  I0701 11:58:10.914557      19 e2e.go:117] Starting e2e run "a65dfce2-a914-45b4-bc26-49557909761b" on Ginkgo node 1
  Jul  1 11:58:10.951: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1688212690 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Jul  1 11:58:11.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 11:58:11.289: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Jul  1 11:58:11.335: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Jul  1 11:58:11.339: INFO: e2e test version: v1.27.3
  Jul  1 11:58:11.341: INFO: kube-apiserver version: v1.27.3
  Jul  1 11:58:11.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 11:58:11.348: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.060 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 07/01/23 11:58:11.826
  Jul  1 11:58:11.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename watch @ 07/01/23 11:58:11.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 11:58:11.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 11:58:11.864
  STEP: creating a watch on configmaps with label A @ 07/01/23 11:58:11.868
  STEP: creating a watch on configmaps with label B @ 07/01/23 11:58:11.87
  STEP: creating a watch on configmaps with label A or B @ 07/01/23 11:58:11.872
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 07/01/23 11:58:11.873
  Jul  1 11:58:11.881: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7823  9afe6bb4-6019-4ce1-9128-a19bd614482e 1835 0 2023-07-01 11:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-01 11:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 11:58:11.882: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7823  9afe6bb4-6019-4ce1-9128-a19bd614482e 1835 0 2023-07-01 11:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-01 11:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 07/01/23 11:58:11.882
  Jul  1 11:58:11.895: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7823  9afe6bb4-6019-4ce1-9128-a19bd614482e 1836 0 2023-07-01 11:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-01 11:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 11:58:11.895: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7823  9afe6bb4-6019-4ce1-9128-a19bd614482e 1836 0 2023-07-01 11:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-01 11:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 07/01/23 11:58:11.895
  Jul  1 11:58:11.908: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7823  9afe6bb4-6019-4ce1-9128-a19bd614482e 1837 0 2023-07-01 11:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-01 11:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 11:58:11.908: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7823  9afe6bb4-6019-4ce1-9128-a19bd614482e 1837 0 2023-07-01 11:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-01 11:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 07/01/23 11:58:11.908
  Jul  1 11:58:11.918: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7823  9afe6bb4-6019-4ce1-9128-a19bd614482e 1838 0 2023-07-01 11:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-01 11:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 11:58:11.918: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7823  9afe6bb4-6019-4ce1-9128-a19bd614482e 1838 0 2023-07-01 11:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-01 11:58:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 07/01/23 11:58:11.918
  Jul  1 11:58:11.924: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7823  e9cc13ee-8865-4c0f-8e94-c31dd268fcf2 1839 0 2023-07-01 11:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-01 11:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 11:58:11.925: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7823  e9cc13ee-8865-4c0f-8e94-c31dd268fcf2 1839 0 2023-07-01 11:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-01 11:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 07/01/23 11:58:21.926
  Jul  1 11:58:21.938: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7823  e9cc13ee-8865-4c0f-8e94-c31dd268fcf2 1872 0 2023-07-01 11:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-01 11:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 11:58:21.938: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7823  e9cc13ee-8865-4c0f-8e94-c31dd268fcf2 1872 0 2023-07-01 11:58:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-01 11:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 11:58:31.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7823" for this suite. @ 07/01/23 11:58:31.947
• [20.134 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 07/01/23 11:58:31.959
  Jul  1 11:58:31.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename services @ 07/01/23 11:58:31.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 11:58:31.996
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 11:58:32
  Jul  1 11:58:32.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3527" for this suite. @ 07/01/23 11:58:32.025
• [0.078 seconds]
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 07/01/23 11:58:32.037
  Jul  1 11:58:32.037: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename runtimeclass @ 07/01/23 11:58:32.041
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 11:58:32.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 11:58:32.076
  Jul  1 11:58:32.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6539" for this suite. @ 07/01/23 11:58:32.097
• [0.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 07/01/23 11:58:32.108
  Jul  1 11:58:32.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename ingress @ 07/01/23 11:58:32.109
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 11:58:32.144
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 11:58:32.148
  STEP: getting /apis @ 07/01/23 11:58:32.152
  STEP: getting /apis/networking.k8s.io @ 07/01/23 11:58:32.158
  STEP: getting /apis/networking.k8s.iov1 @ 07/01/23 11:58:32.16
  STEP: creating @ 07/01/23 11:58:32.161
  STEP: getting @ 07/01/23 11:58:32.185
  STEP: listing @ 07/01/23 11:58:32.202
  STEP: watching @ 07/01/23 11:58:32.21
  Jul  1 11:58:32.211: INFO: starting watch
  STEP: cluster-wide listing @ 07/01/23 11:58:32.213
  STEP: cluster-wide watching @ 07/01/23 11:58:32.22
  Jul  1 11:58:32.221: INFO: starting watch
  STEP: patching @ 07/01/23 11:58:32.223
  STEP: updating @ 07/01/23 11:58:32.242
  Jul  1 11:58:32.258: INFO: waiting for watch events with expected annotations
  Jul  1 11:58:32.259: INFO: saw patched and updated annotations
  STEP: patching /status @ 07/01/23 11:58:32.259
  STEP: updating /status @ 07/01/23 11:58:32.269
  STEP: get /status @ 07/01/23 11:58:32.289
  STEP: deleting @ 07/01/23 11:58:32.303
  STEP: deleting a collection @ 07/01/23 11:58:32.324
  Jul  1 11:58:32.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-7654" for this suite. @ 07/01/23 11:58:32.364
• [0.266 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 07/01/23 11:58:32.399
  Jul  1 11:58:32.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename namespaces @ 07/01/23 11:58:32.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 11:58:32.429
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 11:58:32.433
  STEP: Creating namespace "e2e-ns-wblmz" @ 07/01/23 11:58:32.442
  Jul  1 11:58:32.469: INFO: Namespace "e2e-ns-wblmz-7654" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-wblmz-7654" @ 07/01/23 11:58:32.469
  Jul  1 11:58:32.487: INFO: Namespace "e2e-ns-wblmz-7654" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-wblmz-7654" @ 07/01/23 11:58:32.488
  Jul  1 11:58:32.514: INFO: Namespace "e2e-ns-wblmz-7654" has []v1.FinalizerName{"kubernetes"}
  Jul  1 11:58:32.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-452" for this suite. @ 07/01/23 11:58:32.527
  STEP: Destroying namespace "e2e-ns-wblmz-7654" for this suite. @ 07/01/23 11:58:32.55
• [0.162 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 07/01/23 11:58:32.57
  Jul  1 11:58:32.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 11:58:32.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 11:58:32.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 11:58:32.62
  STEP: Creating configMap with name configmap-projected-all-test-volume-b5011627-0d3d-4b45-83a5-772371610e16 @ 07/01/23 11:58:32.639
  STEP: Creating secret with name secret-projected-all-test-volume-1b19fdc5-8eb6-4706-82aa-458bbc11247a @ 07/01/23 11:58:32.65
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 07/01/23 11:58:32.659
  STEP: Saw pod success @ 07/01/23 11:58:36.697
  Jul  1 11:58:36.704: INFO: Trying to get logs from node ip-172-31-91-66 pod projected-volume-9bfe27ef-43b3-48fc-8525-d43dac40a247 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 07/01/23 11:58:36.728
  Jul  1 11:58:36.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9071" for this suite. @ 07/01/23 11:58:36.761
• [4.203 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 07/01/23 11:58:36.774
  Jul  1 11:58:36.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-probe @ 07/01/23 11:58:36.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 11:58:36.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 11:58:36.808
  STEP: Creating pod liveness-9cd71923-4132-4036-9a93-77f365aedbbb in namespace container-probe-1985 @ 07/01/23 11:58:36.814
  Jul  1 11:58:42.848: INFO: Started pod liveness-9cd71923-4132-4036-9a93-77f365aedbbb in namespace container-probe-1985
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/01/23 11:58:42.848
  Jul  1 11:58:42.852: INFO: Initial restart count of pod liveness-9cd71923-4132-4036-9a93-77f365aedbbb is 0
  Jul  1 12:02:43.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/01/23 12:02:43.635
  STEP: Destroying namespace "container-probe-1985" for this suite. @ 07/01/23 12:02:43.654
• [246.899 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 07/01/23 12:02:43.673
  Jul  1 12:02:43.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename server-version @ 07/01/23 12:02:43.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:02:43.714
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:02:43.719
  STEP: Request ServerVersion @ 07/01/23 12:02:43.725
  STEP: Confirm major version @ 07/01/23 12:02:43.726
  Jul  1 12:02:43.726: INFO: Major version: 1
  STEP: Confirm minor version @ 07/01/23 12:02:43.726
  Jul  1 12:02:43.726: INFO: cleanMinorVersion: 27
  Jul  1 12:02:43.726: INFO: Minor version: 27
  Jul  1 12:02:43.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-9325" for this suite. @ 07/01/23 12:02:43.734
• [0.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 07/01/23 12:02:43.749
  Jul  1 12:02:43.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename disruption @ 07/01/23 12:02:43.752
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:02:43.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:02:43.785
  STEP: Creating a pdb that targets all three pods in a test replica set @ 07/01/23 12:02:43.792
  STEP: Waiting for the pdb to be processed @ 07/01/23 12:02:43.799
  STEP: First trying to evict a pod which shouldn't be evictable @ 07/01/23 12:02:45.819
  STEP: Waiting for all pods to be running @ 07/01/23 12:02:45.819
  Jul  1 12:02:45.824: INFO: pods: 0 < 3
  Jul  1 12:02:47.832: INFO: running pods: 1 < 3
  STEP: locating a running pod @ 07/01/23 12:02:49.831
  STEP: Updating the pdb to allow a pod to be evicted @ 07/01/23 12:02:49.844
  STEP: Waiting for the pdb to be processed @ 07/01/23 12:02:49.859
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 07/01/23 12:02:49.867
  STEP: Waiting for all pods to be running @ 07/01/23 12:02:49.867
  STEP: Waiting for the pdb to observed all healthy pods @ 07/01/23 12:02:49.872
  STEP: Patching the pdb to disallow a pod to be evicted @ 07/01/23 12:02:49.906
  STEP: Waiting for the pdb to be processed @ 07/01/23 12:02:49.921
  STEP: Waiting for all pods to be running @ 07/01/23 12:02:51.944
  STEP: locating a running pod @ 07/01/23 12:02:51.95
  STEP: Deleting the pdb to allow a pod to be evicted @ 07/01/23 12:02:51.963
  STEP: Waiting for the pdb to be deleted @ 07/01/23 12:02:51.972
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 07/01/23 12:02:51.976
  STEP: Waiting for all pods to be running @ 07/01/23 12:02:51.976
  Jul  1 12:02:52.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-3685" for this suite. @ 07/01/23 12:02:52.02
• [8.287 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 07/01/23 12:02:52.036
  Jul  1 12:02:52.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename svcaccounts @ 07/01/23 12:02:52.037
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:02:52.132
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:02:52.136
  Jul  1 12:02:52.160: INFO: created pod
  STEP: Saw pod success @ 07/01/23 12:02:56.18
  Jul  1 12:03:26.181: INFO: polling logs
  Jul  1 12:03:26.205: INFO: Pod logs: 
  I0701 12:02:53.438540       1 log.go:198] OK: Got token
  I0701 12:02:53.438705       1 log.go:198] validating with in-cluster discovery
  I0701 12:02:53.439133       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0701 12:02:53.439311       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-6337:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1688213572, NotBefore:1688212972, IssuedAt:1688212972, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6337", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"bba07b81-88f3-4a62-9bad-5092d04f6dd4"}}}
  I0701 12:02:53.449904       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0701 12:02:53.457470       1 log.go:198] OK: Validated signature on JWT
  I0701 12:02:53.457598       1 log.go:198] OK: Got valid claims from token!
  I0701 12:02:53.457744       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-6337:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1688213572, NotBefore:1688212972, IssuedAt:1688212972, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6337", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"bba07b81-88f3-4a62-9bad-5092d04f6dd4"}}}

  Jul  1 12:03:26.205: INFO: completed pod
  Jul  1 12:03:26.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6337" for this suite. @ 07/01/23 12:03:26.222
• [34.197 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 07/01/23 12:03:26.234
  Jul  1 12:03:26.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename watch @ 07/01/23 12:03:26.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:03:26.259
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:03:26.262
  STEP: getting a starting resourceVersion @ 07/01/23 12:03:26.266
  STEP: starting a background goroutine to produce watch events @ 07/01/23 12:03:26.272
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 07/01/23 12:03:26.272
  Jul  1 12:03:29.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-3649" for this suite. @ 07/01/23 12:03:29.093
• [2.911 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 07/01/23 12:03:29.146
  Jul  1 12:03:29.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename downward-api @ 07/01/23 12:03:29.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:03:29.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:03:29.176
  STEP: Creating a pod to test downward api env vars @ 07/01/23 12:03:29.179
  STEP: Saw pod success @ 07/01/23 12:03:33.213
  Jul  1 12:03:33.218: INFO: Trying to get logs from node ip-172-31-91-66 pod downward-api-a8837811-543f-4bfd-9e76-86f222ede14f container dapi-container: <nil>
  STEP: delete the pod @ 07/01/23 12:03:33.23
  Jul  1 12:03:33.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1390" for this suite. @ 07/01/23 12:03:33.279
• [4.144 seconds]
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 07/01/23 12:03:33.291
  Jul  1 12:03:33.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename services @ 07/01/23 12:03:33.292
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:03:33.324
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:03:33.334
  STEP: creating service multi-endpoint-test in namespace services-8049 @ 07/01/23 12:03:33.338
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8049 to expose endpoints map[] @ 07/01/23 12:03:33.352
  Jul  1 12:03:33.358: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  Jul  1 12:03:34.373: INFO: successfully validated that service multi-endpoint-test in namespace services-8049 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-8049 @ 07/01/23 12:03:34.373
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8049 to expose endpoints map[pod1:[100]] @ 07/01/23 12:03:36.412
  Jul  1 12:03:36.427: INFO: successfully validated that service multi-endpoint-test in namespace services-8049 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-8049 @ 07/01/23 12:03:36.427
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8049 to expose endpoints map[pod1:[100] pod2:[101]] @ 07/01/23 12:03:42.476
  Jul  1 12:03:42.498: INFO: successfully validated that service multi-endpoint-test in namespace services-8049 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 07/01/23 12:03:42.498
  Jul  1 12:03:42.498: INFO: Creating new exec pod
  Jul  1 12:03:45.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-8049 exec execpoddzdzg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Jul  1 12:03:45.902: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Jul  1 12:03:45.902: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 12:03:45.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-8049 exec execpoddzdzg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.200 80'
  Jul  1 12:03:46.095: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.200 80\nConnection to 10.152.183.200 80 port [tcp/http] succeeded!\n"
  Jul  1 12:03:46.095: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 12:03:46.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-8049 exec execpoddzdzg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Jul  1 12:03:46.298: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Jul  1 12:03:46.298: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 12:03:46.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-8049 exec execpoddzdzg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.200 81'
  Jul  1 12:03:46.474: INFO: stderr: "+ nc -v -t -w 2 10.152.183.200 81\n+ echo hostName\nConnection to 10.152.183.200 81 port [tcp/*] succeeded!\n"
  Jul  1 12:03:46.474: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-8049 @ 07/01/23 12:03:46.474
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8049 to expose endpoints map[pod2:[101]] @ 07/01/23 12:03:46.49
  Jul  1 12:03:46.521: INFO: successfully validated that service multi-endpoint-test in namespace services-8049 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-8049 @ 07/01/23 12:03:46.521
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8049 to expose endpoints map[] @ 07/01/23 12:03:46.551
  Jul  1 12:03:46.562: INFO: successfully validated that service multi-endpoint-test in namespace services-8049 exposes endpoints map[]
  Jul  1 12:03:46.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8049" for this suite. @ 07/01/23 12:03:46.594
• [13.317 seconds]
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 07/01/23 12:03:46.608
  Jul  1 12:03:46.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename runtimeclass @ 07/01/23 12:03:46.61
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:03:46.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:03:46.648
  Jul  1 12:03:48.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8140" for this suite. @ 07/01/23 12:03:48.699
• [2.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 07/01/23 12:03:48.712
  Jul  1 12:03:48.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename proxy @ 07/01/23 12:03:48.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:03:48.753
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:03:48.757
  Jul  1 12:03:48.760: INFO: Creating pod...
  Jul  1 12:03:50.783: INFO: Creating service...
  Jul  1 12:03:50.798: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1584/pods/agnhost/proxy?method=DELETE
  Jul  1 12:03:50.805: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul  1 12:03:50.805: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1584/pods/agnhost/proxy?method=OPTIONS
  Jul  1 12:03:50.815: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul  1 12:03:50.815: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1584/pods/agnhost/proxy?method=PATCH
  Jul  1 12:03:50.821: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul  1 12:03:50.821: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1584/pods/agnhost/proxy?method=POST
  Jul  1 12:03:50.827: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul  1 12:03:50.827: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1584/pods/agnhost/proxy?method=PUT
  Jul  1 12:03:50.832: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul  1 12:03:50.833: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1584/services/e2e-proxy-test-service/proxy?method=DELETE
  Jul  1 12:03:50.842: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul  1 12:03:50.842: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1584/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Jul  1 12:03:50.850: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul  1 12:03:50.850: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1584/services/e2e-proxy-test-service/proxy?method=PATCH
  Jul  1 12:03:50.857: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul  1 12:03:50.857: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1584/services/e2e-proxy-test-service/proxy?method=POST
  Jul  1 12:03:50.865: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul  1 12:03:50.865: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1584/services/e2e-proxy-test-service/proxy?method=PUT
  Jul  1 12:03:50.873: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul  1 12:03:50.874: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1584/pods/agnhost/proxy?method=GET
  Jul  1 12:03:50.878: INFO: http.Client request:GET StatusCode:301
  Jul  1 12:03:50.878: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1584/services/e2e-proxy-test-service/proxy?method=GET
  Jul  1 12:03:50.885: INFO: http.Client request:GET StatusCode:301
  Jul  1 12:03:50.886: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1584/pods/agnhost/proxy?method=HEAD
  Jul  1 12:03:50.890: INFO: http.Client request:HEAD StatusCode:301
  Jul  1 12:03:50.890: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1584/services/e2e-proxy-test-service/proxy?method=HEAD
  Jul  1 12:03:50.897: INFO: http.Client request:HEAD StatusCode:301
  Jul  1 12:03:50.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-1584" for this suite. @ 07/01/23 12:03:50.903
• [2.200 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 07/01/23 12:03:50.914
  Jul  1 12:03:50.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl @ 07/01/23 12:03:50.915
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:03:50.941
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:03:50.945
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/01/23 12:03:50.95
  Jul  1 12:03:50.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-2189 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jul  1 12:03:51.102: INFO: stderr: ""
  Jul  1 12:03:51.102: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 07/01/23 12:03:51.102
  STEP: verifying the pod e2e-test-httpd-pod was created @ 07/01/23 12:04:01.157
  Jul  1 12:04:01.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-2189 get pod e2e-test-httpd-pod -o json'
  Jul  1 12:04:01.372: INFO: stderr: ""
  Jul  1 12:04:01.372: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-07-01T12:03:51Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2189\",\n        \"resourceVersion\": \"3243\",\n        \"uid\": \"b256def7-6b58-4480-8fe0-16eed9454c5d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-vlz4l\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-91-66\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-vlz4l\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-01T12:03:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-01T12:03:56Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-01T12:03:56Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-01T12:03:51Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://8ef033f1eaf5526c2b43736c3d83600beca8284a52d3a4b0866bece6d44f7533\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-07-01T12:03:56Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.91.66\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.62.74\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.62.74\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-07-01T12:03:51Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 07/01/23 12:04:01.373
  Jul  1 12:04:01.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-2189 replace -f -'
  Jul  1 12:04:02.907: INFO: stderr: ""
  Jul  1 12:04:02.907: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 07/01/23 12:04:02.907
  Jul  1 12:04:02.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-2189 delete pods e2e-test-httpd-pod'
  Jul  1 12:04:08.654: INFO: stderr: ""
  Jul  1 12:04:08.654: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul  1 12:04:08.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2189" for this suite. @ 07/01/23 12:04:08.68
• [17.782 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 07/01/23 12:04:08.703
  Jul  1 12:04:08.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-probe @ 07/01/23 12:04:08.707
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:04:08.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:04:08.744
  STEP: Creating pod busybox-3898c156-0ffe-4bfb-afef-b09dffa44694 in namespace container-probe-806 @ 07/01/23 12:04:08.756
  Jul  1 12:04:10.782: INFO: Started pod busybox-3898c156-0ffe-4bfb-afef-b09dffa44694 in namespace container-probe-806
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/01/23 12:04:10.782
  Jul  1 12:04:10.789: INFO: Initial restart count of pod busybox-3898c156-0ffe-4bfb-afef-b09dffa44694 is 0
  Jul  1 12:08:11.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/01/23 12:08:11.582
  STEP: Destroying namespace "container-probe-806" for this suite. @ 07/01/23 12:08:11.601
• [242.912 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 07/01/23 12:08:11.615
  Jul  1 12:08:11.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename field-validation @ 07/01/23 12:08:11.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:08:11.642
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:08:11.645
  Jul  1 12:08:11.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  W0701 12:08:14.211862      19 warnings.go:70] unknown field "alpha"
  W0701 12:08:14.211889      19 warnings.go:70] unknown field "beta"
  W0701 12:08:14.211895      19 warnings.go:70] unknown field "delta"
  W0701 12:08:14.211902      19 warnings.go:70] unknown field "epsilon"
  W0701 12:08:14.211907      19 warnings.go:70] unknown field "gamma"
  Jul  1 12:08:14.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-799" for this suite. @ 07/01/23 12:08:14.274
• [2.668 seconds]
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 07/01/23 12:08:14.283
  Jul  1 12:08:14.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename disruption @ 07/01/23 12:08:14.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:08:14.308
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:08:14.313
  STEP: Creating a kubernetes client @ 07/01/23 12:08:14.317
  Jul  1 12:08:14.317: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename disruption-2 @ 07/01/23 12:08:14.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:08:14.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:08:14.395
  STEP: Waiting for the pdb to be processed @ 07/01/23 12:08:14.405
  STEP: Waiting for the pdb to be processed @ 07/01/23 12:08:16.424
  STEP: Waiting for the pdb to be processed @ 07/01/23 12:08:18.442
  STEP: listing a collection of PDBs across all namespaces @ 07/01/23 12:08:18.449
  STEP: listing a collection of PDBs in namespace disruption-4932 @ 07/01/23 12:08:18.453
  STEP: deleting a collection of PDBs @ 07/01/23 12:08:18.458
  STEP: Waiting for the PDB collection to be deleted @ 07/01/23 12:08:18.477
  Jul  1 12:08:18.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  1 12:08:18.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-9285" for this suite. @ 07/01/23 12:08:18.508
  STEP: Destroying namespace "disruption-4932" for this suite. @ 07/01/23 12:08:18.524
• [4.262 seconds]
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 07/01/23 12:08:18.549
  Jul  1 12:08:18.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename daemonsets @ 07/01/23 12:08:18.553
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:08:18.6
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:08:18.635
  Jul  1 12:08:18.714: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/01/23 12:08:18.731
  Jul  1 12:08:18.742: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:18.743: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:18.750: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 12:08:18.750: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 12:08:19.757: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:19.757: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:19.762: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 12:08:19.762: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 12:08:20.759: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:20.759: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:20.765: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  1 12:08:20.765: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 12:08:21.755: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:21.755: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:21.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  1 12:08:21.760: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 12:08:22.757: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:22.758: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:22.764: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  1 12:08:22.764: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 12:08:23.757: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:23.757: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:23.763: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  1 12:08:23.763: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 12:08:24.755: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:24.755: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:24.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul  1 12:08:24.760: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 12:08:25.756: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:25.756: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:25.762: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  1 12:08:25.762: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 07/01/23 12:08:25.798
  STEP: Check that daemon pods images are updated. @ 07/01/23 12:08:25.819
  Jul  1 12:08:25.826: INFO: Wrong image for pod: daemon-set-chnqt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  1 12:08:25.826: INFO: Wrong image for pod: daemon-set-n944g. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  1 12:08:25.826: INFO: Wrong image for pod: daemon-set-rv2pp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  1 12:08:25.835: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:25.835: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:26.841: INFO: Wrong image for pod: daemon-set-chnqt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  1 12:08:26.841: INFO: Wrong image for pod: daemon-set-n944g. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  1 12:08:26.846: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:26.846: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:27.842: INFO: Pod daemon-set-4fzn7 is not available
  Jul  1 12:08:27.842: INFO: Wrong image for pod: daemon-set-chnqt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  1 12:08:27.842: INFO: Wrong image for pod: daemon-set-n944g. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  1 12:08:27.847: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:27.847: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:28.842: INFO: Wrong image for pod: daemon-set-chnqt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  1 12:08:28.847: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:28.847: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:29.842: INFO: Wrong image for pod: daemon-set-chnqt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  1 12:08:29.848: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:29.848: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:30.841: INFO: Wrong image for pod: daemon-set-chnqt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  1 12:08:30.845: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:30.845: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:31.842: INFO: Wrong image for pod: daemon-set-chnqt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  1 12:08:31.847: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:31.847: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:32.842: INFO: Wrong image for pod: daemon-set-chnqt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  1 12:08:32.848: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:32.848: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:33.841: INFO: Wrong image for pod: daemon-set-chnqt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  1 12:08:33.846: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:33.846: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:34.842: INFO: Pod daemon-set-9f59l is not available
  Jul  1 12:08:34.842: INFO: Wrong image for pod: daemon-set-chnqt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  1 12:08:34.849: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:34.849: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:35.847: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:35.847: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:36.849: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:36.850: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:37.846: INFO: Pod daemon-set-p6nh7 is not available
  Jul  1 12:08:37.857: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:37.857: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 07/01/23 12:08:37.857
  Jul  1 12:08:37.863: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:37.863: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:37.868: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul  1 12:08:37.868: INFO: Node ip-172-31-16-94 is running 0 daemon pod, expected 1
  Jul  1 12:08:38.875: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:38.876: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:38.882: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul  1 12:08:38.882: INFO: Node ip-172-31-16-94 is running 0 daemon pod, expected 1
  Jul  1 12:08:39.875: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:39.875: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:39.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul  1 12:08:39.880: INFO: Node ip-172-31-16-94 is running 0 daemon pod, expected 1
  Jul  1 12:08:40.875: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:40.875: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:40.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul  1 12:08:40.880: INFO: Node ip-172-31-16-94 is running 0 daemon pod, expected 1
  Jul  1 12:08:41.874: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:41.874: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:08:41.878: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  1 12:08:41.878: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/01/23 12:08:41.902
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2512, will wait for the garbage collector to delete the pods @ 07/01/23 12:08:41.902
  Jul  1 12:08:41.968: INFO: Deleting DaemonSet.extensions daemon-set took: 10.1098ms
  Jul  1 12:08:42.069: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.0784ms
  Jul  1 12:08:45.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 12:08:45.375: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul  1 12:08:45.379: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4092"},"items":null}

  Jul  1 12:08:45.385: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4092"},"items":null}

  Jul  1 12:08:45.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2512" for this suite. @ 07/01/23 12:08:45.411
• [26.873 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 07/01/23 12:08:45.424
  Jul  1 12:08:45.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-probe @ 07/01/23 12:08:45.425
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:08:45.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:08:45.467
  STEP: Creating pod test-webserver-f85727c0-10e2-4fa2-bade-2852b2faa391 in namespace container-probe-2044 @ 07/01/23 12:08:45.473
  Jul  1 12:08:47.498: INFO: Started pod test-webserver-f85727c0-10e2-4fa2-bade-2852b2faa391 in namespace container-probe-2044
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/01/23 12:08:47.498
  Jul  1 12:08:47.503: INFO: Initial restart count of pod test-webserver-f85727c0-10e2-4fa2-bade-2852b2faa391 is 0
  Jul  1 12:12:48.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/01/23 12:12:48.297
  STEP: Destroying namespace "container-probe-2044" for this suite. @ 07/01/23 12:12:48.317
• [242.906 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 07/01/23 12:12:48.33
  Jul  1 12:12:48.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename csistoragecapacity @ 07/01/23 12:12:48.331
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:12:48.402
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:12:48.406
  STEP: getting /apis @ 07/01/23 12:12:48.409
  STEP: getting /apis/storage.k8s.io @ 07/01/23 12:12:48.416
  STEP: getting /apis/storage.k8s.io/v1 @ 07/01/23 12:12:48.419
  STEP: creating @ 07/01/23 12:12:48.421
  STEP: watching @ 07/01/23 12:12:48.443
  Jul  1 12:12:48.443: INFO: starting watch
  STEP: getting @ 07/01/23 12:12:48.452
  STEP: listing in namespace @ 07/01/23 12:12:48.456
  STEP: listing across namespaces @ 07/01/23 12:12:48.461
  STEP: patching @ 07/01/23 12:12:48.466
  STEP: updating @ 07/01/23 12:12:48.474
  Jul  1 12:12:48.491: INFO: waiting for watch events with expected annotations in namespace
  Jul  1 12:12:48.491: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 07/01/23 12:12:48.491
  STEP: deleting a collection @ 07/01/23 12:12:48.517
  Jul  1 12:12:48.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-7318" for this suite. @ 07/01/23 12:12:48.557
• [0.241 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 07/01/23 12:12:48.572
  Jul  1 12:12:48.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename replication-controller @ 07/01/23 12:12:48.572
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:12:48.618
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:12:48.623
  STEP: Creating replication controller my-hostname-basic-6a7c6126-066e-42a4-8307-07b0fb649dbf @ 07/01/23 12:12:48.626
  Jul  1 12:12:48.641: INFO: Pod name my-hostname-basic-6a7c6126-066e-42a4-8307-07b0fb649dbf: Found 0 pods out of 1
  Jul  1 12:12:53.647: INFO: Pod name my-hostname-basic-6a7c6126-066e-42a4-8307-07b0fb649dbf: Found 1 pods out of 1
  Jul  1 12:12:53.647: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6a7c6126-066e-42a4-8307-07b0fb649dbf" are running
  Jul  1 12:12:53.652: INFO: Pod "my-hostname-basic-6a7c6126-066e-42a4-8307-07b0fb649dbf-w6xk4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-01 12:12:48 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-01 12:12:49 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-01 12:12:49 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-01 12:12:48 +0000 UTC Reason: Message:}])
  Jul  1 12:12:53.652: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 07/01/23 12:12:53.652
  Jul  1 12:12:53.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7788" for this suite. @ 07/01/23 12:12:53.681
• [5.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 07/01/23 12:12:53.694
  Jul  1 12:12:53.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename svc-latency @ 07/01/23 12:12:53.698
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:12:53.721
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:12:53.726
  Jul  1 12:12:53.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-743 @ 07/01/23 12:12:53.769
  I0701 12:12:53.777275      19 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-743, replica count: 1
  I0701 12:12:54.828548      19 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0701 12:12:55.829047      19 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  1 12:12:55.948: INFO: Created: latency-svc-gmx87
  Jul  1 12:12:55.958: INFO: Got endpoints: latency-svc-gmx87 [29.666147ms]
  Jul  1 12:12:55.993: INFO: Created: latency-svc-snmm6
  Jul  1 12:12:56.004: INFO: Got endpoints: latency-svc-snmm6 [45.546236ms]
  Jul  1 12:12:56.021: INFO: Created: latency-svc-46pt2
  Jul  1 12:12:56.030: INFO: Created: latency-svc-w87sq
  Jul  1 12:12:56.030: INFO: Got endpoints: latency-svc-w87sq [70.624692ms]
  Jul  1 12:12:56.031: INFO: Created: latency-svc-6v4k9
  Jul  1 12:12:56.032: INFO: Got endpoints: latency-svc-46pt2 [72.651219ms]
  Jul  1 12:12:56.045: INFO: Got endpoints: latency-svc-6v4k9 [85.258839ms]
  Jul  1 12:12:56.047: INFO: Created: latency-svc-ddhvs
  Jul  1 12:12:56.053: INFO: Created: latency-svc-lvtnl
  Jul  1 12:12:56.173: INFO: Got endpoints: latency-svc-ddhvs [214.348576ms]
  Jul  1 12:12:56.177: INFO: Created: latency-svc-klsgw
  Jul  1 12:12:56.178: INFO: Got endpoints: latency-svc-lvtnl [218.595005ms]
  Jul  1 12:12:56.190: INFO: Created: latency-svc-5qhj4
  Jul  1 12:12:56.190: INFO: Got endpoints: latency-svc-klsgw [230.804432ms]
  Jul  1 12:12:56.196: INFO: Got endpoints: latency-svc-5qhj4 [236.211603ms]
  Jul  1 12:12:56.312: INFO: Created: latency-svc-7tqdj
  Jul  1 12:12:56.316: INFO: Created: latency-svc-z6g65
  Jul  1 12:12:56.316: INFO: Created: latency-svc-2nfnn
  Jul  1 12:12:56.320: INFO: Created: latency-svc-8gr72
  Jul  1 12:12:56.321: INFO: Created: latency-svc-bvzgh
  Jul  1 12:12:56.322: INFO: Created: latency-svc-sztqk
  Jul  1 12:12:56.322: INFO: Created: latency-svc-46dxg
  Jul  1 12:12:56.321: INFO: Created: latency-svc-pksgb
  Jul  1 12:12:56.321: INFO: Created: latency-svc-wphfs
  Jul  1 12:12:56.323: INFO: Created: latency-svc-h2v8c
  Jul  1 12:12:56.323: INFO: Created: latency-svc-8ntkg
  Jul  1 12:12:56.325: INFO: Created: latency-svc-9cn62
  Jul  1 12:12:56.327: INFO: Created: latency-svc-dp6j6
  Jul  1 12:12:56.327: INFO: Created: latency-svc-xmxbc
  Jul  1 12:12:56.327: INFO: Created: latency-svc-d4f2g
  Jul  1 12:12:56.330: INFO: Got endpoints: latency-svc-7tqdj [371.355031ms]
  Jul  1 12:12:56.349: INFO: Got endpoints: latency-svc-bvzgh [390.04623ms]
  Jul  1 12:12:56.350: INFO: Got endpoints: latency-svc-46dxg [390.339264ms]
  Jul  1 12:12:56.354: INFO: Got endpoints: latency-svc-pksgb [394.707462ms]
  Jul  1 12:12:56.358: INFO: Got endpoints: latency-svc-z6g65 [398.960518ms]
  Jul  1 12:12:56.359: INFO: Created: latency-svc-xgfx6
  Jul  1 12:12:56.360: INFO: Got endpoints: latency-svc-8ntkg [181.476964ms]
  Jul  1 12:12:56.363: INFO: Got endpoints: latency-svc-2nfnn [172.592438ms]
  Jul  1 12:12:56.367: INFO: Got endpoints: latency-svc-wphfs [336.949664ms]
  Jul  1 12:12:56.375: INFO: Created: latency-svc-qf9r6
  Jul  1 12:12:56.383: INFO: Got endpoints: latency-svc-8gr72 [186.860757ms]
  Jul  1 12:12:56.386: INFO: Got endpoints: latency-svc-9cn62 [212.491155ms]
  Jul  1 12:12:56.386: INFO: Got endpoints: latency-svc-dp6j6 [426.463695ms]
  Jul  1 12:12:56.386: INFO: Got endpoints: latency-svc-xmxbc [425.857839ms]
  Jul  1 12:12:56.389: INFO: Got endpoints: latency-svc-d4f2g [356.429694ms]
  Jul  1 12:12:56.399: INFO: Created: latency-svc-sfrvh
  Jul  1 12:12:56.405: INFO: Got endpoints: latency-svc-h2v8c [400.852654ms]
  Jul  1 12:12:56.410: INFO: Got endpoints: latency-svc-sztqk [365.692248ms]
  Jul  1 12:12:56.412: INFO: Got endpoints: latency-svc-qf9r6 [62.297162ms]
  Jul  1 12:12:56.412: INFO: Created: latency-svc-fcpbr
  Jul  1 12:12:56.417: INFO: Got endpoints: latency-svc-sfrvh [67.197922ms]
  Jul  1 12:12:56.418: INFO: Got endpoints: latency-svc-xgfx6 [87.536375ms]
  Jul  1 12:12:56.444: INFO: Got endpoints: latency-svc-fcpbr [89.669332ms]
  Jul  1 12:12:56.449: INFO: Created: latency-svc-spwwh
  Jul  1 12:12:56.460: INFO: Got endpoints: latency-svc-spwwh [101.250551ms]
  Jul  1 12:12:56.466: INFO: Created: latency-svc-fxn92
  Jul  1 12:12:56.472: INFO: Created: latency-svc-pcf8c
  Jul  1 12:12:56.475: INFO: Got endpoints: latency-svc-fxn92 [115.065146ms]
  Jul  1 12:12:56.485: INFO: Got endpoints: latency-svc-pcf8c [121.884962ms]
  Jul  1 12:12:56.490: INFO: Created: latency-svc-tmt9j
  Jul  1 12:12:56.500: INFO: Got endpoints: latency-svc-tmt9j [133.429056ms]
  Jul  1 12:12:56.505: INFO: Created: latency-svc-r6j9r
  Jul  1 12:12:56.513: INFO: Got endpoints: latency-svc-r6j9r [130.436675ms]
  Jul  1 12:12:56.522: INFO: Created: latency-svc-xd64v
  Jul  1 12:12:56.528: INFO: Got endpoints: latency-svc-xd64v [141.313333ms]
  Jul  1 12:12:56.534: INFO: Created: latency-svc-zprpg
  Jul  1 12:12:56.544: INFO: Created: latency-svc-p5jh4
  Jul  1 12:12:56.546: INFO: Got endpoints: latency-svc-zprpg [160.07686ms]
  Jul  1 12:12:56.554: INFO: Got endpoints: latency-svc-p5jh4 [165.150252ms]
  Jul  1 12:12:56.554: INFO: Created: latency-svc-dtxl9
  Jul  1 12:12:56.564: INFO: Got endpoints: latency-svc-dtxl9 [175.722422ms]
  Jul  1 12:12:56.568: INFO: Created: latency-svc-qkkn7
  Jul  1 12:12:56.571: INFO: Created: latency-svc-7kw7h
  Jul  1 12:12:56.575: INFO: Got endpoints: latency-svc-qkkn7 [170.070554ms]
  Jul  1 12:12:56.582: INFO: Got endpoints: latency-svc-7kw7h [170.903608ms]
  Jul  1 12:12:56.590: INFO: Created: latency-svc-4vwr8
  Jul  1 12:12:56.607: INFO: Created: latency-svc-wcn49
  Jul  1 12:12:56.608: INFO: Got endpoints: latency-svc-4vwr8 [196.650637ms]
  Jul  1 12:12:56.615: INFO: Got endpoints: latency-svc-wcn49 [197.656384ms]
  Jul  1 12:12:56.620: INFO: Created: latency-svc-p558r
  Jul  1 12:12:56.631: INFO: Created: latency-svc-zsjcs
  Jul  1 12:12:56.633: INFO: Got endpoints: latency-svc-p558r [213.330198ms]
  Jul  1 12:12:56.639: INFO: Got endpoints: latency-svc-zsjcs [194.972036ms]
  Jul  1 12:12:56.645: INFO: Created: latency-svc-xg9tq
  Jul  1 12:12:56.656: INFO: Got endpoints: latency-svc-xg9tq [196.109114ms]
  Jul  1 12:12:56.659: INFO: Created: latency-svc-b4g54
  Jul  1 12:12:56.668: INFO: Created: latency-svc-9gbg7
  Jul  1 12:12:56.679: INFO: Created: latency-svc-wss4p
  Jul  1 12:12:56.689: INFO: Created: latency-svc-4rllt
  Jul  1 12:12:56.696: INFO: Created: latency-svc-ccmpj
  Jul  1 12:12:56.706: INFO: Got endpoints: latency-svc-b4g54 [231.612074ms]
  Jul  1 12:12:56.709: INFO: Created: latency-svc-fhmtf
  Jul  1 12:12:56.721: INFO: Created: latency-svc-dg6sw
  Jul  1 12:12:56.724: INFO: Created: latency-svc-7ljlk
  Jul  1 12:12:56.740: INFO: Created: latency-svc-jsqzt
  Jul  1 12:12:56.751: INFO: Created: latency-svc-c9vsf
  Jul  1 12:12:56.756: INFO: Got endpoints: latency-svc-9gbg7 [270.796696ms]
  Jul  1 12:12:56.763: INFO: Created: latency-svc-r6dgn
  Jul  1 12:12:56.776: INFO: Created: latency-svc-4zgz8
  Jul  1 12:12:56.787: INFO: Created: latency-svc-2gww2
  Jul  1 12:12:56.795: INFO: Created: latency-svc-7zh2f
  Jul  1 12:12:56.804: INFO: Got endpoints: latency-svc-wss4p [303.647722ms]
  Jul  1 12:12:56.806: INFO: Created: latency-svc-brvr9
  Jul  1 12:12:56.812: INFO: Created: latency-svc-856tz
  Jul  1 12:12:56.819: INFO: Created: latency-svc-8pgl4
  Jul  1 12:12:56.830: INFO: Created: latency-svc-8jj9r
  Jul  1 12:12:56.855: INFO: Got endpoints: latency-svc-4rllt [342.109755ms]
  Jul  1 12:12:56.875: INFO: Created: latency-svc-72nd5
  Jul  1 12:12:56.904: INFO: Got endpoints: latency-svc-ccmpj [375.571321ms]
  Jul  1 12:12:56.919: INFO: Created: latency-svc-wwkng
  Jul  1 12:12:56.956: INFO: Got endpoints: latency-svc-fhmtf [409.899398ms]
  Jul  1 12:12:56.974: INFO: Created: latency-svc-6bpdn
  Jul  1 12:12:57.009: INFO: Got endpoints: latency-svc-dg6sw [453.864527ms]
  Jul  1 12:12:57.025: INFO: Created: latency-svc-dpwm8
  Jul  1 12:12:57.054: INFO: Got endpoints: latency-svc-7ljlk [489.452991ms]
  Jul  1 12:12:57.067: INFO: Created: latency-svc-rhkck
  Jul  1 12:12:57.105: INFO: Got endpoints: latency-svc-jsqzt [529.574547ms]
  Jul  1 12:12:57.120: INFO: Created: latency-svc-qcgzq
  Jul  1 12:12:57.155: INFO: Got endpoints: latency-svc-c9vsf [572.625843ms]
  Jul  1 12:12:57.171: INFO: Created: latency-svc-dbnlr
  Jul  1 12:12:57.203: INFO: Got endpoints: latency-svc-r6dgn [595.045929ms]
  Jul  1 12:12:57.217: INFO: Created: latency-svc-ksgcj
  Jul  1 12:12:57.254: INFO: Got endpoints: latency-svc-4zgz8 [638.729283ms]
  Jul  1 12:12:57.270: INFO: Created: latency-svc-knt9v
  Jul  1 12:12:57.310: INFO: Got endpoints: latency-svc-2gww2 [677.218141ms]
  Jul  1 12:12:57.329: INFO: Created: latency-svc-9wpdr
  Jul  1 12:12:57.356: INFO: Got endpoints: latency-svc-7zh2f [716.992217ms]
  Jul  1 12:12:57.370: INFO: Created: latency-svc-xg6q4
  Jul  1 12:12:57.405: INFO: Got endpoints: latency-svc-brvr9 [747.105768ms]
  Jul  1 12:12:57.423: INFO: Created: latency-svc-swrhh
  Jul  1 12:12:57.453: INFO: Got endpoints: latency-svc-856tz [746.588247ms]
  Jul  1 12:12:57.469: INFO: Created: latency-svc-wq68c
  Jul  1 12:12:57.502: INFO: Got endpoints: latency-svc-8pgl4 [746.708906ms]
  Jul  1 12:12:57.517: INFO: Created: latency-svc-fcv8g
  Jul  1 12:12:57.554: INFO: Got endpoints: latency-svc-8jj9r [749.923721ms]
  Jul  1 12:12:57.567: INFO: Created: latency-svc-6f8c4
  Jul  1 12:12:57.604: INFO: Got endpoints: latency-svc-72nd5 [748.660651ms]
  Jul  1 12:12:57.620: INFO: Created: latency-svc-56f87
  Jul  1 12:12:57.654: INFO: Got endpoints: latency-svc-wwkng [749.361017ms]
  Jul  1 12:12:57.672: INFO: Created: latency-svc-tx8bd
  Jul  1 12:12:57.703: INFO: Got endpoints: latency-svc-6bpdn [746.888184ms]
  Jul  1 12:12:57.721: INFO: Created: latency-svc-tc4nt
  Jul  1 12:12:57.755: INFO: Got endpoints: latency-svc-dpwm8 [746.767421ms]
  Jul  1 12:12:57.769: INFO: Created: latency-svc-v4kkv
  Jul  1 12:12:57.803: INFO: Got endpoints: latency-svc-rhkck [749.410606ms]
  Jul  1 12:12:57.819: INFO: Created: latency-svc-mk5wq
  Jul  1 12:12:57.854: INFO: Got endpoints: latency-svc-qcgzq [749.233554ms]
  Jul  1 12:12:57.887: INFO: Created: latency-svc-8kmcz
  Jul  1 12:12:57.903: INFO: Got endpoints: latency-svc-dbnlr [747.861795ms]
  Jul  1 12:12:57.920: INFO: Created: latency-svc-7flhj
  Jul  1 12:12:57.952: INFO: Got endpoints: latency-svc-ksgcj [748.921914ms]
  Jul  1 12:12:57.969: INFO: Created: latency-svc-cf986
  Jul  1 12:12:58.004: INFO: Got endpoints: latency-svc-knt9v [750.087661ms]
  Jul  1 12:12:58.025: INFO: Created: latency-svc-pfjgr
  Jul  1 12:12:58.055: INFO: Got endpoints: latency-svc-9wpdr [744.817747ms]
  Jul  1 12:12:58.068: INFO: Created: latency-svc-nx96c
  Jul  1 12:12:58.107: INFO: Got endpoints: latency-svc-xg6q4 [751.345482ms]
  Jul  1 12:12:58.121: INFO: Created: latency-svc-k27zs
  Jul  1 12:12:58.155: INFO: Got endpoints: latency-svc-swrhh [749.64893ms]
  Jul  1 12:12:58.171: INFO: Created: latency-svc-d6pf8
  Jul  1 12:12:58.206: INFO: Got endpoints: latency-svc-wq68c [752.233347ms]
  Jul  1 12:12:58.225: INFO: Created: latency-svc-w869w
  Jul  1 12:12:58.253: INFO: Got endpoints: latency-svc-fcv8g [750.625625ms]
  Jul  1 12:12:58.265: INFO: Created: latency-svc-mgjkz
  Jul  1 12:12:58.307: INFO: Got endpoints: latency-svc-6f8c4 [753.114089ms]
  Jul  1 12:12:58.323: INFO: Created: latency-svc-6wrrg
  Jul  1 12:12:58.355: INFO: Got endpoints: latency-svc-56f87 [750.526602ms]
  Jul  1 12:12:58.370: INFO: Created: latency-svc-p475x
  Jul  1 12:12:58.403: INFO: Got endpoints: latency-svc-tx8bd [748.977987ms]
  Jul  1 12:12:58.422: INFO: Created: latency-svc-phz7q
  Jul  1 12:12:58.455: INFO: Got endpoints: latency-svc-tc4nt [751.420587ms]
  Jul  1 12:12:58.475: INFO: Created: latency-svc-6cvl4
  Jul  1 12:12:58.505: INFO: Got endpoints: latency-svc-v4kkv [749.719577ms]
  Jul  1 12:12:58.520: INFO: Created: latency-svc-wnqvp
  Jul  1 12:12:58.554: INFO: Got endpoints: latency-svc-mk5wq [750.887124ms]
  Jul  1 12:12:58.571: INFO: Created: latency-svc-62whp
  Jul  1 12:12:58.605: INFO: Got endpoints: latency-svc-8kmcz [751.295311ms]
  Jul  1 12:12:58.626: INFO: Created: latency-svc-cxkt5
  Jul  1 12:12:58.655: INFO: Got endpoints: latency-svc-7flhj [751.128315ms]
  Jul  1 12:12:58.668: INFO: Created: latency-svc-4g59w
  Jul  1 12:12:58.704: INFO: Got endpoints: latency-svc-cf986 [750.887399ms]
  Jul  1 12:12:58.725: INFO: Created: latency-svc-kd6p2
  Jul  1 12:12:58.754: INFO: Got endpoints: latency-svc-pfjgr [749.686052ms]
  Jul  1 12:12:58.771: INFO: Created: latency-svc-mt8lx
  Jul  1 12:12:58.807: INFO: Got endpoints: latency-svc-nx96c [752.105138ms]
  Jul  1 12:12:58.827: INFO: Created: latency-svc-mlxhj
  Jul  1 12:12:58.854: INFO: Got endpoints: latency-svc-k27zs [747.32983ms]
  Jul  1 12:12:58.874: INFO: Created: latency-svc-wnwkv
  Jul  1 12:12:58.906: INFO: Got endpoints: latency-svc-d6pf8 [750.828905ms]
  Jul  1 12:12:58.921: INFO: Created: latency-svc-qpn4r
  Jul  1 12:12:58.954: INFO: Got endpoints: latency-svc-w869w [747.6242ms]
  Jul  1 12:12:58.974: INFO: Created: latency-svc-qxpgz
  Jul  1 12:12:59.008: INFO: Got endpoints: latency-svc-mgjkz [754.858206ms]
  Jul  1 12:12:59.023: INFO: Created: latency-svc-gtbxp
  Jul  1 12:12:59.060: INFO: Got endpoints: latency-svc-6wrrg [752.149235ms]
  Jul  1 12:12:59.077: INFO: Created: latency-svc-tjpkt
  Jul  1 12:12:59.107: INFO: Got endpoints: latency-svc-p475x [751.982845ms]
  Jul  1 12:12:59.124: INFO: Created: latency-svc-zb7k2
  Jul  1 12:12:59.155: INFO: Got endpoints: latency-svc-phz7q [752.310659ms]
  Jul  1 12:12:59.168: INFO: Created: latency-svc-rx27w
  Jul  1 12:12:59.205: INFO: Got endpoints: latency-svc-6cvl4 [749.025469ms]
  Jul  1 12:12:59.220: INFO: Created: latency-svc-97d4d
  Jul  1 12:12:59.264: INFO: Got endpoints: latency-svc-wnqvp [758.430654ms]
  Jul  1 12:12:59.279: INFO: Created: latency-svc-qc7mg
  Jul  1 12:12:59.308: INFO: Got endpoints: latency-svc-62whp [754.373652ms]
  Jul  1 12:12:59.329: INFO: Created: latency-svc-qmpmv
  Jul  1 12:12:59.369: INFO: Got endpoints: latency-svc-cxkt5 [763.285797ms]
  Jul  1 12:12:59.388: INFO: Created: latency-svc-wp68v
  Jul  1 12:12:59.406: INFO: Got endpoints: latency-svc-4g59w [751.622028ms]
  Jul  1 12:12:59.422: INFO: Created: latency-svc-2wc6s
  Jul  1 12:12:59.454: INFO: Got endpoints: latency-svc-kd6p2 [750.363072ms]
  Jul  1 12:12:59.467: INFO: Created: latency-svc-kq4mb
  Jul  1 12:12:59.503: INFO: Got endpoints: latency-svc-mt8lx [749.429326ms]
  Jul  1 12:12:59.520: INFO: Created: latency-svc-rpk42
  Jul  1 12:12:59.561: INFO: Got endpoints: latency-svc-mlxhj [753.704185ms]
  Jul  1 12:12:59.578: INFO: Created: latency-svc-gdx4s
  Jul  1 12:12:59.605: INFO: Got endpoints: latency-svc-wnwkv [750.951835ms]
  Jul  1 12:12:59.618: INFO: Created: latency-svc-st29r
  Jul  1 12:12:59.655: INFO: Got endpoints: latency-svc-qpn4r [747.832263ms]
  Jul  1 12:12:59.669: INFO: Created: latency-svc-mf2rk
  Jul  1 12:12:59.704: INFO: Got endpoints: latency-svc-qxpgz [750.232329ms]
  Jul  1 12:12:59.719: INFO: Created: latency-svc-v8tzr
  Jul  1 12:12:59.752: INFO: Got endpoints: latency-svc-gtbxp [744.188513ms]
  Jul  1 12:12:59.768: INFO: Created: latency-svc-5l2cj
  Jul  1 12:12:59.802: INFO: Got endpoints: latency-svc-tjpkt [742.162809ms]
  Jul  1 12:12:59.818: INFO: Created: latency-svc-5d89t
  Jul  1 12:12:59.856: INFO: Got endpoints: latency-svc-zb7k2 [748.778698ms]
  Jul  1 12:12:59.870: INFO: Created: latency-svc-nk84d
  Jul  1 12:12:59.904: INFO: Got endpoints: latency-svc-rx27w [748.397572ms]
  Jul  1 12:12:59.918: INFO: Created: latency-svc-x98z9
  Jul  1 12:12:59.952: INFO: Got endpoints: latency-svc-97d4d [747.617738ms]
  Jul  1 12:12:59.977: INFO: Created: latency-svc-jkrv7
  Jul  1 12:13:00.015: INFO: Got endpoints: latency-svc-qc7mg [750.993251ms]
  Jul  1 12:13:00.038: INFO: Created: latency-svc-6l976
  Jul  1 12:13:00.053: INFO: Got endpoints: latency-svc-qmpmv [745.081726ms]
  Jul  1 12:13:00.067: INFO: Created: latency-svc-khxsz
  Jul  1 12:13:00.105: INFO: Got endpoints: latency-svc-wp68v [736.094596ms]
  Jul  1 12:13:00.121: INFO: Created: latency-svc-8xxt4
  Jul  1 12:13:00.155: INFO: Got endpoints: latency-svc-2wc6s [748.47159ms]
  Jul  1 12:13:00.169: INFO: Created: latency-svc-pbwn4
  Jul  1 12:13:00.208: INFO: Got endpoints: latency-svc-kq4mb [753.113103ms]
  Jul  1 12:13:00.225: INFO: Created: latency-svc-t6h88
  Jul  1 12:13:00.252: INFO: Got endpoints: latency-svc-rpk42 [749.111662ms]
  Jul  1 12:13:00.271: INFO: Created: latency-svc-9dxzn
  Jul  1 12:13:00.304: INFO: Got endpoints: latency-svc-gdx4s [742.522471ms]
  Jul  1 12:13:00.318: INFO: Created: latency-svc-fmgn9
  Jul  1 12:13:00.354: INFO: Got endpoints: latency-svc-st29r [748.701251ms]
  Jul  1 12:13:00.368: INFO: Created: latency-svc-5wfqw
  Jul  1 12:13:00.404: INFO: Got endpoints: latency-svc-mf2rk [749.335837ms]
  Jul  1 12:13:00.424: INFO: Created: latency-svc-86zdr
  Jul  1 12:13:00.455: INFO: Got endpoints: latency-svc-v8tzr [750.699569ms]
  Jul  1 12:13:00.472: INFO: Created: latency-svc-2lgft
  Jul  1 12:13:00.504: INFO: Got endpoints: latency-svc-5l2cj [751.50089ms]
  Jul  1 12:13:00.517: INFO: Created: latency-svc-xk2cl
  Jul  1 12:13:00.559: INFO: Got endpoints: latency-svc-5d89t [756.890364ms]
  Jul  1 12:13:00.575: INFO: Created: latency-svc-x9szk
  Jul  1 12:13:00.606: INFO: Got endpoints: latency-svc-nk84d [748.965151ms]
  Jul  1 12:13:00.619: INFO: Created: latency-svc-8zbd2
  Jul  1 12:13:00.663: INFO: Got endpoints: latency-svc-x98z9 [759.356848ms]
  Jul  1 12:13:00.679: INFO: Created: latency-svc-227bq
  Jul  1 12:13:00.702: INFO: Got endpoints: latency-svc-jkrv7 [749.81708ms]
  Jul  1 12:13:00.719: INFO: Created: latency-svc-6982g
  Jul  1 12:13:00.763: INFO: Got endpoints: latency-svc-6l976 [748.098168ms]
  Jul  1 12:13:00.776: INFO: Created: latency-svc-ltlvs
  Jul  1 12:13:00.804: INFO: Got endpoints: latency-svc-khxsz [750.331894ms]
  Jul  1 12:13:00.818: INFO: Created: latency-svc-lmlxf
  Jul  1 12:13:00.855: INFO: Got endpoints: latency-svc-8xxt4 [749.742008ms]
  Jul  1 12:13:00.870: INFO: Created: latency-svc-7z8c4
  Jul  1 12:13:00.909: INFO: Got endpoints: latency-svc-pbwn4 [753.982848ms]
  Jul  1 12:13:00.924: INFO: Created: latency-svc-n4clb
  Jul  1 12:13:00.967: INFO: Got endpoints: latency-svc-t6h88 [759.171566ms]
  Jul  1 12:13:00.985: INFO: Created: latency-svc-brdlx
  Jul  1 12:13:01.020: INFO: Got endpoints: latency-svc-9dxzn [767.27914ms]
  Jul  1 12:13:01.033: INFO: Created: latency-svc-952mp
  Jul  1 12:13:01.057: INFO: Got endpoints: latency-svc-fmgn9 [752.956592ms]
  Jul  1 12:13:01.072: INFO: Created: latency-svc-x2pd8
  Jul  1 12:13:01.110: INFO: Got endpoints: latency-svc-5wfqw [755.444426ms]
  Jul  1 12:13:01.128: INFO: Created: latency-svc-x5zdj
  Jul  1 12:13:01.155: INFO: Got endpoints: latency-svc-86zdr [750.860007ms]
  Jul  1 12:13:01.174: INFO: Created: latency-svc-t5rbn
  Jul  1 12:13:01.204: INFO: Got endpoints: latency-svc-2lgft [748.115697ms]
  Jul  1 12:13:01.219: INFO: Created: latency-svc-mwpck
  Jul  1 12:13:01.265: INFO: Got endpoints: latency-svc-xk2cl [760.692579ms]
  Jul  1 12:13:01.279: INFO: Created: latency-svc-q6hst
  Jul  1 12:13:01.302: INFO: Got endpoints: latency-svc-x9szk [742.650169ms]
  Jul  1 12:13:01.318: INFO: Created: latency-svc-567xp
  Jul  1 12:13:01.355: INFO: Got endpoints: latency-svc-8zbd2 [749.019464ms]
  Jul  1 12:13:01.370: INFO: Created: latency-svc-rj7nd
  Jul  1 12:13:01.412: INFO: Got endpoints: latency-svc-227bq [749.085315ms]
  Jul  1 12:13:01.425: INFO: Created: latency-svc-vw8xc
  Jul  1 12:13:01.456: INFO: Got endpoints: latency-svc-6982g [754.10161ms]
  Jul  1 12:13:01.484: INFO: Created: latency-svc-mtlkb
  Jul  1 12:13:01.505: INFO: Got endpoints: latency-svc-ltlvs [741.543288ms]
  Jul  1 12:13:01.524: INFO: Created: latency-svc-4lsm6
  Jul  1 12:13:01.552: INFO: Got endpoints: latency-svc-lmlxf [748.276868ms]
  Jul  1 12:13:01.567: INFO: Created: latency-svc-qtclb
  Jul  1 12:13:01.603: INFO: Got endpoints: latency-svc-7z8c4 [748.242198ms]
  Jul  1 12:13:01.618: INFO: Created: latency-svc-hpdbs
  Jul  1 12:13:01.657: INFO: Got endpoints: latency-svc-n4clb [747.876167ms]
  Jul  1 12:13:01.679: INFO: Created: latency-svc-stnrt
  Jul  1 12:13:01.709: INFO: Got endpoints: latency-svc-brdlx [741.268967ms]
  Jul  1 12:13:01.723: INFO: Created: latency-svc-2ls5z
  Jul  1 12:13:01.752: INFO: Got endpoints: latency-svc-952mp [732.445277ms]
  Jul  1 12:13:01.769: INFO: Created: latency-svc-67dx7
  Jul  1 12:13:01.804: INFO: Got endpoints: latency-svc-x2pd8 [747.530961ms]
  Jul  1 12:13:01.822: INFO: Created: latency-svc-4bw98
  Jul  1 12:13:01.855: INFO: Got endpoints: latency-svc-x5zdj [745.418115ms]
  Jul  1 12:13:01.872: INFO: Created: latency-svc-49c26
  Jul  1 12:13:01.907: INFO: Got endpoints: latency-svc-t5rbn [751.802691ms]
  Jul  1 12:13:01.924: INFO: Created: latency-svc-2bcgl
  Jul  1 12:13:01.960: INFO: Got endpoints: latency-svc-mwpck [755.860947ms]
  Jul  1 12:13:01.981: INFO: Created: latency-svc-snwcx
  Jul  1 12:13:02.011: INFO: Got endpoints: latency-svc-q6hst [745.720838ms]
  Jul  1 12:13:02.034: INFO: Created: latency-svc-mcs6j
  Jul  1 12:13:02.054: INFO: Got endpoints: latency-svc-567xp [751.769099ms]
  Jul  1 12:13:02.076: INFO: Created: latency-svc-k594c
  Jul  1 12:13:02.103: INFO: Got endpoints: latency-svc-rj7nd [748.04871ms]
  Jul  1 12:13:02.120: INFO: Created: latency-svc-r6slg
  Jul  1 12:13:02.155: INFO: Got endpoints: latency-svc-vw8xc [742.117535ms]
  Jul  1 12:13:02.169: INFO: Created: latency-svc-7xjm7
  Jul  1 12:13:02.204: INFO: Got endpoints: latency-svc-mtlkb [747.180345ms]
  Jul  1 12:13:02.218: INFO: Created: latency-svc-rlkgb
  Jul  1 12:13:02.255: INFO: Got endpoints: latency-svc-4lsm6 [749.244966ms]
  Jul  1 12:13:02.274: INFO: Created: latency-svc-zfg8s
  Jul  1 12:13:02.303: INFO: Got endpoints: latency-svc-qtclb [751.260086ms]
  Jul  1 12:13:02.319: INFO: Created: latency-svc-qzpkf
  Jul  1 12:13:02.352: INFO: Got endpoints: latency-svc-hpdbs [749.130304ms]
  Jul  1 12:13:02.367: INFO: Created: latency-svc-k7wg5
  Jul  1 12:13:02.496: INFO: Got endpoints: latency-svc-stnrt [838.567714ms]
  Jul  1 12:13:02.497: INFO: Got endpoints: latency-svc-2ls5z [787.472705ms]
  Jul  1 12:13:02.506: INFO: Got endpoints: latency-svc-67dx7 [754.083645ms]
  Jul  1 12:13:02.515: INFO: Created: latency-svc-sd28s
  Jul  1 12:13:02.521: INFO: Created: latency-svc-tb4x4
  Jul  1 12:13:02.531: INFO: Created: latency-svc-9tlj7
  Jul  1 12:13:02.560: INFO: Got endpoints: latency-svc-4bw98 [755.596733ms]
  Jul  1 12:13:02.576: INFO: Created: latency-svc-28xrk
  Jul  1 12:13:02.608: INFO: Got endpoints: latency-svc-49c26 [751.837783ms]
  Jul  1 12:13:02.623: INFO: Created: latency-svc-6xrnt
  Jul  1 12:13:02.654: INFO: Got endpoints: latency-svc-2bcgl [746.3098ms]
  Jul  1 12:13:02.670: INFO: Created: latency-svc-7n9m6
  Jul  1 12:13:02.711: INFO: Got endpoints: latency-svc-snwcx [750.88339ms]
  Jul  1 12:13:02.728: INFO: Created: latency-svc-fb4r9
  Jul  1 12:13:02.753: INFO: Got endpoints: latency-svc-mcs6j [741.598115ms]
  Jul  1 12:13:02.772: INFO: Created: latency-svc-spc4s
  Jul  1 12:13:02.803: INFO: Got endpoints: latency-svc-k594c [748.721902ms]
  Jul  1 12:13:02.820: INFO: Created: latency-svc-9pp46
  Jul  1 12:13:02.859: INFO: Got endpoints: latency-svc-r6slg [755.870055ms]
  Jul  1 12:13:02.872: INFO: Created: latency-svc-kdzgw
  Jul  1 12:13:02.906: INFO: Got endpoints: latency-svc-7xjm7 [750.957051ms]
  Jul  1 12:13:02.919: INFO: Created: latency-svc-mr2t2
  Jul  1 12:13:02.956: INFO: Got endpoints: latency-svc-rlkgb [752.458266ms]
  Jul  1 12:13:02.979: INFO: Created: latency-svc-2v48v
  Jul  1 12:13:03.014: INFO: Got endpoints: latency-svc-zfg8s [759.437737ms]
  Jul  1 12:13:03.036: INFO: Created: latency-svc-bn22b
  Jul  1 12:13:03.057: INFO: Got endpoints: latency-svc-qzpkf [753.817199ms]
  Jul  1 12:13:03.073: INFO: Created: latency-svc-pxrd9
  Jul  1 12:13:03.103: INFO: Got endpoints: latency-svc-k7wg5 [750.453955ms]
  Jul  1 12:13:03.124: INFO: Created: latency-svc-dhxg7
  Jul  1 12:13:03.156: INFO: Got endpoints: latency-svc-sd28s [659.728634ms]
  Jul  1 12:13:03.170: INFO: Created: latency-svc-lr5b9
  Jul  1 12:13:03.204: INFO: Got endpoints: latency-svc-tb4x4 [707.770885ms]
  Jul  1 12:13:03.218: INFO: Created: latency-svc-j78bq
  Jul  1 12:13:03.254: INFO: Got endpoints: latency-svc-9tlj7 [747.399116ms]
  Jul  1 12:13:03.276: INFO: Created: latency-svc-rggrd
  Jul  1 12:13:03.303: INFO: Got endpoints: latency-svc-28xrk [742.970474ms]
  Jul  1 12:13:03.317: INFO: Created: latency-svc-pwvtl
  Jul  1 12:13:03.357: INFO: Got endpoints: latency-svc-6xrnt [748.90308ms]
  Jul  1 12:13:03.370: INFO: Created: latency-svc-2qmqh
  Jul  1 12:13:03.406: INFO: Got endpoints: latency-svc-7n9m6 [751.420635ms]
  Jul  1 12:13:03.425: INFO: Created: latency-svc-k7cls
  Jul  1 12:13:03.455: INFO: Got endpoints: latency-svc-fb4r9 [744.582219ms]
  Jul  1 12:13:03.470: INFO: Created: latency-svc-drxct
  Jul  1 12:13:03.502: INFO: Got endpoints: latency-svc-spc4s [749.342416ms]
  Jul  1 12:13:03.517: INFO: Created: latency-svc-tflnt
  Jul  1 12:13:03.554: INFO: Got endpoints: latency-svc-9pp46 [750.620182ms]
  Jul  1 12:13:03.568: INFO: Created: latency-svc-pkk49
  Jul  1 12:13:03.605: INFO: Got endpoints: latency-svc-kdzgw [746.109047ms]
  Jul  1 12:13:03.620: INFO: Created: latency-svc-8ftgp
  Jul  1 12:13:03.653: INFO: Got endpoints: latency-svc-mr2t2 [747.865734ms]
  Jul  1 12:13:03.667: INFO: Created: latency-svc-txdrl
  Jul  1 12:13:03.707: INFO: Got endpoints: latency-svc-2v48v [751.117193ms]
  Jul  1 12:13:03.723: INFO: Created: latency-svc-kwtgd
  Jul  1 12:13:03.753: INFO: Got endpoints: latency-svc-bn22b [738.698655ms]
  Jul  1 12:13:03.767: INFO: Created: latency-svc-wtfg4
  Jul  1 12:13:03.804: INFO: Got endpoints: latency-svc-pxrd9 [746.52945ms]
  Jul  1 12:13:03.854: INFO: Got endpoints: latency-svc-dhxg7 [751.305574ms]
  Jul  1 12:13:03.904: INFO: Got endpoints: latency-svc-lr5b9 [748.022264ms]
  Jul  1 12:13:03.957: INFO: Got endpoints: latency-svc-j78bq [752.306701ms]
  Jul  1 12:13:04.009: INFO: Got endpoints: latency-svc-rggrd [755.047275ms]
  Jul  1 12:13:04.054: INFO: Got endpoints: latency-svc-pwvtl [750.662843ms]
  Jul  1 12:13:04.102: INFO: Got endpoints: latency-svc-2qmqh [745.575423ms]
  Jul  1 12:13:04.153: INFO: Got endpoints: latency-svc-k7cls [747.353159ms]
  Jul  1 12:13:04.205: INFO: Got endpoints: latency-svc-drxct [748.881212ms]
  Jul  1 12:13:04.258: INFO: Got endpoints: latency-svc-tflnt [755.723276ms]
  Jul  1 12:13:04.302: INFO: Got endpoints: latency-svc-pkk49 [748.528464ms]
  Jul  1 12:13:04.355: INFO: Got endpoints: latency-svc-8ftgp [749.132201ms]
  Jul  1 12:13:04.404: INFO: Got endpoints: latency-svc-txdrl [750.085239ms]
  Jul  1 12:13:04.454: INFO: Got endpoints: latency-svc-kwtgd [746.824031ms]
  Jul  1 12:13:04.503: INFO: Got endpoints: latency-svc-wtfg4 [749.528685ms]
  Jul  1 12:13:04.504: INFO: Latencies: [45.546236ms 62.297162ms 67.197922ms 70.624692ms 72.651219ms 85.258839ms 87.536375ms 89.669332ms 101.250551ms 115.065146ms 121.884962ms 130.436675ms 133.429056ms 141.313333ms 160.07686ms 165.150252ms 170.070554ms 170.903608ms 172.592438ms 175.722422ms 181.476964ms 186.860757ms 194.972036ms 196.109114ms 196.650637ms 197.656384ms 212.491155ms 213.330198ms 214.348576ms 218.595005ms 230.804432ms 231.612074ms 236.211603ms 270.796696ms 303.647722ms 336.949664ms 342.109755ms 356.429694ms 365.692248ms 371.355031ms 375.571321ms 390.04623ms 390.339264ms 394.707462ms 398.960518ms 400.852654ms 409.899398ms 425.857839ms 426.463695ms 453.864527ms 489.452991ms 529.574547ms 572.625843ms 595.045929ms 638.729283ms 659.728634ms 677.218141ms 707.770885ms 716.992217ms 732.445277ms 736.094596ms 738.698655ms 741.268967ms 741.543288ms 741.598115ms 742.117535ms 742.162809ms 742.522471ms 742.650169ms 742.970474ms 744.188513ms 744.582219ms 744.817747ms 745.081726ms 745.418115ms 745.575423ms 745.720838ms 746.109047ms 746.3098ms 746.52945ms 746.588247ms 746.708906ms 746.767421ms 746.824031ms 746.888184ms 747.105768ms 747.180345ms 747.32983ms 747.353159ms 747.399116ms 747.530961ms 747.617738ms 747.6242ms 747.832263ms 747.861795ms 747.865734ms 747.876167ms 748.022264ms 748.04871ms 748.098168ms 748.115697ms 748.242198ms 748.276868ms 748.397572ms 748.47159ms 748.528464ms 748.660651ms 748.701251ms 748.721902ms 748.778698ms 748.881212ms 748.90308ms 748.921914ms 748.965151ms 748.977987ms 749.019464ms 749.025469ms 749.085315ms 749.111662ms 749.130304ms 749.132201ms 749.233554ms 749.244966ms 749.335837ms 749.342416ms 749.361017ms 749.410606ms 749.429326ms 749.528685ms 749.64893ms 749.686052ms 749.719577ms 749.742008ms 749.81708ms 749.923721ms 750.085239ms 750.087661ms 750.232329ms 750.331894ms 750.363072ms 750.453955ms 750.526602ms 750.620182ms 750.625625ms 750.662843ms 750.699569ms 750.828905ms 750.860007ms 750.88339ms 750.887124ms 750.887399ms 750.951835ms 750.957051ms 750.993251ms 751.117193ms 751.128315ms 751.260086ms 751.295311ms 751.305574ms 751.345482ms 751.420587ms 751.420635ms 751.50089ms 751.622028ms 751.769099ms 751.802691ms 751.837783ms 751.982845ms 752.105138ms 752.149235ms 752.233347ms 752.306701ms 752.310659ms 752.458266ms 752.956592ms 753.113103ms 753.114089ms 753.704185ms 753.817199ms 753.982848ms 754.083645ms 754.10161ms 754.373652ms 754.858206ms 755.047275ms 755.444426ms 755.596733ms 755.723276ms 755.860947ms 755.870055ms 756.890364ms 758.430654ms 759.171566ms 759.356848ms 759.437737ms 760.692579ms 763.285797ms 767.27914ms 787.472705ms 838.567714ms]
  Jul  1 12:13:04.504: INFO: 50 %ile: 748.115697ms
  Jul  1 12:13:04.504: INFO: 90 %ile: 754.083645ms
  Jul  1 12:13:04.504: INFO: 99 %ile: 787.472705ms
  Jul  1 12:13:04.504: INFO: Total sample count: 200
  Jul  1 12:13:04.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-743" for this suite. @ 07/01/23 12:13:04.514
• [10.830 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 07/01/23 12:13:04.528
  Jul  1 12:13:04.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename secrets @ 07/01/23 12:13:04.53
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:13:04.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:13:04.564
  STEP: Creating secret with name secret-test-map-c81b6fa2-d091-4eae-985a-cc1839dd977a @ 07/01/23 12:13:04.568
  STEP: Creating a pod to test consume secrets @ 07/01/23 12:13:04.575
  STEP: Saw pod success @ 07/01/23 12:13:08.604
  Jul  1 12:13:08.609: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-secrets-3ff4269a-3338-4a56-9ad8-f39345eb7b6e container secret-volume-test: <nil>
  STEP: delete the pod @ 07/01/23 12:13:08.634
  Jul  1 12:13:08.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3566" for this suite. @ 07/01/23 12:13:08.671
• [4.155 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 07/01/23 12:13:08.685
  Jul  1 12:13:08.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename init-container @ 07/01/23 12:13:08.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:13:08.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:13:08.721
  STEP: creating the pod @ 07/01/23 12:13:08.727
  Jul  1 12:13:08.727: INFO: PodSpec: initContainers in spec.initContainers
  Jul  1 12:13:14.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-4253" for this suite. @ 07/01/23 12:13:14.307
• [5.632 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 07/01/23 12:13:14.32
  Jul  1 12:13:14.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename secrets @ 07/01/23 12:13:14.321
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:13:14.35
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:13:14.354
  Jul  1 12:13:14.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6564" for this suite. @ 07/01/23 12:13:14.421
• [0.111 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 07/01/23 12:13:14.432
  Jul  1 12:13:14.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/01/23 12:13:14.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:13:14.56
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:13:14.566
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 07/01/23 12:13:14.57
  Jul  1 12:13:14.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:13:16.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:13:21.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2638" for this suite. @ 07/01/23 12:13:21.839
• [7.416 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 07/01/23 12:13:21.85
  Jul  1 12:13:21.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename sysctl @ 07/01/23 12:13:21.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:13:21.888
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:13:21.892
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 07/01/23 12:13:21.895
  STEP: Watching for error events or started pod @ 07/01/23 12:13:21.908
  STEP: Waiting for pod completion @ 07/01/23 12:13:23.915
  STEP: Checking that the pod succeeded @ 07/01/23 12:13:25.931
  STEP: Getting logs from the pod @ 07/01/23 12:13:25.931
  STEP: Checking that the sysctl is actually updated @ 07/01/23 12:13:25.941
  Jul  1 12:13:25.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-8292" for this suite. @ 07/01/23 12:13:25.946
• [4.108 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 07/01/23 12:13:25.962
  Jul  1 12:13:25.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir @ 07/01/23 12:13:25.968
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:13:25.998
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:13:26.002
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 07/01/23 12:13:26.005
  STEP: Saw pod success @ 07/01/23 12:13:30.047
  Jul  1 12:13:30.054: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-a32c917e-7a3c-4f27-a46e-34e7e45463c0 container test-container: <nil>
  STEP: delete the pod @ 07/01/23 12:13:30.066
  Jul  1 12:13:30.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5521" for this suite. @ 07/01/23 12:13:30.098
• [4.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 07/01/23 12:13:30.11
  Jul  1 12:13:30.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename downward-api @ 07/01/23 12:13:30.111
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:13:30.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:13:30.155
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 12:13:30.16
  STEP: Saw pod success @ 07/01/23 12:13:36.207
  Jul  1 12:13:36.212: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-b37b8447-eea6-4232-bcf9-1c99f9320a60 container client-container: <nil>
  STEP: delete the pod @ 07/01/23 12:13:36.222
  Jul  1 12:13:36.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8864" for this suite. @ 07/01/23 12:13:36.263
• [6.166 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 07/01/23 12:13:36.276
  Jul  1 12:13:36.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir @ 07/01/23 12:13:36.279
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:13:36.311
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:13:36.316
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 07/01/23 12:13:36.322
  STEP: Saw pod success @ 07/01/23 12:13:40.355
  Jul  1 12:13:40.361: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-319ff115-b226-4c11-bcb2-87778c80f988 container test-container: <nil>
  STEP: delete the pod @ 07/01/23 12:13:40.371
  Jul  1 12:13:40.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3087" for this suite. @ 07/01/23 12:13:40.397
• [4.131 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 07/01/23 12:13:40.408
  Jul  1 12:13:40.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename crd-webhook @ 07/01/23 12:13:40.409
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:13:40.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:13:40.438
  STEP: Setting up server cert @ 07/01/23 12:13:40.441
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 07/01/23 12:13:40.915
  STEP: Deploying the custom resource conversion webhook pod @ 07/01/23 12:13:40.926
  STEP: Wait for the deployment to be ready @ 07/01/23 12:13:40.941
  Jul  1 12:13:40.951: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  Jul  1 12:13:42.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 1, 12, 13, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 13, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 13, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 13, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 07/01/23 12:13:44.976
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 12:13:44.99
  Jul  1 12:13:45.991: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jul  1 12:13:45.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Creating a v1 custom resource @ 07/01/23 12:13:48.613
  STEP: Create a v2 custom resource @ 07/01/23 12:13:48.644
  STEP: List CRs in v1 @ 07/01/23 12:13:48.655
  STEP: List CRs in v2 @ 07/01/23 12:13:48.713
  Jul  1 12:13:48.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-6435" for this suite. @ 07/01/23 12:13:49.322
• [8.929 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 07/01/23 12:13:49.346
  Jul  1 12:13:49.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename dns @ 07/01/23 12:13:49.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:13:49.385
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:13:49.389
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4955.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4955.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 07/01/23 12:13:49.393
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4955.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4955.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 07/01/23 12:13:49.393
  STEP: creating a pod to probe /etc/hosts @ 07/01/23 12:13:49.393
  STEP: submitting the pod to kubernetes @ 07/01/23 12:13:49.394
  STEP: retrieving the pod @ 07/01/23 12:13:59.449
  STEP: looking for the results for each expected name from probers @ 07/01/23 12:13:59.454
  Jul  1 12:13:59.477: INFO: DNS probes using dns-4955/dns-test-a8c18111-aacf-4a3e-9549-28f12bbaca13 succeeded

  Jul  1 12:13:59.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/01/23 12:13:59.484
  STEP: Destroying namespace "dns-4955" for this suite. @ 07/01/23 12:13:59.499
• [10.164 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 07/01/23 12:13:59.511
  Jul  1 12:13:59.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir @ 07/01/23 12:13:59.512
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:13:59.549
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:13:59.553
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 07/01/23 12:13:59.558
  STEP: Saw pod success @ 07/01/23 12:14:11.611
  Jul  1 12:14:11.616: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-3c27d7f9-8ddf-4d64-acc5-c26db73f0126 container test-container: <nil>
  STEP: delete the pod @ 07/01/23 12:14:11.624
  Jul  1 12:14:11.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7627" for this suite. @ 07/01/23 12:14:11.665
• [12.163 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 07/01/23 12:14:11.678
  Jul  1 12:14:11.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 12:14:11.682
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:14:11.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:14:11.718
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 12:14:11.726
  STEP: Saw pod success @ 07/01/23 12:14:15.76
  Jul  1 12:14:15.765: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-4568d62a-f2e4-41db-a3f5-09a9af009cb8 container client-container: <nil>
  STEP: delete the pod @ 07/01/23 12:14:15.777
  Jul  1 12:14:15.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7033" for this suite. @ 07/01/23 12:14:15.81
• [4.142 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 07/01/23 12:14:15.82
  Jul  1 12:14:15.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename hostport @ 07/01/23 12:14:15.821
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:14:15.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:14:15.855
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 07/01/23 12:14:15.868
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.12.125 on the node which pod1 resides and expect scheduled @ 07/01/23 12:14:17.901
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.12.125 but use UDP protocol on the node which pod2 resides @ 07/01/23 12:14:19.921
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 07/01/23 12:14:34
  Jul  1 12:14:34.000: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.12.125 http://127.0.0.1:54323/hostname] Namespace:hostport-6794 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 12:14:34.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:14:34.001: INFO: ExecWithOptions: Clientset creation
  Jul  1 12:14:34.001: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-6794/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.12.125+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.12.125, port: 54323 @ 07/01/23 12:14:34.113
  Jul  1 12:14:34.113: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.12.125:54323/hostname] Namespace:hostport-6794 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 12:14:34.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:14:34.113: INFO: ExecWithOptions: Clientset creation
  Jul  1 12:14:34.113: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-6794/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.12.125%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.12.125, port: 54323 UDP @ 07/01/23 12:14:34.221
  Jul  1 12:14:34.221: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.12.125 54323] Namespace:hostport-6794 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 12:14:34.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:14:34.222: INFO: ExecWithOptions: Clientset creation
  Jul  1 12:14:34.222: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-6794/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.12.125+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  Jul  1 12:14:39.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-6794" for this suite. @ 07/01/23 12:14:39.31
• [23.500 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 07/01/23 12:14:39.322
  Jul  1 12:14:39.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename containers @ 07/01/23 12:14:39.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:14:39.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:14:39.353
  STEP: Creating a pod to test override all @ 07/01/23 12:14:39.357
  STEP: Saw pod success @ 07/01/23 12:14:43.388
  Jul  1 12:14:43.395: INFO: Trying to get logs from node ip-172-31-91-66 pod client-containers-45b04dce-c8b4-4c39-ba8c-c2ddf7d4c567 container agnhost-container: <nil>
  STEP: delete the pod @ 07/01/23 12:14:43.404
  Jul  1 12:14:43.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-9796" for this suite. @ 07/01/23 12:14:43.434
• [4.121 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 07/01/23 12:14:43.445
  Jul  1 12:14:43.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename init-container @ 07/01/23 12:14:43.446
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:14:43.474
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:14:43.478
  STEP: creating the pod @ 07/01/23 12:14:43.481
  Jul  1 12:14:43.481: INFO: PodSpec: initContainers in spec.initContainers
  Jul  1 12:14:48.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1302" for this suite. @ 07/01/23 12:14:48.191
• [4.756 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 07/01/23 12:14:48.203
  Jul  1 12:14:48.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename sched-pred @ 07/01/23 12:14:48.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:14:48.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:14:48.244
  Jul  1 12:14:48.252: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul  1 12:14:48.267: INFO: Waiting for terminating namespaces to be deleted...
  Jul  1 12:14:48.272: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-12-125 before test
  Jul  1 12:14:48.278: INFO: nginx-ingress-controller-kubernetes-worker-tjncz from ingress-nginx-kubernetes-worker started at 2023-07-01 11:52:18 +0000 UTC (1 container statuses recorded)
  Jul  1 12:14:48.278: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  1 12:14:48.279: INFO: calico-kube-controllers-9c5cff4fb-6hfws from kube-system started at 2023-07-01 11:52:25 +0000 UTC (1 container statuses recorded)
  Jul  1 12:14:48.279: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul  1 12:14:48.279: INFO: sonobuoy-e2e-job-e4b68f70fcf04452 from sonobuoy started at 2023-07-01 11:57:54 +0000 UTC (2 container statuses recorded)
  Jul  1 12:14:48.279: INFO: 	Container e2e ready: true, restart count 0
  Jul  1 12:14:48.279: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  1 12:14:48.279: INFO: sonobuoy-systemd-logs-daemon-set-0aed011ee39b4f14-g2zvl from sonobuoy started at 2023-07-01 11:57:54 +0000 UTC (2 container statuses recorded)
  Jul  1 12:14:48.279: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  1 12:14:48.279: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul  1 12:14:48.279: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-16-94 before test
  Jul  1 12:14:48.289: INFO: nginx-ingress-controller-kubernetes-worker-ps7xr from ingress-nginx-kubernetes-worker started at 2023-07-01 11:52:10 +0000 UTC (1 container statuses recorded)
  Jul  1 12:14:48.289: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  1 12:14:48.289: INFO: coredns-5c7f76ccb8-zxmnn from kube-system started at 2023-07-01 11:52:04 +0000 UTC (1 container statuses recorded)
  Jul  1 12:14:48.289: INFO: 	Container coredns ready: true, restart count 0
  Jul  1 12:14:48.289: INFO: kube-state-metrics-5b95b4459c-5klzn from kube-system started at 2023-07-01 11:52:04 +0000 UTC (1 container statuses recorded)
  Jul  1 12:14:48.289: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul  1 12:14:48.289: INFO: metrics-server-v0.5.2-6cf8c8b69c-lw48q from kube-system started at 2023-07-01 11:52:04 +0000 UTC (2 container statuses recorded)
  Jul  1 12:14:48.289: INFO: 	Container metrics-server ready: true, restart count 0
  Jul  1 12:14:48.289: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul  1 12:14:48.289: INFO: dashboard-metrics-scraper-6b8586b5c9-8wfzw from kubernetes-dashboard started at 2023-07-01 11:52:04 +0000 UTC (1 container statuses recorded)
  Jul  1 12:14:48.289: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul  1 12:14:48.289: INFO: kubernetes-dashboard-6869f4cd5f-f7c25 from kubernetes-dashboard started at 2023-07-01 11:52:04 +0000 UTC (1 container statuses recorded)
  Jul  1 12:14:48.289: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul  1 12:14:48.289: INFO: sonobuoy-systemd-logs-daemon-set-0aed011ee39b4f14-bbwnl from sonobuoy started at 2023-07-01 11:57:54 +0000 UTC (2 container statuses recorded)
  Jul  1 12:14:48.289: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  1 12:14:48.289: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul  1 12:14:48.289: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-91-66 before test
  Jul  1 12:14:48.296: INFO: default-http-backend-kubernetes-worker-65fc475d49-7llc8 from ingress-nginx-kubernetes-worker started at 2023-07-01 11:52:10 +0000 UTC (1 container statuses recorded)
  Jul  1 12:14:48.296: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul  1 12:14:48.296: INFO: nginx-ingress-controller-kubernetes-worker-rs6r4 from ingress-nginx-kubernetes-worker started at 2023-07-01 11:52:14 +0000 UTC (1 container statuses recorded)
  Jul  1 12:14:48.296: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  1 12:14:48.296: INFO: pod-init-182fd857-cdb4-48d4-8002-43f0fe12a137 from init-container-1302 started at 2023-07-01 12:14:43 +0000 UTC (1 container statuses recorded)
  Jul  1 12:14:48.296: INFO: 	Container run1 ready: true, restart count 0
  Jul  1 12:14:48.296: INFO: sonobuoy from sonobuoy started at 2023-07-01 11:57:50 +0000 UTC (1 container statuses recorded)
  Jul  1 12:14:48.296: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul  1 12:14:48.297: INFO: sonobuoy-systemd-logs-daemon-set-0aed011ee39b4f14-g92dx from sonobuoy started at 2023-07-01 11:57:54 +0000 UTC (2 container statuses recorded)
  Jul  1 12:14:48.297: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  1 12:14:48.297: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ip-172-31-12-125 @ 07/01/23 12:14:48.317
  STEP: verifying the node has the label node ip-172-31-16-94 @ 07/01/23 12:14:48.333
  STEP: verifying the node has the label node ip-172-31-91-66 @ 07/01/23 12:14:48.348
  Jul  1 12:14:48.370: INFO: Pod default-http-backend-kubernetes-worker-65fc475d49-7llc8 requesting resource cpu=10m on Node ip-172-31-91-66
  Jul  1 12:14:48.371: INFO: Pod nginx-ingress-controller-kubernetes-worker-ps7xr requesting resource cpu=0m on Node ip-172-31-16-94
  Jul  1 12:14:48.371: INFO: Pod nginx-ingress-controller-kubernetes-worker-rs6r4 requesting resource cpu=0m on Node ip-172-31-91-66
  Jul  1 12:14:48.372: INFO: Pod nginx-ingress-controller-kubernetes-worker-tjncz requesting resource cpu=0m on Node ip-172-31-12-125
  Jul  1 12:14:48.372: INFO: Pod pod-init-182fd857-cdb4-48d4-8002-43f0fe12a137 requesting resource cpu=100m on Node ip-172-31-91-66
  Jul  1 12:14:48.372: INFO: Pod calico-kube-controllers-9c5cff4fb-6hfws requesting resource cpu=0m on Node ip-172-31-12-125
  Jul  1 12:14:48.372: INFO: Pod coredns-5c7f76ccb8-zxmnn requesting resource cpu=100m on Node ip-172-31-16-94
  Jul  1 12:14:48.373: INFO: Pod kube-state-metrics-5b95b4459c-5klzn requesting resource cpu=0m on Node ip-172-31-16-94
  Jul  1 12:14:48.373: INFO: Pod metrics-server-v0.5.2-6cf8c8b69c-lw48q requesting resource cpu=5m on Node ip-172-31-16-94
  Jul  1 12:14:48.373: INFO: Pod dashboard-metrics-scraper-6b8586b5c9-8wfzw requesting resource cpu=0m on Node ip-172-31-16-94
  Jul  1 12:14:48.373: INFO: Pod kubernetes-dashboard-6869f4cd5f-f7c25 requesting resource cpu=0m on Node ip-172-31-16-94
  Jul  1 12:14:48.374: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-91-66
  Jul  1 12:14:48.379: INFO: Pod sonobuoy-e2e-job-e4b68f70fcf04452 requesting resource cpu=0m on Node ip-172-31-12-125
  Jul  1 12:14:48.379: INFO: Pod sonobuoy-systemd-logs-daemon-set-0aed011ee39b4f14-bbwnl requesting resource cpu=0m on Node ip-172-31-16-94
  Jul  1 12:14:48.379: INFO: Pod sonobuoy-systemd-logs-daemon-set-0aed011ee39b4f14-g2zvl requesting resource cpu=0m on Node ip-172-31-12-125
  Jul  1 12:14:48.379: INFO: Pod sonobuoy-systemd-logs-daemon-set-0aed011ee39b4f14-g92dx requesting resource cpu=0m on Node ip-172-31-91-66
  STEP: Starting Pods to consume most of the cluster CPU. @ 07/01/23 12:14:48.379
  Jul  1 12:14:48.380: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-12-125
  Jul  1 12:14:48.394: INFO: Creating a pod which consumes cpu=1326m on Node ip-172-31-16-94
  Jul  1 12:14:48.402: INFO: Creating a pod which consumes cpu=1323m on Node ip-172-31-91-66
  STEP: Creating another pod that requires unavailable amount of CPU. @ 07/01/23 12:14:50.433
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-018c7b41-18f1-41bb-b160-d85519b351d7.176dbd67b3a02fd8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4710/filler-pod-018c7b41-18f1-41bb-b160-d85519b351d7 to ip-172-31-91-66] @ 07/01/23 12:14:50.438
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-018c7b41-18f1-41bb-b160-d85519b351d7.176dbd67e182205b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/01/23 12:14:50.439
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-018c7b41-18f1-41bb-b160-d85519b351d7.176dbd67e2ce1740], Reason = [Created], Message = [Created container filler-pod-018c7b41-18f1-41bb-b160-d85519b351d7] @ 07/01/23 12:14:50.439
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-018c7b41-18f1-41bb-b160-d85519b351d7.176dbd67e720da07], Reason = [Started], Message = [Started container filler-pod-018c7b41-18f1-41bb-b160-d85519b351d7] @ 07/01/23 12:14:50.439
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-0a63f120-96d3-469e-9123-e8fb021e59f5.176dbd67b27adeac], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4710/filler-pod-0a63f120-96d3-469e-9123-e8fb021e59f5 to ip-172-31-12-125] @ 07/01/23 12:14:50.439
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-0a63f120-96d3-469e-9123-e8fb021e59f5.176dbd67df649aeb], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/01/23 12:14:50.439
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-0a63f120-96d3-469e-9123-e8fb021e59f5.176dbd67e0917833], Reason = [Created], Message = [Created container filler-pod-0a63f120-96d3-469e-9123-e8fb021e59f5] @ 07/01/23 12:14:50.439
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-0a63f120-96d3-469e-9123-e8fb021e59f5.176dbd67e55943b3], Reason = [Started], Message = [Started container filler-pod-0a63f120-96d3-469e-9123-e8fb021e59f5] @ 07/01/23 12:14:50.439
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c2f89f9f-8950-4254-b7cd-8640fb54857a.176dbd67b30dbd07], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4710/filler-pod-c2f89f9f-8950-4254-b7cd-8640fb54857a to ip-172-31-16-94] @ 07/01/23 12:14:50.439
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c2f89f9f-8950-4254-b7cd-8640fb54857a.176dbd67e11230da], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/01/23 12:14:50.439
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c2f89f9f-8950-4254-b7cd-8640fb54857a.176dbd67e21f5004], Reason = [Created], Message = [Created container filler-pod-c2f89f9f-8950-4254-b7cd-8640fb54857a] @ 07/01/23 12:14:50.439
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c2f89f9f-8950-4254-b7cd-8640fb54857a.176dbd67e6dc7d89], Reason = [Started], Message = [Started container filler-pod-c2f89f9f-8950-4254-b7cd-8640fb54857a] @ 07/01/23 12:14:50.439
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.176dbd682c7319f1], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/5 nodes are available: 2 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] @ 07/01/23 12:14:50.476
  STEP: removing the label node off the node ip-172-31-16-94 @ 07/01/23 12:14:51.456
  STEP: verifying the node doesn't have the label node @ 07/01/23 12:14:51.472
  STEP: removing the label node off the node ip-172-31-91-66 @ 07/01/23 12:14:51.476
  STEP: verifying the node doesn't have the label node @ 07/01/23 12:14:51.49
  STEP: removing the label node off the node ip-172-31-12-125 @ 07/01/23 12:14:51.499
  STEP: verifying the node doesn't have the label node @ 07/01/23 12:14:51.524
  Jul  1 12:14:51.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-4710" for this suite. @ 07/01/23 12:14:51.535
• [3.349 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 07/01/23 12:14:51.554
  Jul  1 12:14:51.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename watch @ 07/01/23 12:14:51.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:14:51.584
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:14:51.595
  STEP: creating a new configmap @ 07/01/23 12:14:51.601
  STEP: modifying the configmap once @ 07/01/23 12:14:51.608
  STEP: modifying the configmap a second time @ 07/01/23 12:14:51.621
  STEP: deleting the configmap @ 07/01/23 12:14:51.634
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 07/01/23 12:14:51.645
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 07/01/23 12:14:51.647
  Jul  1 12:14:51.647: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2432  1b341f9a-e886-4983-b3af-3385f7c37b23 7423 0 2023-07-01 12:14:51 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-01 12:14:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 12:14:51.648: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2432  1b341f9a-e886-4983-b3af-3385f7c37b23 7424 0 2023-07-01 12:14:51 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-01 12:14:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 12:14:51.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2432" for this suite. @ 07/01/23 12:14:51.655
• [0.113 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 07/01/23 12:14:51.668
  Jul  1 12:14:51.669: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename certificates @ 07/01/23 12:14:51.67
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:14:51.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:14:51.711
  STEP: getting /apis @ 07/01/23 12:14:52.848
  STEP: getting /apis/certificates.k8s.io @ 07/01/23 12:14:52.854
  STEP: getting /apis/certificates.k8s.io/v1 @ 07/01/23 12:14:52.856
  STEP: creating @ 07/01/23 12:14:52.861
  STEP: getting @ 07/01/23 12:14:52.9
  STEP: listing @ 07/01/23 12:14:52.909
  STEP: watching @ 07/01/23 12:14:52.914
  Jul  1 12:14:52.914: INFO: starting watch
  STEP: patching @ 07/01/23 12:14:52.916
  STEP: updating @ 07/01/23 12:14:52.941
  Jul  1 12:14:52.949: INFO: waiting for watch events with expected annotations
  Jul  1 12:14:52.949: INFO: saw patched and updated annotations
  STEP: getting /approval @ 07/01/23 12:14:52.95
  STEP: patching /approval @ 07/01/23 12:14:52.955
  STEP: updating /approval @ 07/01/23 12:14:52.964
  STEP: getting /status @ 07/01/23 12:14:52.971
  STEP: patching /status @ 07/01/23 12:14:52.976
  STEP: updating /status @ 07/01/23 12:14:52.985
  STEP: deleting @ 07/01/23 12:14:52.993
  STEP: deleting a collection @ 07/01/23 12:14:53.011
  Jul  1 12:14:53.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-1136" for this suite. @ 07/01/23 12:14:53.052
• [1.399 seconds]
------------------------------
SSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 07/01/23 12:14:53.068
  Jul  1 12:14:53.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename conformance-tests @ 07/01/23 12:14:53.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:14:53.102
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:14:53.105
  STEP: Getting node addresses @ 07/01/23 12:14:53.109
  Jul  1 12:14:53.109: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Jul  1 12:14:53.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-9282" for this suite. @ 07/01/23 12:14:53.125
• [0.074 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 07/01/23 12:14:53.143
  Jul  1 12:14:53.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename downward-api @ 07/01/23 12:14:53.144
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:14:53.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:14:53.187
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 12:14:53.19
  STEP: Saw pod success @ 07/01/23 12:14:57.221
  Jul  1 12:14:57.228: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-f929126a-c145-4165-9f1b-dd60c427eaf6 container client-container: <nil>
  STEP: delete the pod @ 07/01/23 12:14:57.239
  Jul  1 12:14:57.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7756" for this suite. @ 07/01/23 12:14:57.268
• [4.135 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 07/01/23 12:14:57.278
  Jul  1 12:14:57.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubelet-test @ 07/01/23 12:14:57.278
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:14:57.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:14:57.325
  Jul  1 12:14:59.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4602" for this suite. @ 07/01/23 12:14:59.38
• [2.111 seconds]
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 07/01/23 12:14:59.389
  Jul  1 12:14:59.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename pods @ 07/01/23 12:14:59.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:14:59.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:14:59.428
  STEP: creating the pod @ 07/01/23 12:14:59.43
  STEP: submitting the pod to kubernetes @ 07/01/23 12:14:59.431
  STEP: verifying QOS class is set on the pod @ 07/01/23 12:14:59.444
  Jul  1 12:14:59.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1616" for this suite. @ 07/01/23 12:14:59.454
• [0.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 07/01/23 12:14:59.469
  Jul  1 12:14:59.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-probe @ 07/01/23 12:14:59.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:14:59.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:14:59.512
  STEP: Creating pod liveness-6bae7c23-5431-4688-aafb-51c2773b6ce5 in namespace container-probe-4914 @ 07/01/23 12:14:59.515
  Jul  1 12:15:01.545: INFO: Started pod liveness-6bae7c23-5431-4688-aafb-51c2773b6ce5 in namespace container-probe-4914
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/01/23 12:15:01.545
  Jul  1 12:15:01.552: INFO: Initial restart count of pod liveness-6bae7c23-5431-4688-aafb-51c2773b6ce5 is 0
  Jul  1 12:15:21.643: INFO: Restart count of pod container-probe-4914/liveness-6bae7c23-5431-4688-aafb-51c2773b6ce5 is now 1 (20.091857265s elapsed)
  Jul  1 12:15:21.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/01/23 12:15:21.649
  STEP: Destroying namespace "container-probe-4914" for this suite. @ 07/01/23 12:15:21.679
• [22.220 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 07/01/23 12:15:21.691
  Jul  1 12:15:21.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-probe @ 07/01/23 12:15:21.692
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:15:21.768
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:15:21.772
  STEP: Creating pod liveness-34bcf6cc-8cff-4d8a-ab61-2835cc2217b4 in namespace container-probe-7850 @ 07/01/23 12:15:21.776
  Jul  1 12:15:25.811: INFO: Started pod liveness-34bcf6cc-8cff-4d8a-ab61-2835cc2217b4 in namespace container-probe-7850
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/01/23 12:15:25.811
  Jul  1 12:15:25.816: INFO: Initial restart count of pod liveness-34bcf6cc-8cff-4d8a-ab61-2835cc2217b4 is 0
  Jul  1 12:15:43.872: INFO: Restart count of pod container-probe-7850/liveness-34bcf6cc-8cff-4d8a-ab61-2835cc2217b4 is now 1 (18.056083127s elapsed)
  Jul  1 12:16:03.939: INFO: Restart count of pod container-probe-7850/liveness-34bcf6cc-8cff-4d8a-ab61-2835cc2217b4 is now 2 (38.123136381s elapsed)
  Jul  1 12:16:24.005: INFO: Restart count of pod container-probe-7850/liveness-34bcf6cc-8cff-4d8a-ab61-2835cc2217b4 is now 3 (58.189543349s elapsed)
  Jul  1 12:16:44.075: INFO: Restart count of pod container-probe-7850/liveness-34bcf6cc-8cff-4d8a-ab61-2835cc2217b4 is now 4 (1m18.259009253s elapsed)
  Jul  1 12:17:44.267: INFO: Restart count of pod container-probe-7850/liveness-34bcf6cc-8cff-4d8a-ab61-2835cc2217b4 is now 5 (2m18.451130145s elapsed)
  Jul  1 12:17:44.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/01/23 12:17:44.272
  STEP: Destroying namespace "container-probe-7850" for this suite. @ 07/01/23 12:17:44.296
• [142.614 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 07/01/23 12:17:44.306
  Jul  1 12:17:44.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename dns @ 07/01/23 12:17:44.308
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:17:44.338
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:17:44.351
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 07/01/23 12:17:44.36
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 07/01/23 12:17:44.36
  STEP: creating a pod to probe DNS @ 07/01/23 12:17:44.36
  STEP: submitting the pod to kubernetes @ 07/01/23 12:17:44.36
  STEP: retrieving the pod @ 07/01/23 12:17:46.387
  STEP: looking for the results for each expected name from probers @ 07/01/23 12:17:46.391
  Jul  1 12:17:46.417: INFO: DNS probes using dns-6957/dns-test-28576d8a-88fe-40ca-afa4-380396c0872c succeeded

  Jul  1 12:17:46.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/01/23 12:17:46.422
  STEP: Destroying namespace "dns-6957" for this suite. @ 07/01/23 12:17:46.442
• [2.144 seconds]
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 07/01/23 12:17:46.451
  Jul  1 12:17:46.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename runtimeclass @ 07/01/23 12:17:46.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:17:46.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:17:46.485
  STEP: Deleting RuntimeClass runtimeclass-3294-delete-me @ 07/01/23 12:17:46.496
  STEP: Waiting for the RuntimeClass to disappear @ 07/01/23 12:17:46.504
  Jul  1 12:17:46.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3294" for this suite. @ 07/01/23 12:17:46.531
• [0.091 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 07/01/23 12:17:46.547
  Jul  1 12:17:46.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir-wrapper @ 07/01/23 12:17:46.554
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:17:46.581
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:17:46.585
  Jul  1 12:17:48.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 07/01/23 12:17:48.645
  STEP: Cleaning up the configmap @ 07/01/23 12:17:48.654
  STEP: Cleaning up the pod @ 07/01/23 12:17:48.662
  STEP: Destroying namespace "emptydir-wrapper-4555" for this suite. @ 07/01/23 12:17:48.686
• [2.149 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 07/01/23 12:17:48.697
  Jul  1 12:17:48.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl @ 07/01/23 12:17:48.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:17:48.734
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:17:48.737
  STEP: creating Agnhost RC @ 07/01/23 12:17:48.741
  Jul  1 12:17:48.741: INFO: namespace kubectl-1656
  Jul  1 12:17:48.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-1656 create -f -'
  Jul  1 12:17:50.119: INFO: stderr: ""
  Jul  1 12:17:50.119: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/01/23 12:17:50.119
  Jul  1 12:17:51.125: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  1 12:17:51.125: INFO: Found 0 / 1
  Jul  1 12:17:52.129: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  1 12:17:52.129: INFO: Found 1 / 1
  Jul  1 12:17:52.129: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jul  1 12:17:52.136: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  1 12:17:52.136: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul  1 12:17:52.137: INFO: wait on agnhost-primary startup in kubectl-1656 
  Jul  1 12:17:52.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-1656 logs agnhost-primary-jdjqk agnhost-primary'
  Jul  1 12:17:52.310: INFO: stderr: ""
  Jul  1 12:17:52.310: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 07/01/23 12:17:52.31
  Jul  1 12:17:52.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-1656 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Jul  1 12:17:52.493: INFO: stderr: ""
  Jul  1 12:17:52.493: INFO: stdout: "service/rm2 exposed\n"
  Jul  1 12:17:52.499: INFO: Service rm2 in namespace kubectl-1656 found.
  STEP: exposing service @ 07/01/23 12:17:54.514
  Jul  1 12:17:54.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-1656 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Jul  1 12:17:54.732: INFO: stderr: ""
  Jul  1 12:17:54.732: INFO: stdout: "service/rm3 exposed\n"
  Jul  1 12:17:54.744: INFO: Service rm3 in namespace kubectl-1656 found.
  Jul  1 12:17:56.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1656" for this suite. @ 07/01/23 12:17:56.761
• [8.073 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 07/01/23 12:17:56.77
  Jul  1 12:17:56.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename job @ 07/01/23 12:17:56.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:17:56.848
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:17:56.854
  STEP: Creating Indexed job @ 07/01/23 12:17:56.862
  STEP: Ensuring job reaches completions @ 07/01/23 12:17:56.872
  STEP: Ensuring pods with index for job exist @ 07/01/23 12:18:06.897
  Jul  1 12:18:06.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5032" for this suite. @ 07/01/23 12:18:06.944
• [10.189 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 07/01/23 12:18:06.962
  Jul  1 12:18:06.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename deployment @ 07/01/23 12:18:06.972
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:18:07.005
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:18:07.035
  Jul  1 12:18:07.069: INFO: Pod name rollover-pod: Found 0 pods out of 1
  Jul  1 12:18:12.127: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/01/23 12:18:12.127
  Jul  1 12:18:12.127: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  Jul  1 12:18:14.134: INFO: Creating deployment "test-rollover-deployment"
  Jul  1 12:18:14.149: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  Jul  1 12:18:16.162: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Jul  1 12:18:16.172: INFO: Ensure that both replica sets have 1 created replica
  Jul  1 12:18:16.183: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Jul  1 12:18:16.199: INFO: Updating deployment test-rollover-deployment
  Jul  1 12:18:16.199: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  Jul  1 12:18:18.216: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Jul  1 12:18:18.228: INFO: Make sure deployment "test-rollover-deployment" is complete
  Jul  1 12:18:18.242: INFO: all replica sets need to contain the pod-template-hash label
  Jul  1 12:18:18.242: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 18, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 18, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul  1 12:18:20.253: INFO: all replica sets need to contain the pod-template-hash label
  Jul  1 12:18:20.253: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 18, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 18, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul  1 12:18:22.258: INFO: all replica sets need to contain the pod-template-hash label
  Jul  1 12:18:22.259: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 18, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 18, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul  1 12:18:24.262: INFO: all replica sets need to contain the pod-template-hash label
  Jul  1 12:18:24.262: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 18, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 18, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul  1 12:18:26.256: INFO: all replica sets need to contain the pod-template-hash label
  Jul  1 12:18:26.257: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 18, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 18, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul  1 12:18:28.254: INFO: 
  Jul  1 12:18:28.254: INFO: Ensure that both old replica sets have no replicas
  Jul  1 12:18:28.272: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-354  31fa2284-eb7b-425d-9f31-186f88120ec5 8507 2 2023-07-01 12:18:14 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-01 12:18:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 12:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bd5c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-01 12:18:14 +0000 UTC,LastTransitionTime:2023-07-01 12:18:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-07-01 12:18:27 +0000 UTC,LastTransitionTime:2023-07-01 12:18:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul  1 12:18:28.278: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-354  529ed313-695d-401e-91d0-281a211bb67b 8496 2 2023-07-01 12:18:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 31fa2284-eb7b-425d-9f31-186f88120ec5 0xc003ed4157 0xc003ed4158}] [] [{kube-controller-manager Update apps/v1 2023-07-01 12:18:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"31fa2284-eb7b-425d-9f31-186f88120ec5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 12:18:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ed4208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul  1 12:18:28.278: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Jul  1 12:18:28.278: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-354  8a894d77-fe45-435e-96d3-6daaa7138bb8 8506 2 2023-07-01 12:18:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 31fa2284-eb7b-425d-9f31-186f88120ec5 0xc003ed402f 0xc003ed4040}] [] [{e2e.test Update apps/v1 2023-07-01 12:18:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 12:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"31fa2284-eb7b-425d-9f31-186f88120ec5\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-01 12:18:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003ed40f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul  1 12:18:28.278: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-354  45a07532-f458-4d37-a39e-64efbf9d76ef 8452 2 2023-07-01 12:18:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 31fa2284-eb7b-425d-9f31-186f88120ec5 0xc003ed4267 0xc003ed4268}] [] [{kube-controller-manager Update apps/v1 2023-07-01 12:18:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"31fa2284-eb7b-425d-9f31-186f88120ec5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 12:18:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ed4318 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul  1 12:18:28.285: INFO: Pod "test-rollover-deployment-57777854c9-26s5w" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-26s5w test-rollover-deployment-57777854c9- deployment-354  33c47812-addb-4d65-ba0b-c2af6501f2af 8474 0 2023-07-01 12:18:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 529ed313-695d-401e-91d0-281a211bb67b 0xc003ed4867 0xc003ed4868}] [] [{kube-controller-manager Update v1 2023-07-01 12:18:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"529ed313-695d-401e-91d0-281a211bb67b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:18:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.62.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ppshg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ppshg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:18:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:18:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:18:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:18:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:192.168.62.103,StartTime:2023-07-01 12:18:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-01 12:18:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://fb6af97594954dfc5125323f2194aad600906083ef9aeb172cbaf40f746d5cd3,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.62.103,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:18:28.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-354" for this suite. @ 07/01/23 12:18:28.29
• [21.339 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 07/01/23 12:18:28.302
  Jul  1 12:18:28.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename cronjob @ 07/01/23 12:18:28.302
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:18:28.335
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:18:28.339
  STEP: Creating a cronjob @ 07/01/23 12:18:28.344
  STEP: Ensuring more than one job is running at a time @ 07/01/23 12:18:28.354
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 07/01/23 12:20:00.36
  STEP: Removing cronjob @ 07/01/23 12:20:00.365
  Jul  1 12:20:00.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6883" for this suite. @ 07/01/23 12:20:00.379
• [92.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 07/01/23 12:20:00.391
  Jul  1 12:20:00.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename gc @ 07/01/23 12:20:00.392
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:20:00.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:20:00.463
  STEP: create the rc @ 07/01/23 12:20:00.473
  W0701 12:20:00.482189      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 07/01/23 12:20:06.49
  STEP: wait for the rc to be deleted @ 07/01/23 12:20:06.499
  Jul  1 12:20:07.528: INFO: 80 pods remaining
  Jul  1 12:20:07.528: INFO: 80 pods has nil DeletionTimestamp
  Jul  1 12:20:07.531: INFO: 
  Jul  1 12:20:08.536: INFO: 71 pods remaining
  Jul  1 12:20:08.538: INFO: 70 pods has nil DeletionTimestamp
  Jul  1 12:20:08.538: INFO: 
  Jul  1 12:20:09.543: INFO: 60 pods remaining
  Jul  1 12:20:09.543: INFO: 60 pods has nil DeletionTimestamp
  Jul  1 12:20:09.543: INFO: 
  Jul  1 12:20:10.521: INFO: 40 pods remaining
  Jul  1 12:20:10.522: INFO: 40 pods has nil DeletionTimestamp
  Jul  1 12:20:10.522: INFO: 
  Jul  1 12:20:11.517: INFO: 31 pods remaining
  Jul  1 12:20:11.517: INFO: 31 pods has nil DeletionTimestamp
  Jul  1 12:20:11.517: INFO: 
  Jul  1 12:20:12.513: INFO: 20 pods remaining
  Jul  1 12:20:12.513: INFO: 20 pods has nil DeletionTimestamp
  Jul  1 12:20:12.513: INFO: 
  STEP: Gathering metrics @ 07/01/23 12:20:13.524
  W0701 12:20:13.542273      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul  1 12:20:13.542: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul  1 12:20:13.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1160" for this suite. @ 07/01/23 12:20:13.575
• [13.198 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 07/01/23 12:20:13.59
  Jul  1 12:20:13.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename resourcequota @ 07/01/23 12:20:13.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:20:13.641
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:20:13.661
  STEP: Counting existing ResourceQuota @ 07/01/23 12:20:13.687
  STEP: Creating a ResourceQuota @ 07/01/23 12:20:18.694
  STEP: Ensuring resource quota status is calculated @ 07/01/23 12:20:18.7
  STEP: Creating a ReplicaSet @ 07/01/23 12:20:20.707
  STEP: Ensuring resource quota status captures replicaset creation @ 07/01/23 12:20:20.73
  STEP: Deleting a ReplicaSet @ 07/01/23 12:20:22.741
  STEP: Ensuring resource quota status released usage @ 07/01/23 12:20:22.755
  Jul  1 12:20:24.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-221" for this suite. @ 07/01/23 12:20:24.767
• [11.191 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 07/01/23 12:20:24.782
  Jul  1 12:20:24.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename endpointslice @ 07/01/23 12:20:24.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:20:24.807
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:20:24.811
  STEP: getting /apis @ 07/01/23 12:20:24.816
  STEP: getting /apis/discovery.k8s.io @ 07/01/23 12:20:24.823
  STEP: getting /apis/discovery.k8s.iov1 @ 07/01/23 12:20:24.825
  STEP: creating @ 07/01/23 12:20:24.827
  STEP: getting @ 07/01/23 12:20:24.848
  STEP: listing @ 07/01/23 12:20:24.855
  STEP: watching @ 07/01/23 12:20:24.86
  Jul  1 12:20:24.860: INFO: starting watch
  STEP: cluster-wide listing @ 07/01/23 12:20:24.861
  STEP: cluster-wide watching @ 07/01/23 12:20:24.866
  Jul  1 12:20:24.866: INFO: starting watch
  STEP: patching @ 07/01/23 12:20:24.868
  STEP: updating @ 07/01/23 12:20:24.875
  Jul  1 12:20:24.885: INFO: waiting for watch events with expected annotations
  Jul  1 12:20:24.885: INFO: saw patched and updated annotations
  STEP: deleting @ 07/01/23 12:20:24.885
  STEP: deleting a collection @ 07/01/23 12:20:24.907
  Jul  1 12:20:24.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4269" for this suite. @ 07/01/23 12:20:24.937
• [0.163 seconds]
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 07/01/23 12:20:24.946
  Jul  1 12:20:24.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename field-validation @ 07/01/23 12:20:24.947
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:20:24.972
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:20:24.976
  Jul  1 12:20:24.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:20:27.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7568" for this suite. @ 07/01/23 12:20:27.59
• [2.653 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 07/01/23 12:20:27.6
  Jul  1 12:20:27.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename containers @ 07/01/23 12:20:27.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:20:27.631
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:20:27.636
  Jul  1 12:20:33.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-1537" for this suite. @ 07/01/23 12:20:33.732
• [6.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 07/01/23 12:20:33.745
  Jul  1 12:20:33.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename pod-network-test @ 07/01/23 12:20:33.746
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:20:33.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:20:33.782
  STEP: Performing setup for networking test in namespace pod-network-test-782 @ 07/01/23 12:20:33.785
  STEP: creating a selector @ 07/01/23 12:20:33.786
  STEP: Creating the service pods in kubernetes @ 07/01/23 12:20:33.786
  Jul  1 12:20:33.786: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 07/01/23 12:20:56.019
  Jul  1 12:20:58.045: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul  1 12:20:58.045: INFO: Breadth first check of 192.168.175.49 on host 172.31.12.125...
  Jul  1 12:20:58.050: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.62.75:9080/dial?request=hostname&protocol=http&host=192.168.175.49&port=8083&tries=1'] Namespace:pod-network-test-782 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 12:20:58.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:20:58.050: INFO: ExecWithOptions: Clientset creation
  Jul  1 12:20:58.050: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-782/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.62.75%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.175.49%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul  1 12:20:58.147: INFO: Waiting for responses: map[]
  Jul  1 12:20:58.147: INFO: reached 192.168.175.49 after 0/1 tries
  Jul  1 12:20:58.147: INFO: Breadth first check of 192.168.85.42 on host 172.31.16.94...
  Jul  1 12:20:58.152: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.62.75:9080/dial?request=hostname&protocol=http&host=192.168.85.42&port=8083&tries=1'] Namespace:pod-network-test-782 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 12:20:58.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:20:58.153: INFO: ExecWithOptions: Clientset creation
  Jul  1 12:20:58.153: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-782/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.62.75%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.85.42%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul  1 12:20:58.241: INFO: Waiting for responses: map[]
  Jul  1 12:20:58.241: INFO: reached 192.168.85.42 after 0/1 tries
  Jul  1 12:20:58.242: INFO: Breadth first check of 192.168.62.76 on host 172.31.91.66...
  Jul  1 12:20:58.247: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.62.75:9080/dial?request=hostname&protocol=http&host=192.168.62.76&port=8083&tries=1'] Namespace:pod-network-test-782 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 12:20:58.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:20:58.249: INFO: ExecWithOptions: Clientset creation
  Jul  1 12:20:58.249: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-782/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.62.75%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.62.76%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul  1 12:20:58.320: INFO: Waiting for responses: map[]
  Jul  1 12:20:58.320: INFO: reached 192.168.62.76 after 0/1 tries
  Jul  1 12:20:58.320: INFO: Going to retry 0 out of 3 pods....
  Jul  1 12:20:58.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-782" for this suite. @ 07/01/23 12:20:58.326
• [24.591 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 07/01/23 12:20:58.338
  Jul  1 12:20:58.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename field-validation @ 07/01/23 12:20:58.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:20:58.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:20:58.373
  STEP: apply creating a deployment @ 07/01/23 12:20:58.377
  Jul  1 12:20:58.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9575" for this suite. @ 07/01/23 12:20:58.413
• [0.085 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 07/01/23 12:20:58.423
  Jul  1 12:20:58.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename statefulset @ 07/01/23 12:20:58.424
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:20:58.449
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:20:58.452
  STEP: Creating service test in namespace statefulset-9068 @ 07/01/23 12:20:58.455
  STEP: Creating statefulset ss in namespace statefulset-9068 @ 07/01/23 12:20:58.466
  Jul  1 12:20:58.477: INFO: Found 0 stateful pods, waiting for 1
  Jul  1 12:21:08.489: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 07/01/23 12:21:08.5
  STEP: Getting /status @ 07/01/23 12:21:08.512
  Jul  1 12:21:08.518: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 07/01/23 12:21:08.518
  Jul  1 12:21:08.531: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 07/01/23 12:21:08.532
  Jul  1 12:21:08.536: INFO: Observed &StatefulSet event: ADDED
  Jul  1 12:21:08.536: INFO: Found Statefulset ss in namespace statefulset-9068 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul  1 12:21:08.536: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 07/01/23 12:21:08.536
  Jul  1 12:21:08.536: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul  1 12:21:08.556: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 07/01/23 12:21:08.557
  Jul  1 12:21:08.560: INFO: Observed &StatefulSet event: ADDED
  Jul  1 12:21:08.561: INFO: Deleting all statefulset in ns statefulset-9068
  Jul  1 12:21:08.565: INFO: Scaling statefulset ss to 0
  Jul  1 12:21:18.593: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  1 12:21:18.598: INFO: Deleting statefulset ss
  Jul  1 12:21:18.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9068" for this suite. @ 07/01/23 12:21:18.625
• [20.213 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 07/01/23 12:21:18.636
  Jul  1 12:21:18.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 12:21:18.637
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:21:18.665
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:21:18.67
  STEP: Setting up server cert @ 07/01/23 12:21:18.718
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 12:21:19.627
  STEP: Deploying the webhook pod @ 07/01/23 12:21:19.641
  STEP: Wait for the deployment to be ready @ 07/01/23 12:21:19.66
  Jul  1 12:21:19.673: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/01/23 12:21:21.69
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 12:21:21.705
  Jul  1 12:21:22.705: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul  1 12:21:22.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 07/01/23 12:21:23.226
  STEP: Creating a custom resource that should be denied by the webhook @ 07/01/23 12:21:23.245
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 07/01/23 12:21:25.275
  STEP: Updating the custom resource with disallowed data should be denied @ 07/01/23 12:21:25.283
  STEP: Deleting the custom resource should be denied @ 07/01/23 12:21:25.299
  STEP: Remove the offending key and value from the custom resource data @ 07/01/23 12:21:25.314
  STEP: Deleting the updated custom resource should be successful @ 07/01/23 12:21:25.334
  Jul  1 12:21:25.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3701" for this suite. @ 07/01/23 12:21:26.007
  STEP: Destroying namespace "webhook-markers-6779" for this suite. @ 07/01/23 12:21:26.019
• [7.392 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 07/01/23 12:21:26.033
  Jul  1 12:21:26.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename var-expansion @ 07/01/23 12:21:26.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:21:26.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:21:26.061
  STEP: creating the pod @ 07/01/23 12:21:26.064
  STEP: waiting for pod running @ 07/01/23 12:21:26.079
  STEP: creating a file in subpath @ 07/01/23 12:21:28.093
  Jul  1 12:21:28.101: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9286 PodName:var-expansion-a0b8591c-63ff-49e5-9616-622cf01cf152 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 12:21:28.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:21:28.101: INFO: ExecWithOptions: Clientset creation
  Jul  1 12:21:28.101: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-9286/pods/var-expansion-a0b8591c-63ff-49e5-9616-622cf01cf152/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 07/01/23 12:21:28.211
  Jul  1 12:21:28.217: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9286 PodName:var-expansion-a0b8591c-63ff-49e5-9616-622cf01cf152 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 12:21:28.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:21:28.218: INFO: ExecWithOptions: Clientset creation
  Jul  1 12:21:28.218: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-9286/pods/var-expansion-a0b8591c-63ff-49e5-9616-622cf01cf152/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 07/01/23 12:21:28.341
  Jul  1 12:21:28.861: INFO: Successfully updated pod "var-expansion-a0b8591c-63ff-49e5-9616-622cf01cf152"
  STEP: waiting for annotated pod running @ 07/01/23 12:21:28.861
  STEP: deleting the pod gracefully @ 07/01/23 12:21:28.869
  Jul  1 12:21:28.869: INFO: Deleting pod "var-expansion-a0b8591c-63ff-49e5-9616-622cf01cf152" in namespace "var-expansion-9286"
  Jul  1 12:21:28.882: INFO: Wait up to 5m0s for pod "var-expansion-a0b8591c-63ff-49e5-9616-622cf01cf152" to be fully deleted
  Jul  1 12:22:03.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9286" for this suite. @ 07/01/23 12:22:03.029
• [37.009 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 07/01/23 12:22:03.042
  Jul  1 12:22:03.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename aggregator @ 07/01/23 12:22:03.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:22:03.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:22:03.089
  Jul  1 12:22:03.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Registering the sample API server. @ 07/01/23 12:22:03.097
  Jul  1 12:22:03.662: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Jul  1 12:22:03.701: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  Jul  1 12:22:05.783: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul  1 12:22:07.790: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul  1 12:22:09.791: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul  1 12:22:11.791: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul  1 12:22:13.790: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul  1 12:22:15.791: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul  1 12:22:17.790: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul  1 12:22:19.792: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul  1 12:22:21.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul  1 12:22:23.790: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul  1 12:22:25.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul  1 12:22:27.915: INFO: Waited 117.6983ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 07/01/23 12:22:27.984
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 07/01/23 12:22:27.989
  STEP: List APIServices @ 07/01/23 12:22:27.997
  Jul  1 12:22:28.004: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 07/01/23 12:22:28.004
  Jul  1 12:22:28.023: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 07/01/23 12:22:28.023
  Jul  1 12:22:28.040: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.July, 1, 12, 22, 27, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 07/01/23 12:22:28.04
  Jul  1 12:22:28.048: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-07-01 12:22:27 +0000 UTC Passed all checks passed}
  Jul  1 12:22:28.048: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul  1 12:22:28.048: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 07/01/23 12:22:28.048
  Jul  1 12:22:28.065: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-332793337" @ 07/01/23 12:22:28.065
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 07/01/23 12:22:28.091
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 07/01/23 12:22:28.1
  STEP: Patch APIService Status @ 07/01/23 12:22:28.105
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 07/01/23 12:22:28.116
  Jul  1 12:22:28.121: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-07-01 12:22:27 +0000 UTC Passed all checks passed}
  Jul  1 12:22:28.121: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul  1 12:22:28.121: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Jul  1 12:22:28.121: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 07/01/23 12:22:28.121
  STEP: Confirm that the generated APIService has been deleted @ 07/01/23 12:22:28.133
  Jul  1 12:22:28.133: INFO: Requesting list of APIServices to confirm quantity
  Jul  1 12:22:28.139: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Jul  1 12:22:28.139: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Jul  1 12:22:28.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-6883" for this suite. @ 07/01/23 12:22:28.295
• [25.261 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 07/01/23 12:22:28.305
  Jul  1 12:22:28.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl @ 07/01/23 12:22:28.305
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:22:28.337
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:22:28.341
  STEP: validating api versions @ 07/01/23 12:22:28.344
  Jul  1 12:22:28.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-8895 api-versions'
  Jul  1 12:22:28.453: INFO: stderr: ""
  Jul  1 12:22:28.453: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Jul  1 12:22:28.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8895" for this suite. @ 07/01/23 12:22:28.459
• [0.165 seconds]
------------------------------
SS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 07/01/23 12:22:28.469
  Jul  1 12:22:28.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename runtimeclass @ 07/01/23 12:22:28.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:22:28.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:22:28.504
  STEP: getting /apis @ 07/01/23 12:22:28.508
  STEP: getting /apis/node.k8s.io @ 07/01/23 12:22:28.513
  STEP: getting /apis/node.k8s.io/v1 @ 07/01/23 12:22:28.514
  STEP: creating @ 07/01/23 12:22:28.516
  STEP: watching @ 07/01/23 12:22:28.54
  Jul  1 12:22:28.540: INFO: starting watch
  STEP: getting @ 07/01/23 12:22:28.562
  STEP: listing @ 07/01/23 12:22:28.567
  STEP: patching @ 07/01/23 12:22:28.572
  STEP: updating @ 07/01/23 12:22:28.578
  Jul  1 12:22:28.586: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 07/01/23 12:22:28.587
  STEP: deleting a collection @ 07/01/23 12:22:28.604
  Jul  1 12:22:28.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-963" for this suite. @ 07/01/23 12:22:28.633
• [0.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 07/01/23 12:22:28.644
  Jul  1 12:22:28.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename field-validation @ 07/01/23 12:22:28.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:22:28.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:22:28.689
  STEP: apply creating a deployment @ 07/01/23 12:22:28.691
  Jul  1 12:22:28.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9071" for this suite. @ 07/01/23 12:22:28.722
• [0.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 07/01/23 12:22:28.747
  Jul  1 12:22:28.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename configmap @ 07/01/23 12:22:28.748
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:22:28.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:22:28.794
  STEP: Creating configMap with name cm-test-opt-del-c835a03c-09f8-4d2a-88d7-24731fd7cce9 @ 07/01/23 12:22:28.813
  STEP: Creating configMap with name cm-test-opt-upd-2ac95ae2-acb8-4115-be31-760eadda6201 @ 07/01/23 12:22:28.822
  STEP: Creating the pod @ 07/01/23 12:22:28.828
  STEP: Deleting configmap cm-test-opt-del-c835a03c-09f8-4d2a-88d7-24731fd7cce9 @ 07/01/23 12:22:34.922
  STEP: Updating configmap cm-test-opt-upd-2ac95ae2-acb8-4115-be31-760eadda6201 @ 07/01/23 12:22:34.93
  STEP: Creating configMap with name cm-test-opt-create-b0d60bf7-db76-4169-991a-74a737e0bac1 @ 07/01/23 12:22:34.938
  STEP: waiting to observe update in volume @ 07/01/23 12:22:34.944
  Jul  1 12:22:36.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1961" for this suite. @ 07/01/23 12:22:36.992
• [8.255 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 07/01/23 12:22:37.005
  Jul  1 12:22:37.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/01/23 12:22:37.006
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:22:37.039
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:22:37.044
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 07/01/23 12:22:37.047
  Jul  1 12:22:37.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:22:38.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:22:45.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6448" for this suite. @ 07/01/23 12:22:45.023
• [8.030 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 07/01/23 12:22:45.037
  Jul  1 12:22:45.037: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/01/23 12:22:45.038
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:22:45.064
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:22:45.069
  STEP: create the container to handle the HTTPGet hook request. @ 07/01/23 12:22:45.082
  STEP: create the pod with lifecycle hook @ 07/01/23 12:22:47.119
  STEP: check poststart hook @ 07/01/23 12:22:49.159
  STEP: delete the pod with lifecycle hook @ 07/01/23 12:22:49.186
  Jul  1 12:22:51.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3676" for this suite. @ 07/01/23 12:22:51.225
• [6.201 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 07/01/23 12:22:51.238
  Jul  1 12:22:51.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename configmap @ 07/01/23 12:22:51.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:22:51.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:22:51.264
  STEP: Creating configMap with name configmap-test-volume-2ab74745-054a-4bd3-8f41-973d82b8e950 @ 07/01/23 12:22:51.27
  STEP: Creating a pod to test consume configMaps @ 07/01/23 12:22:51.277
  STEP: Saw pod success @ 07/01/23 12:22:55.313
  Jul  1 12:22:55.320: INFO: Trying to get logs from node ip-172-31-12-125 pod pod-configmaps-3f6800d8-1c9f-40c1-a467-2d22dac8884d container configmap-volume-test: <nil>
  STEP: delete the pod @ 07/01/23 12:22:55.371
  Jul  1 12:22:55.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2268" for this suite. @ 07/01/23 12:22:55.408
• [4.184 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 07/01/23 12:22:55.422
  Jul  1 12:22:55.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename var-expansion @ 07/01/23 12:22:55.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:22:55.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:22:55.496
  STEP: Creating a pod to test substitution in container's command @ 07/01/23 12:22:55.504
  STEP: Saw pod success @ 07/01/23 12:22:59.558
  Jul  1 12:22:59.565: INFO: Trying to get logs from node ip-172-31-12-125 pod var-expansion-09555e79-46dc-44e5-acfc-a45056b705fc container dapi-container: <nil>
  STEP: delete the pod @ 07/01/23 12:22:59.577
  Jul  1 12:22:59.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1842" for this suite. @ 07/01/23 12:22:59.609
• [4.197 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 07/01/23 12:22:59.62
  Jul  1 12:22:59.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename events @ 07/01/23 12:22:59.621
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:22:59.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:22:59.654
  STEP: Create set of events @ 07/01/23 12:22:59.66
  Jul  1 12:22:59.669: INFO: created test-event-1
  Jul  1 12:22:59.677: INFO: created test-event-2
  Jul  1 12:22:59.685: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 07/01/23 12:22:59.685
  STEP: delete collection of events @ 07/01/23 12:22:59.69
  Jul  1 12:22:59.691: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 07/01/23 12:22:59.728
  Jul  1 12:22:59.728: INFO: requesting list of events to confirm quantity
  Jul  1 12:22:59.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7814" for this suite. @ 07/01/23 12:22:59.745
• [0.136 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 07/01/23 12:22:59.757
  Jul  1 12:22:59.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename downward-api @ 07/01/23 12:22:59.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:22:59.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:22:59.795
  STEP: Creating a pod to test downward api env vars @ 07/01/23 12:22:59.804
  STEP: Saw pod success @ 07/01/23 12:23:03.848
  Jul  1 12:23:03.854: INFO: Trying to get logs from node ip-172-31-91-66 pod downward-api-0838ad13-6a12-4ad3-9131-7cfdb9ba7129 container dapi-container: <nil>
  STEP: delete the pod @ 07/01/23 12:23:03.867
  Jul  1 12:23:03.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1737" for this suite. @ 07/01/23 12:23:03.902
• [4.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 07/01/23 12:23:03.922
  Jul  1 12:23:03.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename subpath @ 07/01/23 12:23:03.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:23:03.947
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:23:03.952
  STEP: Setting up data @ 07/01/23 12:23:03.958
  STEP: Creating pod pod-subpath-test-configmap-x5hp @ 07/01/23 12:23:03.973
  STEP: Creating a pod to test atomic-volume-subpath @ 07/01/23 12:23:03.973
  STEP: Saw pod success @ 07/01/23 12:23:28.1
  Jul  1 12:23:28.108: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-subpath-test-configmap-x5hp container test-container-subpath-configmap-x5hp: <nil>
  STEP: delete the pod @ 07/01/23 12:23:28.121
  STEP: Deleting pod pod-subpath-test-configmap-x5hp @ 07/01/23 12:23:28.144
  Jul  1 12:23:28.144: INFO: Deleting pod "pod-subpath-test-configmap-x5hp" in namespace "subpath-2337"
  Jul  1 12:23:28.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2337" for this suite. @ 07/01/23 12:23:28.158
• [24.254 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 07/01/23 12:23:28.176
  Jul  1 12:23:28.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename resourcequota @ 07/01/23 12:23:28.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:23:28.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:23:28.212
  STEP: Counting existing ResourceQuota @ 07/01/23 12:23:28.218
  STEP: Creating a ResourceQuota @ 07/01/23 12:23:33.227
  STEP: Ensuring resource quota status is calculated @ 07/01/23 12:23:33.236
  STEP: Creating a ReplicationController @ 07/01/23 12:23:35.242
  STEP: Ensuring resource quota status captures replication controller creation @ 07/01/23 12:23:35.269
  STEP: Deleting a ReplicationController @ 07/01/23 12:23:37.277
  STEP: Ensuring resource quota status released usage @ 07/01/23 12:23:37.286
  Jul  1 12:23:39.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-843" for this suite. @ 07/01/23 12:23:39.303
• [11.137 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 07/01/23 12:23:39.315
  Jul  1 12:23:39.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/01/23 12:23:39.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:23:39.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:23:39.363
  Jul  1 12:23:39.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:23:42.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9206" for this suite. @ 07/01/23 12:23:42.659
• [3.354 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 07/01/23 12:23:42.671
  Jul  1 12:23:42.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 12:23:42.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:23:42.697
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:23:42.708
  STEP: Setting up server cert @ 07/01/23 12:23:42.748
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 12:23:43.538
  STEP: Deploying the webhook pod @ 07/01/23 12:23:43.553
  STEP: Wait for the deployment to be ready @ 07/01/23 12:23:43.598
  Jul  1 12:23:43.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 23, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 23, 43, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-7497495989\""}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 07/01/23 12:23:45.629
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 12:23:45.657
  Jul  1 12:23:46.657: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 07/01/23 12:23:46.766
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/01/23 12:23:46.827
  STEP: Deleting the collection of validation webhooks @ 07/01/23 12:23:46.897
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/01/23 12:23:46.985
  Jul  1 12:23:47.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2017" for this suite. @ 07/01/23 12:23:47.114
  STEP: Destroying namespace "webhook-markers-4137" for this suite. @ 07/01/23 12:23:47.13
• [4.473 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 07/01/23 12:23:47.145
  Jul  1 12:23:47.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename security-context-test @ 07/01/23 12:23:47.146
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:23:47.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:23:47.173
  Jul  1 12:23:51.268: INFO: Got logs for pod "busybox-privileged-false-463c8a4c-a470-4560-983b-db56478ae974": "ip: RTNETLINK answers: Operation not permitted\n"
  Jul  1 12:23:51.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4610" for this suite. @ 07/01/23 12:23:51.274
• [4.138 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 07/01/23 12:23:51.283
  Jul  1 12:23:51.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename watch @ 07/01/23 12:23:51.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:23:51.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:23:51.314
  STEP: creating a watch on configmaps with a certain label @ 07/01/23 12:23:51.32
  STEP: creating a new configmap @ 07/01/23 12:23:51.323
  STEP: modifying the configmap once @ 07/01/23 12:23:51.334
  STEP: changing the label value of the configmap @ 07/01/23 12:23:51.348
  STEP: Expecting to observe a delete notification for the watched object @ 07/01/23 12:23:51.363
  Jul  1 12:23:51.364: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6768  bda45913-b947-414f-843b-c0e371db717f 11895 0 2023-07-01 12:23:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-01 12:23:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 12:23:51.364: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6768  bda45913-b947-414f-843b-c0e371db717f 11896 0 2023-07-01 12:23:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-01 12:23:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 12:23:51.364: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6768  bda45913-b947-414f-843b-c0e371db717f 11897 0 2023-07-01 12:23:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-01 12:23:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 07/01/23 12:23:51.364
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 07/01/23 12:23:51.38
  STEP: changing the label value of the configmap back @ 07/01/23 12:24:01.381
  STEP: modifying the configmap a third time @ 07/01/23 12:24:01.396
  STEP: deleting the configmap @ 07/01/23 12:24:01.41
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 07/01/23 12:24:01.422
  Jul  1 12:24:01.422: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6768  bda45913-b947-414f-843b-c0e371db717f 11948 0 2023-07-01 12:23:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-01 12:24:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 12:24:01.422: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6768  bda45913-b947-414f-843b-c0e371db717f 11949 0 2023-07-01 12:23:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-01 12:24:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 12:24:01.422: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6768  bda45913-b947-414f-843b-c0e371db717f 11950 0 2023-07-01 12:23:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-01 12:24:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 12:24:01.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-6768" for this suite. @ 07/01/23 12:24:01.433
• [10.160 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 07/01/23 12:24:01.444
  Jul  1 12:24:01.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename downward-api @ 07/01/23 12:24:01.445
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:24:01.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:24:01.479
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 12:24:01.484
  STEP: Saw pod success @ 07/01/23 12:24:05.521
  Jul  1 12:24:05.528: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-f9ee765e-75f4-4ac7-b294-f0d9dc451bc3 container client-container: <nil>
  STEP: delete the pod @ 07/01/23 12:24:05.539
  Jul  1 12:24:05.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2000" for this suite. @ 07/01/23 12:24:05.568
• [4.133 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 07/01/23 12:24:05.582
  Jul  1 12:24:05.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 12:24:05.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:24:05.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:24:05.626
  STEP: Setting up server cert @ 07/01/23 12:24:05.68
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 12:24:06.233
  STEP: Deploying the webhook pod @ 07/01/23 12:24:06.241
  STEP: Wait for the deployment to be ready @ 07/01/23 12:24:06.268
  Jul  1 12:24:06.289: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/01/23 12:24:08.307
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 12:24:08.323
  Jul  1 12:24:09.323: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 07/01/23 12:24:09.331
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 07/01/23 12:24:09.362
  STEP: Creating a configMap that should not be mutated @ 07/01/23 12:24:09.375
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 07/01/23 12:24:09.397
  STEP: Creating a configMap that should be mutated @ 07/01/23 12:24:09.436
  Jul  1 12:24:09.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9810" for this suite. @ 07/01/23 12:24:09.581
  STEP: Destroying namespace "webhook-markers-1915" for this suite. @ 07/01/23 12:24:09.592
• [4.044 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 07/01/23 12:24:09.622
  Jul  1 12:24:09.622: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 12:24:09.623
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:24:09.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:24:09.668
  STEP: Setting up server cert @ 07/01/23 12:24:09.717
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 12:24:10.336
  STEP: Deploying the webhook pod @ 07/01/23 12:24:10.345
  STEP: Wait for the deployment to be ready @ 07/01/23 12:24:10.366
  Jul  1 12:24:10.386: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Jul  1 12:24:12.405: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 1, 12, 24, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 24, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 12, 24, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 12, 24, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 07/01/23 12:24:14.412
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 12:24:14.43
  Jul  1 12:24:15.432: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul  1 12:24:15.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4570-crds.webhook.example.com via the AdmissionRegistration API @ 07/01/23 12:24:15.953
  STEP: Creating a custom resource that should be mutated by the webhook @ 07/01/23 12:24:15.98
  Jul  1 12:24:18.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9816" for this suite. @ 07/01/23 12:24:18.663
  STEP: Destroying namespace "webhook-markers-8426" for this suite. @ 07/01/23 12:24:18.677
• [9.071 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 07/01/23 12:24:18.693
  Jul  1 12:24:18.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 12:24:18.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:24:18.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:24:18.73
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 12:24:18.742
  STEP: Saw pod success @ 07/01/23 12:24:22.783
  Jul  1 12:24:22.789: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-8ca76eb5-d8dd-44be-b6ae-c6af8ff39661 container client-container: <nil>
  STEP: delete the pod @ 07/01/23 12:24:22.801
  Jul  1 12:24:22.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4006" for this suite. @ 07/01/23 12:24:22.832
• [4.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 07/01/23 12:24:22.846
  Jul  1 12:24:22.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename namespaces @ 07/01/23 12:24:22.847
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:24:22.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:24:22.875
  STEP: Read namespace status @ 07/01/23 12:24:22.882
  Jul  1 12:24:22.888: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 07/01/23 12:24:22.888
  Jul  1 12:24:22.897: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 07/01/23 12:24:22.897
  Jul  1 12:24:22.915: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Jul  1 12:24:22.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9970" for this suite. @ 07/01/23 12:24:22.922
• [0.086 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 07/01/23 12:24:22.933
  Jul  1 12:24:22.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename configmap @ 07/01/23 12:24:22.934
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:24:22.956
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:24:22.962
  STEP: Creating configMap with name configmap-test-volume-map-478fdc0f-ec0d-4f04-a182-0b758ee71a0b @ 07/01/23 12:24:22.97
  STEP: Creating a pod to test consume configMaps @ 07/01/23 12:24:22.977
  STEP: Saw pod success @ 07/01/23 12:24:27.013
  Jul  1 12:24:27.018: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-configmaps-2b0dfc72-5ce7-4337-bb0e-881353726306 container agnhost-container: <nil>
  STEP: delete the pod @ 07/01/23 12:24:27.03
  Jul  1 12:24:27.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2601" for this suite. @ 07/01/23 12:24:27.065
• [4.143 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 07/01/23 12:24:27.077
  Jul  1 12:24:27.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/01/23 12:24:27.078
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:24:27.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:24:27.11
  STEP: create the container to handle the HTTPGet hook request. @ 07/01/23 12:24:27.123
  STEP: create the pod with lifecycle hook @ 07/01/23 12:24:29.159
  STEP: delete the pod with lifecycle hook @ 07/01/23 12:24:31.187
  STEP: check prestop hook @ 07/01/23 12:24:33.219
  Jul  1 12:24:33.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-254" for this suite. @ 07/01/23 12:24:33.253
• [6.189 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 07/01/23 12:24:33.266
  Jul  1 12:24:33.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename downward-api @ 07/01/23 12:24:33.268
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:24:33.296
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:24:33.3
  STEP: Creating the pod @ 07/01/23 12:24:33.31
  Jul  1 12:24:35.881: INFO: Successfully updated pod "annotationupdate8da63b44-ed1c-4d1c-b189-70e2c605a3de"
  Jul  1 12:24:37.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4837" for this suite. @ 07/01/23 12:24:37.914
• [4.667 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 07/01/23 12:24:37.933
  Jul  1 12:24:37.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename containers @ 07/01/23 12:24:37.935
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:24:37.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:24:37.991
  STEP: Creating a pod to test override command @ 07/01/23 12:24:37.997
  STEP: Saw pod success @ 07/01/23 12:24:42.042
  Jul  1 12:24:42.049: INFO: Trying to get logs from node ip-172-31-91-66 pod client-containers-ce28312d-10e3-4f04-b73b-58198105c321 container agnhost-container: <nil>
  STEP: delete the pod @ 07/01/23 12:24:42.063
  Jul  1 12:24:42.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7377" for this suite. @ 07/01/23 12:24:42.094
• [4.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 07/01/23 12:24:42.107
  Jul  1 12:24:42.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl @ 07/01/23 12:24:42.108
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:24:42.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:24:42.139
  STEP: creating the pod @ 07/01/23 12:24:42.152
  Jul  1 12:24:42.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-9071 create -f -'
  Jul  1 12:24:43.703: INFO: stderr: ""
  Jul  1 12:24:43.703: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 07/01/23 12:24:45.717
  Jul  1 12:24:45.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-9071 label pods pause testing-label=testing-label-value'
  Jul  1 12:24:45.891: INFO: stderr: ""
  Jul  1 12:24:45.891: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 07/01/23 12:24:45.891
  Jul  1 12:24:45.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-9071 get pod pause -L testing-label'
  Jul  1 12:24:46.009: INFO: stderr: ""
  Jul  1 12:24:46.009: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 07/01/23 12:24:46.009
  Jul  1 12:24:46.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-9071 label pods pause testing-label-'
  Jul  1 12:24:46.140: INFO: stderr: ""
  Jul  1 12:24:46.140: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 07/01/23 12:24:46.14
  Jul  1 12:24:46.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-9071 get pod pause -L testing-label'
  Jul  1 12:24:46.286: INFO: stderr: ""
  Jul  1 12:24:46.286: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 07/01/23 12:24:46.286
  Jul  1 12:24:46.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-9071 delete --grace-period=0 --force -f -'
  Jul  1 12:24:46.422: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  1 12:24:46.422: INFO: stdout: "pod \"pause\" force deleted\n"
  Jul  1 12:24:46.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-9071 get rc,svc -l name=pause --no-headers'
  Jul  1 12:24:46.592: INFO: stderr: "No resources found in kubectl-9071 namespace.\n"
  Jul  1 12:24:46.592: INFO: stdout: ""
  Jul  1 12:24:46.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-9071 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul  1 12:24:46.737: INFO: stderr: ""
  Jul  1 12:24:46.737: INFO: stdout: ""
  Jul  1 12:24:46.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9071" for this suite. @ 07/01/23 12:24:46.745
• [4.652 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 07/01/23 12:24:46.759
  Jul  1 12:24:46.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename subpath @ 07/01/23 12:24:46.76
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:24:46.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:24:46.795
  STEP: Setting up data @ 07/01/23 12:24:46.799
  STEP: Creating pod pod-subpath-test-projected-pt5l @ 07/01/23 12:24:46.812
  STEP: Creating a pod to test atomic-volume-subpath @ 07/01/23 12:24:46.812
  STEP: Saw pod success @ 07/01/23 12:25:10.928
  Jul  1 12:25:10.933: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-subpath-test-projected-pt5l container test-container-subpath-projected-pt5l: <nil>
  STEP: delete the pod @ 07/01/23 12:25:10.945
  STEP: Deleting pod pod-subpath-test-projected-pt5l @ 07/01/23 12:25:10.979
  Jul  1 12:25:10.979: INFO: Deleting pod "pod-subpath-test-projected-pt5l" in namespace "subpath-178"
  Jul  1 12:25:10.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-178" for this suite. @ 07/01/23 12:25:10.995
• [24.250 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 07/01/23 12:25:11.01
  Jul  1 12:25:11.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename replication-controller @ 07/01/23 12:25:11.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:25:11.043
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:25:11.051
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 07/01/23 12:25:11.056
  STEP: When a replication controller with a matching selector is created @ 07/01/23 12:25:13.093
  STEP: Then the orphan pod is adopted @ 07/01/23 12:25:13.105
  Jul  1 12:25:14.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2217" for this suite. @ 07/01/23 12:25:14.135
• [3.136 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 07/01/23 12:25:14.147
  Jul  1 12:25:14.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename resourcequota @ 07/01/23 12:25:14.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:25:14.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:25:14.177
  STEP: Creating a ResourceQuota @ 07/01/23 12:25:14.19
  STEP: Getting a ResourceQuota @ 07/01/23 12:25:14.201
  STEP: Updating a ResourceQuota @ 07/01/23 12:25:14.207
  STEP: Verifying a ResourceQuota was modified @ 07/01/23 12:25:14.216
  STEP: Deleting a ResourceQuota @ 07/01/23 12:25:14.222
  STEP: Verifying the deleted ResourceQuota @ 07/01/23 12:25:14.235
  Jul  1 12:25:14.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9379" for this suite. @ 07/01/23 12:25:14.249
• [0.114 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 07/01/23 12:25:14.262
  Jul  1 12:25:14.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 12:25:14.263
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:25:14.29
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:25:14.298
  STEP: Creating projection with secret that has name projected-secret-test-map-ce95d720-8189-47ca-a5a5-d0f05f6d9c7c @ 07/01/23 12:25:14.304
  STEP: Creating a pod to test consume secrets @ 07/01/23 12:25:14.31
  STEP: Saw pod success @ 07/01/23 12:25:18.351
  Jul  1 12:25:18.358: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-projected-secrets-a9efbea8-501b-4d2c-b3cc-b29b3c2cda60 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/01/23 12:25:18.37
  Jul  1 12:25:18.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5616" for this suite. @ 07/01/23 12:25:18.4
• [4.153 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 07/01/23 12:25:18.416
  Jul  1 12:25:18.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename gc @ 07/01/23 12:25:18.417
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:25:18.446
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:25:18.456
  STEP: create the deployment @ 07/01/23 12:25:18.464
  W0701 12:25:18.474754      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 07/01/23 12:25:18.475
  STEP: delete the deployment @ 07/01/23 12:25:18.484
  STEP: wait for all rs to be garbage collected @ 07/01/23 12:25:18.499
  STEP: expected 0 rs, got 1 rs @ 07/01/23 12:25:18.531
  STEP: expected 0 pods, got 2 pods @ 07/01/23 12:25:18.551
  STEP: Gathering metrics @ 07/01/23 12:25:19.07
  W0701 12:25:19.079638      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul  1 12:25:19.079: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul  1 12:25:19.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2682" for this suite. @ 07/01/23 12:25:19.085
• [0.681 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 07/01/23 12:25:19.098
  Jul  1 12:25:19.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename services @ 07/01/23 12:25:19.099
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:25:19.123
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:25:19.129
  STEP: creating service in namespace services-4199 @ 07/01/23 12:25:19.136
  STEP: creating service affinity-clusterip-transition in namespace services-4199 @ 07/01/23 12:25:19.136
  STEP: creating replication controller affinity-clusterip-transition in namespace services-4199 @ 07/01/23 12:25:19.155
  I0701 12:25:19.188079      19 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-4199, replica count: 3
  I0701 12:25:22.239489      19 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  1 12:25:22.253: INFO: Creating new exec pod
  Jul  1 12:25:25.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-4199 exec execpod-affinityr7gcc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Jul  1 12:25:25.465: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Jul  1 12:25:25.465: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 12:25:25.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-4199 exec execpod-affinityr7gcc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.227 80'
  Jul  1 12:25:25.641: INFO: stderr: "+ nc -v -t -w 2 10.152.183.227 80\n+ echo hostName\nConnection to 10.152.183.227 80 port [tcp/http] succeeded!\n"
  Jul  1 12:25:25.641: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 12:25:25.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-4199 exec execpod-affinityr7gcc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.227:80/ ; done'
  Jul  1 12:25:26.010: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n"
  Jul  1 12:25:26.010: INFO: stdout: "\naffinity-clusterip-transition-jf64x\naffinity-clusterip-transition-c6b2r\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-c6b2r\naffinity-clusterip-transition-jf64x\naffinity-clusterip-transition-c6b2r\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-c6b2r\naffinity-clusterip-transition-jf64x\naffinity-clusterip-transition-jf64x\naffinity-clusterip-transition-jf64x\naffinity-clusterip-transition-jf64x\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-c6b2r\naffinity-clusterip-transition-jf64x\naffinity-clusterip-transition-jf64x"
  Jul  1 12:25:26.010: INFO: Received response from host: affinity-clusterip-transition-jf64x
  Jul  1 12:25:26.010: INFO: Received response from host: affinity-clusterip-transition-c6b2r
  Jul  1 12:25:26.010: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.010: INFO: Received response from host: affinity-clusterip-transition-c6b2r
  Jul  1 12:25:26.010: INFO: Received response from host: affinity-clusterip-transition-jf64x
  Jul  1 12:25:26.010: INFO: Received response from host: affinity-clusterip-transition-c6b2r
  Jul  1 12:25:26.010: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.010: INFO: Received response from host: affinity-clusterip-transition-c6b2r
  Jul  1 12:25:26.010: INFO: Received response from host: affinity-clusterip-transition-jf64x
  Jul  1 12:25:26.010: INFO: Received response from host: affinity-clusterip-transition-jf64x
  Jul  1 12:25:26.010: INFO: Received response from host: affinity-clusterip-transition-jf64x
  Jul  1 12:25:26.010: INFO: Received response from host: affinity-clusterip-transition-jf64x
  Jul  1 12:25:26.010: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.010: INFO: Received response from host: affinity-clusterip-transition-c6b2r
  Jul  1 12:25:26.010: INFO: Received response from host: affinity-clusterip-transition-jf64x
  Jul  1 12:25:26.010: INFO: Received response from host: affinity-clusterip-transition-jf64x
  Jul  1 12:25:26.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-4199 exec execpod-affinityr7gcc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.227:80/ ; done'
  Jul  1 12:25:26.285: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.227:80/\n"
  Jul  1 12:25:26.285: INFO: stdout: "\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-8g2g8\naffinity-clusterip-transition-8g2g8"
  Jul  1 12:25:26.285: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.285: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.285: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.285: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.285: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.285: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.285: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.285: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.285: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.285: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.285: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.285: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.285: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.285: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.285: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.285: INFO: Received response from host: affinity-clusterip-transition-8g2g8
  Jul  1 12:25:26.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  1 12:25:26.291: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4199, will wait for the garbage collector to delete the pods @ 07/01/23 12:25:26.319
  Jul  1 12:25:26.393: INFO: Deleting ReplicationController affinity-clusterip-transition took: 12.060598ms
  Jul  1 12:25:26.494: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.045396ms
  STEP: Destroying namespace "services-4199" for this suite. @ 07/01/23 12:25:29.022
• [9.934 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 07/01/23 12:25:29.033
  Jul  1 12:25:29.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename sched-pred @ 07/01/23 12:25:29.036
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:25:29.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:25:29.072
  Jul  1 12:25:29.078: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul  1 12:25:29.092: INFO: Waiting for terminating namespaces to be deleted...
  Jul  1 12:25:29.111: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-12-125 before test
  Jul  1 12:25:29.123: INFO: nginx-ingress-controller-kubernetes-worker-tjncz from ingress-nginx-kubernetes-worker started at 2023-07-01 11:52:18 +0000 UTC (1 container statuses recorded)
  Jul  1 12:25:29.123: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  1 12:25:29.123: INFO: calico-kube-controllers-9c5cff4fb-6hfws from kube-system started at 2023-07-01 11:52:25 +0000 UTC (1 container statuses recorded)
  Jul  1 12:25:29.123: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul  1 12:25:29.123: INFO: sonobuoy-e2e-job-e4b68f70fcf04452 from sonobuoy started at 2023-07-01 11:57:54 +0000 UTC (2 container statuses recorded)
  Jul  1 12:25:29.123: INFO: 	Container e2e ready: true, restart count 0
  Jul  1 12:25:29.123: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  1 12:25:29.123: INFO: sonobuoy-systemd-logs-daemon-set-0aed011ee39b4f14-g2zvl from sonobuoy started at 2023-07-01 11:57:54 +0000 UTC (2 container statuses recorded)
  Jul  1 12:25:29.123: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  1 12:25:29.123: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul  1 12:25:29.123: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-16-94 before test
  Jul  1 12:25:29.132: INFO: nginx-ingress-controller-kubernetes-worker-ps7xr from ingress-nginx-kubernetes-worker started at 2023-07-01 11:52:10 +0000 UTC (1 container statuses recorded)
  Jul  1 12:25:29.132: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  1 12:25:29.132: INFO: coredns-5c7f76ccb8-zxmnn from kube-system started at 2023-07-01 11:52:04 +0000 UTC (1 container statuses recorded)
  Jul  1 12:25:29.132: INFO: 	Container coredns ready: true, restart count 0
  Jul  1 12:25:29.132: INFO: kube-state-metrics-5b95b4459c-5klzn from kube-system started at 2023-07-01 11:52:04 +0000 UTC (1 container statuses recorded)
  Jul  1 12:25:29.132: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul  1 12:25:29.132: INFO: metrics-server-v0.5.2-6cf8c8b69c-lw48q from kube-system started at 2023-07-01 11:52:04 +0000 UTC (2 container statuses recorded)
  Jul  1 12:25:29.132: INFO: 	Container metrics-server ready: true, restart count 0
  Jul  1 12:25:29.132: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul  1 12:25:29.132: INFO: dashboard-metrics-scraper-6b8586b5c9-8wfzw from kubernetes-dashboard started at 2023-07-01 11:52:04 +0000 UTC (1 container statuses recorded)
  Jul  1 12:25:29.132: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul  1 12:25:29.132: INFO: kubernetes-dashboard-6869f4cd5f-f7c25 from kubernetes-dashboard started at 2023-07-01 11:52:04 +0000 UTC (1 container statuses recorded)
  Jul  1 12:25:29.132: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul  1 12:25:29.133: INFO: sonobuoy-systemd-logs-daemon-set-0aed011ee39b4f14-bbwnl from sonobuoy started at 2023-07-01 11:57:54 +0000 UTC (2 container statuses recorded)
  Jul  1 12:25:29.133: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  1 12:25:29.133: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul  1 12:25:29.133: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-91-66 before test
  Jul  1 12:25:29.141: INFO: default-http-backend-kubernetes-worker-65fc475d49-7llc8 from ingress-nginx-kubernetes-worker started at 2023-07-01 11:52:10 +0000 UTC (1 container statuses recorded)
  Jul  1 12:25:29.141: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul  1 12:25:29.141: INFO: nginx-ingress-controller-kubernetes-worker-rs6r4 from ingress-nginx-kubernetes-worker started at 2023-07-01 11:52:14 +0000 UTC (1 container statuses recorded)
  Jul  1 12:25:29.141: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  1 12:25:29.141: INFO: sonobuoy from sonobuoy started at 2023-07-01 11:57:50 +0000 UTC (1 container statuses recorded)
  Jul  1 12:25:29.141: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul  1 12:25:29.141: INFO: sonobuoy-systemd-logs-daemon-set-0aed011ee39b4f14-g92dx from sonobuoy started at 2023-07-01 11:57:54 +0000 UTC (2 container statuses recorded)
  Jul  1 12:25:29.141: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  1 12:25:29.141: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 07/01/23 12:25:29.141
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.176dbdfce3555c0c], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] @ 07/01/23 12:25:29.184
  Jul  1 12:25:30.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-6908" for this suite. @ 07/01/23 12:25:30.194
• [1.173 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 07/01/23 12:25:30.206
  Jul  1 12:25:30.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir @ 07/01/23 12:25:30.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:25:30.245
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:25:30.25
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 07/01/23 12:25:30.255
  STEP: Saw pod success @ 07/01/23 12:25:34.29
  Jul  1 12:25:34.295: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-20d548c8-605f-4ebc-a449-7baf34b4664d container test-container: <nil>
  STEP: delete the pod @ 07/01/23 12:25:34.319
  Jul  1 12:25:34.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9406" for this suite. @ 07/01/23 12:25:34.363
• [4.179 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 07/01/23 12:25:34.386
  Jul  1 12:25:34.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 12:25:34.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:25:34.42
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:25:34.448
  STEP: Setting up server cert @ 07/01/23 12:25:34.514
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 12:25:35.105
  STEP: Deploying the webhook pod @ 07/01/23 12:25:35.118
  STEP: Wait for the deployment to be ready @ 07/01/23 12:25:35.135
  Jul  1 12:25:35.153: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/01/23 12:25:37.175
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 12:25:37.195
  Jul  1 12:25:38.196: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 07/01/23 12:25:38.203
  STEP: create a namespace for the webhook @ 07/01/23 12:25:38.226
  STEP: create a configmap should be unconditionally rejected by the webhook @ 07/01/23 12:25:38.248
  Jul  1 12:25:38.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3194" for this suite. @ 07/01/23 12:25:38.446
  STEP: Destroying namespace "webhook-markers-6442" for this suite. @ 07/01/23 12:25:38.474
  STEP: Destroying namespace "fail-closed-namespace-3061" for this suite. @ 07/01/23 12:25:38.486
• [4.112 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 07/01/23 12:25:38.5
  Jul  1 12:25:38.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename resourcequota @ 07/01/23 12:25:38.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:25:38.531
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:25:38.546
  STEP: Counting existing ResourceQuota @ 07/01/23 12:25:55.562
  STEP: Creating a ResourceQuota @ 07/01/23 12:26:00.568
  STEP: Ensuring resource quota status is calculated @ 07/01/23 12:26:00.579
  STEP: Creating a ConfigMap @ 07/01/23 12:26:02.586
  STEP: Ensuring resource quota status captures configMap creation @ 07/01/23 12:26:02.603
  STEP: Deleting a ConfigMap @ 07/01/23 12:26:04.61
  STEP: Ensuring resource quota status released usage @ 07/01/23 12:26:04.619
  Jul  1 12:26:06.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1903" for this suite. @ 07/01/23 12:26:06.634
• [28.143 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 07/01/23 12:26:06.644
  Jul  1 12:26:06.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename statefulset @ 07/01/23 12:26:06.645
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:26:06.668
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:26:06.673
  STEP: Creating service test in namespace statefulset-2779 @ 07/01/23 12:26:06.682
  STEP: Creating a new StatefulSet @ 07/01/23 12:26:06.691
  Jul  1 12:26:06.715: INFO: Found 0 stateful pods, waiting for 3
  Jul  1 12:26:16.724: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul  1 12:26:16.724: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul  1 12:26:16.724: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Jul  1 12:26:16.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-2779 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  1 12:26:16.982: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  1 12:26:16.982: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  1 12:26:16.982: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 07/01/23 12:26:27.018
  Jul  1 12:26:27.049: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 07/01/23 12:26:27.049
  STEP: Updating Pods in reverse ordinal order @ 07/01/23 12:26:37.106
  Jul  1 12:26:37.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-2779 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  1 12:26:37.294: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul  1 12:26:37.294: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  1 12:26:37.294: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul  1 12:26:37.324: INFO: Waiting for StatefulSet statefulset-2779/ss2 to complete update
  Jul  1 12:26:37.324: INFO: Waiting for Pod statefulset-2779/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jul  1 12:26:37.324: INFO: Waiting for Pod statefulset-2779/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jul  1 12:26:37.324: INFO: Waiting for Pod statefulset-2779/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jul  1 12:26:47.344: INFO: Waiting for StatefulSet statefulset-2779/ss2 to complete update
  Jul  1 12:26:47.344: INFO: Waiting for Pod statefulset-2779/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Rolling back to a previous revision @ 07/01/23 12:26:57.337
  Jul  1 12:26:57.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-2779 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  1 12:26:57.597: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  1 12:26:57.597: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  1 12:26:57.597: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  1 12:27:07.654: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 07/01/23 12:27:17.682
  Jul  1 12:27:17.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-2779 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  1 12:27:17.903: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul  1 12:27:17.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  1 12:27:17.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul  1 12:27:27.942: INFO: Waiting for StatefulSet statefulset-2779/ss2 to complete update
  Jul  1 12:27:27.942: INFO: Waiting for Pod statefulset-2779/ss2-0 to have revision ss2-7b6c9599d5 update revision ss2-5459d8585b
  Jul  1 12:27:37.965: INFO: Deleting all statefulset in ns statefulset-2779
  Jul  1 12:27:37.973: INFO: Scaling statefulset ss2 to 0
  Jul  1 12:27:48.001: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  1 12:27:48.006: INFO: Deleting statefulset ss2
  Jul  1 12:27:48.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2779" for this suite. @ 07/01/23 12:27:48.046
• [101.421 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 07/01/23 12:27:48.066
  Jul  1 12:27:48.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename resourcequota @ 07/01/23 12:27:48.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:27:48.109
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:27:48.115
  STEP: Creating a ResourceQuota with best effort scope @ 07/01/23 12:27:48.121
  STEP: Ensuring ResourceQuota status is calculated @ 07/01/23 12:27:48.13
  STEP: Creating a ResourceQuota with not best effort scope @ 07/01/23 12:27:50.137
  STEP: Ensuring ResourceQuota status is calculated @ 07/01/23 12:27:50.147
  STEP: Creating a best-effort pod @ 07/01/23 12:27:52.155
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 07/01/23 12:27:52.183
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 07/01/23 12:27:54.195
  STEP: Deleting the pod @ 07/01/23 12:27:56.2
  STEP: Ensuring resource quota status released the pod usage @ 07/01/23 12:27:56.231
  STEP: Creating a not best-effort pod @ 07/01/23 12:27:58.239
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 07/01/23 12:27:58.26
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 07/01/23 12:28:00.27
  STEP: Deleting the pod @ 07/01/23 12:28:02.277
  STEP: Ensuring resource quota status released the pod usage @ 07/01/23 12:28:02.298
  Jul  1 12:28:04.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7452" for this suite. @ 07/01/23 12:28:04.314
• [16.260 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 07/01/23 12:28:04.327
  Jul  1 12:28:04.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename namespaces @ 07/01/23 12:28:04.328
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:28:04.372
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:28:04.384
  STEP: Updating Namespace "namespaces-9000" @ 07/01/23 12:28:04.394
  Jul  1 12:28:04.407: INFO: Namespace "namespaces-9000" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"a65dfce2-a914-45b4-bc26-49557909761b", "kubernetes.io/metadata.name":"namespaces-9000", "namespaces-9000":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Jul  1 12:28:04.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9000" for this suite. @ 07/01/23 12:28:04.414
• [0.097 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 07/01/23 12:28:04.424
  Jul  1 12:28:04.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename configmap @ 07/01/23 12:28:04.426
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:28:04.449
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:28:04.456
  STEP: Creating configMap with name configmap-test-volume-6760caf5-caba-4fea-b73d-971acf499e67 @ 07/01/23 12:28:04.463
  STEP: Creating a pod to test consume configMaps @ 07/01/23 12:28:04.472
  STEP: Saw pod success @ 07/01/23 12:28:08.516
  Jul  1 12:28:08.532: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-configmaps-92759d3f-5439-41fa-9dc7-95c60ea00178 container agnhost-container: <nil>
  STEP: delete the pod @ 07/01/23 12:28:08.563
  Jul  1 12:28:08.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-16" for this suite. @ 07/01/23 12:28:08.63
• [4.218 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 07/01/23 12:28:08.644
  Jul  1 12:28:08.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename deployment @ 07/01/23 12:28:08.645
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:28:08.675
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:28:08.685
  Jul  1 12:28:08.696: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Jul  1 12:28:08.711: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul  1 12:28:13.719: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/01/23 12:28:13.719
  Jul  1 12:28:13.719: INFO: Creating deployment "test-rolling-update-deployment"
  Jul  1 12:28:13.727: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Jul  1 12:28:13.735: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  Jul  1 12:28:15.750: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Jul  1 12:28:15.758: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Jul  1 12:28:15.781: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6887  c597fd86-df60-49f5-b9ed-7accd9b25807 13986 1 2023-07-01 12:28:13 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-07-01 12:28:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 12:28:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038154c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-01 12:28:13 +0000 UTC,LastTransitionTime:2023-07-01 12:28:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-07-01 12:28:15 +0000 UTC,LastTransitionTime:2023-07-01 12:28:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul  1 12:28:15.787: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-6887  6fb6d3e3-bfd4-4d08-ab3f-c8512e19c6e3 13975 1 2023-07-01 12:28:13 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment c597fd86-df60-49f5-b9ed-7accd9b25807 0xc001f452d7 0xc001f452d8}] [] [{kube-controller-manager Update apps/v1 2023-07-01 12:28:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c597fd86-df60-49f5-b9ed-7accd9b25807\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 12:28:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001f45388 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul  1 12:28:15.787: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Jul  1 12:28:15.788: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6887  50240697-828d-401b-8ab7-c63d96e278b5 13984 2 2023-07-01 12:28:08 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment c597fd86-df60-49f5-b9ed-7accd9b25807 0xc001f451a7 0xc001f451a8}] [] [{e2e.test Update apps/v1 2023-07-01 12:28:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 12:28:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c597fd86-df60-49f5-b9ed-7accd9b25807\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-01 12:28:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001f45268 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul  1 12:28:15.795: INFO: Pod "test-rolling-update-deployment-656d657cd8-przzl" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-przzl test-rolling-update-deployment-656d657cd8- deployment-6887  7b5abac8-d079-483f-8ed9-ce9c87d0f12c 13974 0 2023-07-01 12:28:13 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 6fb6d3e3-bfd4-4d08-ab3f-c8512e19c6e3 0xc001f457d7 0xc001f457d8}] [] [{kube-controller-manager Update v1 2023-07-01 12:28:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fb6d3e3-bfd4-4d08-ab3f-c8512e19c6e3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:28:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.62.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r7stj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r7stj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:28:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:28:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:28:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:28:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:192.168.62.108,StartTime:2023-07-01 12:28:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-01 12:28:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://48301daa7df579258d4ea06427c9e9e4f91a6873596f3067b75b46ec82ab878e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.62.108,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:28:15.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6887" for this suite. @ 07/01/23 12:28:15.803
• [7.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 07/01/23 12:28:15.819
  Jul  1 12:28:15.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 12:28:15.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:28:15.847
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:28:15.859
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 12:28:15.865
  STEP: Saw pod success @ 07/01/23 12:28:19.911
  Jul  1 12:28:19.915: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-2942ca83-5b83-43fb-8d34-5918ab726546 container client-container: <nil>
  STEP: delete the pod @ 07/01/23 12:28:19.93
  Jul  1 12:28:19.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3116" for this suite. @ 07/01/23 12:28:19.959
• [4.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 07/01/23 12:28:19.972
  Jul  1 12:28:19.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename sched-preemption @ 07/01/23 12:28:19.972
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:28:20.002
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:28:20.022
  Jul  1 12:28:20.063: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul  1 12:29:20.095: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 07/01/23 12:29:20.101
  Jul  1 12:29:20.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename sched-preemption-path @ 07/01/23 12:29:20.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:29:20.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:29:20.165
  STEP: Finding an available node @ 07/01/23 12:29:20.18
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/01/23 12:29:20.182
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/01/23 12:29:22.232
  Jul  1 12:29:22.250: INFO: found a healthy node: ip-172-31-91-66
  Jul  1 12:29:30.375: INFO: pods created so far: [1 1 1]
  Jul  1 12:29:30.375: INFO: length of pods created so far: 3
  Jul  1 12:29:34.394: INFO: pods created so far: [2 2 1]
  Jul  1 12:29:41.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  1 12:29:41.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-8451" for this suite. @ 07/01/23 12:29:41.524
  STEP: Destroying namespace "sched-preemption-277" for this suite. @ 07/01/23 12:29:41.538
• [81.577 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 07/01/23 12:29:41.549
  Jul  1 12:29:41.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename secrets @ 07/01/23 12:29:41.55
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:29:41.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:29:41.588
  STEP: Creating projection with secret that has name secret-emptykey-test-6283f0d9-fcf0-4064-b6b2-f56131e60e3e @ 07/01/23 12:29:41.592
  Jul  1 12:29:41.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4920" for this suite. @ 07/01/23 12:29:41.607
• [0.067 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 07/01/23 12:29:41.616
  Jul  1 12:29:41.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename sched-preemption @ 07/01/23 12:29:41.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:29:41.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:29:41.654
  Jul  1 12:29:41.686: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul  1 12:30:41.717: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 07/01/23 12:30:41.723
  Jul  1 12:30:41.762: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jul  1 12:30:41.774: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jul  1 12:30:41.803: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jul  1 12:30:41.816: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jul  1 12:30:41.847: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jul  1 12:30:41.864: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 07/01/23 12:30:41.865
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 07/01/23 12:30:45.932
  Jul  1 12:30:52.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-676" for this suite. @ 07/01/23 12:30:52.166
• [70.562 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 07/01/23 12:30:52.179
  Jul  1 12:30:52.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 12:30:52.18
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:30:52.204
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:30:52.211
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 12:30:52.218
  STEP: Saw pod success @ 07/01/23 12:30:56.276
  Jul  1 12:30:56.281: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-c0150dc2-267d-44c1-abfa-5ef900fbc502 container client-container: <nil>
  STEP: delete the pod @ 07/01/23 12:30:56.302
  Jul  1 12:30:56.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4205" for this suite. @ 07/01/23 12:30:56.339
• [4.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 07/01/23 12:30:56.355
  Jul  1 12:30:56.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename statefulset @ 07/01/23 12:30:56.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:30:56.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:30:56.394
  STEP: Creating service test in namespace statefulset-347 @ 07/01/23 12:30:56.406
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 07/01/23 12:30:56.422
  STEP: Creating stateful set ss in namespace statefulset-347 @ 07/01/23 12:30:56.428
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-347 @ 07/01/23 12:30:56.441
  Jul  1 12:30:56.449: INFO: Found 0 stateful pods, waiting for 1
  Jul  1 12:31:06.457: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 07/01/23 12:31:06.457
  Jul  1 12:31:06.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-347 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  1 12:31:06.898: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  1 12:31:06.898: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  1 12:31:06.898: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  1 12:31:06.906: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Jul  1 12:31:16.914: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul  1 12:31:16.914: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  1 12:31:16.945: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999642s
  Jul  1 12:31:17.951: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993797045s
  Jul  1 12:31:18.960: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987913823s
  Jul  1 12:31:19.967: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.979405805s
  Jul  1 12:31:20.975: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.971492354s
  Jul  1 12:31:21.985: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.964543608s
  Jul  1 12:31:22.992: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.954594204s
  Jul  1 12:31:24.018: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.947539282s
  Jul  1 12:31:25.023: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.921689227s
  Jul  1 12:31:26.031: INFO: Verifying statefulset ss doesn't scale past 1 for another 915.508762ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-347 @ 07/01/23 12:31:27.032
  Jul  1 12:31:27.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-347 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  1 12:31:27.315: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul  1 12:31:27.315: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  1 12:31:27.315: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul  1 12:31:27.322: INFO: Found 1 stateful pods, waiting for 3
  Jul  1 12:31:37.331: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul  1 12:31:37.331: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul  1 12:31:37.331: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 07/01/23 12:31:37.331
  STEP: Scale down will halt with unhealthy stateful pod @ 07/01/23 12:31:37.331
  Jul  1 12:31:37.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-347 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  1 12:31:37.656: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  1 12:31:37.656: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  1 12:31:37.656: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  1 12:31:37.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-347 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  1 12:31:38.004: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  1 12:31:38.004: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  1 12:31:38.004: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  1 12:31:38.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-347 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  1 12:31:38.241: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  1 12:31:38.241: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  1 12:31:38.241: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  1 12:31:38.241: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  1 12:31:38.253: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
  Jul  1 12:31:48.270: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul  1 12:31:48.270: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jul  1 12:31:48.270: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jul  1 12:31:48.295: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999622s
  Jul  1 12:31:49.305: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991537737s
  Jul  1 12:31:50.313: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980931144s
  Jul  1 12:31:51.323: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.973415792s
  Jul  1 12:31:52.331: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.96347358s
  Jul  1 12:31:53.346: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.954370278s
  Jul  1 12:31:54.353: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.939907168s
  Jul  1 12:31:55.367: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.932364002s
  Jul  1 12:31:56.374: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.919095571s
  Jul  1 12:31:57.383: INFO: Verifying statefulset ss doesn't scale past 3 for another 911.797948ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-347 @ 07/01/23 12:31:58.384
  Jul  1 12:31:58.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-347 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  1 12:31:58.679: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul  1 12:31:58.679: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  1 12:31:58.679: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul  1 12:31:58.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-347 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  1 12:31:58.966: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul  1 12:31:58.966: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  1 12:31:58.966: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul  1 12:31:58.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-347 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  1 12:31:59.221: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul  1 12:31:59.221: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  1 12:31:59.221: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul  1 12:31:59.221: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 07/01/23 12:32:09.258
  Jul  1 12:32:09.259: INFO: Deleting all statefulset in ns statefulset-347
  Jul  1 12:32:09.265: INFO: Scaling statefulset ss to 0
  Jul  1 12:32:09.284: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  1 12:32:09.290: INFO: Deleting statefulset ss
  Jul  1 12:32:09.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-347" for this suite. @ 07/01/23 12:32:09.328
• [72.986 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 07/01/23 12:32:09.342
  Jul  1 12:32:09.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename statefulset @ 07/01/23 12:32:09.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:32:09.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:32:09.38
  STEP: Creating service test in namespace statefulset-8963 @ 07/01/23 12:32:09.387
  STEP: Looking for a node to schedule stateful set and pod @ 07/01/23 12:32:09.403
  STEP: Creating pod with conflicting port in namespace statefulset-8963 @ 07/01/23 12:32:09.415
  STEP: Waiting until pod test-pod will start running in namespace statefulset-8963 @ 07/01/23 12:32:09.428
  STEP: Creating statefulset with conflicting port in namespace statefulset-8963 @ 07/01/23 12:32:11.448
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8963 @ 07/01/23 12:32:11.465
  Jul  1 12:32:11.485: INFO: Observed stateful pod in namespace: statefulset-8963, name: ss-0, uid: d58caf7a-e773-4c19-842c-7fa83f72f493, status phase: Pending. Waiting for statefulset controller to delete.
  Jul  1 12:32:11.507: INFO: Observed stateful pod in namespace: statefulset-8963, name: ss-0, uid: d58caf7a-e773-4c19-842c-7fa83f72f493, status phase: Failed. Waiting for statefulset controller to delete.
  Jul  1 12:32:11.522: INFO: Observed stateful pod in namespace: statefulset-8963, name: ss-0, uid: d58caf7a-e773-4c19-842c-7fa83f72f493, status phase: Failed. Waiting for statefulset controller to delete.
  Jul  1 12:32:11.532: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8963
  STEP: Removing pod with conflicting port in namespace statefulset-8963 @ 07/01/23 12:32:11.532
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8963 and will be in running state @ 07/01/23 12:32:11.561
  Jul  1 12:32:13.577: INFO: Deleting all statefulset in ns statefulset-8963
  Jul  1 12:32:13.585: INFO: Scaling statefulset ss to 0
  Jul  1 12:32:23.615: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  1 12:32:23.620: INFO: Deleting statefulset ss
  Jul  1 12:32:23.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8963" for this suite. @ 07/01/23 12:32:23.652
• [14.319 seconds]
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 07/01/23 12:32:23.662
  Jul  1 12:32:23.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 12:32:23.663
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:32:23.689
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:32:23.696
  STEP: Creating configMap with name projected-configmap-test-volume-b805399e-bef7-4692-8e2a-8497dddb6bfe @ 07/01/23 12:32:23.705
  STEP: Creating a pod to test consume configMaps @ 07/01/23 12:32:23.714
  STEP: Saw pod success @ 07/01/23 12:32:27.755
  Jul  1 12:32:27.761: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-projected-configmaps-c9d1c33a-2267-46c0-84f4-6e98fd6b8d57 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 07/01/23 12:32:27.779
  Jul  1 12:32:27.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6152" for this suite. @ 07/01/23 12:32:27.864
• [4.222 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 07/01/23 12:32:27.886
  Jul  1 12:32:27.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 12:32:27.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:32:27.912
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:32:27.926
  STEP: Creating configMap with name projected-configmap-test-volume-ccbbe493-b0c9-4232-9c88-467ae568a0c4 @ 07/01/23 12:32:27.931
  STEP: Creating a pod to test consume configMaps @ 07/01/23 12:32:27.937
  STEP: Saw pod success @ 07/01/23 12:32:31.983
  Jul  1 12:32:31.990: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-projected-configmaps-00dd88b2-444d-4ed2-9f66-346603617c9c container agnhost-container: <nil>
  STEP: delete the pod @ 07/01/23 12:32:32.004
  Jul  1 12:32:32.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7276" for this suite. @ 07/01/23 12:32:32.035
• [4.161 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 07/01/23 12:32:32.049
  Jul  1 12:32:32.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename configmap @ 07/01/23 12:32:32.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:32:32.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:32:32.091
  STEP: Creating configMap with name configmap-test-volume-map-c67cbc34-e77b-4289-b105-f1723faa0fe2 @ 07/01/23 12:32:32.097
  STEP: Creating a pod to test consume configMaps @ 07/01/23 12:32:32.107
  STEP: Saw pod success @ 07/01/23 12:32:36.149
  Jul  1 12:32:36.154: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-configmaps-09205b28-7337-43db-bedb-510baaeb98f3 container agnhost-container: <nil>
  STEP: delete the pod @ 07/01/23 12:32:36.166
  Jul  1 12:32:36.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6074" for this suite. @ 07/01/23 12:32:36.201
• [4.166 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 07/01/23 12:32:36.216
  Jul  1 12:32:36.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename job @ 07/01/23 12:32:36.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:32:36.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:32:36.258
  STEP: Creating a job @ 07/01/23 12:32:36.265
  STEP: Ensuring active pods == parallelism @ 07/01/23 12:32:36.276
  STEP: delete a job @ 07/01/23 12:32:40.286
  STEP: deleting Job.batch foo in namespace job-9743, will wait for the garbage collector to delete the pods @ 07/01/23 12:32:40.288
  Jul  1 12:32:40.363: INFO: Deleting Job.batch foo took: 18.046447ms
  Jul  1 12:32:40.464: INFO: Terminating Job.batch foo pods took: 101.103031ms
  STEP: Ensuring job was deleted @ 07/01/23 12:33:13.165
  Jul  1 12:33:13.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9743" for this suite. @ 07/01/23 12:33:13.181
• [36.976 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 07/01/23 12:33:13.197
  Jul  1 12:33:13.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename replicaset @ 07/01/23 12:33:13.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:33:13.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:33:13.235
  Jul  1 12:33:13.241: INFO: Creating ReplicaSet my-hostname-basic-0c7372a6-f905-439d-abfb-530c5d1b8a21
  Jul  1 12:33:13.256: INFO: Pod name my-hostname-basic-0c7372a6-f905-439d-abfb-530c5d1b8a21: Found 0 pods out of 1
  Jul  1 12:33:18.265: INFO: Pod name my-hostname-basic-0c7372a6-f905-439d-abfb-530c5d1b8a21: Found 1 pods out of 1
  Jul  1 12:33:18.265: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0c7372a6-f905-439d-abfb-530c5d1b8a21" is running
  Jul  1 12:33:18.273: INFO: Pod "my-hostname-basic-0c7372a6-f905-439d-abfb-530c5d1b8a21-ggmck" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-01 12:33:13 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-01 12:33:15 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-01 12:33:15 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-01 12:33:13 +0000 UTC Reason: Message:}])
  Jul  1 12:33:18.273: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 07/01/23 12:33:18.273
  Jul  1 12:33:18.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2354" for this suite. @ 07/01/23 12:33:18.309
• [5.124 seconds]
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 07/01/23 12:33:18.321
  Jul  1 12:33:18.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename namespaces @ 07/01/23 12:33:18.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:33:18.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:33:18.381
  STEP: creating a Namespace @ 07/01/23 12:33:18.388
  STEP: patching the Namespace @ 07/01/23 12:33:18.418
  STEP: get the Namespace and ensuring it has the label @ 07/01/23 12:33:18.43
  Jul  1 12:33:18.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-2029" for this suite. @ 07/01/23 12:33:18.451
  STEP: Destroying namespace "nspatchtest-d9cc7743-0416-4f4c-afb8-09b046361c47-1945" for this suite. @ 07/01/23 12:33:18.464
• [0.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 07/01/23 12:33:18.48
  Jul  1 12:33:18.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename downward-api @ 07/01/23 12:33:18.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:33:18.509
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:33:18.516
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 12:33:18.529
  STEP: Saw pod success @ 07/01/23 12:33:22.581
  Jul  1 12:33:22.592: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-cd1b28ab-d2b6-4fa1-b1c6-42780c0605de container client-container: <nil>
  STEP: delete the pod @ 07/01/23 12:33:22.608
  Jul  1 12:33:22.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1762" for this suite. @ 07/01/23 12:33:22.644
• [4.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 07/01/23 12:33:22.668
  Jul  1 12:33:22.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 12:33:22.669
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:33:22.692
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:33:22.702
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 12:33:22.718
  STEP: Saw pod success @ 07/01/23 12:33:26.759
  Jul  1 12:33:26.766: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-3492264e-df37-4e98-9a10-1c9d46dd8c56 container client-container: <nil>
  STEP: delete the pod @ 07/01/23 12:33:26.776
  Jul  1 12:33:26.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8435" for this suite. @ 07/01/23 12:33:26.806
• [4.151 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 07/01/23 12:33:26.82
  Jul  1 12:33:26.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir @ 07/01/23 12:33:26.821
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:33:26.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:33:26.856
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 07/01/23 12:33:26.861
  STEP: Saw pod success @ 07/01/23 12:33:30.9
  Jul  1 12:33:30.905: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-ba3bc32c-81e1-44f0-a68b-28eb642ca1a5 container test-container: <nil>
  STEP: delete the pod @ 07/01/23 12:33:30.917
  Jul  1 12:33:30.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3181" for this suite. @ 07/01/23 12:33:30.959
• [4.152 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 07/01/23 12:33:30.973
  Jul  1 12:33:30.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename replicaset @ 07/01/23 12:33:30.974
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:33:31.002
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:33:31.01
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 07/01/23 12:33:31.025
  Jul  1 12:33:31.043: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul  1 12:33:36.056: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/01/23 12:33:36.056
  STEP: getting scale subresource @ 07/01/23 12:33:36.056
  STEP: updating a scale subresource @ 07/01/23 12:33:36.064
  STEP: verifying the replicaset Spec.Replicas was modified @ 07/01/23 12:33:36.076
  STEP: Patch a scale subresource @ 07/01/23 12:33:36.084
  Jul  1 12:33:36.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6591" for this suite. @ 07/01/23 12:33:36.117
• [5.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 07/01/23 12:33:36.134
  Jul  1 12:33:36.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename init-container @ 07/01/23 12:33:36.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:33:36.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:33:36.184
  STEP: creating the pod @ 07/01/23 12:33:36.19
  Jul  1 12:33:36.190: INFO: PodSpec: initContainers in spec.initContainers
  Jul  1 12:34:19.262: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-79b98eb1-3a9f-4884-bfb1-9ccadf009aa7", GenerateName:"", Namespace:"init-container-2206", SelfLink:"", UID:"c8b9ca84-0b26-49b8-a1c6-86d164005cc1", ResourceVersion:"15976", Generation:0, CreationTimestamp:time.Date(2023, time.July, 1, 12, 33, 36, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"190427742"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 1, 12, 33, 36, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0008bf980), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 1, 12, 34, 19, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0008bf9b0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-g7mll", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc002dae320), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-g7mll", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-g7mll", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-g7mll", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002354df8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-91-66", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0003629a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002354e80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002354ea0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002354ea8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002354eac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0047ff9e0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 1, 12, 33, 36, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 1, 12, 33, 36, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 1, 12, 33, 36, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 1, 12, 33, 36, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.91.66", PodIP:"192.168.62.73", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.62.73"}}, StartTime:time.Date(2023, time.July, 1, 12, 33, 36, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000362bd0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000362e00)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://70311b05c2caf4d9e8ae14e9cd37d35f15c65e9fc27e8a841d138a6219662608", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002dae3a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002dae380), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc002354f2f), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Jul  1 12:34:19.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-2206" for this suite. @ 07/01/23 12:34:19.294
• [43.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 07/01/23 12:34:19.313
  Jul  1 12:34:19.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubelet-test @ 07/01/23 12:34:19.314
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:34:19.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:34:19.369
  Jul  1 12:34:19.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6512" for this suite. @ 07/01/23 12:34:19.481
• [0.186 seconds]
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 07/01/23 12:34:19.499
  Jul  1 12:34:19.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename deployment @ 07/01/23 12:34:19.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:34:19.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:34:19.538
  Jul  1 12:34:19.554: INFO: Creating deployment "webserver-deployment"
  Jul  1 12:34:19.567: INFO: Waiting for observed generation 1
  Jul  1 12:34:21.602: INFO: Waiting for all required pods to come up
  Jul  1 12:34:21.658: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 07/01/23 12:34:21.659
  Jul  1 12:34:23.684: INFO: Waiting for deployment "webserver-deployment" to complete
  Jul  1 12:34:23.695: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Jul  1 12:34:23.713: INFO: Updating deployment webserver-deployment
  Jul  1 12:34:23.713: INFO: Waiting for observed generation 2
  Jul  1 12:34:25.729: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Jul  1 12:34:25.736: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Jul  1 12:34:25.741: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jul  1 12:34:25.761: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Jul  1 12:34:25.762: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Jul  1 12:34:25.768: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jul  1 12:34:25.779: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Jul  1 12:34:25.782: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Jul  1 12:34:25.803: INFO: Updating deployment webserver-deployment
  Jul  1 12:34:25.803: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Jul  1 12:34:25.836: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Jul  1 12:34:27.872: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Jul  1 12:34:27.885: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-8299  b89105ac-141a-46d4-9f72-ed318f249a80 16341 3 2023-07-01 12:34:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00407d718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-01 12:34:25 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-07-01 12:34:25 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Jul  1 12:34:27.894: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-8299  b435afe1-a123-4317-a80c-f80d20b7e654 16331 3 2023-07-01 12:34:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment b89105ac-141a-46d4-9f72-ed318f249a80 0xc003b523b7 0xc003b523b8}] [] [{kube-controller-manager Update apps/v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b89105ac-141a-46d4-9f72-ed318f249a80\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b52498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul  1 12:34:27.894: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Jul  1 12:34:27.894: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-8299  8d6c5039-bd29-4b9b-989a-e13da234c519 16330 3 2023-07-01 12:34:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment b89105ac-141a-46d4-9f72-ed318f249a80 0xc003b522c7 0xc003b522c8}] [] [{kube-controller-manager Update apps/v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b89105ac-141a-46d4-9f72-ed318f249a80\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b52358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Jul  1 12:34:27.904: INFO: Pod "webserver-deployment-67bd4bf6dc-49gvl" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-49gvl webserver-deployment-67bd4bf6dc- deployment-8299  ab21adbf-7b50-404a-990d-a09e1bf6e3e7 16318 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc003b52ae7 0xc003b52ae8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bsdp8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bsdp8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.125,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.905: INFO: Pod "webserver-deployment-67bd4bf6dc-5sgq6" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5sgq6 webserver-deployment-67bd4bf6dc- deployment-8299  42faca8b-37b3-4404-a1cc-12fffa66661e 16098 0 2023-07-01 12:34:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc003b52cb7 0xc003b52cb8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.85.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f7xpr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f7xpr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-94,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.94,PodIP:192.168.85.51,StartTime:2023-07-01 12:34:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-01 12:34:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e415bd64a6b8f3b33f4cbfb8a658c4e1bb864474a406dc140b4ef5e3cfae2895,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.85.51,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.906: INFO: Pod "webserver-deployment-67bd4bf6dc-5vznk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5vznk webserver-deployment-67bd4bf6dc- deployment-8299  ec63ce42-deca-4f01-aba2-3a7078094862 16338 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc003b52ea7 0xc003b52ea8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nb5qg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nb5qg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.915: INFO: Pod "webserver-deployment-67bd4bf6dc-6ptjr" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6ptjr webserver-deployment-67bd4bf6dc- deployment-8299  37168549-bcfe-4c1b-a5ca-af2f31af5f38 16113 0 2023-07-01 12:34:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc003b53077 0xc003b53078}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.62.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2nwjh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2nwjh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:192.168.62.69,StartTime:2023-07-01 12:34:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-01 12:34:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4d61ed1c40eff3f8b40eb32068af849bd904dc6655bf923f0278feda23f6d121,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.62.69,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.916: INFO: Pod "webserver-deployment-67bd4bf6dc-8glgx" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8glgx webserver-deployment-67bd4bf6dc- deployment-8299  dc3ac02d-7798-4d12-b170-1eaaadcfae4c 16136 0 2023-07-01 12:34:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc003b53267 0xc003b53268}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.175.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hr57t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hr57t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.125,PodIP:192.168.175.4,StartTime:2023-07-01 12:34:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-01 12:34:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://851f029ccdfbd741fd9a1d103cbc41ca9010b20dcb17bebee09fdcff8e194186,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.175.4,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.916: INFO: Pod "webserver-deployment-67bd4bf6dc-blml2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-blml2 webserver-deployment-67bd4bf6dc- deployment-8299  40bf8809-c3c6-475f-9089-d92cec8482dd 16366 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc003b53467 0xc003b53468}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z8tzg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z8tzg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.125,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.917: INFO: Pod "webserver-deployment-67bd4bf6dc-dwgql" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dwgql webserver-deployment-67bd4bf6dc- deployment-8299  02a5a3dd-64d9-46a0-8c31-46ecf89c5e13 16354 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc003b53637 0xc003b53638}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xhxlz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xhxlz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-94,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.94,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.917: INFO: Pod "webserver-deployment-67bd4bf6dc-fqqvb" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-fqqvb webserver-deployment-67bd4bf6dc- deployment-8299  297214d8-3bfa-4166-bfa5-815bb87bf4cc 16142 0 2023-07-01 12:34:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc003b53807 0xc003b53808}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.175.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j8ccv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j8ccv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.125,PodIP:192.168.175.3,StartTime:2023-07-01 12:34:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-01 12:34:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d778410ab2eaaa6dd072dde3309d7674ae89ec8224f9174825dd0ad0a69ed7c5,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.175.3,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.918: INFO: Pod "webserver-deployment-67bd4bf6dc-g98gm" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-g98gm webserver-deployment-67bd4bf6dc- deployment-8299  687a9bcf-4df4-4048-b62e-7c3532323461 16350 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc003b539f7 0xc003b539f8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vwxtg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vwxtg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.125,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.918: INFO: Pod "webserver-deployment-67bd4bf6dc-j8vgs" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-j8vgs webserver-deployment-67bd4bf6dc- deployment-8299  50e42f0c-3538-4d6d-9a37-9f99ad303dde 16111 0 2023-07-01 12:34:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc003b53bc7 0xc003b53bc8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.62.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dd7z9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dd7z9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:192.168.62.72,StartTime:2023-07-01 12:34:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-01 12:34:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b9d31a20db0b299fa1d878a72d5a00d7efcdb355302082d997300373c1ada5c0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.62.72,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.919: INFO: Pod "webserver-deployment-67bd4bf6dc-k8cc7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-k8cc7 webserver-deployment-67bd4bf6dc- deployment-8299  4c3228cc-3252-40a6-9385-97bfe160381c 16348 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc003b53db7 0xc003b53db8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lthnc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lthnc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-94,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.94,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.919: INFO: Pod "webserver-deployment-67bd4bf6dc-p56pp" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-p56pp webserver-deployment-67bd4bf6dc- deployment-8299  7c8b5a15-7f1a-48aa-b22d-0f1a8102975a 16090 0 2023-07-01 12:34:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc003b53f87 0xc003b53f88}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.85.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4gmvw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4gmvw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-94,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.94,PodIP:192.168.85.52,StartTime:2023-07-01 12:34:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-01 12:34:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4577158b872abc8385f8fe5c957a95818c8a509419aa8e2244e453ff87ea24d4,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.85.52,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.919: INFO: Pod "webserver-deployment-67bd4bf6dc-rm4rl" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rm4rl webserver-deployment-67bd4bf6dc- deployment-8299  9dc3ffc3-00ea-472b-808b-a69610a709fc 16335 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc0017884e7 0xc0017884e8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nlrsb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nlrsb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-94,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.94,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.920: INFO: Pod "webserver-deployment-67bd4bf6dc-s4mqb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-s4mqb webserver-deployment-67bd4bf6dc- deployment-8299  5f98cb39-a6ff-4343-bddc-cad4f6330696 16365 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc001789647 0xc001789648}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kgkf7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kgkf7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.125,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.920: INFO: Pod "webserver-deployment-67bd4bf6dc-sm8ps" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-sm8ps webserver-deployment-67bd4bf6dc- deployment-8299  3cbe9863-424c-498b-8c0d-0e66b0d3b844 16129 0 2023-07-01 12:34:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc001789ae7 0xc001789ae8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.85.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wfcmp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wfcmp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-94,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.94,PodIP:192.168.85.53,StartTime:2023-07-01 12:34:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-01 12:34:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2899e470f055b8d56469c5939d1af45779a18615395dd6bae4a5266462888679,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.85.53,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.920: INFO: Pod "webserver-deployment-67bd4bf6dc-t5tbh" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-t5tbh webserver-deployment-67bd4bf6dc- deployment-8299  ab11d535-e39b-43ee-8246-8da677a64861 16361 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc001789cd7 0xc001789cd8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbzzx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbzzx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.934: INFO: Pod "webserver-deployment-67bd4bf6dc-tk9nk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tk9nk webserver-deployment-67bd4bf6dc- deployment-8299  0ab2ff1b-3903-425c-9bcd-ad8d5fa576cd 16297 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc001789ea7 0xc001789ea8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lvk5w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lvk5w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.125,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.934: INFO: Pod "webserver-deployment-67bd4bf6dc-tvcl5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tvcl5 webserver-deployment-67bd4bf6dc- deployment-8299  6bc64547-0f44-42b8-a1bc-7e7e0c63990a 16349 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc0052bc0e7 0xc0052bc0e8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lz96h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lz96h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.934: INFO: Pod "webserver-deployment-67bd4bf6dc-w766g" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-w766g webserver-deployment-67bd4bf6dc- deployment-8299  43713624-13a4-4c32-bbe7-48abf9e301bc 16329 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc0052bc2d7 0xc0052bc2d8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-grslp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-grslp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.935: INFO: Pod "webserver-deployment-67bd4bf6dc-zmcfh" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-zmcfh webserver-deployment-67bd4bf6dc- deployment-8299  b5834c33-43df-4ee8-a3ff-8128dd2efff2 16108 0 2023-07-01 12:34:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 8d6c5039-bd29-4b9b-989a-e13da234c519 0xc0052bc4a7 0xc0052bc4a8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d6c5039-bd29-4b9b-989a-e13da234c519\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.62.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ck2c7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ck2c7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:192.168.62.74,StartTime:2023-07-01 12:34:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-01 12:34:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://50ae99387008a06a7e524c805fceb600acb081f5c6edd60d63eab911cbcb84cd,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.62.74,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.935: INFO: Pod "webserver-deployment-7b75d79cf5-7bxxw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-7bxxw webserver-deployment-7b75d79cf5- deployment-8299  b705333d-db40-4632-bafe-56c7d21d95d6 16342 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b435afe1-a123-4317-a80c-f80d20b7e654 0xc0052bc697 0xc0052bc698}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b435afe1-a123-4317-a80c-f80d20b7e654\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbkb2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbkb2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.125,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.947: INFO: Pod "webserver-deployment-7b75d79cf5-c4l48" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-c4l48 webserver-deployment-7b75d79cf5- deployment-8299  64696c33-db64-4f0b-b363-bf22ab84f570 16259 0 2023-07-01 12:34:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b435afe1-a123-4317-a80c-f80d20b7e654 0xc0052bc887 0xc0052bc888}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b435afe1-a123-4317-a80c-f80d20b7e654\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.62.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n5xgf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n5xgf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:192.168.62.104,StartTime:2023-07-01 12:34:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.62.104,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.947: INFO: Pod "webserver-deployment-7b75d79cf5-c55qn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-c55qn webserver-deployment-7b75d79cf5- deployment-8299  457461aa-316a-4254-ae41-e13b1babc772 16420 0 2023-07-01 12:34:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b435afe1-a123-4317-a80c-f80d20b7e654 0xc0052bcab7 0xc0052bcab8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b435afe1-a123-4317-a80c-f80d20b7e654\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.175.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4l2tc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4l2tc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.125,PodIP:192.168.175.6,StartTime:2023-07-01 12:34:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.175.6,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.949: INFO: Pod "webserver-deployment-7b75d79cf5-djcjg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-djcjg webserver-deployment-7b75d79cf5- deployment-8299  5e3568ed-5218-41ce-9d7e-df1193528850 16303 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b435afe1-a123-4317-a80c-f80d20b7e654 0xc0052bcce7 0xc0052bcce8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b435afe1-a123-4317-a80c-f80d20b7e654\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b9288,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b9288,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.949: INFO: Pod "webserver-deployment-7b75d79cf5-g79s6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-g79s6 webserver-deployment-7b75d79cf5- deployment-8299  22658bbd-80ae-4b7f-91e7-d20ffabeaa9a 16360 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b435afe1-a123-4317-a80c-f80d20b7e654 0xc0052bced7 0xc0052bced8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b435afe1-a123-4317-a80c-f80d20b7e654\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zxksw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zxksw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.125,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.950: INFO: Pod "webserver-deployment-7b75d79cf5-hm4qj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-hm4qj webserver-deployment-7b75d79cf5- deployment-8299  6d2ba0dc-156a-4394-aa6f-828fb6121871 16345 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b435afe1-a123-4317-a80c-f80d20b7e654 0xc0052bd0c7 0xc0052bd0c8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b435afe1-a123-4317-a80c-f80d20b7e654\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qq4df,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qq4df,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.959: INFO: Pod "webserver-deployment-7b75d79cf5-jldxs" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-jldxs webserver-deployment-7b75d79cf5- deployment-8299  17753436-315c-434a-8168-538f2f98d4d8 16299 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b435afe1-a123-4317-a80c-f80d20b7e654 0xc0052bd2b7 0xc0052bd2b8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b435afe1-a123-4317-a80c-f80d20b7e654\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rbq2b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbq2b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-94,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.94,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.959: INFO: Pod "webserver-deployment-7b75d79cf5-jlsp6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-jlsp6 webserver-deployment-7b75d79cf5- deployment-8299  f737ea5a-fdb6-4835-a73b-829a9b6c0cbc 16247 0 2023-07-01 12:34:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b435afe1-a123-4317-a80c-f80d20b7e654 0xc0052bd4a7 0xc0052bd4a8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b435afe1-a123-4317-a80c-f80d20b7e654\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.85.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gzmlz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gzmlz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-94,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.94,PodIP:192.168.85.54,StartTime:2023-07-01 12:34:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.85.54,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.959: INFO: Pod "webserver-deployment-7b75d79cf5-nd22f" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-nd22f webserver-deployment-7b75d79cf5- deployment-8299  93de2d1f-1ad3-452d-8fda-cb7758fdb846 16324 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b435afe1-a123-4317-a80c-f80d20b7e654 0xc0052bd6c7 0xc0052bd6c8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b435afe1-a123-4317-a80c-f80d20b7e654\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nl2w2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nl2w2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-94,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.94,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.960: INFO: Pod "webserver-deployment-7b75d79cf5-thr2r" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-thr2r webserver-deployment-7b75d79cf5- deployment-8299  1223e8f7-8f2d-4926-9025-b9abfcb20793 16262 0 2023-07-01 12:34:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b435afe1-a123-4317-a80c-f80d20b7e654 0xc0052bd8b7 0xc0052bd8b8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b435afe1-a123-4317-a80c-f80d20b7e654\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.62.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4fz6b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4fz6b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:192.168.62.68,StartTime:2023-07-01 12:34:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.62.68,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.960: INFO: Pod "webserver-deployment-7b75d79cf5-ttktq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-ttktq webserver-deployment-7b75d79cf5- deployment-8299  8625cd49-540e-476d-a4ab-e910a425f781 16424 0 2023-07-01 12:34:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b435afe1-a123-4317-a80c-f80d20b7e654 0xc0052bdad7 0xc0052bdad8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b435afe1-a123-4317-a80c-f80d20b7e654\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.175.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r8dbv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r8dbv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.125,PodIP:192.168.175.5,StartTime:2023-07-01 12:34:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.175.5,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.964: INFO: Pod "webserver-deployment-7b75d79cf5-vbm57" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-vbm57 webserver-deployment-7b75d79cf5- deployment-8299  cb7f9ec8-982e-41ec-b316-68e463ba11f8 16355 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b435afe1-a123-4317-a80c-f80d20b7e654 0xc0052bdcf7 0xc0052bdcf8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b435afe1-a123-4317-a80c-f80d20b7e654\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xhjqr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xhjqr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.964: INFO: Pod "webserver-deployment-7b75d79cf5-wdktk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-wdktk webserver-deployment-7b75d79cf5- deployment-8299  b5fc6af2-2769-4829-a4b1-617965147acf 16344 0 2023-07-01 12:34:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b435afe1-a123-4317-a80c-f80d20b7e654 0xc0052bdee7 0xc0052bdee8}] [] [{kube-controller-manager Update v1 2023-07-01 12:34:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b435afe1-a123-4317-a80c-f80d20b7e654\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 12:34:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2mx8d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2mx8d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-94,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 12:34:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.94,PodIP:,StartTime:2023-07-01 12:34:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 12:34:27.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8299" for this suite. @ 07/01/23 12:34:27.972
• [8.485 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 07/01/23 12:34:27.999
  Jul  1 12:34:27.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-probe @ 07/01/23 12:34:28.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:34:28.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:34:28.051
  STEP: Creating pod test-grpc-ad8b6228-6e08-4204-acdb-5f9ca02a3d63 in namespace container-probe-2032 @ 07/01/23 12:34:28.064
  Jul  1 12:34:30.116: INFO: Started pod test-grpc-ad8b6228-6e08-4204-acdb-5f9ca02a3d63 in namespace container-probe-2032
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/01/23 12:34:30.116
  Jul  1 12:34:30.121: INFO: Initial restart count of pod test-grpc-ad8b6228-6e08-4204-acdb-5f9ca02a3d63 is 0
  Jul  1 12:35:36.398: INFO: Restart count of pod container-probe-2032/test-grpc-ad8b6228-6e08-4204-acdb-5f9ca02a3d63 is now 1 (1m6.265432482s elapsed)
  Jul  1 12:35:36.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/01/23 12:35:36.404
  STEP: Destroying namespace "container-probe-2032" for this suite. @ 07/01/23 12:35:36.433
• [68.444 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 07/01/23 12:35:36.444
  Jul  1 12:35:36.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 12:35:36.445
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:35:36.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:35:36.493
  STEP: Creating the pod @ 07/01/23 12:35:36.498
  Jul  1 12:35:39.093: INFO: Successfully updated pod "labelsupdate64c8d451-c849-4b89-aa0e-c399fa5f9938"
  Jul  1 12:35:41.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4421" for this suite. @ 07/01/23 12:35:41.129
• [4.697 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 07/01/23 12:35:41.142
  Jul  1 12:35:41.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl @ 07/01/23 12:35:41.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:35:41.181
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:35:41.187
  STEP: starting the proxy server @ 07/01/23 12:35:41.193
  Jul  1 12:35:41.193: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-9336 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 07/01/23 12:35:41.28
  Jul  1 12:35:41.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9336" for this suite. @ 07/01/23 12:35:41.3
• [0.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 07/01/23 12:35:41.315
  Jul  1 12:35:41.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename statefulset @ 07/01/23 12:35:41.323
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:35:41.354
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:35:41.365
  STEP: Creating service test in namespace statefulset-1693 @ 07/01/23 12:35:41.372
  STEP: Creating a new StatefulSet @ 07/01/23 12:35:41.381
  Jul  1 12:35:41.406: INFO: Found 0 stateful pods, waiting for 3
  Jul  1 12:35:51.417: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul  1 12:35:51.417: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul  1 12:35:51.417: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 07/01/23 12:35:51.437
  Jul  1 12:35:51.474: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 07/01/23 12:35:51.474
  STEP: Not applying an update when the partition is greater than the number of replicas @ 07/01/23 12:36:01.505
  STEP: Performing a canary update @ 07/01/23 12:36:01.505
  Jul  1 12:36:01.540: INFO: Updating stateful set ss2
  Jul  1 12:36:01.554: INFO: Waiting for Pod statefulset-1693/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 07/01/23 12:36:11.566
  Jul  1 12:36:11.635: INFO: Found 2 stateful pods, waiting for 3
  Jul  1 12:36:21.641: INFO: Found 2 stateful pods, waiting for 3
  Jul  1 12:36:31.643: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul  1 12:36:31.643: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul  1 12:36:31.643: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 07/01/23 12:36:31.654
  Jul  1 12:36:31.687: INFO: Updating stateful set ss2
  Jul  1 12:36:31.712: INFO: Waiting for Pod statefulset-1693/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jul  1 12:36:41.784: INFO: Updating stateful set ss2
  Jul  1 12:36:41.804: INFO: Waiting for StatefulSet statefulset-1693/ss2 to complete update
  Jul  1 12:36:41.804: INFO: Waiting for Pod statefulset-1693/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jul  1 12:36:51.815: INFO: Deleting all statefulset in ns statefulset-1693
  Jul  1 12:36:51.821: INFO: Scaling statefulset ss2 to 0
  Jul  1 12:37:01.849: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  1 12:37:01.855: INFO: Deleting statefulset ss2
  Jul  1 12:37:01.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1693" for this suite. @ 07/01/23 12:37:01.899
• [80.599 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 07/01/23 12:37:01.916
  Jul  1 12:37:01.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir @ 07/01/23 12:37:01.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:37:01.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:37:01.969
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 07/01/23 12:37:01.987
  STEP: Saw pod success @ 07/01/23 12:37:06.04
  Jul  1 12:37:06.045: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-f85f276f-eccc-4073-9c24-6ed5fe090e14 container test-container: <nil>
  STEP: delete the pod @ 07/01/23 12:37:06.064
  Jul  1 12:37:06.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9961" for this suite. @ 07/01/23 12:37:06.106
• [4.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 07/01/23 12:37:06.123
  Jul  1 12:37:06.123: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename cronjob @ 07/01/23 12:37:06.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:37:06.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:37:06.159
  STEP: Creating a suspended cronjob @ 07/01/23 12:37:06.17
  STEP: Ensuring no jobs are scheduled @ 07/01/23 12:37:06.184
  STEP: Ensuring no job exists by listing jobs explicitly @ 07/01/23 12:42:06.201
  STEP: Removing cronjob @ 07/01/23 12:42:06.206
  Jul  1 12:42:06.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9897" for this suite. @ 07/01/23 12:42:06.223
• [300.111 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 07/01/23 12:42:06.234
  Jul  1 12:42:06.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 12:42:06.235
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:42:06.267
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:42:06.274
  STEP: Creating configMap with name cm-test-opt-del-42793f3b-5e34-4374-9121-d583843a0e4f @ 07/01/23 12:42:06.292
  STEP: Creating configMap with name cm-test-opt-upd-f30e93c0-8f7a-468d-a756-8c21419052d1 @ 07/01/23 12:42:06.303
  STEP: Creating the pod @ 07/01/23 12:42:06.314
  STEP: Deleting configmap cm-test-opt-del-42793f3b-5e34-4374-9121-d583843a0e4f @ 07/01/23 12:42:08.4
  STEP: Updating configmap cm-test-opt-upd-f30e93c0-8f7a-468d-a756-8c21419052d1 @ 07/01/23 12:42:08.41
  STEP: Creating configMap with name cm-test-opt-create-c8d4b9c5-d23d-43e1-bf9c-7aca2fbdf108 @ 07/01/23 12:42:08.421
  STEP: waiting to observe update in volume @ 07/01/23 12:42:08.429
  Jul  1 12:43:20.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-757" for this suite. @ 07/01/23 12:43:20.969
• [74.748 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 07/01/23 12:43:20.982
  Jul  1 12:43:20.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename namespaces @ 07/01/23 12:43:20.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:43:21.004
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:43:21.01
  STEP: Creating a test namespace @ 07/01/23 12:43:21.025
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:43:21.053
  STEP: Creating a service in the namespace @ 07/01/23 12:43:21.061
  STEP: Deleting the namespace @ 07/01/23 12:43:21.084
  STEP: Waiting for the namespace to be removed. @ 07/01/23 12:43:21.099
  STEP: Recreating the namespace @ 07/01/23 12:43:27.104
  STEP: Verifying there is no service in the namespace @ 07/01/23 12:43:27.126
  Jul  1 12:43:27.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3813" for this suite. @ 07/01/23 12:43:27.136
  STEP: Destroying namespace "nsdeletetest-4226" for this suite. @ 07/01/23 12:43:27.145
  Jul  1 12:43:27.151: INFO: Namespace nsdeletetest-4226 was already deleted
  STEP: Destroying namespace "nsdeletetest-1514" for this suite. @ 07/01/23 12:43:27.151
• [6.177 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 07/01/23 12:43:27.16
  Jul  1 12:43:27.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/01/23 12:43:27.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:43:27.181
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:43:27.187
  Jul  1 12:43:27.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/01/23 12:43:28.61
  Jul  1 12:43:28.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-9767 --namespace=crd-publish-openapi-9767 create -f -'
  Jul  1 12:43:29.629: INFO: stderr: ""
  Jul  1 12:43:29.629: INFO: stdout: "e2e-test-crd-publish-openapi-7063-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jul  1 12:43:29.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-9767 --namespace=crd-publish-openapi-9767 delete e2e-test-crd-publish-openapi-7063-crds test-cr'
  Jul  1 12:43:29.815: INFO: stderr: ""
  Jul  1 12:43:29.815: INFO: stdout: "e2e-test-crd-publish-openapi-7063-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Jul  1 12:43:29.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-9767 --namespace=crd-publish-openapi-9767 apply -f -'
  Jul  1 12:43:30.387: INFO: stderr: ""
  Jul  1 12:43:30.387: INFO: stdout: "e2e-test-crd-publish-openapi-7063-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jul  1 12:43:30.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-9767 --namespace=crd-publish-openapi-9767 delete e2e-test-crd-publish-openapi-7063-crds test-cr'
  Jul  1 12:43:30.505: INFO: stderr: ""
  Jul  1 12:43:30.505: INFO: stdout: "e2e-test-crd-publish-openapi-7063-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 07/01/23 12:43:30.505
  Jul  1 12:43:30.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-9767 explain e2e-test-crd-publish-openapi-7063-crds'
  Jul  1 12:43:30.795: INFO: stderr: ""
  Jul  1 12:43:30.795: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-7063-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Jul  1 12:43:32.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9767" for this suite. @ 07/01/23 12:43:32.373
• [5.226 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 07/01/23 12:43:32.387
  Jul  1 12:43:32.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename job @ 07/01/23 12:43:32.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:43:32.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:43:32.419
  STEP: Creating a job @ 07/01/23 12:43:32.429
  STEP: Ensuring job reaches completions @ 07/01/23 12:43:32.44
  Jul  1 12:43:44.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7230" for this suite. @ 07/01/23 12:43:44.454
• [12.077 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 07/01/23 12:43:44.465
  Jul  1 12:43:44.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename csiinlinevolumes @ 07/01/23 12:43:44.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:43:44.49
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:43:44.502
  STEP: creating @ 07/01/23 12:43:44.511
  STEP: getting @ 07/01/23 12:43:44.539
  STEP: listing @ 07/01/23 12:43:44.553
  STEP: deleting @ 07/01/23 12:43:44.559
  Jul  1 12:43:44.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-5064" for this suite. @ 07/01/23 12:43:44.605
• [0.150 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 07/01/23 12:43:44.616
  Jul  1 12:43:44.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename replication-controller @ 07/01/23 12:43:44.617
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:43:44.64
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:43:44.65
  STEP: Given a ReplicationController is created @ 07/01/23 12:43:44.656
  STEP: When the matched label of one of its pods change @ 07/01/23 12:43:44.664
  Jul  1 12:43:44.673: INFO: Pod name pod-release: Found 0 pods out of 1
  Jul  1 12:43:49.683: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 07/01/23 12:43:49.706
  Jul  1 12:43:50.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4240" for this suite. @ 07/01/23 12:43:50.725
• [6.119 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 07/01/23 12:43:50.736
  Jul  1 12:43:50.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename pods @ 07/01/23 12:43:50.737
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:43:50.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:43:50.768
  STEP: Create a pod @ 07/01/23 12:43:50.78
  STEP: patching /status @ 07/01/23 12:43:52.807
  Jul  1 12:43:52.820: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Jul  1 12:43:52.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7093" for this suite. @ 07/01/23 12:43:52.83
• [2.105 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 07/01/23 12:43:52.841
  Jul  1 12:43:52.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename podtemplate @ 07/01/23 12:43:52.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:43:52.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:43:52.875
  STEP: Create set of pod templates @ 07/01/23 12:43:52.88
  Jul  1 12:43:52.889: INFO: created test-podtemplate-1
  Jul  1 12:43:52.902: INFO: created test-podtemplate-2
  Jul  1 12:43:52.912: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 07/01/23 12:43:52.912
  STEP: delete collection of pod templates @ 07/01/23 12:43:52.919
  Jul  1 12:43:52.919: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 07/01/23 12:43:52.954
  Jul  1 12:43:52.954: INFO: requesting list of pod templates to confirm quantity
  Jul  1 12:43:52.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-2981" for this suite. @ 07/01/23 12:43:52.97
• [0.149 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 07/01/23 12:43:52.991
  Jul  1 12:43:52.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir @ 07/01/23 12:43:52.992
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:43:53.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:43:53.027
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 07/01/23 12:43:53.035
  STEP: Saw pod success @ 07/01/23 12:43:57.079
  Jul  1 12:43:57.084: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-86ccb237-fe23-474f-9b39-adefebee4d1a container test-container: <nil>
  STEP: delete the pod @ 07/01/23 12:43:57.097
  Jul  1 12:43:57.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4612" for this suite. @ 07/01/23 12:43:57.129
• [4.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 07/01/23 12:43:57.152
  Jul  1 12:43:57.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename limitrange @ 07/01/23 12:43:57.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:43:57.18
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:43:57.184
  STEP: Creating LimitRange "e2e-limitrange-zxwqb" in namespace "limitrange-6464" @ 07/01/23 12:43:57.19
  STEP: Creating another limitRange in another namespace @ 07/01/23 12:43:57.197
  Jul  1 12:43:57.221: INFO: Namespace "e2e-limitrange-zxwqb-9000" created
  Jul  1 12:43:57.221: INFO: Creating LimitRange "e2e-limitrange-zxwqb" in namespace "e2e-limitrange-zxwqb-9000"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-zxwqb" @ 07/01/23 12:43:57.229
  Jul  1 12:43:57.236: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-zxwqb" in "limitrange-6464" namespace @ 07/01/23 12:43:57.236
  Jul  1 12:43:57.245: INFO: LimitRange "e2e-limitrange-zxwqb" has been patched
  STEP: Delete LimitRange "e2e-limitrange-zxwqb" by Collection with labelSelector: "e2e-limitrange-zxwqb=patched" @ 07/01/23 12:43:57.245
  STEP: Confirm that the limitRange "e2e-limitrange-zxwqb" has been deleted @ 07/01/23 12:43:57.261
  Jul  1 12:43:57.261: INFO: Requesting list of LimitRange to confirm quantity
  Jul  1 12:43:57.265: INFO: Found 0 LimitRange with label "e2e-limitrange-zxwqb=patched"
  Jul  1 12:43:57.265: INFO: LimitRange "e2e-limitrange-zxwqb" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-zxwqb" @ 07/01/23 12:43:57.265
  Jul  1 12:43:57.271: INFO: Found 1 limitRange
  Jul  1 12:43:57.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-6464" for this suite. @ 07/01/23 12:43:57.278
  STEP: Destroying namespace "e2e-limitrange-zxwqb-9000" for this suite. @ 07/01/23 12:43:57.286
• [0.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 07/01/23 12:43:57.302
  Jul  1 12:43:57.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename downward-api @ 07/01/23 12:43:57.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:43:57.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:43:57.332
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 12:43:57.336
  STEP: Saw pod success @ 07/01/23 12:44:01.368
  Jul  1 12:44:01.374: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-99ace79a-9ffb-46c4-9474-2b83a9e407cf container client-container: <nil>
  STEP: delete the pod @ 07/01/23 12:44:01.386
  Jul  1 12:44:01.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4136" for this suite. @ 07/01/23 12:44:01.423
• [4.132 seconds]
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 07/01/23 12:44:01.435
  Jul  1 12:44:01.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename sched-pred @ 07/01/23 12:44:01.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:44:01.464
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:44:01.479
  Jul  1 12:44:01.485: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul  1 12:44:01.501: INFO: Waiting for terminating namespaces to be deleted...
  Jul  1 12:44:01.507: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-12-125 before test
  Jul  1 12:44:01.515: INFO: nginx-ingress-controller-kubernetes-worker-tjncz from ingress-nginx-kubernetes-worker started at 2023-07-01 11:52:18 +0000 UTC (1 container statuses recorded)
  Jul  1 12:44:01.515: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  1 12:44:01.515: INFO: calico-kube-controllers-9c5cff4fb-6hfws from kube-system started at 2023-07-01 11:52:25 +0000 UTC (1 container statuses recorded)
  Jul  1 12:44:01.515: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul  1 12:44:01.515: INFO: sonobuoy-e2e-job-e4b68f70fcf04452 from sonobuoy started at 2023-07-01 11:57:54 +0000 UTC (2 container statuses recorded)
  Jul  1 12:44:01.515: INFO: 	Container e2e ready: true, restart count 0
  Jul  1 12:44:01.516: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  1 12:44:01.516: INFO: sonobuoy-systemd-logs-daemon-set-0aed011ee39b4f14-g2zvl from sonobuoy started at 2023-07-01 11:57:54 +0000 UTC (2 container statuses recorded)
  Jul  1 12:44:01.516: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  1 12:44:01.516: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul  1 12:44:01.516: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-16-94 before test
  Jul  1 12:44:01.526: INFO: nginx-ingress-controller-kubernetes-worker-ps7xr from ingress-nginx-kubernetes-worker started at 2023-07-01 11:52:10 +0000 UTC (1 container statuses recorded)
  Jul  1 12:44:01.526: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  1 12:44:01.526: INFO: coredns-5c7f76ccb8-zxmnn from kube-system started at 2023-07-01 11:52:04 +0000 UTC (1 container statuses recorded)
  Jul  1 12:44:01.526: INFO: 	Container coredns ready: true, restart count 0
  Jul  1 12:44:01.526: INFO: kube-state-metrics-5b95b4459c-5klzn from kube-system started at 2023-07-01 11:52:04 +0000 UTC (1 container statuses recorded)
  Jul  1 12:44:01.526: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul  1 12:44:01.526: INFO: metrics-server-v0.5.2-6cf8c8b69c-lw48q from kube-system started at 2023-07-01 11:52:04 +0000 UTC (2 container statuses recorded)
  Jul  1 12:44:01.526: INFO: 	Container metrics-server ready: true, restart count 0
  Jul  1 12:44:01.526: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul  1 12:44:01.526: INFO: dashboard-metrics-scraper-6b8586b5c9-8wfzw from kubernetes-dashboard started at 2023-07-01 11:52:04 +0000 UTC (1 container statuses recorded)
  Jul  1 12:44:01.526: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul  1 12:44:01.526: INFO: kubernetes-dashboard-6869f4cd5f-f7c25 from kubernetes-dashboard started at 2023-07-01 11:52:04 +0000 UTC (1 container statuses recorded)
  Jul  1 12:44:01.526: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul  1 12:44:01.526: INFO: sonobuoy-systemd-logs-daemon-set-0aed011ee39b4f14-bbwnl from sonobuoy started at 2023-07-01 11:57:54 +0000 UTC (2 container statuses recorded)
  Jul  1 12:44:01.526: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  1 12:44:01.526: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul  1 12:44:01.526: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-91-66 before test
  Jul  1 12:44:01.537: INFO: default-http-backend-kubernetes-worker-65fc475d49-7llc8 from ingress-nginx-kubernetes-worker started at 2023-07-01 11:52:10 +0000 UTC (1 container statuses recorded)
  Jul  1 12:44:01.537: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul  1 12:44:01.537: INFO: nginx-ingress-controller-kubernetes-worker-rs6r4 from ingress-nginx-kubernetes-worker started at 2023-07-01 11:52:14 +0000 UTC (1 container statuses recorded)
  Jul  1 12:44:01.537: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  1 12:44:01.537: INFO: sonobuoy from sonobuoy started at 2023-07-01 11:57:50 +0000 UTC (1 container statuses recorded)
  Jul  1 12:44:01.537: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul  1 12:44:01.537: INFO: sonobuoy-systemd-logs-daemon-set-0aed011ee39b4f14-g92dx from sonobuoy started at 2023-07-01 11:57:54 +0000 UTC (2 container statuses recorded)
  Jul  1 12:44:01.537: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  1 12:44:01.537: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/01/23 12:44:01.538
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/01/23 12:44:03.575
  STEP: Trying to apply a random label on the found node. @ 07/01/23 12:44:03.606
  STEP: verifying the node has the label kubernetes.io/e2e-a3613f9a-a616-43b5-9187-d614cc4f50d3 95 @ 07/01/23 12:44:03.62
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 07/01/23 12:44:03.628
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.91.66 on the node which pod4 resides and expect not scheduled @ 07/01/23 12:44:05.654
  STEP: removing the label kubernetes.io/e2e-a3613f9a-a616-43b5-9187-d614cc4f50d3 off the node ip-172-31-91-66 @ 07/01/23 12:49:05.668
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-a3613f9a-a616-43b5-9187-d614cc4f50d3 @ 07/01/23 12:49:05.687
  Jul  1 12:49:05.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1396" for this suite. @ 07/01/23 12:49:05.701
• [304.280 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 07/01/23 12:49:05.715
  Jul  1 12:49:05.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename dns @ 07/01/23 12:49:05.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:49:05.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:49:05.759
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 07/01/23 12:49:05.774
  Jul  1 12:49:05.789: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9837  8abbc96c-19c7-4bf1-9a18-e13e0b51eb1d 19543 0 2023-07-01 12:49:05 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-07-01 12:49:05 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2xd9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2xd9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 07/01/23 12:49:07.804
  Jul  1 12:49:07.804: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9837 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 12:49:07.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:49:07.805: INFO: ExecWithOptions: Clientset creation
  Jul  1 12:49:07.805: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-9837/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 07/01/23 12:49:07.927
  Jul  1 12:49:07.927: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9837 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 12:49:07.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:49:07.927: INFO: ExecWithOptions: Clientset creation
  Jul  1 12:49:07.927: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-9837/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul  1 12:49:08.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  1 12:49:08.065: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-9837" for this suite. @ 07/01/23 12:49:08.082
• [2.385 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 07/01/23 12:49:08.105
  Jul  1 12:49:08.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 12:49:08.107
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:49:08.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:49:08.132
  STEP: Setting up server cert @ 07/01/23 12:49:08.172
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 12:49:08.65
  STEP: Deploying the webhook pod @ 07/01/23 12:49:08.662
  STEP: Wait for the deployment to be ready @ 07/01/23 12:49:08.676
  Jul  1 12:49:08.686: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/01/23 12:49:10.706
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 12:49:10.733
  Jul  1 12:49:11.734: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 07/01/23 12:49:11.838
  STEP: Creating a configMap that should be mutated @ 07/01/23 12:49:11.855
  STEP: Deleting the collection of validation webhooks @ 07/01/23 12:49:11.907
  STEP: Creating a configMap that should not be mutated @ 07/01/23 12:49:12.003
  Jul  1 12:49:12.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3042" for this suite. @ 07/01/23 12:49:12.116
  STEP: Destroying namespace "webhook-markers-45" for this suite. @ 07/01/23 12:49:12.129
• [4.042 seconds]
------------------------------
SS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 07/01/23 12:49:12.148
  Jul  1 12:49:12.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename containers @ 07/01/23 12:49:12.149
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:49:12.18
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:49:12.186
  STEP: Creating a pod to test override arguments @ 07/01/23 12:49:12.191
  STEP: Saw pod success @ 07/01/23 12:49:16.232
  Jul  1 12:49:16.237: INFO: Trying to get logs from node ip-172-31-91-66 pod client-containers-6dd42c25-4b14-4e3b-a00d-fe3b73dc61ef container agnhost-container: <nil>
  STEP: delete the pod @ 07/01/23 12:49:16.27
  Jul  1 12:49:16.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-3558" for this suite. @ 07/01/23 12:49:16.311
• [4.183 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 07/01/23 12:49:16.331
  Jul  1 12:49:16.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename resourcequota @ 07/01/23 12:49:16.332
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:49:16.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:49:16.374
  STEP: Counting existing ResourceQuota @ 07/01/23 12:49:16.387
  STEP: Creating a ResourceQuota @ 07/01/23 12:49:21.395
  STEP: Ensuring resource quota status is calculated @ 07/01/23 12:49:21.404
  STEP: Creating a Service @ 07/01/23 12:49:23.412
  STEP: Creating a NodePort Service @ 07/01/23 12:49:23.449
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 07/01/23 12:49:23.495
  STEP: Ensuring resource quota status captures service creation @ 07/01/23 12:49:23.537
  STEP: Deleting Services @ 07/01/23 12:49:25.545
  STEP: Ensuring resource quota status released usage @ 07/01/23 12:49:25.627
  Jul  1 12:49:27.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6525" for this suite. @ 07/01/23 12:49:27.64
• [11.319 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 07/01/23 12:49:27.654
  Jul  1 12:49:27.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 12:49:27.655
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:49:27.681
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:49:27.694
  STEP: Creating configMap with name projected-configmap-test-volume-f89bd67b-4caa-4b9c-8864-2885828a8536 @ 07/01/23 12:49:27.704
  STEP: Creating a pod to test consume configMaps @ 07/01/23 12:49:27.716
  STEP: Saw pod success @ 07/01/23 12:49:31.755
  Jul  1 12:49:31.761: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-projected-configmaps-4018434c-b708-42fe-bc92-9c39630be7b0 container agnhost-container: <nil>
  STEP: delete the pod @ 07/01/23 12:49:31.772
  Jul  1 12:49:31.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9781" for this suite. @ 07/01/23 12:49:31.8
• [4.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 07/01/23 12:49:31.815
  Jul  1 12:49:31.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir @ 07/01/23 12:49:31.816
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:49:31.839
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:49:31.844
  STEP: Creating a pod to test emptydir volume type on node default medium @ 07/01/23 12:49:31.85
  STEP: Saw pod success @ 07/01/23 12:49:35.894
  Jul  1 12:49:35.901: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-bcd37a6e-a621-4f7f-a6a2-ce0e6d0448e4 container test-container: <nil>
  STEP: delete the pod @ 07/01/23 12:49:35.912
  Jul  1 12:49:35.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8926" for this suite. @ 07/01/23 12:49:35.941
• [4.139 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 07/01/23 12:49:35.956
  Jul  1 12:49:35.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename var-expansion @ 07/01/23 12:49:35.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:49:35.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:49:35.985
  Jul  1 12:49:38.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  1 12:49:38.036: INFO: Deleting pod "var-expansion-e98e2b21-7494-453e-9931-1ea2b594b111" in namespace "var-expansion-7782"
  Jul  1 12:49:38.053: INFO: Wait up to 5m0s for pod "var-expansion-e98e2b21-7494-453e-9931-1ea2b594b111" to be fully deleted
  STEP: Destroying namespace "var-expansion-7782" for this suite. @ 07/01/23 12:49:40.066
• [4.121 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 07/01/23 12:49:40.078
  Jul  1 12:49:40.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename pods @ 07/01/23 12:49:40.08
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:49:40.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:49:40.127
  STEP: creating a Pod with a static label @ 07/01/23 12:49:40.144
  STEP: watching for Pod to be ready @ 07/01/23 12:49:40.16
  Jul  1 12:49:40.169: INFO: observed Pod pod-test in namespace pods-4486 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Jul  1 12:49:40.169: INFO: observed Pod pod-test in namespace pods-4486 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 12:49:40 +0000 UTC  }]
  Jul  1 12:49:40.194: INFO: observed Pod pod-test in namespace pods-4486 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 12:49:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-01 12:49:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-01 12:49:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 12:49:40 +0000 UTC  }]
  Jul  1 12:49:41.668: INFO: Found Pod pod-test in namespace pods-4486 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 12:49:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 12:49:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 12:49:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 12:49:40 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 07/01/23 12:49:41.674
  STEP: getting the Pod and ensuring that it's patched @ 07/01/23 12:49:41.694
  STEP: replacing the Pod's status Ready condition to False @ 07/01/23 12:49:41.699
  STEP: check the Pod again to ensure its Ready conditions are False @ 07/01/23 12:49:41.721
  STEP: deleting the Pod via a Collection with a LabelSelector @ 07/01/23 12:49:41.721
  STEP: watching for the Pod to be deleted @ 07/01/23 12:49:41.735
  Jul  1 12:49:41.740: INFO: observed event type MODIFIED
  Jul  1 12:49:43.678: INFO: observed event type MODIFIED
  Jul  1 12:49:44.101: INFO: observed event type MODIFIED
  Jul  1 12:49:44.807: INFO: observed event type MODIFIED
  Jul  1 12:49:44.841: INFO: observed event type MODIFIED
  Jul  1 12:49:44.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4486" for this suite. @ 07/01/23 12:49:44.862
• [4.795 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 07/01/23 12:49:44.877
  Jul  1 12:49:44.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/01/23 12:49:44.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:49:44.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:49:44.916
  STEP: set up a multi version CRD @ 07/01/23 12:49:44.921
  Jul  1 12:49:44.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: rename a version @ 07/01/23 12:49:48.921
  STEP: check the new version name is served @ 07/01/23 12:49:48.94
  STEP: check the old version name is removed @ 07/01/23 12:49:50.48
  STEP: check the other version is not changed @ 07/01/23 12:49:51.253
  Jul  1 12:49:54.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8431" for this suite. @ 07/01/23 12:49:54.214
• [9.347 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 07/01/23 12:49:54.225
  Jul  1 12:49:54.225: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename lease-test @ 07/01/23 12:49:54.226
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:49:54.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:49:54.257
  Jul  1 12:49:54.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-6621" for this suite. @ 07/01/23 12:49:54.35
• [0.134 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 07/01/23 12:49:54.36
  Jul  1 12:49:54.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename downward-api @ 07/01/23 12:49:54.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:49:54.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:49:54.391
  STEP: Creating a pod to test downward api env vars @ 07/01/23 12:49:54.394
  STEP: Saw pod success @ 07/01/23 12:49:58.427
  Jul  1 12:49:58.432: INFO: Trying to get logs from node ip-172-31-91-66 pod downward-api-af457de5-456d-4b45-aa5d-1a01b0336ea8 container dapi-container: <nil>
  STEP: delete the pod @ 07/01/23 12:49:58.452
  Jul  1 12:49:58.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-529" for this suite. @ 07/01/23 12:49:58.475
• [4.124 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 07/01/23 12:49:58.485
  Jul  1 12:49:58.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename events @ 07/01/23 12:49:58.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:49:58.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:49:58.524
  STEP: Create set of events @ 07/01/23 12:49:58.541
  STEP: get a list of Events with a label in the current namespace @ 07/01/23 12:49:58.568
  STEP: delete a list of events @ 07/01/23 12:49:58.574
  Jul  1 12:49:58.574: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 07/01/23 12:49:58.608
  Jul  1 12:49:58.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9420" for this suite. @ 07/01/23 12:49:58.619
• [0.143 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 07/01/23 12:49:58.629
  Jul  1 12:49:58.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename dns @ 07/01/23 12:49:58.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:49:58.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:49:58.682
  STEP: Creating a test externalName service @ 07/01/23 12:49:58.687
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3384.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3384.svc.cluster.local; sleep 1; done
   @ 07/01/23 12:49:58.694
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3384.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3384.svc.cluster.local; sleep 1; done
   @ 07/01/23 12:49:58.694
  STEP: creating a pod to probe DNS @ 07/01/23 12:49:58.694
  STEP: submitting the pod to kubernetes @ 07/01/23 12:49:58.694
  STEP: retrieving the pod @ 07/01/23 12:50:02.726
  STEP: looking for the results for each expected name from probers @ 07/01/23 12:50:02.732
  Jul  1 12:50:02.743: INFO: DNS probes using dns-test-bb015f68-d0da-4b6f-8097-f1d025917c38 succeeded

  STEP: changing the externalName to bar.example.com @ 07/01/23 12:50:02.743
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3384.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3384.svc.cluster.local; sleep 1; done
   @ 07/01/23 12:50:02.759
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3384.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3384.svc.cluster.local; sleep 1; done
   @ 07/01/23 12:50:02.759
  STEP: creating a second pod to probe DNS @ 07/01/23 12:50:02.759
  STEP: submitting the pod to kubernetes @ 07/01/23 12:50:02.759
  STEP: retrieving the pod @ 07/01/23 12:50:12.808
  STEP: looking for the results for each expected name from probers @ 07/01/23 12:50:12.813
  Jul  1 12:50:12.824: INFO: DNS probes using dns-test-6574b229-0974-4076-b72b-4dba13253eb1 succeeded

  STEP: changing the service to type=ClusterIP @ 07/01/23 12:50:12.824
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3384.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3384.svc.cluster.local; sleep 1; done
   @ 07/01/23 12:50:12.852
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3384.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3384.svc.cluster.local; sleep 1; done
   @ 07/01/23 12:50:12.852
  STEP: creating a third pod to probe DNS @ 07/01/23 12:50:12.852
  STEP: submitting the pod to kubernetes @ 07/01/23 12:50:12.858
  STEP: retrieving the pod @ 07/01/23 12:50:14.89
  STEP: looking for the results for each expected name from probers @ 07/01/23 12:50:14.895
  Jul  1 12:50:14.911: INFO: DNS probes using dns-test-f32e7131-14f8-4745-adbd-11df4b28b434 succeeded

  Jul  1 12:50:14.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/01/23 12:50:14.917
  STEP: deleting the pod @ 07/01/23 12:50:14.934
  STEP: deleting the pod @ 07/01/23 12:50:14.958
  STEP: deleting the test externalName service @ 07/01/23 12:50:14.98
  STEP: Destroying namespace "dns-3384" for this suite. @ 07/01/23 12:50:15.01
• [16.396 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 07/01/23 12:50:15.04
  Jul  1 12:50:15.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/01/23 12:50:15.041
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:50:15.077
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:50:15.107
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 07/01/23 12:50:15.133
  Jul  1 12:50:15.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 07/01/23 12:50:21.262
  Jul  1 12:50:21.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:50:22.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:50:29.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9958" for this suite. @ 07/01/23 12:50:29.06
• [14.033 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 07/01/23 12:50:29.074
  Jul  1 12:50:29.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename subpath @ 07/01/23 12:50:29.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:50:29.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:50:29.111
  STEP: Setting up data @ 07/01/23 12:50:29.117
  STEP: Creating pod pod-subpath-test-downwardapi-8rf5 @ 07/01/23 12:50:29.139
  STEP: Creating a pod to test atomic-volume-subpath @ 07/01/23 12:50:29.139
  STEP: Saw pod success @ 07/01/23 12:50:53.254
  Jul  1 12:50:53.261: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-subpath-test-downwardapi-8rf5 container test-container-subpath-downwardapi-8rf5: <nil>
  STEP: delete the pod @ 07/01/23 12:50:53.274
  STEP: Deleting pod pod-subpath-test-downwardapi-8rf5 @ 07/01/23 12:50:53.296
  Jul  1 12:50:53.296: INFO: Deleting pod "pod-subpath-test-downwardapi-8rf5" in namespace "subpath-9298"
  Jul  1 12:50:53.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9298" for this suite. @ 07/01/23 12:50:53.309
• [24.355 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 07/01/23 12:50:53.432
  Jul  1 12:50:53.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename dns @ 07/01/23 12:50:53.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:50:53.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:50:53.465
  STEP: Creating a test headless service @ 07/01/23 12:50:53.476
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9133.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9133.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9133.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9133.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9133.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9133.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9133.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9133.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9133.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9133.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9133.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9133.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 191.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.191_udp@PTR;check="$$(dig +tcp +noall +answer +search 191.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.191_tcp@PTR;sleep 1; done
   @ 07/01/23 12:50:53.504
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9133.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9133.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9133.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9133.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9133.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9133.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9133.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9133.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9133.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9133.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9133.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9133.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 191.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.191_udp@PTR;check="$$(dig +tcp +noall +answer +search 191.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.191_tcp@PTR;sleep 1; done
   @ 07/01/23 12:50:53.504
  STEP: creating a pod to probe DNS @ 07/01/23 12:50:53.504
  STEP: submitting the pod to kubernetes @ 07/01/23 12:50:53.505
  STEP: retrieving the pod @ 07/01/23 12:50:55.552
  STEP: looking for the results for each expected name from probers @ 07/01/23 12:50:55.557
  Jul  1 12:50:55.566: INFO: Unable to read wheezy_udp@dns-test-service.dns-9133.svc.cluster.local from pod dns-9133/dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae: the server could not find the requested resource (get pods dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae)
  Jul  1 12:50:55.573: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9133.svc.cluster.local from pod dns-9133/dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae: the server could not find the requested resource (get pods dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae)
  Jul  1 12:50:55.583: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9133.svc.cluster.local from pod dns-9133/dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae: the server could not find the requested resource (get pods dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae)
  Jul  1 12:50:55.592: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9133.svc.cluster.local from pod dns-9133/dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae: the server could not find the requested resource (get pods dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae)
  Jul  1 12:50:55.630: INFO: Unable to read jessie_udp@dns-test-service.dns-9133.svc.cluster.local from pod dns-9133/dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae: the server could not find the requested resource (get pods dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae)
  Jul  1 12:50:55.637: INFO: Unable to read jessie_tcp@dns-test-service.dns-9133.svc.cluster.local from pod dns-9133/dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae: the server could not find the requested resource (get pods dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae)
  Jul  1 12:50:55.645: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9133.svc.cluster.local from pod dns-9133/dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae: the server could not find the requested resource (get pods dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae)
  Jul  1 12:50:55.651: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9133.svc.cluster.local from pod dns-9133/dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae: the server could not find the requested resource (get pods dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae)
  Jul  1 12:50:55.679: INFO: Lookups using dns-9133/dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae failed for: [wheezy_udp@dns-test-service.dns-9133.svc.cluster.local wheezy_tcp@dns-test-service.dns-9133.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9133.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9133.svc.cluster.local jessie_udp@dns-test-service.dns-9133.svc.cluster.local jessie_tcp@dns-test-service.dns-9133.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9133.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9133.svc.cluster.local]

  Jul  1 12:51:00.878: INFO: DNS probes using dns-9133/dns-test-f29ab801-605c-4701-9ef3-3b6957b24dae succeeded

  Jul  1 12:51:00.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/01/23 12:51:00.886
  STEP: deleting the test service @ 07/01/23 12:51:00.919
  STEP: deleting the test headless service @ 07/01/23 12:51:01.032
  STEP: Destroying namespace "dns-9133" for this suite. @ 07/01/23 12:51:01.062
• [7.643 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 07/01/23 12:51:01.076
  Jul  1 12:51:01.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename downward-api @ 07/01/23 12:51:01.077
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:51:01.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:51:01.1
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 12:51:01.105
  STEP: Saw pod success @ 07/01/23 12:51:05.149
  Jul  1 12:51:05.155: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-2a00ec8c-4b27-4ea3-8bb6-c35f4fd07288 container client-container: <nil>
  STEP: delete the pod @ 07/01/23 12:51:05.172
  Jul  1 12:51:05.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7093" for this suite. @ 07/01/23 12:51:05.201
• [4.136 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 07/01/23 12:51:05.213
  Jul  1 12:51:05.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl @ 07/01/23 12:51:05.214
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:51:05.242
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:51:05.256
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/01/23 12:51:05.266
  Jul  1 12:51:05.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5915 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jul  1 12:51:05.385: INFO: stderr: ""
  Jul  1 12:51:05.385: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 07/01/23 12:51:05.385
  Jul  1 12:51:05.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5915 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Jul  1 12:51:05.491: INFO: stderr: ""
  Jul  1 12:51:05.492: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/01/23 12:51:05.492
  Jul  1 12:51:05.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5915 delete pods e2e-test-httpd-pod'
  Jul  1 12:51:08.138: INFO: stderr: ""
  Jul  1 12:51:08.138: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul  1 12:51:08.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5915" for this suite. @ 07/01/23 12:51:08.147
• [2.955 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 07/01/23 12:51:08.168
  Jul  1 12:51:08.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename svcaccounts @ 07/01/23 12:51:08.169
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:51:08.192
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:51:08.198
  STEP: creating a ServiceAccount @ 07/01/23 12:51:08.203
  STEP: watching for the ServiceAccount to be added @ 07/01/23 12:51:08.233
  STEP: patching the ServiceAccount @ 07/01/23 12:51:08.239
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 07/01/23 12:51:08.248
  STEP: deleting the ServiceAccount @ 07/01/23 12:51:08.255
  Jul  1 12:51:08.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3067" for this suite. @ 07/01/23 12:51:08.288
• [0.130 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 07/01/23 12:51:08.3
  Jul  1 12:51:08.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename daemonsets @ 07/01/23 12:51:08.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:51:08.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:51:08.351
  Jul  1 12:51:08.393: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 07/01/23 12:51:08.406
  Jul  1 12:51:08.411: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 12:51:08.411: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 07/01/23 12:51:08.411
  Jul  1 12:51:08.450: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 12:51:08.450: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 12:51:09.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 12:51:09.460: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 12:51:10.456: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  1 12:51:10.456: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 07/01/23 12:51:10.462
  Jul  1 12:51:10.496: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  1 12:51:10.496: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  Jul  1 12:51:11.505: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 12:51:11.505: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 07/01/23 12:51:11.505
  Jul  1 12:51:11.524: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 12:51:11.524: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 12:51:12.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 12:51:12.532: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 12:51:13.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  1 12:51:13.532: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/01/23 12:51:13.546
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1904, will wait for the garbage collector to delete the pods @ 07/01/23 12:51:13.546
  Jul  1 12:51:13.618: INFO: Deleting DaemonSet.extensions daemon-set took: 13.75734ms
  Jul  1 12:51:13.719: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.018484ms
  Jul  1 12:51:15.330: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 12:51:15.330: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul  1 12:51:15.335: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20696"},"items":null}

  Jul  1 12:51:15.341: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20696"},"items":null}

  Jul  1 12:51:15.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1904" for this suite. @ 07/01/23 12:51:15.398
• [7.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 07/01/23 12:51:15.431
  Jul  1 12:51:15.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename resourcequota @ 07/01/23 12:51:15.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:51:15.482
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:51:15.493
  STEP: Creating a ResourceQuota @ 07/01/23 12:51:15.51
  STEP: Getting a ResourceQuota @ 07/01/23 12:51:15.526
  STEP: Listing all ResourceQuotas with LabelSelector @ 07/01/23 12:51:15.551
  STEP: Patching the ResourceQuota @ 07/01/23 12:51:15.56
  STEP: Deleting a Collection of ResourceQuotas @ 07/01/23 12:51:15.571
  STEP: Verifying the deleted ResourceQuota @ 07/01/23 12:51:15.594
  Jul  1 12:51:15.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6279" for this suite. @ 07/01/23 12:51:15.609
• [0.187 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 07/01/23 12:51:15.619
  Jul  1 12:51:15.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename prestop @ 07/01/23 12:51:15.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:51:15.648
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:51:15.653
  STEP: Creating server pod server in namespace prestop-718 @ 07/01/23 12:51:15.675
  STEP: Waiting for pods to come up. @ 07/01/23 12:51:15.692
  STEP: Creating tester pod tester in namespace prestop-718 @ 07/01/23 12:51:17.709
  STEP: Deleting pre-stop pod @ 07/01/23 12:51:19.735
  Jul  1 12:51:24.756: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Jul  1 12:51:24.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 07/01/23 12:51:24.764
  STEP: Destroying namespace "prestop-718" for this suite. @ 07/01/23 12:51:24.786
• [9.180 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 07/01/23 12:51:24.799
  Jul  1 12:51:24.799: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/01/23 12:51:24.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:51:24.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:51:24.833
  STEP: fetching the /apis discovery document @ 07/01/23 12:51:24.84
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 07/01/23 12:51:24.842
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 07/01/23 12:51:24.842
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 07/01/23 12:51:24.842
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 07/01/23 12:51:24.844
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 07/01/23 12:51:24.844
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 07/01/23 12:51:24.846
  Jul  1 12:51:24.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1156" for this suite. @ 07/01/23 12:51:24.851
• [0.064 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 07/01/23 12:51:24.865
  Jul  1 12:51:24.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/01/23 12:51:24.866
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:51:24.894
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:51:24.903
  Jul  1 12:51:24.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 07/01/23 12:51:26.317
  Jul  1 12:51:26.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-7901 --namespace=crd-publish-openapi-7901 create -f -'
  Jul  1 12:51:27.241: INFO: stderr: ""
  Jul  1 12:51:27.241: INFO: stdout: "e2e-test-crd-publish-openapi-276-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jul  1 12:51:27.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-7901 --namespace=crd-publish-openapi-7901 delete e2e-test-crd-publish-openapi-276-crds test-foo'
  Jul  1 12:51:27.343: INFO: stderr: ""
  Jul  1 12:51:27.343: INFO: stdout: "e2e-test-crd-publish-openapi-276-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Jul  1 12:51:27.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-7901 --namespace=crd-publish-openapi-7901 apply -f -'
  Jul  1 12:51:28.270: INFO: stderr: ""
  Jul  1 12:51:28.270: INFO: stdout: "e2e-test-crd-publish-openapi-276-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jul  1 12:51:28.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-7901 --namespace=crd-publish-openapi-7901 delete e2e-test-crd-publish-openapi-276-crds test-foo'
  Jul  1 12:51:28.365: INFO: stderr: ""
  Jul  1 12:51:28.365: INFO: stdout: "e2e-test-crd-publish-openapi-276-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 07/01/23 12:51:28.365
  Jul  1 12:51:28.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-7901 --namespace=crd-publish-openapi-7901 create -f -'
  Jul  1 12:51:28.804: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 07/01/23 12:51:28.804
  Jul  1 12:51:28.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-7901 --namespace=crd-publish-openapi-7901 create -f -'
  Jul  1 12:51:29.241: INFO: rc: 1
  Jul  1 12:51:29.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-7901 --namespace=crd-publish-openapi-7901 apply -f -'
  Jul  1 12:51:30.007: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 07/01/23 12:51:30.007
  Jul  1 12:51:30.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-7901 --namespace=crd-publish-openapi-7901 create -f -'
  Jul  1 12:51:30.357: INFO: rc: 1
  Jul  1 12:51:30.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-7901 --namespace=crd-publish-openapi-7901 apply -f -'
  Jul  1 12:51:30.930: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 07/01/23 12:51:30.93
  Jul  1 12:51:30.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-7901 explain e2e-test-crd-publish-openapi-276-crds'
  Jul  1 12:51:31.270: INFO: stderr: ""
  Jul  1 12:51:31.270: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-276-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 07/01/23 12:51:31.271
  Jul  1 12:51:31.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-7901 explain e2e-test-crd-publish-openapi-276-crds.metadata'
  Jul  1 12:51:31.640: INFO: stderr: ""
  Jul  1 12:51:31.640: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-276-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Jul  1 12:51:31.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-7901 explain e2e-test-crd-publish-openapi-276-crds.spec'
  Jul  1 12:51:32.105: INFO: stderr: ""
  Jul  1 12:51:32.105: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-276-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Jul  1 12:51:32.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-7901 explain e2e-test-crd-publish-openapi-276-crds.spec.bars'
  Jul  1 12:51:32.611: INFO: stderr: ""
  Jul  1 12:51:32.611: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-276-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 07/01/23 12:51:32.612
  Jul  1 12:51:32.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-7901 explain e2e-test-crd-publish-openapi-276-crds.spec.bars2'
  Jul  1 12:51:33.155: INFO: rc: 1
  Jul  1 12:51:34.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7901" for this suite. @ 07/01/23 12:51:34.663
• [9.807 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 07/01/23 12:51:34.673
  Jul  1 12:51:34.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename secrets @ 07/01/23 12:51:34.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:51:34.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:51:34.71
  STEP: Creating secret with name s-test-opt-del-4a5266c3-414f-480c-9330-c3ecb2b0542b @ 07/01/23 12:51:34.719
  STEP: Creating secret with name s-test-opt-upd-fa5f6395-ef60-4d99-95fa-547b4af7d8c0 @ 07/01/23 12:51:34.726
  STEP: Creating the pod @ 07/01/23 12:51:34.731
  STEP: Deleting secret s-test-opt-del-4a5266c3-414f-480c-9330-c3ecb2b0542b @ 07/01/23 12:51:36.795
  STEP: Updating secret s-test-opt-upd-fa5f6395-ef60-4d99-95fa-547b4af7d8c0 @ 07/01/23 12:51:36.806
  STEP: Creating secret with name s-test-opt-create-ca387618-9bc0-47b7-8da3-c733894fb89d @ 07/01/23 12:51:36.814
  STEP: waiting to observe update in volume @ 07/01/23 12:51:36.82
  Jul  1 12:52:41.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2726" for this suite. @ 07/01/23 12:52:41.249
• [66.586 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 07/01/23 12:52:41.259
  Jul  1 12:52:41.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename dns @ 07/01/23 12:52:41.261
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:52:41.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:52:41.291
  STEP: Creating a test headless service @ 07/01/23 12:52:41.294
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3070.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3070.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 07/01/23 12:52:41.302
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3070.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3070.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 07/01/23 12:52:41.302
  STEP: creating a pod to probe DNS @ 07/01/23 12:52:41.302
  STEP: submitting the pod to kubernetes @ 07/01/23 12:52:41.303
  STEP: retrieving the pod @ 07/01/23 12:52:43.328
  STEP: looking for the results for each expected name from probers @ 07/01/23 12:52:43.332
  Jul  1 12:52:43.354: INFO: DNS probes using dns-3070/dns-test-76760951-6406-4b23-a7af-91db5a797ebf succeeded

  Jul  1 12:52:43.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/01/23 12:52:43.359
  STEP: deleting the test headless service @ 07/01/23 12:52:43.384
  STEP: Destroying namespace "dns-3070" for this suite. @ 07/01/23 12:52:43.401
• [2.150 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 07/01/23 12:52:43.412
  Jul  1 12:52:43.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/01/23 12:52:43.413
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:52:43.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:52:43.442
  STEP: create the container to handle the HTTPGet hook request. @ 07/01/23 12:52:43.451
  STEP: create the pod with lifecycle hook @ 07/01/23 12:52:45.491
  STEP: delete the pod with lifecycle hook @ 07/01/23 12:52:47.518
  STEP: check prestop hook @ 07/01/23 12:52:49.54
  Jul  1 12:52:49.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9973" for this suite. @ 07/01/23 12:52:49.572
• [6.169 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 07/01/23 12:52:49.58
  Jul  1 12:52:49.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-probe @ 07/01/23 12:52:49.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:52:49.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:52:49.61
  Jul  1 12:53:49.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-539" for this suite. @ 07/01/23 12:53:49.635
• [60.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 07/01/23 12:53:49.646
  Jul  1 12:53:49.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename configmap @ 07/01/23 12:53:49.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:53:49.673
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:53:49.677
  STEP: Creating configMap with name configmap-test-volume-cbd875af-d57a-49df-aaad-9c2754137de7 @ 07/01/23 12:53:49.681
  STEP: Creating a pod to test consume configMaps @ 07/01/23 12:53:49.692
  STEP: Saw pod success @ 07/01/23 12:53:53.723
  Jul  1 12:53:53.729: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-configmaps-4fcd2366-4f01-4686-a4df-c231e3025a2c container agnhost-container: <nil>
  STEP: delete the pod @ 07/01/23 12:53:53.738
  Jul  1 12:53:53.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4152" for this suite. @ 07/01/23 12:53:53.762
• [4.125 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 07/01/23 12:53:53.773
  Jul  1 12:53:53.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename downward-api @ 07/01/23 12:53:53.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:53:53.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:53:53.805
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 12:53:53.809
  STEP: Saw pod success @ 07/01/23 12:53:57.841
  Jul  1 12:53:57.847: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-c8291797-bae4-4f6e-aa68-6c81336276e1 container client-container: <nil>
  STEP: delete the pod @ 07/01/23 12:53:57.858
  Jul  1 12:53:57.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9506" for this suite. @ 07/01/23 12:53:57.89
• [4.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 07/01/23 12:53:57.912
  Jul  1 12:53:57.912: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename disruption @ 07/01/23 12:53:57.913
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:53:57.942
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:53:57.947
  STEP: creating the pdb @ 07/01/23 12:53:57.957
  STEP: Waiting for the pdb to be processed @ 07/01/23 12:53:57.965
  STEP: updating the pdb @ 07/01/23 12:53:59.979
  STEP: Waiting for the pdb to be processed @ 07/01/23 12:53:59.991
  STEP: patching the pdb @ 07/01/23 12:53:59.999
  STEP: Waiting for the pdb to be processed @ 07/01/23 12:54:00.014
  STEP: Waiting for the pdb to be deleted @ 07/01/23 12:54:00.035
  Jul  1 12:54:00.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4941" for this suite. @ 07/01/23 12:54:00.049
• [2.149 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 07/01/23 12:54:00.062
  Jul  1 12:54:00.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename services @ 07/01/23 12:54:00.063
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:54:00.098
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:54:00.104
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-3713 @ 07/01/23 12:54:00.109
  STEP: changing the ExternalName service to type=ClusterIP @ 07/01/23 12:54:00.115
  STEP: creating replication controller externalname-service in namespace services-3713 @ 07/01/23 12:54:00.151
  I0701 12:54:00.165191      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-3713, replica count: 2
  I0701 12:54:03.216068      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  1 12:54:03.216: INFO: Creating new exec pod
  Jul  1 12:54:06.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-3713 exec execpodjmxrp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul  1 12:54:06.448: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul  1 12:54:06.448: INFO: stdout: "externalname-service-nk2bf"
  Jul  1 12:54:06.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-3713 exec execpodjmxrp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.186 80'
  Jul  1 12:54:06.678: INFO: stderr: "+ nc -v -t -w 2 10.152.183.186 80\nConnection to 10.152.183.186 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jul  1 12:54:06.678: INFO: stdout: "externalname-service-nk2bf"
  Jul  1 12:54:06.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  1 12:54:06.685: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-3713" for this suite. @ 07/01/23 12:54:06.708
• [6.665 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 07/01/23 12:54:06.732
  Jul  1 12:54:06.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-runtime @ 07/01/23 12:54:06.738
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:54:06.773
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:54:06.786
  STEP: create the container @ 07/01/23 12:54:06.793
  W0701 12:54:06.812495      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/01/23 12:54:06.812
  STEP: get the container status @ 07/01/23 12:54:10.846
  STEP: the container should be terminated @ 07/01/23 12:54:10.852
  STEP: the termination message should be set @ 07/01/23 12:54:10.852
  Jul  1 12:54:10.852: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 07/01/23 12:54:10.852
  Jul  1 12:54:10.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7438" for this suite. @ 07/01/23 12:54:10.884
• [4.162 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 07/01/23 12:54:10.895
  Jul  1 12:54:10.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename resourcequota @ 07/01/23 12:54:10.897
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:54:10.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:54:10.927
  STEP: Creating a ResourceQuota with terminating scope @ 07/01/23 12:54:10.931
  STEP: Ensuring ResourceQuota status is calculated @ 07/01/23 12:54:10.937
  STEP: Creating a ResourceQuota with not terminating scope @ 07/01/23 12:54:12.945
  STEP: Ensuring ResourceQuota status is calculated @ 07/01/23 12:54:12.953
  STEP: Creating a long running pod @ 07/01/23 12:54:14.96
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 07/01/23 12:54:14.979
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 07/01/23 12:54:16.987
  STEP: Deleting the pod @ 07/01/23 12:54:18.996
  STEP: Ensuring resource quota status released the pod usage @ 07/01/23 12:54:19.012
  STEP: Creating a terminating pod @ 07/01/23 12:54:21.017
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 07/01/23 12:54:21.03
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 07/01/23 12:54:23.038
  STEP: Deleting the pod @ 07/01/23 12:54:25.043
  STEP: Ensuring resource quota status released the pod usage @ 07/01/23 12:54:25.062
  Jul  1 12:54:27.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6203" for this suite. @ 07/01/23 12:54:27.074
• [16.187 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 07/01/23 12:54:27.084
  Jul  1 12:54:27.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/01/23 12:54:27.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:54:27.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:54:27.11
  STEP: set up a multi version CRD @ 07/01/23 12:54:27.113
  Jul  1 12:54:27.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: mark a version not serverd @ 07/01/23 12:54:31.115
  STEP: check the unserved version gets removed @ 07/01/23 12:54:31.14
  STEP: check the other version is not changed @ 07/01/23 12:54:32.009
  Jul  1 12:54:35.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2042" for this suite. @ 07/01/23 12:54:35.034
• [7.961 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 07/01/23 12:54:35.046
  Jul  1 12:54:35.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename podtemplate @ 07/01/23 12:54:35.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:54:35.076
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:54:35.082
  Jul  1 12:54:35.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-8813" for this suite. @ 07/01/23 12:54:35.155
• [0.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 07/01/23 12:54:35.17
  Jul  1 12:54:35.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/01/23 12:54:35.171
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:54:35.194
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:54:35.2
  Jul  1 12:54:35.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:54:41.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5539" for this suite. @ 07/01/23 12:54:41.598
• [6.439 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 07/01/23 12:54:41.611
  Jul  1 12:54:41.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 12:54:41.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:54:41.64
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:54:41.645
  STEP: Creating projection with secret that has name projected-secret-test-bd575b3e-73ae-450e-94f3-1aba675c4f73 @ 07/01/23 12:54:41.655
  STEP: Creating a pod to test consume secrets @ 07/01/23 12:54:41.665
  STEP: Saw pod success @ 07/01/23 12:54:45.7
  Jul  1 12:54:45.707: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-projected-secrets-36f90e29-e3da-40c8-91d1-5c653a675793 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/01/23 12:54:45.745
  Jul  1 12:54:45.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1161" for this suite. @ 07/01/23 12:54:45.784
• [4.183 seconds]
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 07/01/23 12:54:45.796
  Jul  1 12:54:45.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename statefulset @ 07/01/23 12:54:45.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:54:45.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:54:45.825
  STEP: Creating service test in namespace statefulset-4004 @ 07/01/23 12:54:45.831
  STEP: Creating statefulset ss in namespace statefulset-4004 @ 07/01/23 12:54:45.84
  Jul  1 12:54:45.858: INFO: Found 0 stateful pods, waiting for 1
  Jul  1 12:54:55.867: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 07/01/23 12:54:55.877
  STEP: updating a scale subresource @ 07/01/23 12:54:55.883
  STEP: verifying the statefulset Spec.Replicas was modified @ 07/01/23 12:54:55.894
  STEP: Patch a scale subresource @ 07/01/23 12:54:55.9
  STEP: verifying the statefulset Spec.Replicas was modified @ 07/01/23 12:54:55.924
  Jul  1 12:54:55.934: INFO: Deleting all statefulset in ns statefulset-4004
  Jul  1 12:54:55.943: INFO: Scaling statefulset ss to 0
  Jul  1 12:55:06.025: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  1 12:55:06.031: INFO: Deleting statefulset ss
  Jul  1 12:55:06.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4004" for this suite. @ 07/01/23 12:55:06.078
• [20.292 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 07/01/23 12:55:06.087
  Jul  1 12:55:06.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir @ 07/01/23 12:55:06.088
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:55:06.116
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:55:06.125
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 07/01/23 12:55:06.13
  STEP: Saw pod success @ 07/01/23 12:55:10.167
  Jul  1 12:55:10.171: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-092f4f67-2ba1-4b71-912c-623298abc16c container test-container: <nil>
  STEP: delete the pod @ 07/01/23 12:55:10.183
  Jul  1 12:55:10.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4467" for this suite. @ 07/01/23 12:55:10.226
• [4.148 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 07/01/23 12:55:10.236
  Jul  1 12:55:10.236: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename services @ 07/01/23 12:55:10.237
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:55:10.26
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:55:10.268
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-3338 @ 07/01/23 12:55:10.274
  STEP: changing the ExternalName service to type=NodePort @ 07/01/23 12:55:10.28
  STEP: creating replication controller externalname-service in namespace services-3338 @ 07/01/23 12:55:10.312
  I0701 12:55:10.335098      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-3338, replica count: 2
  I0701 12:55:13.386346      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  1 12:55:13.386: INFO: Creating new exec pod
  Jul  1 12:55:16.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-3338 exec execpodbrgdq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul  1 12:55:16.621: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul  1 12:55:16.621: INFO: stdout: "externalname-service-c8g8v"
  Jul  1 12:55:16.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-3338 exec execpodbrgdq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.238 80'
  Jul  1 12:55:16.805: INFO: stderr: "+ nc -v -t -w 2 10.152.183.238 80\nConnection to 10.152.183.238 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jul  1 12:55:16.805: INFO: stdout: "externalname-service-fw44s"
  Jul  1 12:55:16.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-3338 exec execpodbrgdq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.94 30862'
  Jul  1 12:55:16.994: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.16.94 30862\nConnection to 172.31.16.94 30862 port [tcp/*] succeeded!\n"
  Jul  1 12:55:16.994: INFO: stdout: ""
  Jul  1 12:55:17.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-3338 exec execpodbrgdq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.94 30862'
  Jul  1 12:55:18.242: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.16.94 30862\nConnection to 172.31.16.94 30862 port [tcp/*] succeeded!\n"
  Jul  1 12:55:18.242: INFO: stdout: ""
  Jul  1 12:55:18.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-3338 exec execpodbrgdq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.94 30862'
  Jul  1 12:55:19.204: INFO: stderr: "+ nc -v -t -w 2 172.31.16.94 30862\n+ echo hostName\nConnection to 172.31.16.94 30862 port [tcp/*] succeeded!\n"
  Jul  1 12:55:19.204: INFO: stdout: "externalname-service-fw44s"
  Jul  1 12:55:19.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-3338 exec execpodbrgdq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.91.66 30862'
  Jul  1 12:55:19.383: INFO: stderr: "+ nc -v -t -w 2 172.31.91.66 30862\nConnection to 172.31.91.66 30862 port [tcp/*] succeeded!\n+ echo hostName\n"
  Jul  1 12:55:19.383: INFO: stdout: "externalname-service-fw44s"
  Jul  1 12:55:19.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  1 12:55:19.390: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-3338" for this suite. @ 07/01/23 12:55:19.452
• [9.234 seconds]
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 07/01/23 12:55:19.47
  Jul  1 12:55:19.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename pod-network-test @ 07/01/23 12:55:19.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:55:19.522
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:55:19.538
  STEP: Performing setup for networking test in namespace pod-network-test-5531 @ 07/01/23 12:55:19.544
  STEP: creating a selector @ 07/01/23 12:55:19.544
  STEP: Creating the service pods in kubernetes @ 07/01/23 12:55:19.544
  Jul  1 12:55:19.544: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 07/01/23 12:55:41.8
  Jul  1 12:55:45.864: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul  1 12:55:45.864: INFO: Going to poll 192.168.175.25 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul  1 12:55:45.870: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.175.25:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5531 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 12:55:45.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:55:45.870: INFO: ExecWithOptions: Clientset creation
  Jul  1 12:55:45.870: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-5531/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.175.25%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul  1 12:55:45.984: INFO: Found all 1 expected endpoints: [netserver-0]
  Jul  1 12:55:45.984: INFO: Going to poll 192.168.85.8 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul  1 12:55:45.991: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.85.8:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5531 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 12:55:45.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:55:45.992: INFO: ExecWithOptions: Clientset creation
  Jul  1 12:55:45.992: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-5531/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.85.8%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul  1 12:55:46.082: INFO: Found all 1 expected endpoints: [netserver-1]
  Jul  1 12:55:46.082: INFO: Going to poll 192.168.62.70 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul  1 12:55:46.087: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.62.70:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5531 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 12:55:46.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 12:55:46.087: INFO: ExecWithOptions: Clientset creation
  Jul  1 12:55:46.087: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-5531/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.62.70%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul  1 12:55:46.175: INFO: Found all 1 expected endpoints: [netserver-2]
  Jul  1 12:55:46.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-5531" for this suite. @ 07/01/23 12:55:46.181
• [26.721 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 07/01/23 12:55:46.191
  Jul  1 12:55:46.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 12:55:46.192
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:55:46.231
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:55:46.246
  STEP: Creating the pod @ 07/01/23 12:55:46.251
  Jul  1 12:55:48.839: INFO: Successfully updated pod "annotationupdate98e9dcea-9f14-4443-a7c5-daaff456b805"
  Jul  1 12:55:50.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5083" for this suite. @ 07/01/23 12:55:50.87
• [4.689 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 07/01/23 12:55:50.88
  Jul  1 12:55:50.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename gc @ 07/01/23 12:55:50.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:55:50.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:55:50.931
  Jul  1 12:55:50.995: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"08cd85ad-fa8f-4688-9111-1822e331d559", Controller:(*bool)(0xc004e1d61e), BlockOwnerDeletion:(*bool)(0xc004e1d61f)}}
  Jul  1 12:55:51.010: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"81a1f396-3311-44c8-8596-dc4330a73a1b", Controller:(*bool)(0xc004f40056), BlockOwnerDeletion:(*bool)(0xc004f40057)}}
  Jul  1 12:55:51.021: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"20b576f5-a0b3-4cb6-9008-e8d2f482a996", Controller:(*bool)(0xc004f402e6), BlockOwnerDeletion:(*bool)(0xc004f402e7)}}
  Jul  1 12:55:56.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8509" for this suite. @ 07/01/23 12:55:56.062
• [5.193 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 07/01/23 12:55:56.08
  Jul  1 12:55:56.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename replication-controller @ 07/01/23 12:55:56.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:55:56.112
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:55:56.129
  STEP: Creating ReplicationController "e2e-rc-g67qx" @ 07/01/23 12:55:56.136
  Jul  1 12:55:56.156: INFO: Get Replication Controller "e2e-rc-g67qx" to confirm replicas
  Jul  1 12:55:57.165: INFO: Get Replication Controller "e2e-rc-g67qx" to confirm replicas
  Jul  1 12:55:57.170: INFO: Found 1 replicas for "e2e-rc-g67qx" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-g67qx" @ 07/01/23 12:55:57.171
  STEP: Updating a scale subresource @ 07/01/23 12:55:57.175
  STEP: Verifying replicas where modified for replication controller "e2e-rc-g67qx" @ 07/01/23 12:55:57.186
  Jul  1 12:55:57.186: INFO: Get Replication Controller "e2e-rc-g67qx" to confirm replicas
  Jul  1 12:55:58.195: INFO: Get Replication Controller "e2e-rc-g67qx" to confirm replicas
  Jul  1 12:55:58.201: INFO: Found 2 replicas for "e2e-rc-g67qx" replication controller
  Jul  1 12:55:58.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9650" for this suite. @ 07/01/23 12:55:58.209
• [2.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 07/01/23 12:55:58.222
  Jul  1 12:55:58.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename security-context-test @ 07/01/23 12:55:58.223
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:55:58.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:55:58.264
  Jul  1 12:56:02.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8032" for this suite. @ 07/01/23 12:56:02.314
• [4.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 07/01/23 12:56:02.339
  Jul  1 12:56:02.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename secrets @ 07/01/23 12:56:02.34
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:56:02.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:56:02.428
  STEP: Creating secret with name secret-test-0611ec54-a0c7-4ba4-bd40-73f4f3386a08 @ 07/01/23 12:56:02.47
  STEP: Creating a pod to test consume secrets @ 07/01/23 12:56:02.478
  STEP: Saw pod success @ 07/01/23 12:56:06.52
  Jul  1 12:56:06.525: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-secrets-14403a29-f715-4918-8a33-380a507f10be container secret-volume-test: <nil>
  STEP: delete the pod @ 07/01/23 12:56:06.536
  Jul  1 12:56:06.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3690" for this suite. @ 07/01/23 12:56:06.571
  STEP: Destroying namespace "secret-namespace-4252" for this suite. @ 07/01/23 12:56:06.581
• [4.254 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 07/01/23 12:56:06.594
  Jul  1 12:56:06.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir @ 07/01/23 12:56:06.595
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:56:06.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:56:06.622
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 07/01/23 12:56:06.628
  STEP: Saw pod success @ 07/01/23 12:56:10.672
  Jul  1 12:56:10.678: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-688f0eec-e076-42c0-9f9a-ed95f1f80a08 container test-container: <nil>
  STEP: delete the pod @ 07/01/23 12:56:10.688
  Jul  1 12:56:10.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3209" for this suite. @ 07/01/23 12:56:10.732
• [4.146 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 07/01/23 12:56:10.743
  Jul  1 12:56:10.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 12:56:10.744
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:56:10.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:56:10.771
  STEP: Creating secret with name s-test-opt-del-23988b4a-2716-4b29-b315-280841d3cbcd @ 07/01/23 12:56:10.786
  STEP: Creating secret with name s-test-opt-upd-fd00f956-c440-48fb-8ec7-8fd9ceb2392d @ 07/01/23 12:56:10.794
  STEP: Creating the pod @ 07/01/23 12:56:10.803
  STEP: Deleting secret s-test-opt-del-23988b4a-2716-4b29-b315-280841d3cbcd @ 07/01/23 12:56:14.887
  STEP: Updating secret s-test-opt-upd-fd00f956-c440-48fb-8ec7-8fd9ceb2392d @ 07/01/23 12:56:14.901
  STEP: Creating secret with name s-test-opt-create-b3ffce63-1d41-46b9-9680-6867d800e115 @ 07/01/23 12:56:14.908
  STEP: waiting to observe update in volume @ 07/01/23 12:56:14.919
  Jul  1 12:57:27.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2452" for this suite. @ 07/01/23 12:57:27.492
• [76.763 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 07/01/23 12:57:27.507
  Jul  1 12:57:27.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename namespaces @ 07/01/23 12:57:27.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:57:27.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:57:27.544
  STEP: Creating a test namespace @ 07/01/23 12:57:27.55
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:57:27.577
  STEP: Creating a pod in the namespace @ 07/01/23 12:57:27.584
  STEP: Waiting for the pod to have running status @ 07/01/23 12:57:27.633
  STEP: Deleting the namespace @ 07/01/23 12:57:29.659
  STEP: Waiting for the namespace to be removed. @ 07/01/23 12:57:29.671
  STEP: Recreating the namespace @ 07/01/23 12:57:40.677
  STEP: Verifying there are no pods in the namespace @ 07/01/23 12:57:40.696
  Jul  1 12:57:40.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4663" for this suite. @ 07/01/23 12:57:40.716
  STEP: Destroying namespace "nsdeletetest-9376" for this suite. @ 07/01/23 12:57:40.728
  Jul  1 12:57:40.733: INFO: Namespace nsdeletetest-9376 was already deleted
  STEP: Destroying namespace "nsdeletetest-2463" for this suite. @ 07/01/23 12:57:40.733
• [13.243 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 07/01/23 12:57:40.751
  Jul  1 12:57:40.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename gc @ 07/01/23 12:57:40.752
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:57:40.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:57:40.781
  STEP: create the deployment @ 07/01/23 12:57:40.788
  W0701 12:57:40.803007      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 07/01/23 12:57:40.803
  STEP: delete the deployment @ 07/01/23 12:57:40.927
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 07/01/23 12:57:40.937
  STEP: Gathering metrics @ 07/01/23 12:57:41.496
  W0701 12:57:41.505638      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul  1 12:57:41.505: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul  1 12:57:41.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5059" for this suite. @ 07/01/23 12:57:41.518
• [0.781 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 07/01/23 12:57:41.563
  Jul  1 12:57:41.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl @ 07/01/23 12:57:41.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:57:41.6
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:57:41.61
  Jul  1 12:57:41.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-2545 version'
  Jul  1 12:57:41.825: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Jul  1 12:57:41.825: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.3\", GitCommit:\"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:53:42Z\", GoVersion:\"go1.20.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.3\", GitCommit:\"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", GitTreeState:\"clean\", BuildDate:\"2023-06-15T02:06:40Z\", GoVersion:\"go1.20.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Jul  1 12:57:41.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2545" for this suite. @ 07/01/23 12:57:41.832
• [0.282 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 07/01/23 12:57:41.845
  Jul  1 12:57:41.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename sched-pred @ 07/01/23 12:57:41.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:57:41.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:57:41.878
  Jul  1 12:57:41.885: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul  1 12:57:41.897: INFO: Waiting for terminating namespaces to be deleted...
  Jul  1 12:57:41.905: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-12-125 before test
  Jul  1 12:57:41.918: INFO: simpletest.deployment-7cf547c767-g5b4v from gc-5059 started at 2023-07-01 12:57:40 +0000 UTC (1 container statuses recorded)
  Jul  1 12:57:41.925: INFO: 	Container nginx ready: false, restart count 0
  Jul  1 12:57:41.925: INFO: nginx-ingress-controller-kubernetes-worker-tjncz from ingress-nginx-kubernetes-worker started at 2023-07-01 11:52:18 +0000 UTC (1 container statuses recorded)
  Jul  1 12:57:41.925: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  1 12:57:41.925: INFO: calico-kube-controllers-9c5cff4fb-6hfws from kube-system started at 2023-07-01 11:52:25 +0000 UTC (1 container statuses recorded)
  Jul  1 12:57:41.925: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul  1 12:57:41.925: INFO: sonobuoy-e2e-job-e4b68f70fcf04452 from sonobuoy started at 2023-07-01 11:57:54 +0000 UTC (2 container statuses recorded)
  Jul  1 12:57:41.925: INFO: 	Container e2e ready: true, restart count 0
  Jul  1 12:57:41.925: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  1 12:57:41.925: INFO: sonobuoy-systemd-logs-daemon-set-0aed011ee39b4f14-g2zvl from sonobuoy started at 2023-07-01 11:57:54 +0000 UTC (2 container statuses recorded)
  Jul  1 12:57:41.925: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  1 12:57:41.925: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul  1 12:57:41.925: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-16-94 before test
  Jul  1 12:57:41.935: INFO: nginx-ingress-controller-kubernetes-worker-ps7xr from ingress-nginx-kubernetes-worker started at 2023-07-01 11:52:10 +0000 UTC (1 container statuses recorded)
  Jul  1 12:57:41.936: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  1 12:57:41.937: INFO: coredns-5c7f76ccb8-zxmnn from kube-system started at 2023-07-01 11:52:04 +0000 UTC (1 container statuses recorded)
  Jul  1 12:57:41.938: INFO: 	Container coredns ready: true, restart count 0
  Jul  1 12:57:41.938: INFO: kube-state-metrics-5b95b4459c-5klzn from kube-system started at 2023-07-01 11:52:04 +0000 UTC (1 container statuses recorded)
  Jul  1 12:57:41.939: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul  1 12:57:41.939: INFO: metrics-server-v0.5.2-6cf8c8b69c-lw48q from kube-system started at 2023-07-01 11:52:04 +0000 UTC (2 container statuses recorded)
  Jul  1 12:57:41.939: INFO: 	Container metrics-server ready: true, restart count 0
  Jul  1 12:57:41.939: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul  1 12:57:41.939: INFO: dashboard-metrics-scraper-6b8586b5c9-8wfzw from kubernetes-dashboard started at 2023-07-01 11:52:04 +0000 UTC (1 container statuses recorded)
  Jul  1 12:57:41.939: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul  1 12:57:41.939: INFO: kubernetes-dashboard-6869f4cd5f-f7c25 from kubernetes-dashboard started at 2023-07-01 11:52:04 +0000 UTC (1 container statuses recorded)
  Jul  1 12:57:41.939: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul  1 12:57:41.939: INFO: sonobuoy-systemd-logs-daemon-set-0aed011ee39b4f14-bbwnl from sonobuoy started at 2023-07-01 11:57:54 +0000 UTC (2 container statuses recorded)
  Jul  1 12:57:41.939: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  1 12:57:41.939: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul  1 12:57:41.939: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-91-66 before test
  Jul  1 12:57:41.949: INFO: simpletest.deployment-7cf547c767-g82lv from gc-5059 started at 2023-07-01 12:57:40 +0000 UTC (1 container statuses recorded)
  Jul  1 12:57:41.949: INFO: 	Container nginx ready: false, restart count 0
  Jul  1 12:57:41.950: INFO: default-http-backend-kubernetes-worker-65fc475d49-7llc8 from ingress-nginx-kubernetes-worker started at 2023-07-01 11:52:10 +0000 UTC (1 container statuses recorded)
  Jul  1 12:57:41.950: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul  1 12:57:41.950: INFO: nginx-ingress-controller-kubernetes-worker-rs6r4 from ingress-nginx-kubernetes-worker started at 2023-07-01 11:52:14 +0000 UTC (1 container statuses recorded)
  Jul  1 12:57:41.950: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  1 12:57:41.951: INFO: sonobuoy from sonobuoy started at 2023-07-01 11:57:50 +0000 UTC (1 container statuses recorded)
  Jul  1 12:57:41.951: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul  1 12:57:41.951: INFO: sonobuoy-systemd-logs-daemon-set-0aed011ee39b4f14-g92dx from sonobuoy started at 2023-07-01 11:57:54 +0000 UTC (2 container statuses recorded)
  Jul  1 12:57:41.951: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  1 12:57:41.951: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/01/23 12:57:41.951
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/01/23 12:57:43.995
  STEP: Trying to apply a random label on the found node. @ 07/01/23 12:57:44.017
  STEP: verifying the node has the label kubernetes.io/e2e-67c3db37-8ae2-4dd6-97ee-cca0d79a45f7 42 @ 07/01/23 12:57:44.032
  STEP: Trying to relaunch the pod, now with labels. @ 07/01/23 12:57:44.039
  STEP: removing the label kubernetes.io/e2e-67c3db37-8ae2-4dd6-97ee-cca0d79a45f7 off the node ip-172-31-91-66 @ 07/01/23 12:57:46.083
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-67c3db37-8ae2-4dd6-97ee-cca0d79a45f7 @ 07/01/23 12:57:46.103
  Jul  1 12:57:46.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8118" for this suite. @ 07/01/23 12:57:46.117
• [4.287 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 07/01/23 12:57:46.134
  Jul  1 12:57:46.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 12:57:46.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:57:46.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:57:46.166
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-36c4275b-1034-43a3-83e6-5cd5b12af37b @ 07/01/23 12:57:46.181
  STEP: Creating the pod @ 07/01/23 12:57:46.19
  STEP: Updating configmap projected-configmap-test-upd-36c4275b-1034-43a3-83e6-5cd5b12af37b @ 07/01/23 12:57:48.254
  STEP: waiting to observe update in volume @ 07/01/23 12:57:48.261
  Jul  1 12:58:54.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3994" for this suite. @ 07/01/23 12:58:54.739
• [68.628 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 07/01/23 12:58:54.762
  Jul  1 12:58:54.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename svcaccounts @ 07/01/23 12:58:54.763
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:58:54.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:58:54.806
  STEP: Creating a pod to test service account token:  @ 07/01/23 12:58:54.817
  STEP: Saw pod success @ 07/01/23 12:58:58.855
  Jul  1 12:58:58.860: INFO: Trying to get logs from node ip-172-31-91-66 pod test-pod-d3920d1c-012c-420d-b3d8-c5eb17bf5905 container agnhost-container: <nil>
  STEP: delete the pod @ 07/01/23 12:58:58.871
  Jul  1 12:58:58.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9153" for this suite. @ 07/01/23 12:58:58.903
• [4.153 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 07/01/23 12:58:58.916
  Jul  1 12:58:58.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename secrets @ 07/01/23 12:58:58.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:58:58.94
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:58:58.945
  STEP: Creating secret with name secret-test-e485b63e-3a37-4fa3-8abf-5fde2fb75428 @ 07/01/23 12:58:58.951
  STEP: Creating a pod to test consume secrets @ 07/01/23 12:58:58.959
  STEP: Saw pod success @ 07/01/23 12:59:03.011
  Jul  1 12:59:03.020: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-secrets-d328ed88-0139-4d79-8228-72feef85cb88 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/01/23 12:59:03.036
  Jul  1 12:59:03.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2603" for this suite. @ 07/01/23 12:59:03.072
• [4.169 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 07/01/23 12:59:03.091
  Jul  1 12:59:03.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename services @ 07/01/23 12:59:03.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:59:03.139
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:59:03.148
  STEP: creating service nodeport-test with type=NodePort in namespace services-5432 @ 07/01/23 12:59:03.154
  STEP: creating replication controller nodeport-test in namespace services-5432 @ 07/01/23 12:59:03.181
  I0701 12:59:03.197451      19 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-5432, replica count: 2
  I0701 12:59:06.249313      19 runners.go:194] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0701 12:59:09.251463      19 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  1 12:59:09.251: INFO: Creating new exec pod
  Jul  1 12:59:12.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-5432 exec execpod9cb64 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jul  1 12:59:12.483: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jul  1 12:59:12.483: INFO: stdout: ""
  Jul  1 12:59:13.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-5432 exec execpod9cb64 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jul  1 12:59:13.650: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jul  1 12:59:13.650: INFO: stdout: ""
  Jul  1 12:59:14.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-5432 exec execpod9cb64 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jul  1 12:59:14.713: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jul  1 12:59:14.713: INFO: stdout: ""
  Jul  1 12:59:15.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-5432 exec execpod9cb64 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jul  1 12:59:15.695: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jul  1 12:59:15.696: INFO: stdout: "nodeport-test-f7pck"
  Jul  1 12:59:15.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-5432 exec execpod9cb64 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.185 80'
  Jul  1 12:59:15.937: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.185 80\nConnection to 10.152.183.185 80 port [tcp/http] succeeded!\n"
  Jul  1 12:59:15.937: INFO: stdout: "nodeport-test-77jch"
  Jul  1 12:59:15.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-5432 exec execpod9cb64 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.12.125 31440'
  Jul  1 12:59:16.129: INFO: stderr: "+ nc -v -t -w 2 172.31.12.125 31440\n+ echo hostName\nConnection to 172.31.12.125 31440 port [tcp/*] succeeded!\n"
  Jul  1 12:59:16.129: INFO: stdout: "nodeport-test-f7pck"
  Jul  1 12:59:16.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-5432 exec execpod9cb64 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.91.66 31440'
  Jul  1 12:59:16.327: INFO: stderr: "+ nc -v -t -w 2 172.31.91.66 31440\n+ echo hostName\nConnection to 172.31.91.66 31440 port [tcp/*] succeeded!\n"
  Jul  1 12:59:16.327: INFO: stdout: "nodeport-test-77jch"
  Jul  1 12:59:16.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5432" for this suite. @ 07/01/23 12:59:16.332
• [13.250 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 07/01/23 12:59:16.343
  Jul  1 12:59:16.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubelet-test @ 07/01/23 12:59:16.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:59:16.373
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:59:16.38
  Jul  1 12:59:18.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4949" for this suite. @ 07/01/23 12:59:18.443
• [2.110 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 07/01/23 12:59:18.453
  Jul  1 12:59:18.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-runtime @ 07/01/23 12:59:18.454
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:59:18.477
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:59:18.489
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 07/01/23 12:59:18.515
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 07/01/23 12:59:32.635
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 07/01/23 12:59:32.641
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 07/01/23 12:59:32.656
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 07/01/23 12:59:32.656
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 07/01/23 12:59:32.689
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 07/01/23 12:59:35.716
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 07/01/23 12:59:36.73
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 07/01/23 12:59:36.741
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 07/01/23 12:59:36.741
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 07/01/23 12:59:36.786
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 07/01/23 12:59:37.8
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 07/01/23 12:59:39.82
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 07/01/23 12:59:39.835
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 07/01/23 12:59:39.835
  Jul  1 12:59:39.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7852" for this suite. @ 07/01/23 12:59:39.885
• [21.449 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 07/01/23 12:59:39.902
  Jul  1 12:59:39.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename daemonsets @ 07/01/23 12:59:39.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:59:39.929
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:59:39.937
  STEP: Creating simple DaemonSet "daemon-set" @ 07/01/23 12:59:39.993
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/01/23 12:59:40.009
  Jul  1 12:59:40.023: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:59:40.023: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:59:40.030: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 12:59:40.030: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 12:59:41.039: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:59:41.039: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:59:41.046: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 12:59:41.046: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 12:59:42.040: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:59:42.040: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:59:42.046: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  1 12:59:42.046: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 07/01/23 12:59:42.052
  STEP: DeleteCollection of the DaemonSets @ 07/01/23 12:59:42.059
  STEP: Verify that ReplicaSets have been deleted @ 07/01/23 12:59:42.076
  Jul  1 12:59:42.117: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23683"},"items":null}

  Jul  1 12:59:42.126: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23684"},"items":[{"metadata":{"name":"daemon-set-4qdqc","generateName":"daemon-set-","namespace":"daemonsets-55","uid":"5c2155d2-6140-43b6-91b5-83b46c784a6f","resourceVersion":"23684","creationTimestamp":"2023-07-01T12:59:40Z","deletionTimestamp":"2023-07-01T13:00:12Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"eb69d529-60da-4882-8c83-83adf0d0b479","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-01T12:59:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb69d529-60da-4882-8c83-83adf0d0b479\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-01T12:59:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.85.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-7mkgr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-7mkgr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-16-94","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-16-94"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-01T12:59:40Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-01T12:59:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-01T12:59:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-01T12:59:40Z"}],"hostIP":"172.31.16.94","podIP":"192.168.85.9","podIPs":[{"ip":"192.168.85.9"}],"startTime":"2023-07-01T12:59:40Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-01T12:59:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://b48d720dc0d7e167f5260710fdeac4e3458cf84d27e0a0c7f0a9b76cb861fbd6","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-4s5vl","generateName":"daemon-set-","namespace":"daemonsets-55","uid":"12cbd57b-0948-4e06-a596-18d612d6ff9d","resourceVersion":"23682","creationTimestamp":"2023-07-01T12:59:40Z","deletionTimestamp":"2023-07-01T13:00:12Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"eb69d529-60da-4882-8c83-83adf0d0b479","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-01T12:59:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb69d529-60da-4882-8c83-83adf0d0b479\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-01T12:59:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.175.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-tc7bz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-tc7bz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-12-125","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-12-125"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-01T12:59:40Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-01T12:59:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-01T12:59:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-01T12:59:40Z"}],"hostIP":"172.31.12.125","podIP":"192.168.175.33","podIPs":[{"ip":"192.168.175.33"}],"startTime":"2023-07-01T12:59:40Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-01T12:59:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://5ede286f2b4133b3effed76fb570f0015a7577fc7935aca587fc91e771bf074a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hlj6n","generateName":"daemon-set-","namespace":"daemonsets-55","uid":"d0024279-d189-45eb-988e-f0d4aeee4db1","resourceVersion":"23681","creationTimestamp":"2023-07-01T12:59:40Z","deletionTimestamp":"2023-07-01T13:00:12Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"eb69d529-60da-4882-8c83-83adf0d0b479","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-01T12:59:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb69d529-60da-4882-8c83-83adf0d0b479\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-01T12:59:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.62.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kmxrz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kmxrz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-91-66","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-91-66"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-01T12:59:40Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-01T12:59:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-01T12:59:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-01T12:59:40Z"}],"hostIP":"172.31.91.66","podIP":"192.168.62.84","podIPs":[{"ip":"192.168.62.84"}],"startTime":"2023-07-01T12:59:40Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-01T12:59:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://81a623f93508a3e94222979e3e313fddec744fbdbd5275a3685ade4af58d9cfc","started":true}],"qosClass":"BestEffort"}}]}

  Jul  1 12:59:42.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-55" for this suite. @ 07/01/23 12:59:42.17
• [2.281 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 07/01/23 12:59:42.185
  Jul  1 12:59:42.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename services @ 07/01/23 12:59:42.186
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:59:42.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:59:42.231
  STEP: creating service in namespace services-9381 @ 07/01/23 12:59:42.241
  STEP: creating service affinity-clusterip in namespace services-9381 @ 07/01/23 12:59:42.241
  STEP: creating replication controller affinity-clusterip in namespace services-9381 @ 07/01/23 12:59:42.259
  I0701 12:59:42.280168      19 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-9381, replica count: 3
  I0701 12:59:45.331499      19 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  1 12:59:45.346: INFO: Creating new exec pod
  Jul  1 12:59:48.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-9381 exec execpod-affinityp9gtl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Jul  1 12:59:48.558: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\n+ echo hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Jul  1 12:59:48.559: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 12:59:48.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-9381 exec execpod-affinityp9gtl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.90 80'
  Jul  1 12:59:48.745: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.90 80\nConnection to 10.152.183.90 80 port [tcp/http] succeeded!\n"
  Jul  1 12:59:48.745: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 12:59:48.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-9381 exec execpod-affinityp9gtl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.90:80/ ; done'
  Jul  1 12:59:49.013: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.90:80/\n"
  Jul  1 12:59:49.013: INFO: stdout: "\naffinity-clusterip-6vv25\naffinity-clusterip-6vv25\naffinity-clusterip-6vv25\naffinity-clusterip-6vv25\naffinity-clusterip-6vv25\naffinity-clusterip-6vv25\naffinity-clusterip-6vv25\naffinity-clusterip-6vv25\naffinity-clusterip-6vv25\naffinity-clusterip-6vv25\naffinity-clusterip-6vv25\naffinity-clusterip-6vv25\naffinity-clusterip-6vv25\naffinity-clusterip-6vv25\naffinity-clusterip-6vv25\naffinity-clusterip-6vv25"
  Jul  1 12:59:49.013: INFO: Received response from host: affinity-clusterip-6vv25
  Jul  1 12:59:49.013: INFO: Received response from host: affinity-clusterip-6vv25
  Jul  1 12:59:49.013: INFO: Received response from host: affinity-clusterip-6vv25
  Jul  1 12:59:49.013: INFO: Received response from host: affinity-clusterip-6vv25
  Jul  1 12:59:49.013: INFO: Received response from host: affinity-clusterip-6vv25
  Jul  1 12:59:49.013: INFO: Received response from host: affinity-clusterip-6vv25
  Jul  1 12:59:49.013: INFO: Received response from host: affinity-clusterip-6vv25
  Jul  1 12:59:49.013: INFO: Received response from host: affinity-clusterip-6vv25
  Jul  1 12:59:49.013: INFO: Received response from host: affinity-clusterip-6vv25
  Jul  1 12:59:49.013: INFO: Received response from host: affinity-clusterip-6vv25
  Jul  1 12:59:49.013: INFO: Received response from host: affinity-clusterip-6vv25
  Jul  1 12:59:49.013: INFO: Received response from host: affinity-clusterip-6vv25
  Jul  1 12:59:49.014: INFO: Received response from host: affinity-clusterip-6vv25
  Jul  1 12:59:49.014: INFO: Received response from host: affinity-clusterip-6vv25
  Jul  1 12:59:49.014: INFO: Received response from host: affinity-clusterip-6vv25
  Jul  1 12:59:49.014: INFO: Received response from host: affinity-clusterip-6vv25
  Jul  1 12:59:49.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  1 12:59:49.020: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-9381, will wait for the garbage collector to delete the pods @ 07/01/23 12:59:49.044
  Jul  1 12:59:49.120: INFO: Deleting ReplicationController affinity-clusterip took: 11.087238ms
  Jul  1 12:59:49.220: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.333351ms
  STEP: Destroying namespace "services-9381" for this suite. @ 07/01/23 12:59:51.859
• [9.685 seconds]
------------------------------
SS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 07/01/23 12:59:51.87
  Jul  1 12:59:51.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename security-context @ 07/01/23 12:59:51.871
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:59:51.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:59:51.925
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 07/01/23 12:59:51.929
  STEP: Saw pod success @ 07/01/23 12:59:55.971
  Jul  1 12:59:55.978: INFO: Trying to get logs from node ip-172-31-91-66 pod security-context-d4b0312c-6135-44e0-9e28-e8c0c5a4c76a container test-container: <nil>
  STEP: delete the pod @ 07/01/23 12:59:55.996
  Jul  1 12:59:56.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-9997" for this suite. @ 07/01/23 12:59:56.034
• [4.176 seconds]
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 07/01/23 12:59:56.046
  Jul  1 12:59:56.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename daemonsets @ 07/01/23 12:59:56.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 12:59:56.07
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 12:59:56.079
  STEP: Creating a simple DaemonSet "daemon-set" @ 07/01/23 12:59:56.119
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/01/23 12:59:56.13
  Jul  1 12:59:56.138: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:59:56.138: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:59:56.147: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 12:59:56.147: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 12:59:57.156: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:59:57.156: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:59:57.167: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 12:59:57.167: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 12:59:58.154: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:59:58.154: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:59:58.161: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  1 12:59:58.161: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 12:59:59.154: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:59:59.154: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 12:59:59.160: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  1 12:59:59.160: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 13:00:00.155: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:00:00.155: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:00:00.160: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  1 13:00:00.160: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 07/01/23 13:00:00.167
  Jul  1 13:00:00.218: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:00:00.218: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:00:00.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul  1 13:00:00.238: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 13:00:01.246: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:00:01.247: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:00:01.257: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul  1 13:00:01.257: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  Jul  1 13:00:02.249: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:00:02.249: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:00:02.270: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  1 13:00:02.270: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 07/01/23 13:00:02.27
  STEP: Deleting DaemonSet "daemon-set" @ 07/01/23 13:00:02.29
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-528, will wait for the garbage collector to delete the pods @ 07/01/23 13:00:02.29
  Jul  1 13:00:02.361: INFO: Deleting DaemonSet.extensions daemon-set took: 12.696174ms
  Jul  1 13:00:02.463: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.112187ms
  Jul  1 13:00:03.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 13:00:03.970: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul  1 13:00:03.980: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24081"},"items":null}

  Jul  1 13:00:03.989: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24081"},"items":null}

  Jul  1 13:00:04.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-528" for this suite. @ 07/01/23 13:00:04.025
• [7.992 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 07/01/23 13:00:04.039
  Jul  1 13:00:04.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename discovery @ 07/01/23 13:00:04.041
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:00:04.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:00:04.08
  STEP: Setting up server cert @ 07/01/23 13:00:04.092
  Jul  1 13:00:04.686: INFO: Checking APIGroup: apiregistration.k8s.io
  Jul  1 13:00:04.687: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Jul  1 13:00:04.687: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Jul  1 13:00:04.687: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Jul  1 13:00:04.687: INFO: Checking APIGroup: apps
  Jul  1 13:00:04.689: INFO: PreferredVersion.GroupVersion: apps/v1
  Jul  1 13:00:04.689: INFO: Versions found [{apps/v1 v1}]
  Jul  1 13:00:04.689: INFO: apps/v1 matches apps/v1
  Jul  1 13:00:04.689: INFO: Checking APIGroup: events.k8s.io
  Jul  1 13:00:04.692: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Jul  1 13:00:04.692: INFO: Versions found [{events.k8s.io/v1 v1}]
  Jul  1 13:00:04.692: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Jul  1 13:00:04.692: INFO: Checking APIGroup: authentication.k8s.io
  Jul  1 13:00:04.694: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Jul  1 13:00:04.694: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Jul  1 13:00:04.694: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Jul  1 13:00:04.694: INFO: Checking APIGroup: authorization.k8s.io
  Jul  1 13:00:04.696: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Jul  1 13:00:04.696: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Jul  1 13:00:04.696: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Jul  1 13:00:04.696: INFO: Checking APIGroup: autoscaling
  Jul  1 13:00:04.698: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Jul  1 13:00:04.698: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Jul  1 13:00:04.698: INFO: autoscaling/v2 matches autoscaling/v2
  Jul  1 13:00:04.698: INFO: Checking APIGroup: batch
  Jul  1 13:00:04.701: INFO: PreferredVersion.GroupVersion: batch/v1
  Jul  1 13:00:04.701: INFO: Versions found [{batch/v1 v1}]
  Jul  1 13:00:04.701: INFO: batch/v1 matches batch/v1
  Jul  1 13:00:04.701: INFO: Checking APIGroup: certificates.k8s.io
  Jul  1 13:00:04.702: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Jul  1 13:00:04.703: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Jul  1 13:00:04.703: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Jul  1 13:00:04.703: INFO: Checking APIGroup: networking.k8s.io
  Jul  1 13:00:04.704: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Jul  1 13:00:04.704: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Jul  1 13:00:04.704: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Jul  1 13:00:04.704: INFO: Checking APIGroup: policy
  Jul  1 13:00:04.707: INFO: PreferredVersion.GroupVersion: policy/v1
  Jul  1 13:00:04.707: INFO: Versions found [{policy/v1 v1}]
  Jul  1 13:00:04.707: INFO: policy/v1 matches policy/v1
  Jul  1 13:00:04.707: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Jul  1 13:00:04.709: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Jul  1 13:00:04.709: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Jul  1 13:00:04.709: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Jul  1 13:00:04.709: INFO: Checking APIGroup: storage.k8s.io
  Jul  1 13:00:04.711: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Jul  1 13:00:04.711: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Jul  1 13:00:04.711: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Jul  1 13:00:04.711: INFO: Checking APIGroup: admissionregistration.k8s.io
  Jul  1 13:00:04.713: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Jul  1 13:00:04.713: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Jul  1 13:00:04.713: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Jul  1 13:00:04.713: INFO: Checking APIGroup: apiextensions.k8s.io
  Jul  1 13:00:04.716: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Jul  1 13:00:04.716: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Jul  1 13:00:04.716: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Jul  1 13:00:04.716: INFO: Checking APIGroup: scheduling.k8s.io
  Jul  1 13:00:04.718: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Jul  1 13:00:04.718: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Jul  1 13:00:04.718: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Jul  1 13:00:04.718: INFO: Checking APIGroup: coordination.k8s.io
  Jul  1 13:00:04.720: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Jul  1 13:00:04.720: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Jul  1 13:00:04.720: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Jul  1 13:00:04.720: INFO: Checking APIGroup: node.k8s.io
  Jul  1 13:00:04.721: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Jul  1 13:00:04.721: INFO: Versions found [{node.k8s.io/v1 v1}]
  Jul  1 13:00:04.721: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Jul  1 13:00:04.722: INFO: Checking APIGroup: discovery.k8s.io
  Jul  1 13:00:04.724: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Jul  1 13:00:04.724: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Jul  1 13:00:04.724: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Jul  1 13:00:04.724: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Jul  1 13:00:04.726: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Jul  1 13:00:04.726: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Jul  1 13:00:04.726: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Jul  1 13:00:04.726: INFO: Checking APIGroup: metrics.k8s.io
  Jul  1 13:00:04.728: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Jul  1 13:00:04.728: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Jul  1 13:00:04.728: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Jul  1 13:00:04.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-1483" for this suite. @ 07/01/23 13:00:04.736
• [0.712 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 07/01/23 13:00:04.752
  Jul  1 13:00:04.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename secrets @ 07/01/23 13:00:04.753
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:00:04.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:00:04.792
  STEP: Creating secret with name secret-test-a1b5e795-5742-4fad-9534-cf71af8e9342 @ 07/01/23 13:00:04.811
  STEP: Creating a pod to test consume secrets @ 07/01/23 13:00:04.827
  STEP: Saw pod success @ 07/01/23 13:00:08.868
  Jul  1 13:00:08.875: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-secrets-8f920cbd-23a0-4031-9f94-9ee57c281c85 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/01/23 13:00:08.902
  Jul  1 13:00:08.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4211" for this suite. @ 07/01/23 13:00:08.942
• [4.202 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 07/01/23 13:00:08.956
  Jul  1 13:00:08.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename secrets @ 07/01/23 13:00:08.956
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:00:08.99
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:00:08.998
  STEP: creating secret secrets-5693/secret-test-07219f70-58ca-4765-83fd-8f94f5f486ae @ 07/01/23 13:00:09.018
  STEP: Creating a pod to test consume secrets @ 07/01/23 13:00:09.03
  STEP: Saw pod success @ 07/01/23 13:00:13.11
  Jul  1 13:00:13.115: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-configmaps-6681c14d-5398-4060-a86e-baf4857b982a container env-test: <nil>
  STEP: delete the pod @ 07/01/23 13:00:13.13
  Jul  1 13:00:13.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5693" for this suite. @ 07/01/23 13:00:13.193
• [4.252 seconds]
------------------------------
S
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 07/01/23 13:00:13.208
  Jul  1 13:00:13.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename services @ 07/01/23 13:00:13.209
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:00:13.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:00:13.241
  STEP: creating service in namespace services-6450 @ 07/01/23 13:00:13.247
  STEP: creating service affinity-nodeport in namespace services-6450 @ 07/01/23 13:00:13.247
  STEP: creating replication controller affinity-nodeport in namespace services-6450 @ 07/01/23 13:00:13.284
  I0701 13:00:13.299849      19 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-6450, replica count: 3
  I0701 13:00:16.351227      19 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  1 13:00:16.368: INFO: Creating new exec pod
  Jul  1 13:00:19.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-6450 exec execpod-affinityg682c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Jul  1 13:00:19.645: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Jul  1 13:00:19.645: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 13:00:19.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-6450 exec execpod-affinityg682c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.27 80'
  Jul  1 13:00:19.905: INFO: stderr: "+ nc -v -t -w 2 10.152.183.27 80\n+ echo hostName\nConnection to 10.152.183.27 80 port [tcp/http] succeeded!\n"
  Jul  1 13:00:19.905: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 13:00:19.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-6450 exec execpod-affinityg682c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.94 31546'
  Jul  1 13:00:20.149: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.16.94 31546\nConnection to 172.31.16.94 31546 port [tcp/*] succeeded!\n"
  Jul  1 13:00:20.149: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 13:00:20.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-6450 exec execpod-affinityg682c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.12.125 31546'
  Jul  1 13:00:20.332: INFO: stderr: "+ nc -v -t -w 2 172.31.12.125 31546\n+ echo hostName\nConnection to 172.31.12.125 31546 port [tcp/*] succeeded!\n"
  Jul  1 13:00:20.332: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 13:00:20.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-6450 exec execpod-affinityg682c -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.12.125:31546/ ; done'
  Jul  1 13:00:20.625: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:31546/\n"
  Jul  1 13:00:20.625: INFO: stdout: "\naffinity-nodeport-fvbdx\naffinity-nodeport-fvbdx\naffinity-nodeport-fvbdx\naffinity-nodeport-fvbdx\naffinity-nodeport-fvbdx\naffinity-nodeport-fvbdx\naffinity-nodeport-fvbdx\naffinity-nodeport-fvbdx\naffinity-nodeport-fvbdx\naffinity-nodeport-fvbdx\naffinity-nodeport-fvbdx\naffinity-nodeport-fvbdx\naffinity-nodeport-fvbdx\naffinity-nodeport-fvbdx\naffinity-nodeport-fvbdx\naffinity-nodeport-fvbdx"
  Jul  1 13:00:20.625: INFO: Received response from host: affinity-nodeport-fvbdx
  Jul  1 13:00:20.625: INFO: Received response from host: affinity-nodeport-fvbdx
  Jul  1 13:00:20.625: INFO: Received response from host: affinity-nodeport-fvbdx
  Jul  1 13:00:20.625: INFO: Received response from host: affinity-nodeport-fvbdx
  Jul  1 13:00:20.625: INFO: Received response from host: affinity-nodeport-fvbdx
  Jul  1 13:00:20.625: INFO: Received response from host: affinity-nodeport-fvbdx
  Jul  1 13:00:20.625: INFO: Received response from host: affinity-nodeport-fvbdx
  Jul  1 13:00:20.625: INFO: Received response from host: affinity-nodeport-fvbdx
  Jul  1 13:00:20.625: INFO: Received response from host: affinity-nodeport-fvbdx
  Jul  1 13:00:20.625: INFO: Received response from host: affinity-nodeport-fvbdx
  Jul  1 13:00:20.625: INFO: Received response from host: affinity-nodeport-fvbdx
  Jul  1 13:00:20.625: INFO: Received response from host: affinity-nodeport-fvbdx
  Jul  1 13:00:20.625: INFO: Received response from host: affinity-nodeport-fvbdx
  Jul  1 13:00:20.625: INFO: Received response from host: affinity-nodeport-fvbdx
  Jul  1 13:00:20.625: INFO: Received response from host: affinity-nodeport-fvbdx
  Jul  1 13:00:20.625: INFO: Received response from host: affinity-nodeport-fvbdx
  Jul  1 13:00:20.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  1 13:00:20.631: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-6450, will wait for the garbage collector to delete the pods @ 07/01/23 13:00:20.648
  Jul  1 13:00:20.724: INFO: Deleting ReplicationController affinity-nodeport took: 17.806872ms
  Jul  1 13:00:20.825: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.324896ms
  STEP: Destroying namespace "services-6450" for this suite. @ 07/01/23 13:00:23.079
• [9.884 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 07/01/23 13:00:23.093
  Jul  1 13:00:23.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename replicaset @ 07/01/23 13:00:23.094
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:00:23.116
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:00:23.121
  STEP: Create a Replicaset @ 07/01/23 13:00:23.131
  STEP: Verify that the required pods have come up. @ 07/01/23 13:00:23.141
  Jul  1 13:00:23.145: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul  1 13:00:28.154: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/01/23 13:00:28.154
  STEP: Getting /status @ 07/01/23 13:00:28.154
  Jul  1 13:00:28.161: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 07/01/23 13:00:28.161
  Jul  1 13:00:28.178: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 07/01/23 13:00:28.178
  Jul  1 13:00:28.181: INFO: Observed &ReplicaSet event: ADDED
  Jul  1 13:00:28.181: INFO: Observed &ReplicaSet event: MODIFIED
  Jul  1 13:00:28.181: INFO: Observed &ReplicaSet event: MODIFIED
  Jul  1 13:00:28.181: INFO: Observed &ReplicaSet event: MODIFIED
  Jul  1 13:00:28.182: INFO: Found replicaset test-rs in namespace replicaset-9374 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul  1 13:00:28.182: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 07/01/23 13:00:28.182
  Jul  1 13:00:28.182: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul  1 13:00:28.194: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 07/01/23 13:00:28.194
  Jul  1 13:00:28.197: INFO: Observed &ReplicaSet event: ADDED
  Jul  1 13:00:28.197: INFO: Observed &ReplicaSet event: MODIFIED
  Jul  1 13:00:28.197: INFO: Observed &ReplicaSet event: MODIFIED
  Jul  1 13:00:28.198: INFO: Observed &ReplicaSet event: MODIFIED
  Jul  1 13:00:28.198: INFO: Observed replicaset test-rs in namespace replicaset-9374 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul  1 13:00:28.198: INFO: Observed &ReplicaSet event: MODIFIED
  Jul  1 13:00:28.198: INFO: Found replicaset test-rs in namespace replicaset-9374 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Jul  1 13:00:28.198: INFO: Replicaset test-rs has a patched status
  Jul  1 13:00:28.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9374" for this suite. @ 07/01/23 13:00:28.205
• [5.121 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 07/01/23 13:00:28.216
  Jul  1 13:00:28.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename services @ 07/01/23 13:00:28.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:00:28.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:00:28.246
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6370 @ 07/01/23 13:00:28.253
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 07/01/23 13:00:28.27
  STEP: creating service externalsvc in namespace services-6370 @ 07/01/23 13:00:28.27
  STEP: creating replication controller externalsvc in namespace services-6370 @ 07/01/23 13:00:28.292
  I0701 13:00:28.311631      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-6370, replica count: 2
  I0701 13:00:31.362736      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 07/01/23 13:00:31.37
  Jul  1 13:00:31.390: INFO: Creating new exec pod
  Jul  1 13:00:33.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-6370 exec execpodpgv7p -- /bin/sh -x -c nslookup clusterip-service.services-6370.svc.cluster.local'
  Jul  1 13:00:33.701: INFO: stderr: "+ nslookup clusterip-service.services-6370.svc.cluster.local\n"
  Jul  1 13:00:33.701: INFO: stdout: "Server:\t\t10.152.183.140\nAddress:\t10.152.183.140#53\n\nclusterip-service.services-6370.svc.cluster.local\tcanonical name = externalsvc.services-6370.svc.cluster.local.\nName:\texternalsvc.services-6370.svc.cluster.local\nAddress: 10.152.183.90\n\n"
  Jul  1 13:00:33.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-6370, will wait for the garbage collector to delete the pods @ 07/01/23 13:00:33.708
  Jul  1 13:00:33.782: INFO: Deleting ReplicationController externalsvc took: 16.044008ms
  Jul  1 13:00:33.882: INFO: Terminating ReplicationController externalsvc pods took: 100.840029ms
  Jul  1 13:00:36.132: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-6370" for this suite. @ 07/01/23 13:00:36.159
• [7.957 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 07/01/23 13:00:36.174
  Jul  1 13:00:36.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename resourcequota @ 07/01/23 13:00:36.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:00:36.206
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:00:36.211
  STEP: Discovering how many secrets are in namespace by default @ 07/01/23 13:00:36.216
  STEP: Counting existing ResourceQuota @ 07/01/23 13:00:41.224
  STEP: Creating a ResourceQuota @ 07/01/23 13:00:46.231
  STEP: Ensuring resource quota status is calculated @ 07/01/23 13:00:46.239
  STEP: Creating a Secret @ 07/01/23 13:00:48.247
  STEP: Ensuring resource quota status captures secret creation @ 07/01/23 13:00:48.272
  STEP: Deleting a secret @ 07/01/23 13:00:50.277
  STEP: Ensuring resource quota status released usage @ 07/01/23 13:00:50.286
  Jul  1 13:00:52.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9507" for this suite. @ 07/01/23 13:00:52.305
• [16.140 seconds]
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 07/01/23 13:00:52.315
  Jul  1 13:00:52.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename job @ 07/01/23 13:00:52.316
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:00:52.338
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:00:52.344
  STEP: Creating a job @ 07/01/23 13:00:52.349
  STEP: Ensuring active pods == parallelism @ 07/01/23 13:00:52.363
  STEP: Orphaning one of the Job's Pods @ 07/01/23 13:00:54.368
  Jul  1 13:00:54.901: INFO: Successfully updated pod "adopt-release-bnqs2"
  STEP: Checking that the Job readopts the Pod @ 07/01/23 13:00:54.901
  STEP: Removing the labels from the Job's Pod @ 07/01/23 13:00:56.925
  Jul  1 13:00:57.456: INFO: Successfully updated pod "adopt-release-bnqs2"
  STEP: Checking that the Job releases the Pod @ 07/01/23 13:00:57.457
  Jul  1 13:00:59.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3065" for this suite. @ 07/01/23 13:00:59.489
• [7.186 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 07/01/23 13:00:59.508
  Jul  1 13:00:59.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename job @ 07/01/23 13:00:59.512
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:00:59.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:00:59.548
  STEP: Creating a suspended job @ 07/01/23 13:00:59.562
  STEP: Patching the Job @ 07/01/23 13:00:59.572
  STEP: Watching for Job to be patched @ 07/01/23 13:00:59.595
  Jul  1 13:00:59.601: INFO: Event ADDED observed for Job e2e-xfrwc in namespace job-9876 with labels: map[e2e-job-label:e2e-xfrwc] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jul  1 13:00:59.601: INFO: Event MODIFIED observed for Job e2e-xfrwc in namespace job-9876 with labels: map[e2e-job-label:e2e-xfrwc] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jul  1 13:00:59.601: INFO: Event MODIFIED found for Job e2e-xfrwc in namespace job-9876 with labels: map[e2e-job-label:e2e-xfrwc e2e-xfrwc:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 07/01/23 13:00:59.601
  STEP: Watching for Job to be updated @ 07/01/23 13:00:59.617
  Jul  1 13:00:59.624: INFO: Event MODIFIED found for Job e2e-xfrwc in namespace job-9876 with labels: map[e2e-job-label:e2e-xfrwc e2e-xfrwc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul  1 13:00:59.624: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 07/01/23 13:00:59.624
  Jul  1 13:00:59.640: INFO: Job: e2e-xfrwc as labels: map[e2e-job-label:e2e-xfrwc e2e-xfrwc:patched]
  STEP: Waiting for job to complete @ 07/01/23 13:00:59.64
  STEP: Delete a job collection with a labelselector @ 07/01/23 13:01:07.652
  STEP: Watching for Job to be deleted @ 07/01/23 13:01:07.677
  Jul  1 13:01:07.681: INFO: Event MODIFIED observed for Job e2e-xfrwc in namespace job-9876 with labels: map[e2e-job-label:e2e-xfrwc e2e-xfrwc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul  1 13:01:07.681: INFO: Event MODIFIED observed for Job e2e-xfrwc in namespace job-9876 with labels: map[e2e-job-label:e2e-xfrwc e2e-xfrwc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul  1 13:01:07.681: INFO: Event MODIFIED observed for Job e2e-xfrwc in namespace job-9876 with labels: map[e2e-job-label:e2e-xfrwc e2e-xfrwc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul  1 13:01:07.681: INFO: Event MODIFIED observed for Job e2e-xfrwc in namespace job-9876 with labels: map[e2e-job-label:e2e-xfrwc e2e-xfrwc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul  1 13:01:07.681: INFO: Event MODIFIED observed for Job e2e-xfrwc in namespace job-9876 with labels: map[e2e-job-label:e2e-xfrwc e2e-xfrwc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul  1 13:01:07.681: INFO: Event DELETED found for Job e2e-xfrwc in namespace job-9876 with labels: map[e2e-job-label:e2e-xfrwc e2e-xfrwc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 07/01/23 13:01:07.681
  Jul  1 13:01:07.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9876" for this suite. @ 07/01/23 13:01:07.722
• [8.235 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 07/01/23 13:01:07.738
  Jul  1 13:01:07.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename deployment @ 07/01/23 13:01:07.74
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:01:07.767
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:01:07.772
  Jul  1 13:01:07.809: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  Jul  1 13:01:12.817: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/01/23 13:01:12.817
  Jul  1 13:01:12.817: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 07/01/23 13:01:12.835
  Jul  1 13:01:16.882: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8139  b78e056c-d730-4745-a4ec-5e3935388c7d 24917 1 2023-07-01 13:01:12 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-07-01 13:01:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 13:01:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005368558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-01 13:01:12 +0000 UTC,LastTransitionTime:2023-07-01 13:01:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-68b75d69f8" has successfully progressed.,LastUpdateTime:2023-07-01 13:01:15 +0000 UTC,LastTransitionTime:2023-07-01 13:01:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul  1 13:01:16.890: INFO: New ReplicaSet "test-cleanup-deployment-68b75d69f8" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-68b75d69f8  deployment-8139  c0c2b428-406b-4e75-af4b-3321e50b0a8b 24907 1 2023-07-01 13:01:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b78e056c-d730-4745-a4ec-5e3935388c7d 0xc005368947 0xc005368948}] [] [{kube-controller-manager Update apps/v1 2023-07-01 13:01:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b78e056c-d730-4745-a4ec-5e3935388c7d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 13:01:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 68b75d69f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005368a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul  1 13:01:16.899: INFO: Pod "test-cleanup-deployment-68b75d69f8-hj2qs" is available:
  &Pod{ObjectMeta:{test-cleanup-deployment-68b75d69f8-hj2qs test-cleanup-deployment-68b75d69f8- deployment-8139  846f225e-9e58-4ef0-b712-43bf4420115c 24905 0 2023-07-01 13:01:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-68b75d69f8 c0c2b428-406b-4e75-af4b-3321e50b0a8b 0xc005368dd7 0xc005368dd8}] [] [{kube-controller-manager Update v1 2023-07-01 13:01:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c0c2b428-406b-4e75-af4b-3321e50b0a8b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 13:01:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.175.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6ltkq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6ltkq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:01:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:01:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:01:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:01:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.125,PodIP:192.168.175.35,StartTime:2023-07-01 13:01:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-01 13:01:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://a177d02951e473e78fa09671c314c02c46e352dca92db28e186f2b81b9e0b53c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.175.35,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 13:01:16.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8139" for this suite. @ 07/01/23 13:01:16.905
• [9.176 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 07/01/23 13:01:16.916
  Jul  1 13:01:16.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 13:01:16.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:01:16.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:01:16.954
  STEP: Creating configMap with name projected-configmap-test-volume-map-ac57f9c0-2f55-45bc-8b74-ce00cb8feec6 @ 07/01/23 13:01:16.959
  STEP: Creating a pod to test consume configMaps @ 07/01/23 13:01:16.967
  STEP: Saw pod success @ 07/01/23 13:01:21.01
  Jul  1 13:01:21.014: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-projected-configmaps-fa32cca9-5dcd-4cb4-8415-8bc5c8ca12d0 container agnhost-container: <nil>
  STEP: delete the pod @ 07/01/23 13:01:21.026
  Jul  1 13:01:21.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8460" for this suite. @ 07/01/23 13:01:21.057
• [4.151 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 07/01/23 13:01:21.069
  Jul  1 13:01:21.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename events @ 07/01/23 13:01:21.07
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:01:21.099
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:01:21.105
  STEP: creating a test event @ 07/01/23 13:01:21.11
  STEP: listing all events in all namespaces @ 07/01/23 13:01:21.118
  STEP: patching the test event @ 07/01/23 13:01:21.126
  STEP: fetching the test event @ 07/01/23 13:01:21.136
  STEP: updating the test event @ 07/01/23 13:01:21.143
  STEP: getting the test event @ 07/01/23 13:01:21.16
  STEP: deleting the test event @ 07/01/23 13:01:21.164
  STEP: listing all events in all namespaces @ 07/01/23 13:01:21.182
  Jul  1 13:01:21.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5037" for this suite. @ 07/01/23 13:01:21.197
• [0.139 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 07/01/23 13:01:21.21
  Jul  1 13:01:21.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename cronjob @ 07/01/23 13:01:21.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:01:21.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:01:21.248
  STEP: Creating a cronjob @ 07/01/23 13:01:21.252
  STEP: creating @ 07/01/23 13:01:21.252
  STEP: getting @ 07/01/23 13:01:21.263
  STEP: listing @ 07/01/23 13:01:21.269
  STEP: watching @ 07/01/23 13:01:21.275
  Jul  1 13:01:21.275: INFO: starting watch
  STEP: cluster-wide listing @ 07/01/23 13:01:21.277
  STEP: cluster-wide watching @ 07/01/23 13:01:21.286
  Jul  1 13:01:21.286: INFO: starting watch
  STEP: patching @ 07/01/23 13:01:21.288
  STEP: updating @ 07/01/23 13:01:21.299
  Jul  1 13:01:21.313: INFO: waiting for watch events with expected annotations
  Jul  1 13:01:21.313: INFO: saw patched and updated annotations
  STEP: patching /status @ 07/01/23 13:01:21.314
  STEP: updating /status @ 07/01/23 13:01:21.326
  STEP: get /status @ 07/01/23 13:01:21.339
  STEP: deleting @ 07/01/23 13:01:21.345
  STEP: deleting a collection @ 07/01/23 13:01:21.368
  Jul  1 13:01:21.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5186" for this suite. @ 07/01/23 13:01:21.407
• [0.207 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 07/01/23 13:01:21.418
  Jul  1 13:01:21.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-runtime @ 07/01/23 13:01:21.419
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:01:21.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:01:21.454
  STEP: create the container @ 07/01/23 13:01:21.458
  W0701 13:01:21.475603      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 07/01/23 13:01:21.475
  STEP: get the container status @ 07/01/23 13:01:25.516
  STEP: the container should be terminated @ 07/01/23 13:01:25.521
  STEP: the termination message should be set @ 07/01/23 13:01:25.521
  Jul  1 13:01:25.521: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 07/01/23 13:01:25.522
  Jul  1 13:01:25.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6603" for this suite. @ 07/01/23 13:01:25.558
• [4.150 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 07/01/23 13:01:25.57
  Jul  1 13:01:25.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubelet-test @ 07/01/23 13:01:25.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:01:25.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:01:25.604
  STEP: Waiting for pod completion @ 07/01/23 13:01:25.628
  Jul  1 13:01:29.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7746" for this suite. @ 07/01/23 13:01:29.671
• [4.114 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 07/01/23 13:01:29.684
  Jul  1 13:01:29.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename replicaset @ 07/01/23 13:01:29.688
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:01:29.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:01:29.721
  Jul  1 13:01:29.762: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul  1 13:01:34.773: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/01/23 13:01:34.773
  STEP: Scaling up "test-rs" replicaset  @ 07/01/23 13:01:34.773
  Jul  1 13:01:34.786: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 07/01/23 13:01:34.786
  W0701 13:01:34.839796      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul  1 13:01:34.843: INFO: observed ReplicaSet test-rs in namespace replicaset-7677 with ReadyReplicas 1, AvailableReplicas 1
  Jul  1 13:01:34.859: INFO: observed ReplicaSet test-rs in namespace replicaset-7677 with ReadyReplicas 1, AvailableReplicas 1
  Jul  1 13:01:34.888: INFO: observed ReplicaSet test-rs in namespace replicaset-7677 with ReadyReplicas 1, AvailableReplicas 1
  Jul  1 13:01:34.899: INFO: observed ReplicaSet test-rs in namespace replicaset-7677 with ReadyReplicas 1, AvailableReplicas 1
  Jul  1 13:01:36.097: INFO: observed ReplicaSet test-rs in namespace replicaset-7677 with ReadyReplicas 2, AvailableReplicas 2
  Jul  1 13:01:36.854: INFO: observed Replicaset test-rs in namespace replicaset-7677 with ReadyReplicas 3 found true
  Jul  1 13:01:36.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7677" for this suite. @ 07/01/23 13:01:36.861
• [7.187 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 07/01/23 13:01:36.875
  Jul  1 13:01:36.875: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename services @ 07/01/23 13:01:36.876
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:01:36.899
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:01:36.904
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-2142 @ 07/01/23 13:01:36.909
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 07/01/23 13:01:36.936
  STEP: creating service externalsvc in namespace services-2142 @ 07/01/23 13:01:36.936
  STEP: creating replication controller externalsvc in namespace services-2142 @ 07/01/23 13:01:36.964
  I0701 13:01:36.981053      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-2142, replica count: 2
  I0701 13:01:40.031476      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 07/01/23 13:01:40.036
  Jul  1 13:01:40.069: INFO: Creating new exec pod
  Jul  1 13:01:42.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-2142 exec execpodk276w -- /bin/sh -x -c nslookup nodeport-service.services-2142.svc.cluster.local'
  Jul  1 13:01:42.426: INFO: stderr: "+ nslookup nodeport-service.services-2142.svc.cluster.local\n"
  Jul  1 13:01:42.426: INFO: stdout: "Server:\t\t10.152.183.140\nAddress:\t10.152.183.140#53\n\nnodeport-service.services-2142.svc.cluster.local\tcanonical name = externalsvc.services-2142.svc.cluster.local.\nName:\texternalsvc.services-2142.svc.cluster.local\nAddress: 10.152.183.226\n\n"
  Jul  1 13:01:42.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-2142, will wait for the garbage collector to delete the pods @ 07/01/23 13:01:42.432
  Jul  1 13:01:42.506: INFO: Deleting ReplicationController externalsvc took: 14.560772ms
  Jul  1 13:01:42.607: INFO: Terminating ReplicationController externalsvc pods took: 101.359098ms
  Jul  1 13:01:45.242: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-2142" for this suite. @ 07/01/23 13:01:45.275
• [8.417 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 07/01/23 13:01:45.292
  Jul  1 13:01:45.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 13:01:45.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:01:45.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:01:45.327
  STEP: Setting up server cert @ 07/01/23 13:01:45.379
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 13:01:45.754
  STEP: Deploying the webhook pod @ 07/01/23 13:01:45.768
  STEP: Wait for the deployment to be ready @ 07/01/23 13:01:45.791
  Jul  1 13:01:45.806: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/01/23 13:01:47.829
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 13:01:47.858
  Jul  1 13:01:48.859: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 07/01/23 13:01:48.866
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 07/01/23 13:01:48.892
  STEP: Creating a dummy validating-webhook-configuration object @ 07/01/23 13:01:48.918
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 07/01/23 13:01:48.935
  STEP: Creating a dummy mutating-webhook-configuration object @ 07/01/23 13:01:48.946
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 07/01/23 13:01:48.964
  Jul  1 13:01:48.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4540" for this suite. @ 07/01/23 13:01:49.102
  STEP: Destroying namespace "webhook-markers-5968" for this suite. @ 07/01/23 13:01:49.113
• [3.831 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 07/01/23 13:01:49.125
  Jul  1 13:01:49.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename replication-controller @ 07/01/23 13:01:49.126
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:01:49.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:01:49.171
  STEP: creating a ReplicationController @ 07/01/23 13:01:49.186
  STEP: waiting for RC to be added @ 07/01/23 13:01:49.201
  STEP: waiting for available Replicas @ 07/01/23 13:01:49.201
  STEP: patching ReplicationController @ 07/01/23 13:01:50.283
  STEP: waiting for RC to be modified @ 07/01/23 13:01:50.302
  STEP: patching ReplicationController status @ 07/01/23 13:01:50.302
  STEP: waiting for RC to be modified @ 07/01/23 13:01:50.321
  STEP: waiting for available Replicas @ 07/01/23 13:01:50.321
  STEP: fetching ReplicationController status @ 07/01/23 13:01:50.324
  STEP: patching ReplicationController scale @ 07/01/23 13:01:50.331
  STEP: waiting for RC to be modified @ 07/01/23 13:01:50.348
  STEP: waiting for ReplicationController's scale to be the max amount @ 07/01/23 13:01:50.348
  STEP: fetching ReplicationController; ensuring that it's patched @ 07/01/23 13:01:52.161
  STEP: updating ReplicationController status @ 07/01/23 13:01:52.169
  STEP: waiting for RC to be modified @ 07/01/23 13:01:52.181
  STEP: listing all ReplicationControllers @ 07/01/23 13:01:52.181
  STEP: checking that ReplicationController has expected values @ 07/01/23 13:01:52.187
  STEP: deleting ReplicationControllers by collection @ 07/01/23 13:01:52.187
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 07/01/23 13:01:52.202
  Jul  1 13:01:52.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0701 13:01:52.273292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-1787" for this suite. @ 07/01/23 13:01:52.279
• [3.166 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 07/01/23 13:01:52.293
  Jul  1 13:01:52.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename init-container @ 07/01/23 13:01:52.295
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:01:52.315
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:01:52.328
  STEP: creating the pod @ 07/01/23 13:01:52.334
  Jul  1 13:01:52.334: INFO: PodSpec: initContainers in spec.initContainers
  E0701 13:01:53.273529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:01:54.273819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:01:55.273945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:01:56.274971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:01:56.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1858" for this suite. @ 07/01/23 13:01:56.824
• [4.541 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 07/01/23 13:01:56.838
  Jul  1 13:01:56.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 07/01/23 13:01:56.839
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:01:56.862
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:01:56.871
  STEP: creating a target pod @ 07/01/23 13:01:56.876
  E0701 13:01:57.275549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:01:58.275659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 07/01/23 13:01:58.91
  E0701 13:01:59.275840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:00.275980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 07/01/23 13:02:00.936
  Jul  1 13:02:00.936: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4744 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:02:00.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:02:00.937: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:02:00.937: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-4744/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Jul  1 13:02:01.023: INFO: Exec stderr: ""
  Jul  1 13:02:01.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-4744" for this suite. @ 07/01/23 13:02:01.044
• [4.218 seconds]
------------------------------
SSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 07/01/23 13:02:01.057
  Jul  1 13:02:01.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename downward-api @ 07/01/23 13:02:01.058
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:02:01.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:02:01.095
  STEP: Creating a pod to test downward api env vars @ 07/01/23 13:02:01.102
  E0701 13:02:01.276361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:02.276495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:03.277059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:04.277176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:02:05.142
  Jul  1 13:02:05.146: INFO: Trying to get logs from node ip-172-31-91-66 pod downward-api-75a70bfd-707f-49a6-892c-5b7d85422820 container dapi-container: <nil>
  STEP: delete the pod @ 07/01/23 13:02:05.158
  Jul  1 13:02:05.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2023" for this suite. @ 07/01/23 13:02:05.189
• [4.141 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 07/01/23 13:02:05.198
  Jul  1 13:02:05.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 13:02:05.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:02:05.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:02:05.225
  STEP: Setting up server cert @ 07/01/23 13:02:05.261
  E0701 13:02:05.277757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 13:02:06.055
  STEP: Deploying the webhook pod @ 07/01/23 13:02:06.066
  STEP: Wait for the deployment to be ready @ 07/01/23 13:02:06.093
  Jul  1 13:02:06.116: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0701 13:02:06.278467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:07.278671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/01/23 13:02:08.134
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 13:02:08.148
  E0701 13:02:08.279854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:02:09.148: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 07/01/23 13:02:09.155
  STEP: create a pod that should be denied by the webhook @ 07/01/23 13:02:09.182
  STEP: create a pod that causes the webhook to hang @ 07/01/23 13:02:09.203
  E0701 13:02:09.279970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:10.280247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:11.280627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:12.280738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:13.280867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:14.280970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:15.281101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:16.281911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:17.282882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:18.283162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 07/01/23 13:02:19.219
  STEP: create a configmap that should be admitted by the webhook @ 07/01/23 13:02:19.232
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 07/01/23 13:02:19.26
  E0701 13:02:19.283295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 07/01/23 13:02:19.287
  STEP: create a namespace that bypass the webhook @ 07/01/23 13:02:19.298
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 07/01/23 13:02:19.32
  Jul  1 13:02:19.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2091" for this suite. @ 07/01/23 13:02:19.482
  STEP: Destroying namespace "webhook-markers-208" for this suite. @ 07/01/23 13:02:19.499
  STEP: Destroying namespace "exempted-namespace-3441" for this suite. @ 07/01/23 13:02:19.516
• [14.336 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 07/01/23 13:02:19.537
  Jul  1 13:02:19.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename statefulset @ 07/01/23 13:02:19.538
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:02:19.577
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:02:19.591
  STEP: Creating service test in namespace statefulset-5395 @ 07/01/23 13:02:19.602
  Jul  1 13:02:19.655: INFO: Found 0 stateful pods, waiting for 1
  E0701 13:02:20.283478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:21.283607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:22.283662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:23.283809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:24.283854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:25.284017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:26.284243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:27.284699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:28.284781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:29.284988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:02:29.664: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 07/01/23 13:02:29.675
  W0701 13:02:29.691545      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul  1 13:02:29.703: INFO: Found 1 stateful pods, waiting for 2
  E0701 13:02:30.285235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:31.286160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:32.286430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:33.286975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:34.288021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:35.288147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:36.288641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:37.289065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:38.289266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:39.289523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:02:39.711: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul  1 13:02:39.711: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 07/01/23 13:02:39.723
  STEP: Delete all of the StatefulSets @ 07/01/23 13:02:39.729
  STEP: Verify that StatefulSets have been deleted @ 07/01/23 13:02:39.744
  Jul  1 13:02:39.755: INFO: Deleting all statefulset in ns statefulset-5395
  Jul  1 13:02:39.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5395" for this suite. @ 07/01/23 13:02:39.811
• [20.292 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 07/01/23 13:02:39.83
  Jul  1 13:02:39.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename replicaset @ 07/01/23 13:02:39.831
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:02:39.861
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:02:39.873
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 07/01/23 13:02:39.88
  E0701 13:02:40.290960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:41.291893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 07/01/23 13:02:41.938
  STEP: Then the orphan pod is adopted @ 07/01/23 13:02:41.947
  E0701 13:02:42.292019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 07/01/23 13:02:42.96
  Jul  1 13:02:42.967: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 07/01/23 13:02:42.99
  E0701 13:02:43.292136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:02:44.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5" for this suite. @ 07/01/23 13:02:44.026
• [4.208 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 07/01/23 13:02:44.042
  Jul  1 13:02:44.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename configmap @ 07/01/23 13:02:44.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:02:44.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:02:44.078
  STEP: Creating configMap that has name configmap-test-emptyKey-d6500a04-5a87-4219-8714-9bc5e705cdf1 @ 07/01/23 13:02:44.084
  Jul  1 13:02:44.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6657" for this suite. @ 07/01/23 13:02:44.1
• [0.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 07/01/23 13:02:44.124
  Jul  1 13:02:44.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename resourcequota @ 07/01/23 13:02:44.126
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:02:44.146
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:02:44.152
  STEP: Counting existing ResourceQuota @ 07/01/23 13:02:44.159
  E0701 13:02:44.292308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:45.292739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:46.293595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:47.293750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:48.297326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/01/23 13:02:49.164
  STEP: Ensuring resource quota status is calculated @ 07/01/23 13:02:49.181
  E0701 13:02:49.297993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:50.298102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 07/01/23 13:02:51.191
  STEP: Ensuring ResourceQuota status captures the pod usage @ 07/01/23 13:02:51.223
  E0701 13:02:51.298753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:52.298869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 07/01/23 13:02:53.231
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 07/01/23 13:02:53.237
  STEP: Ensuring a pod cannot update its resource requirements @ 07/01/23 13:02:53.241
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 07/01/23 13:02:53.25
  E0701 13:02:53.299593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:54.299793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 07/01/23 13:02:55.256
  STEP: Ensuring resource quota status released the pod usage @ 07/01/23 13:02:55.274
  E0701 13:02:55.300620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:56.300716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:02:57.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1644" for this suite. @ 07/01/23 13:02:57.287
  E0701 13:02:57.300730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [13.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 07/01/23 13:02:57.302
  Jul  1 13:02:57.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename pods @ 07/01/23 13:02:57.302
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:02:57.334
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:02:57.342
  STEP: creating pod @ 07/01/23 13:02:57.347
  E0701 13:02:58.300969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:02:59.301085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:02:59.391: INFO: Pod pod-hostip-93868b9b-a6b7-4e04-9404-4b9da3a34e0e has hostIP: 172.31.91.66
  Jul  1 13:02:59.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7726" for this suite. @ 07/01/23 13:02:59.4
• [2.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 07/01/23 13:02:59.42
  Jul  1 13:02:59.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename field-validation @ 07/01/23 13:02:59.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:02:59.501
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:02:59.514
  Jul  1 13:02:59.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  W0701 13:02:59.524467      19 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc000c38f20 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0701 13:03:00.301242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:01.301880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0701 13:03:02.118339      19 warnings.go:70] unknown field "alpha"
  W0701 13:03:02.118429      19 warnings.go:70] unknown field "beta"
  W0701 13:03:02.118448      19 warnings.go:70] unknown field "delta"
  W0701 13:03:02.118455      19 warnings.go:70] unknown field "epsilon"
  W0701 13:03:02.118488      19 warnings.go:70] unknown field "gamma"
  Jul  1 13:03:02.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4247" for this suite. @ 07/01/23 13:03:02.184
• [2.775 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 07/01/23 13:03:02.196
  Jul  1 13:03:02.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl @ 07/01/23 13:03:02.198
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:03:02.231
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:03:02.236
  STEP: validating cluster-info @ 07/01/23 13:03:02.246
  Jul  1 13:03:02.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-3337 cluster-info'
  E0701 13:03:02.306802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:03:02.380: INFO: stderr: ""
  Jul  1 13:03:02.380: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Jul  1 13:03:02.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3337" for this suite. @ 07/01/23 13:03:02.387
• [0.203 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 07/01/23 13:03:02.399
  Jul  1 13:03:02.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl @ 07/01/23 13:03:02.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:03:02.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:03:02.439
  STEP: create deployment with httpd image @ 07/01/23 13:03:02.445
  Jul  1 13:03:02.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-3501 create -f -'
  E0701 13:03:03.306923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:03:03.794: INFO: stderr: ""
  Jul  1 13:03:03.794: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 07/01/23 13:03:03.794
  Jul  1 13:03:03.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-3501 diff -f -'
  E0701 13:03:04.307798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:03:05.201: INFO: rc: 1
  Jul  1 13:03:05.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-3501 delete -f -'
  Jul  1 13:03:05.289: INFO: stderr: ""
  Jul  1 13:03:05.289: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Jul  1 13:03:05.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3501" for this suite. @ 07/01/23 13:03:05.297
  E0701 13:03:05.308122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [2.916 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 07/01/23 13:03:05.316
  Jul  1 13:03:05.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 13:03:05.323
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:03:05.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:03:05.37
  STEP: Creating configMap with name projected-configmap-test-volume-map-334f46eb-6248-430e-a7d6-c95954ad770e @ 07/01/23 13:03:05.376
  STEP: Creating a pod to test consume configMaps @ 07/01/23 13:03:05.385
  E0701 13:03:06.308270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:07.308941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:08.309089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:09.309364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:03:09.449
  Jul  1 13:03:09.458: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-projected-configmaps-231abe4c-71df-46b0-b27b-e60320933cd8 container agnhost-container: <nil>
  STEP: delete the pod @ 07/01/23 13:03:09.467
  Jul  1 13:03:09.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5297" for this suite. @ 07/01/23 13:03:09.496
• [4.191 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 07/01/23 13:03:09.516
  Jul  1 13:03:09.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename var-expansion @ 07/01/23 13:03:09.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:03:09.546
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:03:09.551
  STEP: Creating a pod to test substitution in container's args @ 07/01/23 13:03:09.56
  E0701 13:03:10.309911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:11.310475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:12.310590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:13.311503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:03:13.602
  Jul  1 13:03:13.608: INFO: Trying to get logs from node ip-172-31-91-66 pod var-expansion-83c97989-978e-4159-97f7-e886fade097d container dapi-container: <nil>
  STEP: delete the pod @ 07/01/23 13:03:13.616
  Jul  1 13:03:13.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4239" for this suite. @ 07/01/23 13:03:13.653
• [4.150 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 07/01/23 13:03:13.688
  Jul  1 13:03:13.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename controllerrevisions @ 07/01/23 13:03:13.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:03:13.725
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:03:13.731
  STEP: Creating DaemonSet "e2e-pnbmf-daemon-set" @ 07/01/23 13:03:13.773
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/01/23 13:03:13.782
  Jul  1 13:03:13.791: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:03:13.791: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:03:13.797: INFO: Number of nodes with available pods controlled by daemonset e2e-pnbmf-daemon-set: 0
  Jul  1 13:03:13.797: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  E0701 13:03:14.313035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:03:14.807: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:03:14.807: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:03:14.814: INFO: Number of nodes with available pods controlled by daemonset e2e-pnbmf-daemon-set: 0
  Jul  1 13:03:14.814: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  E0701 13:03:15.312738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:03:15.805: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:03:15.805: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:03:15.810: INFO: Number of nodes with available pods controlled by daemonset e2e-pnbmf-daemon-set: 3
  Jul  1 13:03:15.810: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-pnbmf-daemon-set
  STEP: Confirm DaemonSet "e2e-pnbmf-daemon-set" successfully created with "daemonset-name=e2e-pnbmf-daemon-set" label @ 07/01/23 13:03:15.816
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-pnbmf-daemon-set" @ 07/01/23 13:03:15.828
  Jul  1 13:03:15.833: INFO: Located ControllerRevision: "e2e-pnbmf-daemon-set-585776bc6b"
  STEP: Patching ControllerRevision "e2e-pnbmf-daemon-set-585776bc6b" @ 07/01/23 13:03:15.84
  Jul  1 13:03:15.850: INFO: e2e-pnbmf-daemon-set-585776bc6b has been patched
  STEP: Create a new ControllerRevision @ 07/01/23 13:03:15.85
  Jul  1 13:03:15.858: INFO: Created ControllerRevision: e2e-pnbmf-daemon-set-d8cf48b96
  STEP: Confirm that there are two ControllerRevisions @ 07/01/23 13:03:15.858
  Jul  1 13:03:15.858: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul  1 13:03:15.865: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-pnbmf-daemon-set-585776bc6b" @ 07/01/23 13:03:15.865
  STEP: Confirm that there is only one ControllerRevision @ 07/01/23 13:03:15.875
  Jul  1 13:03:15.875: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul  1 13:03:15.881: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-pnbmf-daemon-set-d8cf48b96" @ 07/01/23 13:03:15.888
  Jul  1 13:03:15.903: INFO: e2e-pnbmf-daemon-set-d8cf48b96 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 07/01/23 13:03:15.903
  W0701 13:03:15.917060      19 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 07/01/23 13:03:15.917
  Jul  1 13:03:15.917: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0701 13:03:16.313040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:03:16.922: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul  1 13:03:16.929: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-pnbmf-daemon-set-d8cf48b96=updated" @ 07/01/23 13:03:16.929
  STEP: Confirm that there is only one ControllerRevision @ 07/01/23 13:03:16.943
  Jul  1 13:03:16.943: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul  1 13:03:16.949: INFO: Found 1 ControllerRevisions
  Jul  1 13:03:16.954: INFO: ControllerRevision "e2e-pnbmf-daemon-set-777f8bfdbf" has revision 3
  STEP: Deleting DaemonSet "e2e-pnbmf-daemon-set" @ 07/01/23 13:03:16.962
  STEP: deleting DaemonSet.extensions e2e-pnbmf-daemon-set in namespace controllerrevisions-5374, will wait for the garbage collector to delete the pods @ 07/01/23 13:03:16.962
  Jul  1 13:03:17.042: INFO: Deleting DaemonSet.extensions e2e-pnbmf-daemon-set took: 15.000506ms
  Jul  1 13:03:17.143: INFO: Terminating DaemonSet.extensions e2e-pnbmf-daemon-set pods took: 100.764276ms
  E0701 13:03:17.313956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:18.314993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:03:18.750: INFO: Number of nodes with available pods controlled by daemonset e2e-pnbmf-daemon-set: 0
  Jul  1 13:03:18.750: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-pnbmf-daemon-set
  Jul  1 13:03:18.758: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26449"},"items":null}

  Jul  1 13:03:18.763: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26450"},"items":null}

  Jul  1 13:03:18.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-5374" for this suite. @ 07/01/23 13:03:18.795
• [5.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 07/01/23 13:03:18.808
  Jul  1 13:03:18.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename daemonsets @ 07/01/23 13:03:18.809
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:03:18.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:03:18.844
  Jul  1 13:03:18.902: INFO: Create a RollingUpdate DaemonSet
  Jul  1 13:03:18.911: INFO: Check that daemon pods launch on every node of the cluster
  Jul  1 13:03:18.925: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:03:18.925: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:03:18.932: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 13:03:18.932: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  E0701 13:03:19.315896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:03:19.937: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:03:19.938: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:03:19.942: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 13:03:19.942: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  E0701 13:03:20.316004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:03:20.940: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:03:20.940: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:03:20.946: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  1 13:03:20.946: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Jul  1 13:03:20.946: INFO: Update the DaemonSet to trigger a rollout
  Jul  1 13:03:20.963: INFO: Updating DaemonSet daemon-set
  E0701 13:03:21.316609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:22.316808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:03:22.990: INFO: Roll back the DaemonSet before rollout is complete
  Jul  1 13:03:23.005: INFO: Updating DaemonSet daemon-set
  Jul  1 13:03:23.005: INFO: Make sure DaemonSet rollback is complete
  Jul  1 13:03:23.011: INFO: Wrong image for pod: daemon-set-vzr9q. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Jul  1 13:03:23.011: INFO: Pod daemon-set-vzr9q is not available
  Jul  1 13:03:23.018: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:03:23.018: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0701 13:03:23.317684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:03:24.036: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:03:24.036: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0701 13:03:24.318450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:03:25.026: INFO: Pod daemon-set-nq5zf is not available
  Jul  1 13:03:25.033: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:03:25.033: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 07/01/23 13:03:25.047
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5000, will wait for the garbage collector to delete the pods @ 07/01/23 13:03:25.047
  Jul  1 13:03:25.115: INFO: Deleting DaemonSet.extensions daemon-set took: 11.457851ms
  Jul  1 13:03:25.218: INFO: Terminating DaemonSet.extensions daemon-set pods took: 103.193491ms
  E0701 13:03:25.319475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:26.319554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:03:27.225: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 13:03:27.225: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul  1 13:03:27.231: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26631"},"items":null}

  Jul  1 13:03:27.240: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26631"},"items":null}

  Jul  1 13:03:27.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5000" for this suite. @ 07/01/23 13:03:27.271
• [8.474 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 07/01/23 13:03:27.283
  Jul  1 13:03:27.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename subpath @ 07/01/23 13:03:27.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:03:27.309
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:03:27.315
  E0701 13:03:27.320049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up data @ 07/01/23 13:03:27.32
  STEP: Creating pod pod-subpath-test-secret-dxnk @ 07/01/23 13:03:27.333
  STEP: Creating a pod to test atomic-volume-subpath @ 07/01/23 13:03:27.333
  E0701 13:03:28.320233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:29.320400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:30.320585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:31.321012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:32.321122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:33.321238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:34.321778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:35.321913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:36.322049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:37.322166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:38.322296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:39.322529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:40.323051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:41.323699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:42.323896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:43.324643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:44.324761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:45.324876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:46.325836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:47.325961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:48.326446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:49.327534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:50.328246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:51.328679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:03:51.45
  Jul  1 13:03:51.457: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-subpath-test-secret-dxnk container test-container-subpath-secret-dxnk: <nil>
  STEP: delete the pod @ 07/01/23 13:03:51.47
  STEP: Deleting pod pod-subpath-test-secret-dxnk @ 07/01/23 13:03:51.497
  Jul  1 13:03:51.497: INFO: Deleting pod "pod-subpath-test-secret-dxnk" in namespace "subpath-674"
  Jul  1 13:03:51.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-674" for this suite. @ 07/01/23 13:03:51.513
• [24.242 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 07/01/23 13:03:51.525
  Jul  1 13:03:51.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename cronjob @ 07/01/23 13:03:51.526
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:03:51.553
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:03:51.56
  STEP: Creating a ReplaceConcurrent cronjob @ 07/01/23 13:03:51.569
  STEP: Ensuring a job is scheduled @ 07/01/23 13:03:51.583
  E0701 13:03:52.328964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:53.329193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:54.329309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:55.329444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:56.329687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:57.330383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:58.330590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:03:59.331483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:00.331559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:01.331664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 07/01/23 13:04:01.59
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 07/01/23 13:04:01.595
  STEP: Ensuring the job is replaced with a new one @ 07/01/23 13:04:01.602
  E0701 13:04:02.331808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:03.332048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:04.332176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:05.332566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:06.332706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:07.332822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:08.332950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:09.333930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:10.334085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:11.334769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:12.334815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:13.335095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:14.335191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:15.335530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:16.336162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:17.337041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:18.337138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:19.337739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:20.337886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:21.338421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:22.339624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:23.339696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:24.340093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:25.340214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:26.340647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:27.340790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:28.340915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:29.340996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:30.342222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:31.342576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:32.342645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:33.343592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:34.344592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:35.344717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:36.345610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:37.345579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:38.345640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:39.345969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:40.346892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:41.347522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:42.347649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:43.348632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:44.349963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:45.349364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:46.349758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:47.350124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:48.350297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:49.350564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:50.350860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:51.351578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:52.352621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:53.352757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:54.353821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:55.354027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:56.354154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:57.354363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:58.355087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:04:59.355827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:00.355910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:01.356678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 07/01/23 13:05:01.608
  Jul  1 13:05:01.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-501" for this suite. @ 07/01/23 13:05:01.628
• [70.115 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 07/01/23 13:05:01.642
  Jul  1 13:05:01.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename subpath @ 07/01/23 13:05:01.643
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:05:01.673
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:05:01.678
  STEP: Setting up data @ 07/01/23 13:05:01.683
  STEP: Creating pod pod-subpath-test-configmap-pf74 @ 07/01/23 13:05:01.705
  STEP: Creating a pod to test atomic-volume-subpath @ 07/01/23 13:05:01.705
  E0701 13:05:02.356864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:03.357154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:04.357324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:05.358032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:06.358248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:07.359306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:08.359565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:09.359667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:10.360660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:11.361242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:12.362107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:13.362375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:14.362652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:15.362630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:16.362992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:17.363800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:18.364651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:19.364768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:20.364889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:21.365500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:22.366229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:23.366363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:24.366481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:25.366720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:05:25.82
  Jul  1 13:05:25.825: INFO: Trying to get logs from node ip-172-31-12-125 pod pod-subpath-test-configmap-pf74 container test-container-subpath-configmap-pf74: <nil>
  STEP: delete the pod @ 07/01/23 13:05:25.85
  STEP: Deleting pod pod-subpath-test-configmap-pf74 @ 07/01/23 13:05:25.875
  Jul  1 13:05:25.875: INFO: Deleting pod "pod-subpath-test-configmap-pf74" in namespace "subpath-8646"
  Jul  1 13:05:25.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8646" for this suite. @ 07/01/23 13:05:25.89
• [24.259 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 07/01/23 13:05:25.902
  Jul  1 13:05:25.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir @ 07/01/23 13:05:25.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:05:25.925
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:05:25.93
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 07/01/23 13:05:25.936
  E0701 13:05:26.366788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:27.376154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:28.376320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:29.376402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:05:29.972
  Jul  1 13:05:29.983: INFO: Trying to get logs from node ip-172-31-12-125 pod pod-b45786cf-ad99-439b-906a-d89328ec2d86 container test-container: <nil>
  STEP: delete the pod @ 07/01/23 13:05:30.007
  Jul  1 13:05:30.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6151" for this suite. @ 07/01/23 13:05:30.061
• [4.171 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 07/01/23 13:05:30.074
  Jul  1 13:05:30.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename pods @ 07/01/23 13:05:30.078
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:05:30.114
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:05:30.134
  Jul  1 13:05:30.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: creating the pod @ 07/01/23 13:05:30.141
  STEP: submitting the pod to kubernetes @ 07/01/23 13:05:30.141
  E0701 13:05:30.377112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:31.377259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:05:32.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9040" for this suite. @ 07/01/23 13:05:32.26
• [2.198 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 07/01/23 13:05:32.273
  Jul  1 13:05:32.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename deployment @ 07/01/23 13:05:32.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:05:32.295
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:05:32.304
  STEP: creating a Deployment @ 07/01/23 13:05:32.32
  Jul  1 13:05:32.320: INFO: Creating simple deployment test-deployment-wfqjb
  Jul  1 13:05:32.352: INFO: deployment "test-deployment-wfqjb" doesn't have the required revision set
  E0701 13:05:32.378236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:33.378433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Getting /status @ 07/01/23 13:05:34.376
  E0701 13:05:34.379121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:05:34.384: INFO: Deployment test-deployment-wfqjb has Conditions: [{Available True 2023-07-01 13:05:34 +0000 UTC 2023-07-01 13:05:34 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-07-01 13:05:34 +0000 UTC 2023-07-01 13:05:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-wfqjb-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 07/01/23 13:05:34.384
  Jul  1 13:05:34.398: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 13, 5, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 13, 5, 34, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 13, 5, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 13, 5, 32, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-wfqjb-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 07/01/23 13:05:34.399
  Jul  1 13:05:34.402: INFO: Observed &Deployment event: ADDED
  Jul  1 13:05:34.402: INFO: Observed Deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-01 13:05:32 +0000 UTC 2023-07-01 13:05:32 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-wfqjb-5994cf9475"}
  Jul  1 13:05:34.403: INFO: Observed &Deployment event: MODIFIED
  Jul  1 13:05:34.403: INFO: Observed Deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-01 13:05:32 +0000 UTC 2023-07-01 13:05:32 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-wfqjb-5994cf9475"}
  Jul  1 13:05:34.403: INFO: Observed Deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-01 13:05:32 +0000 UTC 2023-07-01 13:05:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul  1 13:05:34.403: INFO: Observed &Deployment event: MODIFIED
  Jul  1 13:05:34.403: INFO: Observed Deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-01 13:05:32 +0000 UTC 2023-07-01 13:05:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul  1 13:05:34.403: INFO: Observed Deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-01 13:05:32 +0000 UTC 2023-07-01 13:05:32 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-wfqjb-5994cf9475" is progressing.}
  Jul  1 13:05:34.404: INFO: Observed &Deployment event: MODIFIED
  Jul  1 13:05:34.404: INFO: Observed Deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-01 13:05:34 +0000 UTC 2023-07-01 13:05:34 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul  1 13:05:34.404: INFO: Observed Deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-01 13:05:34 +0000 UTC 2023-07-01 13:05:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-wfqjb-5994cf9475" has successfully progressed.}
  Jul  1 13:05:34.404: INFO: Observed &Deployment event: MODIFIED
  Jul  1 13:05:34.404: INFO: Observed Deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-01 13:05:34 +0000 UTC 2023-07-01 13:05:34 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul  1 13:05:34.404: INFO: Observed Deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-01 13:05:34 +0000 UTC 2023-07-01 13:05:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-wfqjb-5994cf9475" has successfully progressed.}
  Jul  1 13:05:34.404: INFO: Found Deployment test-deployment-wfqjb in namespace deployment-2710 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul  1 13:05:34.404: INFO: Deployment test-deployment-wfqjb has an updated status
  STEP: patching the Statefulset Status @ 07/01/23 13:05:34.404
  Jul  1 13:05:34.404: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul  1 13:05:34.417: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 07/01/23 13:05:34.417
  Jul  1 13:05:34.420: INFO: Observed &Deployment event: ADDED
  Jul  1 13:05:34.420: INFO: Observed deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-01 13:05:32 +0000 UTC 2023-07-01 13:05:32 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-wfqjb-5994cf9475"}
  Jul  1 13:05:34.421: INFO: Observed &Deployment event: MODIFIED
  Jul  1 13:05:34.421: INFO: Observed deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-01 13:05:32 +0000 UTC 2023-07-01 13:05:32 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-wfqjb-5994cf9475"}
  Jul  1 13:05:34.421: INFO: Observed deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-01 13:05:32 +0000 UTC 2023-07-01 13:05:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul  1 13:05:34.421: INFO: Observed &Deployment event: MODIFIED
  Jul  1 13:05:34.421: INFO: Observed deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-01 13:05:32 +0000 UTC 2023-07-01 13:05:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul  1 13:05:34.421: INFO: Observed deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-01 13:05:32 +0000 UTC 2023-07-01 13:05:32 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-wfqjb-5994cf9475" is progressing.}
  Jul  1 13:05:34.421: INFO: Observed &Deployment event: MODIFIED
  Jul  1 13:05:34.421: INFO: Observed deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-01 13:05:34 +0000 UTC 2023-07-01 13:05:34 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul  1 13:05:34.421: INFO: Observed deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-01 13:05:34 +0000 UTC 2023-07-01 13:05:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-wfqjb-5994cf9475" has successfully progressed.}
  Jul  1 13:05:34.421: INFO: Observed &Deployment event: MODIFIED
  Jul  1 13:05:34.421: INFO: Observed deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-01 13:05:34 +0000 UTC 2023-07-01 13:05:34 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul  1 13:05:34.421: INFO: Observed deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-01 13:05:34 +0000 UTC 2023-07-01 13:05:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-wfqjb-5994cf9475" has successfully progressed.}
  Jul  1 13:05:34.421: INFO: Observed deployment test-deployment-wfqjb in namespace deployment-2710 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul  1 13:05:34.421: INFO: Observed &Deployment event: MODIFIED
  Jul  1 13:05:34.422: INFO: Found deployment test-deployment-wfqjb in namespace deployment-2710 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Jul  1 13:05:34.422: INFO: Deployment test-deployment-wfqjb has a patched status
  Jul  1 13:05:34.429: INFO: Deployment "test-deployment-wfqjb":
  &Deployment{ObjectMeta:{test-deployment-wfqjb  deployment-2710  a06a8d3d-82e7-4cac-9ef4-70e01822bcc7 27142 1 2023-07-01 13:05:32 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-07-01 13:05:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-07-01 13:05:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-07-01 13:05:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005368288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-wfqjb-5994cf9475",LastUpdateTime:2023-07-01 13:05:34 +0000 UTC,LastTransitionTime:2023-07-01 13:05:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul  1 13:05:34.434: INFO: New ReplicaSet "test-deployment-wfqjb-5994cf9475" of Deployment "test-deployment-wfqjb":
  &ReplicaSet{ObjectMeta:{test-deployment-wfqjb-5994cf9475  deployment-2710  2f576f5b-42d9-464c-b801-0908c8c48778 27137 1 2023-07-01 13:05:32 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-wfqjb a06a8d3d-82e7-4cac-9ef4-70e01822bcc7 0xc005368680 0xc005368681}] [] [{kube-controller-manager Update apps/v1 2023-07-01 13:05:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a06a8d3d-82e7-4cac-9ef4-70e01822bcc7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 13:05:34 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005368728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul  1 13:05:34.441: INFO: Pod "test-deployment-wfqjb-5994cf9475-qm7nq" is available:
  &Pod{ObjectMeta:{test-deployment-wfqjb-5994cf9475-qm7nq test-deployment-wfqjb-5994cf9475- deployment-2710  54134b92-fa69-45a3-bf69-2ae0433b288d 27136 0 2023-07-01 13:05:32 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-wfqjb-5994cf9475 2f576f5b-42d9-464c-b801-0908c8c48778 0xc005368ae0 0xc005368ae1}] [] [{kube-controller-manager Update v1 2023-07-01 13:05:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f576f5b-42d9-464c-b801-0908c8c48778\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 13:05:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.62.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7qmds,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7qmds,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:05:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:05:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:05:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:05:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:192.168.62.127,StartTime:2023-07-01 13:05:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-01 13:05:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9b773926fdcf52e65e0de644d395894bdaa0f207f6cd729fd2cb19ebaaca373a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.62.127,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 13:05:34.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2710" for this suite. @ 07/01/23 13:05:34.449
• [2.188 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 07/01/23 13:05:34.464
  Jul  1 13:05:34.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl-logs @ 07/01/23 13:05:34.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:05:34.49
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:05:34.498
  STEP: creating an pod @ 07/01/23 13:05:34.504
  Jul  1 13:05:34.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-logs-8838 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Jul  1 13:05:34.765: INFO: stderr: ""
  Jul  1 13:05:34.765: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 07/01/23 13:05:34.765
  Jul  1 13:05:34.766: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0701 13:05:35.380179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:36.380295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:05:36.782: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 07/01/23 13:05:36.782
  Jul  1 13:05:36.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-logs-8838 logs logs-generator logs-generator'
  Jul  1 13:05:36.955: INFO: stderr: ""
  Jul  1 13:05:36.955: INFO: stdout: "I0701 13:05:35.704768       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/qk8 231\nI0701 13:05:35.904873       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/5s8p 519\nI0701 13:05:36.105432       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/szd 370\nI0701 13:05:36.305753       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/ksqm 408\nI0701 13:05:36.505002       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/xww 334\nI0701 13:05:36.705319       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/qpm 391\nI0701 13:05:36.905617       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/bq8h 475\n"
  STEP: limiting log lines @ 07/01/23 13:05:36.955
  Jul  1 13:05:36.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-logs-8838 logs logs-generator logs-generator --tail=1'
  Jul  1 13:05:37.077: INFO: stderr: ""
  Jul  1 13:05:37.077: INFO: stdout: "I0701 13:05:36.905617       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/bq8h 475\n"
  Jul  1 13:05:37.077: INFO: got output "I0701 13:05:36.905617       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/bq8h 475\n"
  STEP: limiting log bytes @ 07/01/23 13:05:37.077
  Jul  1 13:05:37.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-logs-8838 logs logs-generator logs-generator --limit-bytes=1'
  Jul  1 13:05:37.163: INFO: stderr: ""
  Jul  1 13:05:37.163: INFO: stdout: "I"
  Jul  1 13:05:37.163: INFO: got output "I"
  STEP: exposing timestamps @ 07/01/23 13:05:37.163
  Jul  1 13:05:37.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-logs-8838 logs logs-generator logs-generator --tail=1 --timestamps'
  Jul  1 13:05:37.256: INFO: stderr: ""
  Jul  1 13:05:37.256: INFO: stdout: "2023-07-01T13:05:37.104957626Z I0701 13:05:37.104845       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/vsbs 444\n"
  Jul  1 13:05:37.256: INFO: got output "2023-07-01T13:05:37.104957626Z I0701 13:05:37.104845       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/vsbs 444\n"
  STEP: restricting to a time range @ 07/01/23 13:05:37.256
  E0701 13:05:37.381158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:38.381245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:39.381368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:05:39.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-logs-8838 logs logs-generator logs-generator --since=1s'
  Jul  1 13:05:39.863: INFO: stderr: ""
  Jul  1 13:05:39.863: INFO: stdout: "I0701 13:05:38.905553       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/9pv 340\nI0701 13:05:39.104853       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/b7ph 514\nI0701 13:05:39.305174       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/5g5 400\nI0701 13:05:39.505701       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/r22k 270\nI0701 13:05:39.704873       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/l6d2 432\n"
  Jul  1 13:05:39.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-logs-8838 logs logs-generator logs-generator --since=24h'
  Jul  1 13:05:39.982: INFO: stderr: ""
  Jul  1 13:05:39.982: INFO: stdout: "I0701 13:05:35.704768       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/qk8 231\nI0701 13:05:35.904873       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/5s8p 519\nI0701 13:05:36.105432       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/szd 370\nI0701 13:05:36.305753       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/ksqm 408\nI0701 13:05:36.505002       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/xww 334\nI0701 13:05:36.705319       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/qpm 391\nI0701 13:05:36.905617       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/bq8h 475\nI0701 13:05:37.104845       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/vsbs 444\nI0701 13:05:37.305181       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/ng5 448\nI0701 13:05:37.505496       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/xg5m 412\nI0701 13:05:37.705803       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/z8gv 323\nI0701 13:05:37.905100       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/ppp 599\nI0701 13:05:38.105315       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/6rpg 422\nI0701 13:05:38.305677       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/mj8p 240\nI0701 13:05:38.504964       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/q8j 575\nI0701 13:05:38.705231       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/qcjw 382\nI0701 13:05:38.905553       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/9pv 340\nI0701 13:05:39.104853       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/b7ph 514\nI0701 13:05:39.305174       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/5g5 400\nI0701 13:05:39.505701       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/r22k 270\nI0701 13:05:39.704873       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/l6d2 432\nI0701 13:05:39.905259       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/sxk 380\n"
  Jul  1 13:05:39.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-logs-8838 delete pod logs-generator'
  E0701 13:05:40.382330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:05:41.098: INFO: stderr: ""
  Jul  1 13:05:41.098: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Jul  1 13:05:41.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-8838" for this suite. @ 07/01/23 13:05:41.106
• [6.652 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 07/01/23 13:05:41.116
  Jul  1 13:05:41.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename security-context-test @ 07/01/23 13:05:41.117
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:05:41.141
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:05:41.145
  E0701 13:05:41.382725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:42.382977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:43.383721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:44.383877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:45.384929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:46.385186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:05:47.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4046" for this suite. @ 07/01/23 13:05:47.21
• [6.103 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 07/01/23 13:05:47.219
  Jul  1 13:05:47.219: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename configmap @ 07/01/23 13:05:47.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:05:47.242
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:05:47.248
  STEP: Creating configMap with name configmap-test-volume-b059f9f8-4f66-452f-a794-2b758595fc44 @ 07/01/23 13:05:47.256
  STEP: Creating a pod to test consume configMaps @ 07/01/23 13:05:47.264
  E0701 13:05:47.385894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:48.385937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:49.386964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:50.387463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:05:51.3
  Jul  1 13:05:51.307: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-configmaps-280cd934-0183-4f55-a908-8d1d1f081a6f container agnhost-container: <nil>
  STEP: delete the pod @ 07/01/23 13:05:51.317
  Jul  1 13:05:51.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9013" for this suite. @ 07/01/23 13:05:51.35
• [4.141 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 07/01/23 13:05:51.361
  Jul  1 13:05:51.361: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename services @ 07/01/23 13:05:51.362
  E0701 13:05:51.388060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:05:51.39
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:05:51.395
  STEP: creating an Endpoint @ 07/01/23 13:05:51.405
  STEP: waiting for available Endpoint @ 07/01/23 13:05:51.412
  STEP: listing all Endpoints @ 07/01/23 13:05:51.414
  STEP: updating the Endpoint @ 07/01/23 13:05:51.42
  STEP: fetching the Endpoint @ 07/01/23 13:05:51.431
  STEP: patching the Endpoint @ 07/01/23 13:05:51.438
  STEP: fetching the Endpoint @ 07/01/23 13:05:51.45
  STEP: deleting the Endpoint by Collection @ 07/01/23 13:05:51.461
  STEP: waiting for Endpoint deletion @ 07/01/23 13:05:51.476
  STEP: fetching the Endpoint @ 07/01/23 13:05:51.479
  Jul  1 13:05:51.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2303" for this suite. @ 07/01/23 13:05:51.499
• [0.152 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 07/01/23 13:05:51.514
  Jul  1 13:05:51.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename pods @ 07/01/23 13:05:51.515
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:05:51.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:05:51.542
  STEP: creating the pod @ 07/01/23 13:05:51.547
  STEP: setting up watch @ 07/01/23 13:05:51.547
  STEP: submitting the pod to kubernetes @ 07/01/23 13:05:51.654
  STEP: verifying the pod is in kubernetes @ 07/01/23 13:05:51.668
  STEP: verifying pod creation was observed @ 07/01/23 13:05:51.676
  E0701 13:05:52.388409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:53.389115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 07/01/23 13:05:53.696
  STEP: verifying pod deletion was observed @ 07/01/23 13:05:53.71
  E0701 13:05:54.389975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:05:55.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4871" for this suite. @ 07/01/23 13:05:55.177
• [3.676 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 07/01/23 13:05:55.191
  Jul  1 13:05:55.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename configmap @ 07/01/23 13:05:55.192
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:05:55.303
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:05:55.311
  STEP: Creating configMap configmap-2568/configmap-test-7ac96133-96b7-4ade-890e-47f711b7ff4b @ 07/01/23 13:05:55.317
  STEP: Creating a pod to test consume configMaps @ 07/01/23 13:05:55.328
  E0701 13:05:55.390575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:56.390698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:57.391072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:05:58.391346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:05:59.362
  Jul  1 13:05:59.370: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-configmaps-7ac5cded-c9ff-4fe5-89a8-c1521dffdfea container env-test: <nil>
  STEP: delete the pod @ 07/01/23 13:05:59.379
  E0701 13:05:59.391705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:05:59.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2568" for this suite. @ 07/01/23 13:05:59.411
• [4.230 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 07/01/23 13:05:59.422
  Jul  1 13:05:59.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 13:05:59.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:05:59.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:05:59.452
  STEP: Setting up server cert @ 07/01/23 13:05:59.49
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 13:05:59.969
  STEP: Deploying the webhook pod @ 07/01/23 13:05:59.986
  STEP: Wait for the deployment to be ready @ 07/01/23 13:06:00.014
  Jul  1 13:06:00.031: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0701 13:06:00.392438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:01.392898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:06:02.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 1, 13, 6, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 13, 6, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 1, 13, 6, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 1, 13, 6, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0701 13:06:02.393038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:03.393254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/01/23 13:06:04.054
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 13:06:04.082
  E0701 13:06:04.394546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:06:05.083: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 07/01/23 13:06:05.091
  STEP: create a pod @ 07/01/23 13:06:05.117
  E0701 13:06:05.394675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:06.395215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 07/01/23 13:06:07.144
  Jul  1 13:06:07.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=webhook-8649 attach --namespace=webhook-8649 to-be-attached-pod -i -c=container1'
  Jul  1 13:06:07.258: INFO: rc: 1
  Jul  1 13:06:07.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8649" for this suite. @ 07/01/23 13:06:07.361
  STEP: Destroying namespace "webhook-markers-5976" for this suite. @ 07/01/23 13:06:07.388
  E0701 13:06:07.396238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [7.981 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 07/01/23 13:06:07.405
  Jul  1 13:06:07.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 13:06:07.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:06:07.431
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:06:07.437
  STEP: Setting up server cert @ 07/01/23 13:06:07.506
  E0701 13:06:08.397099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 13:06:08.576
  STEP: Deploying the webhook pod @ 07/01/23 13:06:08.584
  STEP: Wait for the deployment to be ready @ 07/01/23 13:06:08.603
  Jul  1 13:06:08.624: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0701 13:06:09.397447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:10.397722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/01/23 13:06:10.643
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 13:06:10.658
  E0701 13:06:11.398469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:06:11.658: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 07/01/23 13:06:11.665
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/01/23 13:06:11.686
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 07/01/23 13:06:11.698
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/01/23 13:06:11.714
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 07/01/23 13:06:11.739
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/01/23 13:06:11.751
  Jul  1 13:06:11.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8387" for this suite. @ 07/01/23 13:06:11.886
  STEP: Destroying namespace "webhook-markers-9103" for this suite. @ 07/01/23 13:06:11.897
• [4.516 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 07/01/23 13:06:11.921
  Jul  1 13:06:11.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 13:06:11.922
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:06:11.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:06:11.953
  STEP: Setting up server cert @ 07/01/23 13:06:11.998
  E0701 13:06:12.399491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 13:06:12.623
  STEP: Deploying the webhook pod @ 07/01/23 13:06:12.631
  STEP: Wait for the deployment to be ready @ 07/01/23 13:06:12.658
  Jul  1 13:06:12.682: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0701 13:06:13.399619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:14.399716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/01/23 13:06:14.701
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 13:06:14.717
  E0701 13:06:15.400449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:06:15.718: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul  1 13:06:15.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6404-crds.webhook.example.com via the AdmissionRegistration API @ 07/01/23 13:06:16.244
  STEP: Creating a custom resource while v1 is storage version @ 07/01/23 13:06:16.263
  E0701 13:06:16.401074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:17.401185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 07/01/23 13:06:18.322
  STEP: Patching the custom resource while v2 is storage version @ 07/01/23 13:06:18.343
  E0701 13:06:18.401307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:06:18.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9619" for this suite. @ 07/01/23 13:06:19.013
  STEP: Destroying namespace "webhook-markers-43" for this suite. @ 07/01/23 13:06:19.023
• [7.112 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 07/01/23 13:06:19.034
  Jul  1 13:06:19.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename statefulset @ 07/01/23 13:06:19.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:06:19.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:06:19.061
  STEP: Creating service test in namespace statefulset-5969 @ 07/01/23 13:06:19.067
  STEP: Creating stateful set ss in namespace statefulset-5969 @ 07/01/23 13:06:19.082
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5969 @ 07/01/23 13:06:19.094
  Jul  1 13:06:19.101: INFO: Found 0 stateful pods, waiting for 1
  E0701 13:06:19.401380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:20.401531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:21.401751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:22.402204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:23.402331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:24.402485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:25.402854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:26.403280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:27.404068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:28.404177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:06:29.107: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 07/01/23 13:06:29.107
  Jul  1 13:06:29.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-5969 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  1 13:06:29.302: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  1 13:06:29.302: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  1 13:06:29.302: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  1 13:06:29.308: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0701 13:06:29.404633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:30.404939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:31.405499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:32.405843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:33.406253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:34.406326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:35.406450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:36.406929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:37.407443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:38.407574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:06:39.320: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul  1 13:06:39.321: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  1 13:06:39.362: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
  Jul  1 13:06:39.362: INFO: ss-0  ip-172-31-91-66  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:06:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:06:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:06:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:06:19 +0000 UTC  }]
  Jul  1 13:06:39.362: INFO: ss-1                   Pending         []
  Jul  1 13:06:39.362: INFO: 
  Jul  1 13:06:39.362: INFO: StatefulSet ss has not reached scale 3, at 2
  E0701 13:06:39.407855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:06:40.369: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986816393s
  E0701 13:06:40.408772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:06:41.376: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.979225268s
  E0701 13:06:41.409841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:06:42.384: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.972340418s
  E0701 13:06:42.410227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:06:43.390: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.96549089s
  E0701 13:06:43.410664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:06:44.400: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.958652164s
  E0701 13:06:44.411012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:06:45.408: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.948609996s
  E0701 13:06:45.412029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:46.412551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:06:46.417: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.940621811s
  E0701 13:06:47.412653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:06:47.423: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.931881361s
  E0701 13:06:48.413160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:06:48.430: INFO: Verifying statefulset ss doesn't scale past 3 for another 926.344365ms
  E0701 13:06:49.413281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5969 @ 07/01/23 13:06:49.431
  Jul  1 13:06:49.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-5969 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  1 13:06:49.647: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul  1 13:06:49.648: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  1 13:06:49.648: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul  1 13:06:49.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-5969 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  1 13:06:49.844: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jul  1 13:06:49.844: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  1 13:06:49.844: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul  1 13:06:49.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  1 13:06:50.038: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jul  1 13:06:50.038: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  1 13:06:50.038: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul  1 13:06:50.045: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  E0701 13:06:50.413313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:51.414069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:52.414288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:53.414387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:54.414612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:55.414701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:56.415582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:57.415685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:58.415729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:06:59.415851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:00.054: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul  1 13:07:00.054: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul  1 13:07:00.054: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 07/01/23 13:07:00.054
  Jul  1 13:07:00.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-5969 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  1 13:07:00.309: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  1 13:07:00.309: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  1 13:07:00.309: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  1 13:07:00.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-5969 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0701 13:07:00.416210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:00.489: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  1 13:07:00.489: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  1 13:07:00.489: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  1 13:07:00.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=statefulset-5969 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  1 13:07:00.677: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  1 13:07:00.677: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  1 13:07:00.677: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  1 13:07:00.677: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  1 13:07:00.682: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  E0701 13:07:01.416292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:02.416412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:03.416489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:04.416639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:05.416765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:06.416859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:07.416990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:08.417248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:09.417484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:10.417771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:10.696: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul  1 13:07:10.696: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jul  1 13:07:10.696: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jul  1 13:07:10.724: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Jul  1 13:07:10.724: INFO: ss-0  ip-172-31-91-66   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:06:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:07:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:07:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:06:19 +0000 UTC  }]
  Jul  1 13:07:10.724: INFO: ss-1  ip-172-31-12-125  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:06:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:07:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:07:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:06:39 +0000 UTC  }]
  Jul  1 13:07:10.724: INFO: ss-2  ip-172-31-16-94   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:06:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:07:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:07:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:06:39 +0000 UTC  }]
  Jul  1 13:07:10.724: INFO: 
  Jul  1 13:07:10.724: INFO: StatefulSet ss has not reached scale 0, at 3
  E0701 13:07:11.418594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:11.731: INFO: POD   NODE              PHASE      GRACE  CONDITIONS
  Jul  1 13:07:11.731: INFO: ss-0  ip-172-31-91-66   Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:06:19 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:07:00 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:07:00 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:06:19 +0000 UTC  }]
  Jul  1 13:07:11.731: INFO: ss-1  ip-172-31-12-125  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:06:39 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:07:00 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:07:00 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-01 13:06:39 +0000 UTC  }]
  Jul  1 13:07:11.731: INFO: 
  Jul  1 13:07:11.731: INFO: StatefulSet ss has not reached scale 0, at 2
  E0701 13:07:12.418940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:12.737: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.981838048s
  E0701 13:07:13.419592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:13.746: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.9761818s
  E0701 13:07:14.420845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:14.752: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.968078116s
  E0701 13:07:15.421539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:15.761: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.961271298s
  E0701 13:07:16.421812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:16.767: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.951938307s
  E0701 13:07:17.421948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:17.774: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.946775209s
  E0701 13:07:18.423006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:18.781: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.939643994s
  E0701 13:07:19.423850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:19.787: INFO: Verifying statefulset ss doesn't scale past 0 for another 932.96174ms
  E0701 13:07:20.424421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5969 @ 07/01/23 13:07:20.787
  Jul  1 13:07:20.796: INFO: Scaling statefulset ss to 0
  Jul  1 13:07:20.816: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  1 13:07:20.820: INFO: Deleting all statefulset in ns statefulset-5969
  Jul  1 13:07:20.827: INFO: Scaling statefulset ss to 0
  Jul  1 13:07:20.847: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  1 13:07:20.853: INFO: Deleting statefulset ss
  Jul  1 13:07:20.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5969" for this suite. @ 07/01/23 13:07:20.88
• [61.860 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 07/01/23 13:07:20.901
  Jul  1 13:07:20.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl @ 07/01/23 13:07:20.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:07:20.931
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:07:20.935
  STEP: creating Agnhost RC @ 07/01/23 13:07:20.941
  Jul  1 13:07:20.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-6452 create -f -'
  E0701 13:07:21.424627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:21.429: INFO: stderr: ""
  Jul  1 13:07:21.429: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/01/23 13:07:21.429
  E0701 13:07:22.425301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:22.438: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  1 13:07:22.438: INFO: Found 0 / 1
  E0701 13:07:23.425759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:23.445: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  1 13:07:23.445: INFO: Found 1 / 1
  Jul  1 13:07:23.445: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 07/01/23 13:07:23.446
  Jul  1 13:07:23.451: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  1 13:07:23.451: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul  1 13:07:23.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-6452 patch pod agnhost-primary-5n98c -p {"metadata":{"annotations":{"x":"y"}}}'
  Jul  1 13:07:23.637: INFO: stderr: ""
  Jul  1 13:07:23.637: INFO: stdout: "pod/agnhost-primary-5n98c patched\n"
  STEP: checking annotations @ 07/01/23 13:07:23.637
  Jul  1 13:07:23.651: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  1 13:07:23.651: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul  1 13:07:23.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6452" for this suite. @ 07/01/23 13:07:23.658
• [2.768 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 07/01/23 13:07:23.671
  Jul  1 13:07:23.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 13:07:23.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:07:23.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:07:23.708
  STEP: Setting up server cert @ 07/01/23 13:07:23.76
  E0701 13:07:24.426336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 13:07:24.678
  STEP: Deploying the webhook pod @ 07/01/23 13:07:24.693
  STEP: Wait for the deployment to be ready @ 07/01/23 13:07:24.712
  Jul  1 13:07:24.729: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0701 13:07:25.427119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:26.427272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/01/23 13:07:26.747
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 13:07:26.776
  E0701 13:07:27.427546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:27.776: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 07/01/23 13:07:27.786
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 07/01/23 13:07:27.79
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 07/01/23 13:07:27.79
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 07/01/23 13:07:27.79
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 07/01/23 13:07:27.793
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 07/01/23 13:07:27.793
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 07/01/23 13:07:27.795
  Jul  1 13:07:27.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5050" for this suite. @ 07/01/23 13:07:27.873
  STEP: Destroying namespace "webhook-markers-4935" for this suite. @ 07/01/23 13:07:27.888
• [4.233 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 07/01/23 13:07:27.906
  Jul  1 13:07:27.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename pods @ 07/01/23 13:07:27.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:07:27.941
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:07:27.948
  STEP: Create set of pods @ 07/01/23 13:07:27.954
  Jul  1 13:07:27.971: INFO: created test-pod-1
  Jul  1 13:07:27.981: INFO: created test-pod-2
  Jul  1 13:07:28.004: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 07/01/23 13:07:28.004
  E0701 13:07:28.427668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:29.428689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 07/01/23 13:07:30.092
  Jul  1 13:07:30.099: INFO: Pod quantity 3 is different from expected quantity 0
  E0701 13:07:30.428809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:31.108: INFO: Pod quantity 3 is different from expected quantity 0
  E0701 13:07:31.429315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:32.108: INFO: Pod quantity 1 is different from expected quantity 0
  E0701 13:07:32.429932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:33.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2858" for this suite. @ 07/01/23 13:07:33.122
• [5.232 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 07/01/23 13:07:33.14
  Jul  1 13:07:33.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/01/23 13:07:33.141
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:07:33.175
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:07:33.187
  Jul  1 13:07:33.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  E0701 13:07:33.429880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:34.430722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/01/23 13:07:34.7
  Jul  1 13:07:34.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-8576 --namespace=crd-publish-openapi-8576 create -f -'
  E0701 13:07:35.430816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:35.445: INFO: stderr: ""
  Jul  1 13:07:35.445: INFO: stdout: "e2e-test-crd-publish-openapi-9803-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jul  1 13:07:35.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-8576 --namespace=crd-publish-openapi-8576 delete e2e-test-crd-publish-openapi-9803-crds test-cr'
  Jul  1 13:07:35.643: INFO: stderr: ""
  Jul  1 13:07:35.643: INFO: stdout: "e2e-test-crd-publish-openapi-9803-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Jul  1 13:07:35.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-8576 --namespace=crd-publish-openapi-8576 apply -f -'
  E0701 13:07:36.431776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:36.820: INFO: stderr: ""
  Jul  1 13:07:36.820: INFO: stdout: "e2e-test-crd-publish-openapi-9803-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jul  1 13:07:36.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-8576 --namespace=crd-publish-openapi-8576 delete e2e-test-crd-publish-openapi-9803-crds test-cr'
  Jul  1 13:07:36.901: INFO: stderr: ""
  Jul  1 13:07:36.901: INFO: stdout: "e2e-test-crd-publish-openapi-9803-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 07/01/23 13:07:36.901
  Jul  1 13:07:36.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-8576 explain e2e-test-crd-publish-openapi-9803-crds'
  Jul  1 13:07:37.180: INFO: stderr: ""
  Jul  1 13:07:37.180: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-9803-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0701 13:07:37.432160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:38.433093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:07:38.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8576" for this suite. @ 07/01/23 13:07:38.822
• [5.699 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 07/01/23 13:07:38.86
  Jul  1 13:07:38.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-probe @ 07/01/23 13:07:38.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:07:38.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:07:38.887
  E0701 13:07:39.433913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:40.434036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:41.434367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:42.434790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:43.434871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:44.435098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:45.436004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:46.438655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:47.438735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:48.438834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:49.439485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:50.439576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:51.440207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:52.440781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:53.440890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:54.440993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:55.441156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:56.441249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:57.441628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:58.441981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:07:59.442088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:00.442313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:08:01.007: INFO: Container started at 2023-07-01 13:07:39 +0000 UTC, pod became ready at 2023-07-01 13:07:59 +0000 UTC
  Jul  1 13:08:01.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1057" for this suite. @ 07/01/23 13:08:01.013
• [22.171 seconds]
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 07/01/23 13:08:01.031
  Jul  1 13:08:01.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename proxy @ 07/01/23 13:08:01.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:08:01.055
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:08:01.061
  STEP: starting an echo server on multiple ports @ 07/01/23 13:08:01.091
  STEP: creating replication controller proxy-service-8wghd in namespace proxy-9072 @ 07/01/23 13:08:01.091
  I0701 13:08:01.112951      19 runners.go:194] Created replication controller with name: proxy-service-8wghd, namespace: proxy-9072, replica count: 1
  E0701 13:08:01.443314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0701 13:08:02.164297      19 runners.go:194] proxy-service-8wghd Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0701 13:08:02.443821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0701 13:08:03.164451      19 runners.go:194] proxy-service-8wghd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0701 13:08:03.443864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0701 13:08:04.165538      19 runners.go:194] proxy-service-8wghd Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  1 13:08:04.172: INFO: setup took 3.105747358s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 07/01/23 13:08:04.172
  Jul  1 13:08:04.185: INFO: (0) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 13.056184ms)
  Jul  1 13:08:04.191: INFO: (0) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 17.824341ms)
  Jul  1 13:08:04.191: INFO: (0) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 17.706851ms)
  Jul  1 13:08:04.192: INFO: (0) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 19.00646ms)
  Jul  1 13:08:04.193: INFO: (0) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 19.617971ms)
  Jul  1 13:08:04.193: INFO: (0) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 20.278487ms)
  Jul  1 13:08:04.194: INFO: (0) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 20.764033ms)
  Jul  1 13:08:04.194: INFO: (0) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 21.496019ms)
  Jul  1 13:08:04.198: INFO: (0) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 25.238978ms)
  Jul  1 13:08:04.199: INFO: (0) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 25.618102ms)
  Jul  1 13:08:04.200: INFO: (0) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 26.682693ms)
  Jul  1 13:08:04.200: INFO: (0) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 27.181957ms)
  Jul  1 13:08:04.201: INFO: (0) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 28.14245ms)
  Jul  1 13:08:04.201: INFO: (0) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 28.464368ms)
  Jul  1 13:08:04.202: INFO: (0) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 28.821635ms)
  Jul  1 13:08:04.202: INFO: (0) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 29.139106ms)
  Jul  1 13:08:04.209: INFO: (1) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 7.271188ms)
  Jul  1 13:08:04.211: INFO: (1) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 8.92037ms)
  Jul  1 13:08:04.213: INFO: (1) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 10.402697ms)
  Jul  1 13:08:04.214: INFO: (1) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 10.688496ms)
  Jul  1 13:08:04.215: INFO: (1) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 12.608833ms)
  Jul  1 13:08:04.217: INFO: (1) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 13.517201ms)
  Jul  1 13:08:04.217: INFO: (1) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 13.796901ms)
  Jul  1 13:08:04.217: INFO: (1) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 14.234515ms)
  Jul  1 13:08:04.219: INFO: (1) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 15.96633ms)
  Jul  1 13:08:04.219: INFO: (1) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 16.912733ms)
  Jul  1 13:08:04.219: INFO: (1) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 16.749703ms)
  Jul  1 13:08:04.219: INFO: (1) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 17.085239ms)
  Jul  1 13:08:04.221: INFO: (1) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 17.899881ms)
  Jul  1 13:08:04.221: INFO: (1) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 18.283857ms)
  Jul  1 13:08:04.221: INFO: (1) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 18.089338ms)
  Jul  1 13:08:04.228: INFO: (1) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 25.462739ms)
  Jul  1 13:08:04.239: INFO: (2) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 9.481556ms)
  Jul  1 13:08:04.240: INFO: (2) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 11.278218ms)
  Jul  1 13:08:04.241: INFO: (2) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 11.793183ms)
  Jul  1 13:08:04.241: INFO: (2) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 12.520232ms)
  Jul  1 13:08:04.242: INFO: (2) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 13.141806ms)
  Jul  1 13:08:04.244: INFO: (2) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 15.022001ms)
  Jul  1 13:08:04.245: INFO: (2) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 15.859578ms)
  Jul  1 13:08:04.245: INFO: (2) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 16.258655ms)
  Jul  1 13:08:04.246: INFO: (2) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 16.840668ms)
  Jul  1 13:08:04.246: INFO: (2) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 16.742111ms)
  Jul  1 13:08:04.246: INFO: (2) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 16.923236ms)
  Jul  1 13:08:04.246: INFO: (2) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 17.40801ms)
  Jul  1 13:08:04.246: INFO: (2) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 17.043198ms)
  Jul  1 13:08:04.246: INFO: (2) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 17.78782ms)
  Jul  1 13:08:04.246: INFO: (2) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 18.127553ms)
  Jul  1 13:08:04.246: INFO: (2) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 17.537227ms)
  Jul  1 13:08:04.253: INFO: (3) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 5.982727ms)
  Jul  1 13:08:04.255: INFO: (3) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 8.495536ms)
  Jul  1 13:08:04.255: INFO: (3) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 8.365342ms)
  Jul  1 13:08:04.256: INFO: (3) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 8.711622ms)
  Jul  1 13:08:04.256: INFO: (3) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 8.701649ms)
  Jul  1 13:08:04.257: INFO: (3) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 9.051065ms)
  Jul  1 13:08:04.259: INFO: (3) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 11.703527ms)
  Jul  1 13:08:04.259: INFO: (3) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 11.334451ms)
  Jul  1 13:08:04.260: INFO: (3) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 12.521758ms)
  Jul  1 13:08:04.260: INFO: (3) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 12.621894ms)
  Jul  1 13:08:04.260: INFO: (3) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 12.463108ms)
  Jul  1 13:08:04.260: INFO: (3) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 13.45322ms)
  Jul  1 13:08:04.262: INFO: (3) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 14.428584ms)
  Jul  1 13:08:04.262: INFO: (3) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 15.476775ms)
  Jul  1 13:08:04.262: INFO: (3) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 14.823697ms)
  Jul  1 13:08:04.263: INFO: (3) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 16.450123ms)
  Jul  1 13:08:04.270: INFO: (4) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 6.405823ms)
  Jul  1 13:08:04.272: INFO: (4) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 7.673328ms)
  Jul  1 13:08:04.273: INFO: (4) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 9.320034ms)
  Jul  1 13:08:04.276: INFO: (4) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 12.013399ms)
  Jul  1 13:08:04.276: INFO: (4) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 11.780713ms)
  Jul  1 13:08:04.277: INFO: (4) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 13.395127ms)
  Jul  1 13:08:04.278: INFO: (4) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 14.014459ms)
  Jul  1 13:08:04.278: INFO: (4) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 14.532746ms)
  Jul  1 13:08:04.279: INFO: (4) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 14.451666ms)
  Jul  1 13:08:04.279: INFO: (4) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 14.658711ms)
  Jul  1 13:08:04.279: INFO: (4) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 15.000075ms)
  Jul  1 13:08:04.280: INFO: (4) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 15.795475ms)
  Jul  1 13:08:04.280: INFO: (4) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 16.4808ms)
  Jul  1 13:08:04.280: INFO: (4) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 16.146678ms)
  Jul  1 13:08:04.281: INFO: (4) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 17.364102ms)
  Jul  1 13:08:04.284: INFO: (4) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 19.672647ms)
  Jul  1 13:08:04.296: INFO: (5) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 11.073371ms)
  Jul  1 13:08:04.296: INFO: (5) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 12.037517ms)
  Jul  1 13:08:04.299: INFO: (5) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 14.736146ms)
  Jul  1 13:08:04.299: INFO: (5) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 14.483422ms)
  Jul  1 13:08:04.299: INFO: (5) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 14.363837ms)
  Jul  1 13:08:04.299: INFO: (5) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 15.326306ms)
  Jul  1 13:08:04.300: INFO: (5) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 15.524784ms)
  Jul  1 13:08:04.300: INFO: (5) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 16.145826ms)
  Jul  1 13:08:04.300: INFO: (5) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 15.852195ms)
  Jul  1 13:08:04.300: INFO: (5) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 15.156985ms)
  Jul  1 13:08:04.300: INFO: (5) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 15.45111ms)
  Jul  1 13:08:04.300: INFO: (5) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 15.74774ms)
  Jul  1 13:08:04.301: INFO: (5) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 16.502939ms)
  Jul  1 13:08:04.301: INFO: (5) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 16.280405ms)
  Jul  1 13:08:04.301: INFO: (5) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 17.122081ms)
  Jul  1 13:08:04.301: INFO: (5) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 17.434223ms)
  Jul  1 13:08:04.310: INFO: (6) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 7.349158ms)
  Jul  1 13:08:04.310: INFO: (6) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 8.406763ms)
  Jul  1 13:08:04.310: INFO: (6) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 7.86681ms)
  Jul  1 13:08:04.311: INFO: (6) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 8.634155ms)
  Jul  1 13:08:04.314: INFO: (6) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 10.73767ms)
  Jul  1 13:08:04.314: INFO: (6) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 10.594256ms)
  Jul  1 13:08:04.316: INFO: (6) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 12.816944ms)
  Jul  1 13:08:04.317: INFO: (6) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 13.431834ms)
  Jul  1 13:08:04.317: INFO: (6) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 13.783461ms)
  Jul  1 13:08:04.319: INFO: (6) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 15.875919ms)
  Jul  1 13:08:04.319: INFO: (6) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 16.313777ms)
  Jul  1 13:08:04.320: INFO: (6) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 17.841001ms)
  Jul  1 13:08:04.320: INFO: (6) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 16.785744ms)
  Jul  1 13:08:04.320: INFO: (6) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 16.623948ms)
  Jul  1 13:08:04.320: INFO: (6) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 17.633831ms)
  Jul  1 13:08:04.322: INFO: (6) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 18.980935ms)
  Jul  1 13:08:04.328: INFO: (7) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 6.521043ms)
  Jul  1 13:08:04.329: INFO: (7) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 7.062672ms)
  Jul  1 13:08:04.330: INFO: (7) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 7.40139ms)
  Jul  1 13:08:04.333: INFO: (7) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 10.527678ms)
  Jul  1 13:08:04.333: INFO: (7) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 11.057746ms)
  Jul  1 13:08:04.344: INFO: (7) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 21.551097ms)
  Jul  1 13:08:04.347: INFO: (7) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 24.471456ms)
  Jul  1 13:08:04.347: INFO: (7) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 24.770229ms)
  Jul  1 13:08:04.347: INFO: (7) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 24.932423ms)
  Jul  1 13:08:04.348: INFO: (7) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 24.915778ms)
  Jul  1 13:08:04.350: INFO: (7) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 26.690245ms)
  Jul  1 13:08:04.350: INFO: (7) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 27.104113ms)
  Jul  1 13:08:04.351: INFO: (7) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 28.599829ms)
  Jul  1 13:08:04.355: INFO: (7) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 32.827614ms)
  Jul  1 13:08:04.355: INFO: (7) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 32.422887ms)
  Jul  1 13:08:04.357: INFO: (7) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 35.505259ms)
  Jul  1 13:08:04.365: INFO: (8) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 7.264403ms)
  Jul  1 13:08:04.376: INFO: (8) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 18.373463ms)
  Jul  1 13:08:04.376: INFO: (8) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 17.830592ms)
  Jul  1 13:08:04.377: INFO: (8) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 18.809274ms)
  Jul  1 13:08:04.379: INFO: (8) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 20.984975ms)
  Jul  1 13:08:04.380: INFO: (8) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 21.368261ms)
  Jul  1 13:08:04.380: INFO: (8) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 21.848579ms)
  Jul  1 13:08:04.380: INFO: (8) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 22.481082ms)
  Jul  1 13:08:04.381: INFO: (8) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 22.79552ms)
  Jul  1 13:08:04.381: INFO: (8) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 22.747561ms)
  Jul  1 13:08:04.381: INFO: (8) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 22.969728ms)
  Jul  1 13:08:04.381: INFO: (8) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 22.85397ms)
  Jul  1 13:08:04.383: INFO: (8) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 25.176511ms)
  Jul  1 13:08:04.384: INFO: (8) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 25.612847ms)
  Jul  1 13:08:04.384: INFO: (8) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 25.5992ms)
  Jul  1 13:08:04.384: INFO: (8) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 25.846747ms)
  Jul  1 13:08:04.392: INFO: (9) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 7.962522ms)
  Jul  1 13:08:04.392: INFO: (9) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 8.020696ms)
  Jul  1 13:08:04.392: INFO: (9) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 7.812419ms)
  Jul  1 13:08:04.394: INFO: (9) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 10.056742ms)
  Jul  1 13:08:04.395: INFO: (9) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 10.466721ms)
  Jul  1 13:08:04.396: INFO: (9) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 11.39964ms)
  Jul  1 13:08:04.396: INFO: (9) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 12.065039ms)
  Jul  1 13:08:04.402: INFO: (9) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 17.346346ms)
  Jul  1 13:08:04.404: INFO: (9) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 19.458667ms)
  Jul  1 13:08:04.404: INFO: (9) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 19.702081ms)
  Jul  1 13:08:04.404: INFO: (9) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 20.133439ms)
  Jul  1 13:08:04.406: INFO: (9) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 21.256131ms)
  Jul  1 13:08:04.407: INFO: (9) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 22.498679ms)
  Jul  1 13:08:04.407: INFO: (9) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 22.892143ms)
  Jul  1 13:08:04.407: INFO: (9) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 23.136106ms)
  Jul  1 13:08:04.409: INFO: (9) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 24.209453ms)
  Jul  1 13:08:04.418: INFO: (10) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 9.309059ms)
  Jul  1 13:08:04.418: INFO: (10) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 9.403794ms)
  Jul  1 13:08:04.428: INFO: (10) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 18.7003ms)
  Jul  1 13:08:04.429: INFO: (10) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 19.53677ms)
  Jul  1 13:08:04.429: INFO: (10) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 19.972249ms)
  Jul  1 13:08:04.430: INFO: (10) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 20.210881ms)
  Jul  1 13:08:04.431: INFO: (10) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 21.622219ms)
  Jul  1 13:08:04.431: INFO: (10) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 21.565535ms)
  Jul  1 13:08:04.432: INFO: (10) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 22.392837ms)
  Jul  1 13:08:04.440: INFO: (10) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 30.493922ms)
  Jul  1 13:08:04.441: INFO: (10) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 31.892154ms)
  Jul  1 13:08:04.441: INFO: (10) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 31.516662ms)
  Jul  1 13:08:04.441: INFO: (10) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 31.255055ms)
  Jul  1 13:08:04.441: INFO: (10) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 31.764002ms)
  Jul  1 13:08:04.442: INFO: (10) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 32.437552ms)
  Jul  1 13:08:04.443: INFO: (10) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 33.409781ms)
  E0701 13:08:04.443976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:08:04.451: INFO: (11) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 7.799149ms)
  Jul  1 13:08:04.451: INFO: (11) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 7.919992ms)
  Jul  1 13:08:04.452: INFO: (11) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 8.423414ms)
  Jul  1 13:08:04.454: INFO: (11) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 10.433711ms)
  Jul  1 13:08:04.454: INFO: (11) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 10.934548ms)
  Jul  1 13:08:04.454: INFO: (11) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 10.856219ms)
  Jul  1 13:08:04.455: INFO: (11) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 12.423151ms)
  Jul  1 13:08:04.456: INFO: (11) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 12.401545ms)
  Jul  1 13:08:04.461: INFO: (11) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 17.28638ms)
  Jul  1 13:08:04.461: INFO: (11) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 17.536129ms)
  Jul  1 13:08:04.461: INFO: (11) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 17.140804ms)
  Jul  1 13:08:04.462: INFO: (11) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 18.445557ms)
  Jul  1 13:08:04.463: INFO: (11) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 18.717213ms)
  Jul  1 13:08:04.464: INFO: (11) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 21.059754ms)
  Jul  1 13:08:04.464: INFO: (11) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 20.92113ms)
  Jul  1 13:08:04.464: INFO: (11) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 21.183317ms)
  Jul  1 13:08:04.480: INFO: (12) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 13.901293ms)
  Jul  1 13:08:04.480: INFO: (12) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 14.365179ms)
  Jul  1 13:08:04.483: INFO: (12) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 17.419759ms)
  Jul  1 13:08:04.486: INFO: (12) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 19.56566ms)
  Jul  1 13:08:04.487: INFO: (12) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 20.919974ms)
  Jul  1 13:08:04.488: INFO: (12) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 22.036782ms)
  Jul  1 13:08:04.488: INFO: (12) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 22.163472ms)
  Jul  1 13:08:04.491: INFO: (12) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 25.707402ms)
  Jul  1 13:08:04.491: INFO: (12) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 24.833244ms)
  Jul  1 13:08:04.491: INFO: (12) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 24.960431ms)
  Jul  1 13:08:04.491: INFO: (12) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 25.083331ms)
  Jul  1 13:08:04.492: INFO: (12) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 26.644244ms)
  Jul  1 13:08:04.492: INFO: (12) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 25.759199ms)
  Jul  1 13:08:04.492: INFO: (12) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 25.733301ms)
  Jul  1 13:08:04.493: INFO: (12) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 28.371071ms)
  Jul  1 13:08:04.493: INFO: (12) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 27.350137ms)
  Jul  1 13:08:04.506: INFO: (13) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 11.820506ms)
  Jul  1 13:08:04.508: INFO: (13) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 14.237501ms)
  Jul  1 13:08:04.509: INFO: (13) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 14.511438ms)
  Jul  1 13:08:04.511: INFO: (13) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 16.510891ms)
  Jul  1 13:08:04.511: INFO: (13) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 16.668444ms)
  Jul  1 13:08:04.512: INFO: (13) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 18.159733ms)
  Jul  1 13:08:04.516: INFO: (13) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 21.157559ms)
  Jul  1 13:08:04.516: INFO: (13) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 21.92302ms)
  Jul  1 13:08:04.516: INFO: (13) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 21.664599ms)
  Jul  1 13:08:04.516: INFO: (13) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 21.74149ms)
  Jul  1 13:08:04.516: INFO: (13) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 21.902645ms)
  Jul  1 13:08:04.516: INFO: (13) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 21.656743ms)
  Jul  1 13:08:04.517: INFO: (13) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 23.336919ms)
  Jul  1 13:08:04.517: INFO: (13) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 22.901224ms)
  Jul  1 13:08:04.518: INFO: (13) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 22.889232ms)
  Jul  1 13:08:04.518: INFO: (13) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 23.773945ms)
  Jul  1 13:08:04.537: INFO: (14) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 18.744644ms)
  Jul  1 13:08:04.538: INFO: (14) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 19.123246ms)
  Jul  1 13:08:04.540: INFO: (14) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 20.829335ms)
  Jul  1 13:08:04.542: INFO: (14) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 22.322416ms)
  Jul  1 13:08:04.542: INFO: (14) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 23.863235ms)
  Jul  1 13:08:04.542: INFO: (14) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 23.192068ms)
  Jul  1 13:08:04.543: INFO: (14) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 24.142221ms)
  Jul  1 13:08:04.543: INFO: (14) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 23.977519ms)
  Jul  1 13:08:04.543: INFO: (14) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 24.284664ms)
  Jul  1 13:08:04.547: INFO: (14) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 28.463283ms)
  Jul  1 13:08:04.547: INFO: (14) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 28.139176ms)
  Jul  1 13:08:04.547: INFO: (14) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 28.915938ms)
  Jul  1 13:08:04.548: INFO: (14) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 28.627128ms)
  Jul  1 13:08:04.548: INFO: (14) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 28.672188ms)
  Jul  1 13:08:04.548: INFO: (14) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 28.853241ms)
  Jul  1 13:08:04.548: INFO: (14) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 29.780649ms)
  Jul  1 13:08:04.556: INFO: (15) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 7.322701ms)
  Jul  1 13:08:04.557: INFO: (15) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 8.935897ms)
  Jul  1 13:08:04.558: INFO: (15) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 9.590987ms)
  Jul  1 13:08:04.560: INFO: (15) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 11.439171ms)
  Jul  1 13:08:04.562: INFO: (15) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 13.82744ms)
  Jul  1 13:08:04.562: INFO: (15) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 13.18604ms)
  Jul  1 13:08:04.563: INFO: (15) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 14.550434ms)
  Jul  1 13:08:04.564: INFO: (15) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 15.49302ms)
  Jul  1 13:08:04.564: INFO: (15) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 15.126623ms)
  Jul  1 13:08:04.566: INFO: (15) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 16.590899ms)
  Jul  1 13:08:04.566: INFO: (15) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 17.371909ms)
  Jul  1 13:08:04.567: INFO: (15) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 17.934579ms)
  Jul  1 13:08:04.568: INFO: (15) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 19.696052ms)
  Jul  1 13:08:04.568: INFO: (15) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 19.148527ms)
  Jul  1 13:08:04.568: INFO: (15) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 19.153847ms)
  Jul  1 13:08:04.569: INFO: (15) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 20.827103ms)
  Jul  1 13:08:04.578: INFO: (16) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 8.359296ms)
  Jul  1 13:08:04.579: INFO: (16) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 8.488119ms)
  Jul  1 13:08:04.579: INFO: (16) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 8.344802ms)
  Jul  1 13:08:04.581: INFO: (16) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 9.57543ms)
  Jul  1 13:08:04.581: INFO: (16) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 10.129761ms)
  Jul  1 13:08:04.582: INFO: (16) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 11.438102ms)
  Jul  1 13:08:04.583: INFO: (16) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 12.006036ms)
  Jul  1 13:08:04.588: INFO: (16) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 16.620556ms)
  Jul  1 13:08:04.588: INFO: (16) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 16.993291ms)
  Jul  1 13:08:04.588: INFO: (16) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 17.771194ms)
  Jul  1 13:08:04.588: INFO: (16) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 16.757241ms)
  Jul  1 13:08:04.588: INFO: (16) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 17.899945ms)
  Jul  1 13:08:04.588: INFO: (16) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 17.734805ms)
  Jul  1 13:08:04.591: INFO: (16) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 19.788849ms)
  Jul  1 13:08:04.592: INFO: (16) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 20.457345ms)
  Jul  1 13:08:04.592: INFO: (16) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 20.92668ms)
  Jul  1 13:08:04.604: INFO: (17) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 11.809933ms)
  Jul  1 13:08:04.604: INFO: (17) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 11.946231ms)
  Jul  1 13:08:04.604: INFO: (17) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 12.172245ms)
  Jul  1 13:08:04.606: INFO: (17) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 13.710508ms)
  Jul  1 13:08:04.611: INFO: (17) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 18.679439ms)
  Jul  1 13:08:04.612: INFO: (17) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 19.166577ms)
  Jul  1 13:08:04.612: INFO: (17) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 19.821392ms)
  Jul  1 13:08:04.614: INFO: (17) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 21.773726ms)
  Jul  1 13:08:04.621: INFO: (17) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 28.028052ms)
  Jul  1 13:08:04.621: INFO: (17) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 28.599ms)
  Jul  1 13:08:04.621: INFO: (17) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 28.397474ms)
  Jul  1 13:08:04.621: INFO: (17) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 28.172764ms)
  Jul  1 13:08:04.621: INFO: (17) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 28.930584ms)
  Jul  1 13:08:04.621: INFO: (17) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 28.942377ms)
  Jul  1 13:08:04.621: INFO: (17) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 29.108155ms)
  Jul  1 13:08:04.622: INFO: (17) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 29.422809ms)
  Jul  1 13:08:04.635: INFO: (18) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 12.759856ms)
  Jul  1 13:08:04.635: INFO: (18) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 12.355125ms)
  Jul  1 13:08:04.635: INFO: (18) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 12.063357ms)
  Jul  1 13:08:04.638: INFO: (18) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 15.250372ms)
  Jul  1 13:08:04.639: INFO: (18) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 16.260403ms)
  Jul  1 13:08:04.639: INFO: (18) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 16.524264ms)
  Jul  1 13:08:04.639: INFO: (18) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 16.117982ms)
  Jul  1 13:08:04.641: INFO: (18) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 17.889261ms)
  Jul  1 13:08:04.641: INFO: (18) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 18.031803ms)
  Jul  1 13:08:04.641: INFO: (18) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 19.485199ms)
  Jul  1 13:08:04.642: INFO: (18) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 18.741003ms)
  Jul  1 13:08:04.642: INFO: (18) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 19.410406ms)
  Jul  1 13:08:04.643: INFO: (18) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 19.638167ms)
  Jul  1 13:08:04.645: INFO: (18) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 22.190208ms)
  Jul  1 13:08:04.645: INFO: (18) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 22.010772ms)
  Jul  1 13:08:04.647: INFO: (18) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 24.336702ms)
  Jul  1 13:08:04.655: INFO: (19) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 6.976524ms)
  Jul  1 13:08:04.656: INFO: (19) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:443/proxy/tlsrewritem... (200; 8.139216ms)
  Jul  1 13:08:04.661: INFO: (19) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 12.213197ms)
  Jul  1 13:08:04.661: INFO: (19) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">... (200; 12.882849ms)
  Jul  1 13:08:04.661: INFO: (19) /api/v1/namespaces/proxy-9072/pods/http:proxy-service-8wghd-qzfv2:162/proxy/: bar (200; 12.554099ms)
  Jul  1 13:08:04.663: INFO: (19) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:160/proxy/: foo (200; 15.123153ms)
  Jul  1 13:08:04.663: INFO: (19) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2/proxy/rewriteme">test</a> (200; 14.663108ms)
  Jul  1 13:08:04.663: INFO: (19) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:462/proxy/: tls qux (200; 14.673197ms)
  Jul  1 13:08:04.663: INFO: (19) /api/v1/namespaces/proxy-9072/pods/https:proxy-service-8wghd-qzfv2:460/proxy/: tls baz (200; 15.24025ms)
  Jul  1 13:08:04.665: INFO: (19) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname2/proxy/: tls qux (200; 16.858663ms)
  Jul  1 13:08:04.665: INFO: (19) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname1/proxy/: foo (200; 17.161194ms)
  Jul  1 13:08:04.667: INFO: (19) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname1/proxy/: foo (200; 18.449612ms)
  Jul  1 13:08:04.667: INFO: (19) /api/v1/namespaces/proxy-9072/services/http:proxy-service-8wghd:portname2/proxy/: bar (200; 17.985439ms)
  Jul  1 13:08:04.667: INFO: (19) /api/v1/namespaces/proxy-9072/services/https:proxy-service-8wghd:tlsportname1/proxy/: tls baz (200; 18.111191ms)
  Jul  1 13:08:04.668: INFO: (19) /api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9072/pods/proxy-service-8wghd-qzfv2:1080/proxy/rewriteme">test<... (200; 20.358235ms)
  Jul  1 13:08:04.672: INFO: (19) /api/v1/namespaces/proxy-9072/services/proxy-service-8wghd:portname2/proxy/: bar (200; 24.203676ms)
  Jul  1 13:08:04.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-8wghd in namespace proxy-9072, will wait for the garbage collector to delete the pods @ 07/01/23 13:08:04.679
  Jul  1 13:08:04.746: INFO: Deleting ReplicationController proxy-service-8wghd took: 9.842925ms
  Jul  1 13:08:04.847: INFO: Terminating ReplicationController proxy-service-8wghd pods took: 101.049116ms
  E0701 13:08:05.444206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:06.444319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-9072" for this suite. @ 07/01/23 13:08:06.649
• [5.632 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 07/01/23 13:08:06.664
  Jul  1 13:08:06.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename var-expansion @ 07/01/23 13:08:06.665
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:08:06.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:08:06.69
  STEP: creating the pod with failed condition @ 07/01/23 13:08:06.694
  E0701 13:08:07.444609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:08.444727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:09.444972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:10.445265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:11.445520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:12.445799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:13.445905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:14.446048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:15.446149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:16.446448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:17.446510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:18.446636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:19.446783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:20.446920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:21.447662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:22.447889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:23.448664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:24.448754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:25.449843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:26.450861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:27.451310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:28.451555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:29.452625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:30.453024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:31.453919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:32.454075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:33.454192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:34.454444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:35.454591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:36.456270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:37.456707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:38.456755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:39.457781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:40.458016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:41.458853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:42.459045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:43.459177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:44.459307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:45.460089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:46.460201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:47.460341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:48.461001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:49.461864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:50.461992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:51.462446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:52.462670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:53.463633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:54.464667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:55.465443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:56.465747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:57.466625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:58.466896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:08:59.467815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:00.470999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:01.471122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:02.471259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:03.471449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:04.471555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:05.472402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:06.473044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:07.473621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:08.473788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:09.474856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:10.475700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:11.475825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:12.476659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:13.476798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:14.477200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:15.477313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:16.477355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:17.477495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:18.477701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:19.477744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:20.477872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:21.477975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:22.478098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:23.478924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:24.479290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:25.480367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:26.480492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:27.480778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:28.480627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:29.480726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:30.480832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:31.481396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:32.481625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:33.482487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:34.482769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:35.483765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:36.484164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:37.484412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:38.484420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:39.484532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:40.485193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:41.485916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:42.486365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:43.487142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:44.487684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:45.488618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:46.488713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:47.488872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:48.489374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:49.489430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:50.489530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:51.489647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:52.489773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:53.490856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:54.491048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:55.491119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:56.491197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:57.491870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:58.492071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:09:59.492851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:00.493090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:01.493871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:02.494852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:03.494963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:04.495121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:05.496612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:06.497247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 07/01/23 13:10:06.721
  Jul  1 13:10:07.241: INFO: Successfully updated pod "var-expansion-8e301af7-171e-4cb4-960e-11139f26dde0"
  STEP: waiting for pod running @ 07/01/23 13:10:07.241
  E0701 13:10:07.497688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:08.498560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 07/01/23 13:10:09.258
  Jul  1 13:10:09.258: INFO: Deleting pod "var-expansion-8e301af7-171e-4cb4-960e-11139f26dde0" in namespace "var-expansion-7762"
  Jul  1 13:10:09.270: INFO: Wait up to 5m0s for pod "var-expansion-8e301af7-171e-4cb4-960e-11139f26dde0" to be fully deleted
  E0701 13:10:09.498925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:10.499050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:11.500135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:12.500983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:13.501758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:14.501896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:15.502471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:16.502873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:17.503634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:18.503821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:19.504650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:20.504766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:21.505337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:22.505385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:23.505510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:24.505615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:25.505696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:26.506164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:27.507038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:28.507128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:29.507501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:30.507614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:31.508015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:32.508663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:33.508741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:34.508876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:35.509886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:36.509975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:37.510323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:38.511530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:39.512328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:40.512451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:10:41.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7762" for this suite. @ 07/01/23 13:10:41.417
• [154.765 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 07/01/23 13:10:41.43
  Jul  1 13:10:41.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename crd-webhook @ 07/01/23 13:10:41.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:10:41.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:10:41.474
  STEP: Setting up server cert @ 07/01/23 13:10:41.479
  E0701 13:10:41.512885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 07/01/23 13:10:42.144
  STEP: Deploying the custom resource conversion webhook pod @ 07/01/23 13:10:42.154
  STEP: Wait for the deployment to be ready @ 07/01/23 13:10:42.174
  Jul  1 13:10:42.207: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0701 13:10:42.513660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:43.513780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/01/23 13:10:44.234
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 13:10:44.251
  E0701 13:10:44.514487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:10:45.251: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jul  1 13:10:45.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  E0701 13:10:45.514740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:46.514903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:47.514971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 07/01/23 13:10:47.879
  STEP: v2 custom resource should be converted @ 07/01/23 13:10:47.892
  Jul  1 13:10:47.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0701 13:10:48.516628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-1380" for this suite. @ 07/01/23 13:10:48.528
• [7.109 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 07/01/23 13:10:48.541
  Jul  1 13:10:48.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename secrets @ 07/01/23 13:10:48.543
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:10:48.566
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:10:48.578
  STEP: Creating secret with name secret-test-2339738e-b684-4523-91ca-dd4d62856fac @ 07/01/23 13:10:48.585
  STEP: Creating a pod to test consume secrets @ 07/01/23 13:10:48.593
  E0701 13:10:49.516964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:50.517330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:51.517954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:52.518381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:10:52.627
  Jul  1 13:10:52.634: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-secrets-70b99056-d43a-4ed9-aaf7-50f37724c320 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/01/23 13:10:52.662
  Jul  1 13:10:52.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-237" for this suite. @ 07/01/23 13:10:52.692
• [4.164 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 07/01/23 13:10:52.707
  Jul  1 13:10:52.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir @ 07/01/23 13:10:52.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:10:52.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:10:52.743
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 07/01/23 13:10:52.748
  E0701 13:10:53.519362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:54.519547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:55.520624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:56.521131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:10:56.789
  Jul  1 13:10:56.796: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-2925526a-cf2d-4121-a945-22e4aa2a2ef4 container test-container: <nil>
  STEP: delete the pod @ 07/01/23 13:10:56.808
  Jul  1 13:10:56.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1685" for this suite. @ 07/01/23 13:10:56.837
• [4.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 07/01/23 13:10:56.853
  Jul  1 13:10:56.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-probe @ 07/01/23 13:10:56.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:10:56.879
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:10:56.884
  STEP: Creating pod busybox-04eb8e4d-6139-4ed4-be71-bd1424d90995 in namespace container-probe-1923 @ 07/01/23 13:10:56.889
  E0701 13:10:57.521254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:10:58.524582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:10:58.928: INFO: Started pod busybox-04eb8e4d-6139-4ed4-be71-bd1424d90995 in namespace container-probe-1923
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/01/23 13:10:58.928
  Jul  1 13:10:58.933: INFO: Initial restart count of pod busybox-04eb8e4d-6139-4ed4-be71-bd1424d90995 is 0
  E0701 13:10:59.524682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:00.524811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:01.524945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:02.525824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:03.526574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:04.526690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:05.527249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:06.527583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:07.530988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:08.531562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:09.531660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:10.531743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:11.532177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:12.532667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:13.532784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:14.533046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:15.533356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:16.533439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:17.533616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:18.536415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:19.536540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:20.536652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:21.536789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:22.536910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:23.537955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:24.538020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:25.538158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:26.538987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:27.539121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:28.539233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:29.539910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:30.539994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:31.540611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:32.540758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:33.540867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:34.541005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:35.541142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:36.541237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:37.541387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:38.541482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:39.541942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:40.541725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:41.541817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:42.541957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:43.542086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:44.542297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:45.543314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:46.543564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:47.544126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:48.544352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:11:49.113: INFO: Restart count of pod container-probe-1923/busybox-04eb8e4d-6139-4ed4-be71-bd1424d90995 is now 1 (50.179999128s elapsed)
  Jul  1 13:11:49.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/01/23 13:11:49.123
  STEP: Destroying namespace "container-probe-1923" for this suite. @ 07/01/23 13:11:49.14
• [52.299 seconds]
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 07/01/23 13:11:49.152
  Jul  1 13:11:49.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename taint-multiple-pods @ 07/01/23 13:11:49.154
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:11:49.175
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:11:49.19
  Jul  1 13:11:49.197: INFO: Waiting up to 1m0s for all nodes to be ready
  E0701 13:11:49.544557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:50.544736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:51.544910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:52.544876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:53.545264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:54.545331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:55.545920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:56.546104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:57.546713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:58.547931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:11:59.548617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:00.548869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:01.549008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:02.549254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:03.549370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:04.549564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:05.550209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:06.550350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:07.550442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:08.550800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:09.551095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:10.551236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:11.551844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:12.552619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:13.553649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:14.553660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:15.554579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:16.554792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:17.555849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:18.556000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:19.556821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:20.557267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:21.557278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:22.557407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:23.557553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:24.557819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:25.558673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:26.559101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:27.559228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:28.559628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:29.560469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:30.560721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:31.560834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:32.560945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:33.561789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:34.562090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:35.562939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:36.563536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:37.564368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:38.564474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:39.565140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:40.565670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:41.565905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:42.566110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:43.566757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:44.566886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:45.567005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:46.567429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:47.568489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:48.568753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:12:49.222: INFO: Waiting for terminating namespaces to be deleted...
  Jul  1 13:12:49.227: INFO: Starting informer...
  STEP: Starting pods... @ 07/01/23 13:12:49.227
  Jul  1 13:12:49.459: INFO: Pod1 is running on ip-172-31-91-66. Tainting Node
  E0701 13:12:49.569382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:50.570558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:51.571239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:12:51.695: INFO: Pod2 is running on ip-172-31-91-66. Tainting Node
  STEP: Trying to apply a taint on the Node @ 07/01/23 13:12:51.695
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/01/23 13:12:51.709
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 07/01/23 13:12:51.722
  E0701 13:12:52.571889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:53.572668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:54.572743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:55.572851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:56.573701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:57.574121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:12:58.284: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0701 13:12:58.574799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:12:59.575666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:00.575841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:01.575920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:02.576657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:03.576992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:04.577091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:05.577298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:06.577542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:07.577887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:08.578087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:09.578467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:10.578830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:11.579490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:12.579557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:13.579719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:14.579882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:15.580631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:16.581010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:17.581604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:13:17.621: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Jul  1 13:13:17.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/01/23 13:13:17.66
  STEP: Destroying namespace "taint-multiple-pods-4595" for this suite. @ 07/01/23 13:13:17.671
• [88.555 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 07/01/23 13:13:17.707
  Jul  1 13:13:17.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename endpointslicemirroring @ 07/01/23 13:13:17.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:13:17.758
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:13:17.783
  STEP: mirroring a new custom Endpoint @ 07/01/23 13:13:17.808
  Jul  1 13:13:17.845: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0701 13:13:18.581864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:19.582110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 07/01/23 13:13:19.852
  STEP: mirroring deletion of a custom Endpoint @ 07/01/23 13:13:19.868
  Jul  1 13:13:19.885: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0701 13:13:20.582204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:21.582771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:13:21.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-3002" for this suite. @ 07/01/23 13:13:21.9
• [4.206 seconds]
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 07/01/23 13:13:21.913
  Jul  1 13:13:21.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl @ 07/01/23 13:13:21.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:13:21.948
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:13:21.962
  STEP: Starting the proxy @ 07/01/23 13:13:21.969
  Jul  1 13:13:21.969: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-6950 proxy --unix-socket=/tmp/kubectl-proxy-unix3347969619/test'
  STEP: retrieving proxy /api/ output @ 07/01/23 13:13:22.046
  Jul  1 13:13:22.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6950" for this suite. @ 07/01/23 13:13:22.055
• [0.159 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 07/01/23 13:13:22.075
  Jul  1 13:13:22.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 13:13:22.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:13:22.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:13:22.123
  STEP: Setting up server cert @ 07/01/23 13:13:22.22
  E0701 13:13:22.593770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 13:13:22.636
  STEP: Deploying the webhook pod @ 07/01/23 13:13:22.648
  STEP: Wait for the deployment to be ready @ 07/01/23 13:13:22.671
  Jul  1 13:13:22.689: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0701 13:13:23.593850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:24.594877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/01/23 13:13:24.713
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 13:13:24.731
  E0701 13:13:25.595602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:13:25.731: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 07/01/23 13:13:25.739
  STEP: create a configmap that should be updated by the webhook @ 07/01/23 13:13:25.758
  Jul  1 13:13:25.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4551" for this suite. @ 07/01/23 13:13:25.853
  STEP: Destroying namespace "webhook-markers-9416" for this suite. @ 07/01/23 13:13:25.863
• [3.803 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 07/01/23 13:13:25.88
  Jul  1 13:13:25.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename gc @ 07/01/23 13:13:25.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:13:25.905
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:13:25.914
  STEP: create the rc @ 07/01/23 13:13:25.925
  W0701 13:13:25.932838      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0701 13:13:26.597435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:27.596241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:28.596361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:29.596660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:30.596803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:31.597177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 07/01/23 13:13:31.939
  STEP: wait for the rc to be deleted @ 07/01/23 13:13:31.949
  E0701 13:13:32.597306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:33.597746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:34.598829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:35.599203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:36.600464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 07/01/23 13:13:36.961
  E0701 13:13:37.600586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:38.600878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:39.601121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:40.600883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:41.601478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:42.602350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:43.602613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:44.602810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:45.603911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:46.604128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:47.604325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:48.604492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:49.604549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:50.604880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:51.605117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:52.605236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:53.605307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:54.605390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:55.605418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:56.605808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:57.605927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:58.606027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:13:59.606584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:00.607060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:01.607550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:02.607640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:03.607749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:04.608768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:05.608990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:06.609871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/01/23 13:14:06.978
  W0701 13:14:06.985354      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul  1 13:14:06.985: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul  1 13:14:06.985: INFO: Deleting pod "simpletest.rc-2bv6v" in namespace "gc-4191"
  Jul  1 13:14:07.002: INFO: Deleting pod "simpletest.rc-2gqfs" in namespace "gc-4191"
  Jul  1 13:14:07.023: INFO: Deleting pod "simpletest.rc-2hx4v" in namespace "gc-4191"
  Jul  1 13:14:07.047: INFO: Deleting pod "simpletest.rc-2pwkj" in namespace "gc-4191"
  Jul  1 13:14:07.066: INFO: Deleting pod "simpletest.rc-4blc2" in namespace "gc-4191"
  Jul  1 13:14:07.087: INFO: Deleting pod "simpletest.rc-4h68n" in namespace "gc-4191"
  Jul  1 13:14:07.109: INFO: Deleting pod "simpletest.rc-555md" in namespace "gc-4191"
  Jul  1 13:14:07.126: INFO: Deleting pod "simpletest.rc-57rd9" in namespace "gc-4191"
  Jul  1 13:14:07.144: INFO: Deleting pod "simpletest.rc-5d9sk" in namespace "gc-4191"
  Jul  1 13:14:07.161: INFO: Deleting pod "simpletest.rc-5mgnz" in namespace "gc-4191"
  Jul  1 13:14:07.183: INFO: Deleting pod "simpletest.rc-5whwg" in namespace "gc-4191"
  Jul  1 13:14:07.236: INFO: Deleting pod "simpletest.rc-6brt6" in namespace "gc-4191"
  Jul  1 13:14:07.259: INFO: Deleting pod "simpletest.rc-6w85g" in namespace "gc-4191"
  Jul  1 13:14:07.285: INFO: Deleting pod "simpletest.rc-6zhlp" in namespace "gc-4191"
  Jul  1 13:14:07.305: INFO: Deleting pod "simpletest.rc-7bftr" in namespace "gc-4191"
  Jul  1 13:14:07.327: INFO: Deleting pod "simpletest.rc-7bq7x" in namespace "gc-4191"
  Jul  1 13:14:07.347: INFO: Deleting pod "simpletest.rc-7g885" in namespace "gc-4191"
  Jul  1 13:14:07.378: INFO: Deleting pod "simpletest.rc-7rwrq" in namespace "gc-4191"
  Jul  1 13:14:07.400: INFO: Deleting pod "simpletest.rc-7whbt" in namespace "gc-4191"
  Jul  1 13:14:07.430: INFO: Deleting pod "simpletest.rc-86wsv" in namespace "gc-4191"
  Jul  1 13:14:07.468: INFO: Deleting pod "simpletest.rc-88qk2" in namespace "gc-4191"
  Jul  1 13:14:07.495: INFO: Deleting pod "simpletest.rc-89zsb" in namespace "gc-4191"
  Jul  1 13:14:07.516: INFO: Deleting pod "simpletest.rc-8czxz" in namespace "gc-4191"
  Jul  1 13:14:07.544: INFO: Deleting pod "simpletest.rc-8h5gg" in namespace "gc-4191"
  Jul  1 13:14:07.587: INFO: Deleting pod "simpletest.rc-8pl4l" in namespace "gc-4191"
  E0701 13:14:07.610015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:14:07.616: INFO: Deleting pod "simpletest.rc-8q6dm" in namespace "gc-4191"
  Jul  1 13:14:07.661: INFO: Deleting pod "simpletest.rc-8stdj" in namespace "gc-4191"
  Jul  1 13:14:07.686: INFO: Deleting pod "simpletest.rc-8tc6h" in namespace "gc-4191"
  Jul  1 13:14:07.709: INFO: Deleting pod "simpletest.rc-979bd" in namespace "gc-4191"
  Jul  1 13:14:07.737: INFO: Deleting pod "simpletest.rc-9cmsj" in namespace "gc-4191"
  Jul  1 13:14:07.765: INFO: Deleting pod "simpletest.rc-9wx5m" in namespace "gc-4191"
  Jul  1 13:14:07.796: INFO: Deleting pod "simpletest.rc-b548c" in namespace "gc-4191"
  Jul  1 13:14:07.815: INFO: Deleting pod "simpletest.rc-b6qz2" in namespace "gc-4191"
  Jul  1 13:14:07.834: INFO: Deleting pod "simpletest.rc-b9qqr" in namespace "gc-4191"
  Jul  1 13:14:07.861: INFO: Deleting pod "simpletest.rc-bswnk" in namespace "gc-4191"
  Jul  1 13:14:07.887: INFO: Deleting pod "simpletest.rc-bxhq7" in namespace "gc-4191"
  Jul  1 13:14:07.906: INFO: Deleting pod "simpletest.rc-cb8hx" in namespace "gc-4191"
  Jul  1 13:14:07.924: INFO: Deleting pod "simpletest.rc-cbwn8" in namespace "gc-4191"
  Jul  1 13:14:07.945: INFO: Deleting pod "simpletest.rc-clcp4" in namespace "gc-4191"
  Jul  1 13:14:07.964: INFO: Deleting pod "simpletest.rc-cvpgj" in namespace "gc-4191"
  Jul  1 13:14:07.988: INFO: Deleting pod "simpletest.rc-d78z4" in namespace "gc-4191"
  Jul  1 13:14:08.012: INFO: Deleting pod "simpletest.rc-ddb6l" in namespace "gc-4191"
  Jul  1 13:14:08.037: INFO: Deleting pod "simpletest.rc-dnjzk" in namespace "gc-4191"
  Jul  1 13:14:08.067: INFO: Deleting pod "simpletest.rc-ds946" in namespace "gc-4191"
  Jul  1 13:14:08.094: INFO: Deleting pod "simpletest.rc-dvfrc" in namespace "gc-4191"
  Jul  1 13:14:08.112: INFO: Deleting pod "simpletest.rc-dzp8q" in namespace "gc-4191"
  Jul  1 13:14:08.128: INFO: Deleting pod "simpletest.rc-f56nz" in namespace "gc-4191"
  Jul  1 13:14:08.146: INFO: Deleting pod "simpletest.rc-f787s" in namespace "gc-4191"
  Jul  1 13:14:08.164: INFO: Deleting pod "simpletest.rc-f8lnm" in namespace "gc-4191"
  Jul  1 13:14:08.182: INFO: Deleting pod "simpletest.rc-fpj56" in namespace "gc-4191"
  Jul  1 13:14:08.201: INFO: Deleting pod "simpletest.rc-frxf5" in namespace "gc-4191"
  Jul  1 13:14:08.223: INFO: Deleting pod "simpletest.rc-ftp88" in namespace "gc-4191"
  Jul  1 13:14:08.245: INFO: Deleting pod "simpletest.rc-fvv42" in namespace "gc-4191"
  Jul  1 13:14:08.262: INFO: Deleting pod "simpletest.rc-gddv4" in namespace "gc-4191"
  Jul  1 13:14:08.279: INFO: Deleting pod "simpletest.rc-h5lpl" in namespace "gc-4191"
  Jul  1 13:14:08.298: INFO: Deleting pod "simpletest.rc-h7wv4" in namespace "gc-4191"
  Jul  1 13:14:08.318: INFO: Deleting pod "simpletest.rc-hlvnz" in namespace "gc-4191"
  Jul  1 13:14:08.340: INFO: Deleting pod "simpletest.rc-hrj84" in namespace "gc-4191"
  Jul  1 13:14:08.356: INFO: Deleting pod "simpletest.rc-j8cnb" in namespace "gc-4191"
  Jul  1 13:14:08.375: INFO: Deleting pod "simpletest.rc-j8kcg" in namespace "gc-4191"
  Jul  1 13:14:08.390: INFO: Deleting pod "simpletest.rc-jlcdv" in namespace "gc-4191"
  Jul  1 13:14:08.408: INFO: Deleting pod "simpletest.rc-jq58g" in namespace "gc-4191"
  Jul  1 13:14:08.426: INFO: Deleting pod "simpletest.rc-kl46l" in namespace "gc-4191"
  Jul  1 13:14:08.445: INFO: Deleting pod "simpletest.rc-l984j" in namespace "gc-4191"
  Jul  1 13:14:08.458: INFO: Deleting pod "simpletest.rc-l9lpb" in namespace "gc-4191"
  Jul  1 13:14:08.477: INFO: Deleting pod "simpletest.rc-ld9dq" in namespace "gc-4191"
  Jul  1 13:14:08.498: INFO: Deleting pod "simpletest.rc-lgm76" in namespace "gc-4191"
  Jul  1 13:14:08.514: INFO: Deleting pod "simpletest.rc-ljzfl" in namespace "gc-4191"
  Jul  1 13:14:08.533: INFO: Deleting pod "simpletest.rc-lrcvn" in namespace "gc-4191"
  Jul  1 13:14:08.555: INFO: Deleting pod "simpletest.rc-lrrmv" in namespace "gc-4191"
  Jul  1 13:14:08.584: INFO: Deleting pod "simpletest.rc-m275q" in namespace "gc-4191"
  Jul  1 13:14:08.599: INFO: Deleting pod "simpletest.rc-mgkvs" in namespace "gc-4191"
  E0701 13:14:08.610080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:14:08.626: INFO: Deleting pod "simpletest.rc-mjnkm" in namespace "gc-4191"
  Jul  1 13:14:08.645: INFO: Deleting pod "simpletest.rc-mv9kq" in namespace "gc-4191"
  Jul  1 13:14:08.667: INFO: Deleting pod "simpletest.rc-n6l4p" in namespace "gc-4191"
  Jul  1 13:14:08.683: INFO: Deleting pod "simpletest.rc-n8vnh" in namespace "gc-4191"
  Jul  1 13:14:08.701: INFO: Deleting pod "simpletest.rc-n9b6b" in namespace "gc-4191"
  Jul  1 13:14:08.718: INFO: Deleting pod "simpletest.rc-ngn7x" in namespace "gc-4191"
  Jul  1 13:14:08.733: INFO: Deleting pod "simpletest.rc-pldjf" in namespace "gc-4191"
  Jul  1 13:14:08.749: INFO: Deleting pod "simpletest.rc-qbf49" in namespace "gc-4191"
  Jul  1 13:14:08.766: INFO: Deleting pod "simpletest.rc-rkks5" in namespace "gc-4191"
  Jul  1 13:14:08.784: INFO: Deleting pod "simpletest.rc-s5pw5" in namespace "gc-4191"
  Jul  1 13:14:08.803: INFO: Deleting pod "simpletest.rc-s6h94" in namespace "gc-4191"
  Jul  1 13:14:08.822: INFO: Deleting pod "simpletest.rc-svfb6" in namespace "gc-4191"
  Jul  1 13:14:08.841: INFO: Deleting pod "simpletest.rc-svzgn" in namespace "gc-4191"
  Jul  1 13:14:08.858: INFO: Deleting pod "simpletest.rc-t9nn2" in namespace "gc-4191"
  Jul  1 13:14:08.880: INFO: Deleting pod "simpletest.rc-tb9dh" in namespace "gc-4191"
  Jul  1 13:14:08.929: INFO: Deleting pod "simpletest.rc-tbdps" in namespace "gc-4191"
  Jul  1 13:14:08.978: INFO: Deleting pod "simpletest.rc-v4n7r" in namespace "gc-4191"
  Jul  1 13:14:09.030: INFO: Deleting pod "simpletest.rc-v9nfg" in namespace "gc-4191"
  Jul  1 13:14:09.078: INFO: Deleting pod "simpletest.rc-vchb2" in namespace "gc-4191"
  Jul  1 13:14:09.127: INFO: Deleting pod "simpletest.rc-vgrzr" in namespace "gc-4191"
  Jul  1 13:14:09.178: INFO: Deleting pod "simpletest.rc-vnfs2" in namespace "gc-4191"
  Jul  1 13:14:09.231: INFO: Deleting pod "simpletest.rc-w9498" in namespace "gc-4191"
  Jul  1 13:14:09.280: INFO: Deleting pod "simpletest.rc-x59nf" in namespace "gc-4191"
  Jul  1 13:14:09.329: INFO: Deleting pod "simpletest.rc-xc4cn" in namespace "gc-4191"
  Jul  1 13:14:09.379: INFO: Deleting pod "simpletest.rc-xqjvw" in namespace "gc-4191"
  Jul  1 13:14:09.442: INFO: Deleting pod "simpletest.rc-xvf6n" in namespace "gc-4191"
  Jul  1 13:14:09.528: INFO: Deleting pod "simpletest.rc-z5q44" in namespace "gc-4191"
  Jul  1 13:14:09.549: INFO: Deleting pod "simpletest.rc-zbgh4" in namespace "gc-4191"
  Jul  1 13:14:09.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0701 13:14:09.610469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "gc-4191" for this suite. @ 07/01/23 13:14:09.623
• [43.794 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 07/01/23 13:14:09.675
  Jul  1 13:14:09.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 13:14:09.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:14:09.698
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:14:09.703
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 13:14:09.707
  E0701 13:14:10.611514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:11.612500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:12.612778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:13.613266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:14.614431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:15.615606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:16.616470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:17.616573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:18.616672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:19.616948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:20.617100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:21.617627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:14:21.788
  Jul  1 13:14:21.793: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-28a31aca-53ba-47d4-8e09-3d89058f68b2 container client-container: <nil>
  STEP: delete the pod @ 07/01/23 13:14:21.815
  Jul  1 13:14:21.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6307" for this suite. @ 07/01/23 13:14:21.847
• [12.185 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 07/01/23 13:14:21.861
  Jul  1 13:14:21.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename services @ 07/01/23 13:14:21.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:14:21.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:14:21.89
  STEP: creating a collection of services @ 07/01/23 13:14:21.894
  Jul  1 13:14:21.894: INFO: Creating e2e-svc-a-6l2kc
  Jul  1 13:14:21.918: INFO: Creating e2e-svc-b-gh4dn
  Jul  1 13:14:21.936: INFO: Creating e2e-svc-c-svv2c
  STEP: deleting service collection @ 07/01/23 13:14:21.966
  Jul  1 13:14:22.038: INFO: Collection of services has been deleted
  Jul  1 13:14:22.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-228" for this suite. @ 07/01/23 13:14:22.046
• [0.205 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 07/01/23 13:14:22.068
  Jul  1 13:14:22.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename events @ 07/01/23 13:14:22.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:14:22.089
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:14:22.096
  STEP: creating a test event @ 07/01/23 13:14:22.101
  STEP: listing events in all namespaces @ 07/01/23 13:14:22.117
  STEP: listing events in test namespace @ 07/01/23 13:14:22.126
  STEP: listing events with field selection filtering on source @ 07/01/23 13:14:22.134
  STEP: listing events with field selection filtering on reportingController @ 07/01/23 13:14:22.139
  STEP: getting the test event @ 07/01/23 13:14:22.146
  STEP: patching the test event @ 07/01/23 13:14:22.152
  STEP: getting the test event @ 07/01/23 13:14:22.165
  STEP: updating the test event @ 07/01/23 13:14:22.171
  STEP: getting the test event @ 07/01/23 13:14:22.182
  STEP: deleting the test event @ 07/01/23 13:14:22.188
  STEP: listing events in all namespaces @ 07/01/23 13:14:22.203
  STEP: listing events in test namespace @ 07/01/23 13:14:22.212
  Jul  1 13:14:22.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-2184" for this suite. @ 07/01/23 13:14:22.225
• [0.167 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 07/01/23 13:14:22.237
  Jul  1 13:14:22.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename downward-api @ 07/01/23 13:14:22.238
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:14:22.26
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:14:22.266
  STEP: Creating a pod to test downward api env vars @ 07/01/23 13:14:22.272
  E0701 13:14:22.618599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:23.619723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:24.620461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:25.620511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:14:26.308
  Jul  1 13:14:26.314: INFO: Trying to get logs from node ip-172-31-91-66 pod downward-api-2f502d9d-0b76-4572-9ebc-c8b2d2189c2c container dapi-container: <nil>
  STEP: delete the pod @ 07/01/23 13:14:26.327
  Jul  1 13:14:26.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1778" for this suite. @ 07/01/23 13:14:26.367
• [4.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 07/01/23 13:14:26.38
  Jul  1 13:14:26.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 13:14:26.381
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:14:26.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:14:26.409
  STEP: Setting up server cert @ 07/01/23 13:14:26.446
  E0701 13:14:26.621225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 13:14:26.839
  STEP: Deploying the webhook pod @ 07/01/23 13:14:26.853
  STEP: Wait for the deployment to be ready @ 07/01/23 13:14:26.872
  Jul  1 13:14:26.893: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0701 13:14:27.620898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:28.621481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/01/23 13:14:28.911
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 13:14:28.929
  E0701 13:14:29.621629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:14:29.931: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul  1 13:14:29.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2765-crds.webhook.example.com via the AdmissionRegistration API @ 07/01/23 13:14:30.455
  STEP: Creating a custom resource that should be mutated by the webhook @ 07/01/23 13:14:30.483
  E0701 13:14:30.622123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:31.622576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:14:32.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0701 13:14:32.622894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-3772" for this suite. @ 07/01/23 13:14:33.195
  STEP: Destroying namespace "webhook-markers-7097" for this suite. @ 07/01/23 13:14:33.211
• [6.843 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 07/01/23 13:14:33.224
  Jul  1 13:14:33.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename endpointslice @ 07/01/23 13:14:33.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:14:33.248
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:14:33.254
  Jul  1 13:14:33.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1571" for this suite. @ 07/01/23 13:14:33.352
• [0.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 07/01/23 13:14:33.369
  Jul  1 13:14:33.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename limitrange @ 07/01/23 13:14:33.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:14:33.405
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:14:33.41
  STEP: Creating a LimitRange @ 07/01/23 13:14:33.416
  STEP: Setting up watch @ 07/01/23 13:14:33.417
  STEP: Submitting a LimitRange @ 07/01/23 13:14:33.523
  STEP: Verifying LimitRange creation was observed @ 07/01/23 13:14:33.531
  STEP: Fetching the LimitRange to ensure it has proper values @ 07/01/23 13:14:33.531
  Jul  1 13:14:33.538: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jul  1 13:14:33.538: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 07/01/23 13:14:33.538
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 07/01/23 13:14:33.555
  Jul  1 13:14:33.563: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jul  1 13:14:33.564: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 07/01/23 13:14:33.564
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 07/01/23 13:14:33.596
  E0701 13:14:33.623479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:14:33.639: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Jul  1 13:14:33.639: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 07/01/23 13:14:33.639
  STEP: Failing to create a Pod with more than max resources @ 07/01/23 13:14:33.651
  STEP: Updating a LimitRange @ 07/01/23 13:14:33.659
  STEP: Verifying LimitRange updating is effective @ 07/01/23 13:14:33.675
  E0701 13:14:34.624399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:35.624467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 07/01/23 13:14:35.681
  STEP: Failing to create a Pod with more than max resources @ 07/01/23 13:14:35.691
  STEP: Deleting a LimitRange @ 07/01/23 13:14:35.695
  STEP: Verifying the LimitRange was deleted @ 07/01/23 13:14:35.71
  E0701 13:14:36.624569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:37.624772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:38.625003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:39.625071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:40.625723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:14:40.717: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 07/01/23 13:14:40.718
  Jul  1 13:14:40.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-4206" for this suite. @ 07/01/23 13:14:40.744
• [7.384 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 07/01/23 13:14:40.754
  Jul  1 13:14:40.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename var-expansion @ 07/01/23 13:14:40.755
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:14:40.775
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:14:40.781
  STEP: Creating a pod to test substitution in volume subpath @ 07/01/23 13:14:40.786
  E0701 13:14:41.628511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:42.628627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:43.628761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:44.629224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:14:44.853
  Jul  1 13:14:44.860: INFO: Trying to get logs from node ip-172-31-91-66 pod var-expansion-79687b38-9c6d-4b26-8bc4-0aff2c05b8c5 container dapi-container: <nil>
  STEP: delete the pod @ 07/01/23 13:14:44.876
  Jul  1 13:14:44.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5091" for this suite. @ 07/01/23 13:14:44.908
• [4.169 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 07/01/23 13:14:44.924
  Jul  1 13:14:44.924: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir @ 07/01/23 13:14:44.925
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:14:44.949
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:14:44.96
  STEP: Creating Pod @ 07/01/23 13:14:44.967
  E0701 13:14:45.629200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:46.629229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 07/01/23 13:14:47
  Jul  1 13:14:47.000: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7381 PodName:pod-sharedvolume-4dce9a75-a209-4ed0-80c9-5212e2c098c4 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:14:47.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:14:47.001: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:14:47.001: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-7381/pods/pod-sharedvolume-4dce9a75-a209-4ed0-80c9-5212e2c098c4/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Jul  1 13:14:47.079: INFO: Exec stderr: ""
  Jul  1 13:14:47.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7381" for this suite. @ 07/01/23 13:14:47.089
• [2.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 07/01/23 13:14:47.106
  Jul  1 13:14:47.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename downward-api @ 07/01/23 13:14:47.108
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:14:47.137
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:14:47.144
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 13:14:47.151
  E0701 13:14:47.629765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:48.629818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:49.630566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:50.631544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:14:51.191
  Jul  1 13:14:51.197: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-91c19a5b-8eb9-4b55-b47e-7d6d90a176e4 container client-container: <nil>
  STEP: delete the pod @ 07/01/23 13:14:51.208
  Jul  1 13:14:51.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6374" for this suite. @ 07/01/23 13:14:51.238
• [4.144 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 07/01/23 13:14:51.252
  Jul  1 13:14:51.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename podtemplate @ 07/01/23 13:14:51.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:14:51.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:14:51.284
  STEP: Create a pod template @ 07/01/23 13:14:51.289
  STEP: Replace a pod template @ 07/01/23 13:14:51.296
  Jul  1 13:14:51.310: INFO: Found updated podtemplate annotation: "true"

  Jul  1 13:14:51.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4006" for this suite. @ 07/01/23 13:14:51.316
• [0.075 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 07/01/23 13:14:51.328
  Jul  1 13:14:51.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename endpointslice @ 07/01/23 13:14:51.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:14:51.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:14:51.358
  E0701 13:14:51.632184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:52.632317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:53.632417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:54.632673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:55.632821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 07/01/23 13:14:56.48
  E0701 13:14:56.633292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:57.633402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:58.633718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:14:59.633845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:00.634199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 07/01/23 13:15:01.495
  E0701 13:15:01.634986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:02.635414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:03.635600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:04.636754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:05.637153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 07/01/23 13:15:06.508
  E0701 13:15:06.638232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:07.638323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:08.638469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:09.639288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:10.639561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 07/01/23 13:15:11.522
  Jul  1 13:15:11.578: INFO: EndpointSlice for Service endpointslice-4838/example-named-port not found
  E0701 13:15:11.640256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:12.640341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:13.640467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:14.640702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:15.641761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:16.641854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:17.642475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:18.642575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:19.643823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:20.643967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:15:21.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4838" for this suite. @ 07/01/23 13:15:21.601
• [30.282 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 07/01/23 13:15:21.611
  Jul  1 13:15:21.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename daemonsets @ 07/01/23 13:15:21.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:15:21.631
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:15:21.637
  E0701 13:15:21.644211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating simple DaemonSet "daemon-set" @ 07/01/23 13:15:21.689
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/01/23 13:15:21.706
  Jul  1 13:15:21.713: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:21.713: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:21.720: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 13:15:21.721: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  E0701 13:15:22.645284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:15:22.734: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:22.734: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:22.742: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  1 13:15:22.742: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  E0701 13:15:23.645176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:15:23.727: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:23.727: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:23.732: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  1 13:15:23.732: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 07/01/23 13:15:23.736
  Jul  1 13:15:23.743: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 07/01/23 13:15:23.743
  Jul  1 13:15:23.757: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 07/01/23 13:15:23.757
  Jul  1 13:15:23.759: INFO: Observed &DaemonSet event: ADDED
  Jul  1 13:15:23.760: INFO: Observed &DaemonSet event: MODIFIED
  Jul  1 13:15:23.760: INFO: Observed &DaemonSet event: MODIFIED
  Jul  1 13:15:23.760: INFO: Observed &DaemonSet event: MODIFIED
  Jul  1 13:15:23.760: INFO: Observed &DaemonSet event: MODIFIED
  Jul  1 13:15:23.761: INFO: Found daemon set daemon-set in namespace daemonsets-3928 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul  1 13:15:23.761: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 07/01/23 13:15:23.761
  STEP: watching for the daemon set status to be patched @ 07/01/23 13:15:23.775
  Jul  1 13:15:23.781: INFO: Observed &DaemonSet event: ADDED
  Jul  1 13:15:23.782: INFO: Observed &DaemonSet event: MODIFIED
  Jul  1 13:15:23.782: INFO: Observed &DaemonSet event: MODIFIED
  Jul  1 13:15:23.782: INFO: Observed &DaemonSet event: MODIFIED
  Jul  1 13:15:23.782: INFO: Observed &DaemonSet event: MODIFIED
  Jul  1 13:15:23.783: INFO: Observed daemon set daemon-set in namespace daemonsets-3928 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul  1 13:15:23.783: INFO: Observed &DaemonSet event: MODIFIED
  Jul  1 13:15:23.783: INFO: Found daemon set daemon-set in namespace daemonsets-3928 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Jul  1 13:15:23.783: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 07/01/23 13:15:23.794
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3928, will wait for the garbage collector to delete the pods @ 07/01/23 13:15:23.794
  Jul  1 13:15:23.863: INFO: Deleting DaemonSet.extensions daemon-set took: 9.963427ms
  Jul  1 13:15:23.964: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.358405ms
  E0701 13:15:24.646041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:15:25.370: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 13:15:25.370: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul  1 13:15:25.376: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32746"},"items":null}

  Jul  1 13:15:25.381: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32746"},"items":null}

  Jul  1 13:15:25.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3928" for this suite. @ 07/01/23 13:15:25.412
• [3.810 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 07/01/23 13:15:25.426
  Jul  1 13:15:25.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename svcaccounts @ 07/01/23 13:15:25.426
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:15:25.449
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:15:25.454
  STEP: Creating ServiceAccount "e2e-sa-8brf8"  @ 07/01/23 13:15:25.459
  Jul  1 13:15:25.551: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-8brf8"  @ 07/01/23 13:15:25.551
  Jul  1 13:15:25.568: INFO: AutomountServiceAccountToken: true
  Jul  1 13:15:25.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5345" for this suite. @ 07/01/23 13:15:25.574
• [0.157 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 07/01/23 13:15:25.583
  Jul  1 13:15:25.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename daemonsets @ 07/01/23 13:15:25.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:15:25.605
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:15:25.61
  E0701 13:15:25.646596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating simple DaemonSet "daemon-set" @ 07/01/23 13:15:25.647
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/01/23 13:15:25.656
  Jul  1 13:15:25.663: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:25.663: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:25.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 13:15:25.669: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  E0701 13:15:26.647576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:15:26.675: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:26.675: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:26.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 13:15:26.681: INFO: Node ip-172-31-12-125 is running 0 daemon pod, expected 1
  E0701 13:15:27.648622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:15:27.678: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:27.678: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:27.686: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  1 13:15:27.686: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 07/01/23 13:15:27.694
  Jul  1 13:15:27.732: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:27.732: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:27.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul  1 13:15:27.740: INFO: Node ip-172-31-16-94 is running 0 daemon pod, expected 1
  E0701 13:15:28.649036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:15:28.757: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:28.757: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:28.763: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul  1 13:15:28.763: INFO: Node ip-172-31-16-94 is running 0 daemon pod, expected 1
  E0701 13:15:29.649111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:15:29.748: INFO: DaemonSet pods can't tolerate node ip-172-31-34-114 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:29.748: INFO: DaemonSet pods can't tolerate node ip-172-31-68-61 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  1 13:15:29.756: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  1 13:15:29.756: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/01/23 13:15:29.764
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1177, will wait for the garbage collector to delete the pods @ 07/01/23 13:15:29.764
  Jul  1 13:15:29.835: INFO: Deleting DaemonSet.extensions daemon-set took: 14.265146ms
  Jul  1 13:15:29.936: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.887362ms
  E0701 13:15:30.649261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:31.649666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:15:31.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  1 13:15:31.843: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul  1 13:15:31.849: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32941"},"items":null}

  Jul  1 13:15:31.855: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32941"},"items":null}

  Jul  1 13:15:31.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1177" for this suite. @ 07/01/23 13:15:31.884
• [6.311 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 07/01/23 13:15:31.899
  Jul  1 13:15:31.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename sysctl @ 07/01/23 13:15:31.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:15:31.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:15:31.965
  STEP: Creating a pod with one valid and two invalid sysctls @ 07/01/23 13:15:31.974
  Jul  1 13:15:31.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-2716" for this suite. @ 07/01/23 13:15:31.997
• [0.111 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 07/01/23 13:15:32.01
  Jul  1 13:15:32.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename watch @ 07/01/23 13:15:32.014
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:15:32.038
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:15:32.043
  STEP: creating a watch on configmaps @ 07/01/23 13:15:32.048
  STEP: creating a new configmap @ 07/01/23 13:15:32.05
  STEP: modifying the configmap once @ 07/01/23 13:15:32.059
  STEP: closing the watch once it receives two notifications @ 07/01/23 13:15:32.073
  Jul  1 13:15:32.073: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4249  5a68bc66-c522-48c5-8e57-a1958b776c71 32954 0 2023-07-01 13:15:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-01 13:15:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 13:15:32.073: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4249  5a68bc66-c522-48c5-8e57-a1958b776c71 32955 0 2023-07-01 13:15:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-01 13:15:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 07/01/23 13:15:32.074
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 07/01/23 13:15:32.09
  STEP: deleting the configmap @ 07/01/23 13:15:32.092
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 07/01/23 13:15:32.11
  Jul  1 13:15:32.110: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4249  5a68bc66-c522-48c5-8e57-a1958b776c71 32956 0 2023-07-01 13:15:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-01 13:15:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 13:15:32.111: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4249  5a68bc66-c522-48c5-8e57-a1958b776c71 32957 0 2023-07-01 13:15:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-01 13:15:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  1 13:15:32.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4249" for this suite. @ 07/01/23 13:15:32.121
• [0.121 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 07/01/23 13:15:32.131
  Jul  1 13:15:32.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename deployment @ 07/01/23 13:15:32.134
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:15:32.163
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:15:32.178
  Jul  1 13:15:32.184: INFO: Creating simple deployment test-new-deployment
  Jul  1 13:15:32.215: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0701 13:15:32.649814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:33.649932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 07/01/23 13:15:34.242
  STEP: updating a scale subresource @ 07/01/23 13:15:34.247
  STEP: verifying the deployment Spec.Replicas was modified @ 07/01/23 13:15:34.26
  STEP: Patch a scale subresource @ 07/01/23 13:15:34.264
  Jul  1 13:15:34.300: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-2007  a096141c-cbcf-40e9-b30b-200241afa175 32996 3 2023-07-01 13:15:32 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-07-01 13:15:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 13:15:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005130988 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-01 13:15:33 +0000 UTC,LastTransitionTime:2023-07-01 13:15:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-07-01 13:15:33 +0000 UTC,LastTransitionTime:2023-07-01 13:15:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul  1 13:15:34.311: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-2007  c4b6981d-8057-49c3-a10d-92d038288a71 32999 2 2023-07-01 13:15:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment a096141c-cbcf-40e9-b30b-200241afa175 0xc005130da7 0xc005130da8}] [] [{kube-controller-manager Update apps/v1 2023-07-01 13:15:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a096141c-cbcf-40e9-b30b-200241afa175\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 13:15:34 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005130e38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul  1 13:15:34.321: INFO: Pod "test-new-deployment-67bd4bf6dc-bvtbp" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-bvtbp test-new-deployment-67bd4bf6dc- deployment-2007  610af4cb-e752-445a-883d-b6625e90ccdd 33000 0 2023-07-01 13:15:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc c4b6981d-8057-49c3-a10d-92d038288a71 0xc0051311e7 0xc0051311e8}] [] [{kube-controller-manager Update v1 2023-07-01 13:15:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4b6981d-8057-49c3-a10d-92d038288a71\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8x8g4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8x8g4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:15:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 13:15:34.322: INFO: Pod "test-new-deployment-67bd4bf6dc-cvv27" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-cvv27 test-new-deployment-67bd4bf6dc- deployment-2007  f16476c6-aeed-4deb-b4e8-c0a5ad102777 32990 0 2023-07-01 13:15:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc c4b6981d-8057-49c3-a10d-92d038288a71 0xc005131350 0xc005131351}] [] [{kube-controller-manager Update v1 2023-07-01 13:15:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4b6981d-8057-49c3-a10d-92d038288a71\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 13:15:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.62.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t2x5z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t2x5z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:15:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:15:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:192.168.62.69,StartTime:2023-07-01 13:15:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-01 13:15:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ee7e628a99073ab5ddbdf8459a37576329f15957c7855a9d769afe711345bbc2,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.62.69,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 13:15:34.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2007" for this suite. @ 07/01/23 13:15:34.341
• [2.251 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 07/01/23 13:15:34.387
  Jul  1 13:15:34.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-runtime @ 07/01/23 13:15:34.39
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:15:34.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:15:34.457
  STEP: create the container @ 07/01/23 13:15:34.467
  W0701 13:15:34.479212      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/01/23 13:15:34.479
  E0701 13:15:34.650959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:35.651566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:36.652632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:37.653497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/01/23 13:15:38.524
  STEP: the container should be terminated @ 07/01/23 13:15:38.534
  STEP: the termination message should be set @ 07/01/23 13:15:38.534
  Jul  1 13:15:38.534: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 07/01/23 13:15:38.534
  Jul  1 13:15:38.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-949" for this suite. @ 07/01/23 13:15:38.572
• [4.198 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 07/01/23 13:15:38.587
  Jul  1 13:15:38.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 13:15:38.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:15:38.612
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:15:38.623
  E0701 13:15:38.653603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 07/01/23 13:15:38.655
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 13:15:39.064
  STEP: Deploying the webhook pod @ 07/01/23 13:15:39.074
  STEP: Wait for the deployment to be ready @ 07/01/23 13:15:39.09
  Jul  1 13:15:39.104: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0701 13:15:39.654226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:40.654314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/01/23 13:15:41.119
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 13:15:41.215
  E0701 13:15:41.654793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:15:42.216: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 07/01/23 13:15:42.222
  STEP: create a pod that should be updated by the webhook @ 07/01/23 13:15:42.246
  Jul  1 13:15:42.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4990" for this suite. @ 07/01/23 13:15:42.386
  STEP: Destroying namespace "webhook-markers-7187" for this suite. @ 07/01/23 13:15:42.41
• [3.834 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 07/01/23 13:15:42.421
  Jul  1 13:15:42.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename svcaccounts @ 07/01/23 13:15:42.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:15:42.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:15:42.471
  E0701 13:15:42.655343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:43.655611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 07/01/23 13:15:44.518
  Jul  1 13:15:44.518: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8679 pod-service-account-4573201d-7329-412f-b85c-32560195106c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  E0701 13:15:44.656009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 07/01/23 13:15:44.691
  Jul  1 13:15:44.691: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8679 pod-service-account-4573201d-7329-412f-b85c-32560195106c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 07/01/23 13:15:44.877
  Jul  1 13:15:44.877: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8679 pod-service-account-4573201d-7329-412f-b85c-32560195106c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Jul  1 13:15:45.113: INFO: Got root ca configmap in namespace "svcaccounts-8679"
  Jul  1 13:15:45.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8679" for this suite. @ 07/01/23 13:15:45.122
• [2.715 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 07/01/23 13:15:45.136
  Jul  1 13:15:45.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 13:15:45.137
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:15:45.162
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:15:45.167
  STEP: Creating projection with secret that has name projected-secret-test-d0540d93-6a9a-4054-98c7-0e8a2afdff5c @ 07/01/23 13:15:45.176
  STEP: Creating a pod to test consume secrets @ 07/01/23 13:15:45.184
  E0701 13:15:45.656619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:46.657057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:47.657819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:48.658154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:15:49.221
  Jul  1 13:15:49.230: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-projected-secrets-9ed204c0-85dc-48b1-895b-85cc6092608e container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/01/23 13:15:49.241
  Jul  1 13:15:49.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5158" for this suite. @ 07/01/23 13:15:49.268
• [4.141 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 07/01/23 13:15:49.278
  Jul  1 13:15:49.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename field-validation @ 07/01/23 13:15:49.279
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:15:49.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:15:49.307
  Jul  1 13:15:49.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  E0701 13:15:49.659176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:50.660032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:51.660144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0701 13:15:51.979968      19 warnings.go:70] unknown field "alpha"
  W0701 13:15:51.979994      19 warnings.go:70] unknown field "beta"
  W0701 13:15:51.980001      19 warnings.go:70] unknown field "delta"
  W0701 13:15:51.980007      19 warnings.go:70] unknown field "epsilon"
  W0701 13:15:51.980012      19 warnings.go:70] unknown field "gamma"
  Jul  1 13:15:52.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2705" for this suite. @ 07/01/23 13:15:52.037
• [2.770 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 07/01/23 13:15:52.049
  Jul  1 13:15:52.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl @ 07/01/23 13:15:52.049
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:15:52.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:15:52.079
  STEP: creating all guestbook components @ 07/01/23 13:15:52.093
  Jul  1 13:15:52.093: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Jul  1 13:15:52.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-690 create -f -'
  E0701 13:15:52.660640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:15:53.178: INFO: stderr: ""
  Jul  1 13:15:53.178: INFO: stdout: "service/agnhost-replica created\n"
  Jul  1 13:15:53.178: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Jul  1 13:15:53.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-690 create -f -'
  E0701 13:15:53.661599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:15:54.126: INFO: stderr: ""
  Jul  1 13:15:54.126: INFO: stdout: "service/agnhost-primary created\n"
  Jul  1 13:15:54.126: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Jul  1 13:15:54.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-690 create -f -'
  Jul  1 13:15:54.649: INFO: stderr: ""
  Jul  1 13:15:54.650: INFO: stdout: "service/frontend created\n"
  Jul  1 13:15:54.651: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Jul  1 13:15:54.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-690 create -f -'
  E0701 13:15:54.662589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:15:55.190: INFO: stderr: ""
  Jul  1 13:15:55.190: INFO: stdout: "deployment.apps/frontend created\n"
  Jul  1 13:15:55.191: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jul  1 13:15:55.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-690 create -f -'
  E0701 13:15:55.662769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:15:55.668: INFO: stderr: ""
  Jul  1 13:15:55.668: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Jul  1 13:15:55.668: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jul  1 13:15:55.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-690 create -f -'
  Jul  1 13:15:56.167: INFO: stderr: ""
  Jul  1 13:15:56.167: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 07/01/23 13:15:56.167
  Jul  1 13:15:56.167: INFO: Waiting for all frontend pods to be Running.
  E0701 13:15:56.663578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:57.664321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:58.664627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:15:59.664727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:00.664960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:16:01.218: INFO: Waiting for frontend to serve content.
  Jul  1 13:16:01.234: INFO: Trying to add a new entry to the guestbook.
  Jul  1 13:16:01.257: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 07/01/23 13:16:01.271
  Jul  1 13:16:01.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-690 delete --grace-period=0 --force -f -'
  Jul  1 13:16:01.371: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  1 13:16:01.371: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 07/01/23 13:16:01.371
  Jul  1 13:16:01.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-690 delete --grace-period=0 --force -f -'
  Jul  1 13:16:01.514: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  1 13:16:01.514: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 07/01/23 13:16:01.514
  Jul  1 13:16:01.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-690 delete --grace-period=0 --force -f -'
  E0701 13:16:01.665741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:16:01.689: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  1 13:16:01.689: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 07/01/23 13:16:01.689
  Jul  1 13:16:01.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-690 delete --grace-period=0 --force -f -'
  Jul  1 13:16:01.831: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  1 13:16:01.831: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 07/01/23 13:16:01.832
  Jul  1 13:16:01.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-690 delete --grace-period=0 --force -f -'
  Jul  1 13:16:02.048: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  1 13:16:02.048: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 07/01/23 13:16:02.048
  Jul  1 13:16:02.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-690 delete --grace-period=0 --force -f -'
  Jul  1 13:16:02.198: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  1 13:16:02.198: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Jul  1 13:16:02.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-690" for this suite. @ 07/01/23 13:16:02.207
• [10.171 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 07/01/23 13:16:02.22
  Jul  1 13:16:02.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename gc @ 07/01/23 13:16:02.221
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:16:02.246
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:16:02.25
  STEP: create the rc @ 07/01/23 13:16:02.257
  W0701 13:16:02.267516      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0701 13:16:02.666394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:03.666886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:04.667294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:05.667628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:06.667752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 07/01/23 13:16:07.277
  STEP: wait for all pods to be garbage collected @ 07/01/23 13:16:07.292
  E0701 13:16:07.670714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:08.669898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:09.670013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:10.670244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:11.670833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/01/23 13:16:12.309
  W0701 13:16:12.320904      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul  1 13:16:12.320: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul  1 13:16:12.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7137" for this suite. @ 07/01/23 13:16:12.329
• [10.121 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 07/01/23 13:16:12.343
  Jul  1 13:16:12.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/01/23 13:16:12.345
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:16:12.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:16:12.385
  Jul  1 13:16:12.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  E0701 13:16:12.671601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:16:13.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-7826" for this suite. @ 07/01/23 13:16:13.439
• [1.105 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 07/01/23 13:16:13.448
  Jul  1 13:16:13.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename secrets @ 07/01/23 13:16:13.449
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:16:13.468
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:16:13.473
  STEP: Creating secret with name secret-test-map-7d1a0c22-eab5-472e-aa24-805745c97a39 @ 07/01/23 13:16:13.482
  STEP: Creating a pod to test consume secrets @ 07/01/23 13:16:13.49
  E0701 13:16:13.671937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:14.672092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:15.672606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:16.672698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:16:17.53
  Jul  1 13:16:17.536: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-secrets-9fc7690c-a299-4ea7-ba98-d88013b020c5 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/01/23 13:16:17.551
  Jul  1 13:16:17.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9803" for this suite. @ 07/01/23 13:16:17.586
• [4.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 07/01/23 13:16:17.597
  Jul  1 13:16:17.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl @ 07/01/23 13:16:17.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:16:17.623
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:16:17.632
  STEP: creating a replication controller @ 07/01/23 13:16:17.638
  Jul  1 13:16:17.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-8935 create -f -'
  E0701 13:16:17.673383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:16:18.064: INFO: stderr: ""
  Jul  1 13:16:18.065: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/01/23 13:16:18.065
  Jul  1 13:16:18.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-8935 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul  1 13:16:18.152: INFO: stderr: ""
  Jul  1 13:16:18.152: INFO: stdout: "update-demo-nautilus-p44p5 update-demo-nautilus-rszph "
  Jul  1 13:16:18.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-8935 get pods update-demo-nautilus-p44p5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  1 13:16:18.238: INFO: stderr: ""
  Jul  1 13:16:18.238: INFO: stdout: ""
  Jul  1 13:16:18.238: INFO: update-demo-nautilus-p44p5 is created but not running
  E0701 13:16:18.673577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:19.673768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:20.673880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:21.674014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:22.674111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:16:23.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-8935 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul  1 13:16:23.368: INFO: stderr: ""
  Jul  1 13:16:23.368: INFO: stdout: "update-demo-nautilus-p44p5 update-demo-nautilus-rszph "
  Jul  1 13:16:23.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-8935 get pods update-demo-nautilus-p44p5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  1 13:16:23.452: INFO: stderr: ""
  Jul  1 13:16:23.452: INFO: stdout: "true"
  Jul  1 13:16:23.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-8935 get pods update-demo-nautilus-p44p5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul  1 13:16:23.537: INFO: stderr: ""
  Jul  1 13:16:23.537: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul  1 13:16:23.537: INFO: validating pod update-demo-nautilus-p44p5
  Jul  1 13:16:23.545: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul  1 13:16:23.545: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul  1 13:16:23.545: INFO: update-demo-nautilus-p44p5 is verified up and running
  Jul  1 13:16:23.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-8935 get pods update-demo-nautilus-rszph -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  1 13:16:23.642: INFO: stderr: ""
  Jul  1 13:16:23.642: INFO: stdout: ""
  Jul  1 13:16:23.642: INFO: update-demo-nautilus-rszph is created but not running
  E0701 13:16:23.674507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:24.675596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:25.676665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:26.677121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:27.677244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:16:28.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-8935 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0701 13:16:28.678715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:16:28.769: INFO: stderr: ""
  Jul  1 13:16:28.769: INFO: stdout: "update-demo-nautilus-p44p5 update-demo-nautilus-rszph "
  Jul  1 13:16:28.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-8935 get pods update-demo-nautilus-p44p5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  1 13:16:28.894: INFO: stderr: ""
  Jul  1 13:16:28.894: INFO: stdout: "true"
  Jul  1 13:16:28.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-8935 get pods update-demo-nautilus-p44p5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul  1 13:16:29.007: INFO: stderr: ""
  Jul  1 13:16:29.007: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul  1 13:16:29.007: INFO: validating pod update-demo-nautilus-p44p5
  Jul  1 13:16:29.013: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul  1 13:16:29.013: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul  1 13:16:29.013: INFO: update-demo-nautilus-p44p5 is verified up and running
  Jul  1 13:16:29.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-8935 get pods update-demo-nautilus-rszph -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  1 13:16:29.082: INFO: stderr: ""
  Jul  1 13:16:29.083: INFO: stdout: "true"
  Jul  1 13:16:29.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-8935 get pods update-demo-nautilus-rszph -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul  1 13:16:29.150: INFO: stderr: ""
  Jul  1 13:16:29.150: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul  1 13:16:29.150: INFO: validating pod update-demo-nautilus-rszph
  Jul  1 13:16:29.158: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul  1 13:16:29.158: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul  1 13:16:29.158: INFO: update-demo-nautilus-rszph is verified up and running
  STEP: using delete to clean up resources @ 07/01/23 13:16:29.158
  Jul  1 13:16:29.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-8935 delete --grace-period=0 --force -f -'
  Jul  1 13:16:29.248: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  1 13:16:29.248: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jul  1 13:16:29.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-8935 get rc,svc -l name=update-demo --no-headers'
  Jul  1 13:16:29.353: INFO: stderr: "No resources found in kubectl-8935 namespace.\n"
  Jul  1 13:16:29.353: INFO: stdout: ""
  Jul  1 13:16:29.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-8935 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul  1 13:16:29.436: INFO: stderr: ""
  Jul  1 13:16:29.436: INFO: stdout: ""
  Jul  1 13:16:29.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8935" for this suite. @ 07/01/23 13:16:29.443
• [11.857 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 07/01/23 13:16:29.455
  Jul  1 13:16:29.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename replicaset @ 07/01/23 13:16:29.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:16:29.478
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:16:29.483
  STEP: Create a ReplicaSet @ 07/01/23 13:16:29.488
  STEP: Verify that the required pods have come up @ 07/01/23 13:16:29.496
  Jul  1 13:16:29.502: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0701 13:16:29.679652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:30.679774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:31.680632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:32.680921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:33.680988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:16:34.510: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 07/01/23 13:16:34.51
  Jul  1 13:16:34.515: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 07/01/23 13:16:34.516
  STEP: DeleteCollection of the ReplicaSets @ 07/01/23 13:16:34.522
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 07/01/23 13:16:34.539
  Jul  1 13:16:34.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-789" for this suite. @ 07/01/23 13:16:34.554
• [5.118 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 07/01/23 13:16:34.587
  Jul  1 13:16:34.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename dns @ 07/01/23 13:16:34.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:16:34.612
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:16:34.619
  STEP: Creating a test headless service @ 07/01/23 13:16:34.627
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4427 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4427;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4427 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4427;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4427.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4427.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4427.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4427.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4427.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4427.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4427.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4427.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4427.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4427.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4427.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4427.svc;check="$$(dig +notcp +noall +answer +search 239.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.239_tcp@PTR;sleep 1; done
   @ 07/01/23 13:16:34.664
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4427 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4427;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4427 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4427;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4427.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4427.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4427.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4427.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4427.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4427.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4427.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4427.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4427.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4427.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4427.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4427.svc;check="$$(dig +notcp +noall +answer +search 239.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.239_tcp@PTR;sleep 1; done
   @ 07/01/23 13:16:34.664
  STEP: creating a pod to probe DNS @ 07/01/23 13:16:34.664
  STEP: submitting the pod to kubernetes @ 07/01/23 13:16:34.664
  E0701 13:16:34.681833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:35.682652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:36.683513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/01/23 13:16:36.711
  STEP: looking for the results for each expected name from probers @ 07/01/23 13:16:36.718
  Jul  1 13:16:36.726: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4427/dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9: the server could not find the requested resource (get pods dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9)
  Jul  1 13:16:36.731: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4427/dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9: the server could not find the requested resource (get pods dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9)
  Jul  1 13:16:36.739: INFO: Unable to read wheezy_udp@dns-test-service.dns-4427 from pod dns-4427/dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9: the server could not find the requested resource (get pods dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9)
  Jul  1 13:16:36.746: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4427 from pod dns-4427/dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9: the server could not find the requested resource (get pods dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9)
  Jul  1 13:16:36.752: INFO: Unable to read wheezy_udp@dns-test-service.dns-4427.svc from pod dns-4427/dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9: the server could not find the requested resource (get pods dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9)
  Jul  1 13:16:36.759: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4427.svc from pod dns-4427/dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9: the server could not find the requested resource (get pods dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9)
  Jul  1 13:16:36.808: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4427/dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9: the server could not find the requested resource (get pods dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9)
  Jul  1 13:16:36.813: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4427/dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9: the server could not find the requested resource (get pods dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9)
  Jul  1 13:16:36.820: INFO: Unable to read jessie_udp@dns-test-service.dns-4427 from pod dns-4427/dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9: the server could not find the requested resource (get pods dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9)
  Jul  1 13:16:36.827: INFO: Unable to read jessie_tcp@dns-test-service.dns-4427 from pod dns-4427/dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9: the server could not find the requested resource (get pods dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9)
  Jul  1 13:16:36.832: INFO: Unable to read jessie_udp@dns-test-service.dns-4427.svc from pod dns-4427/dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9: the server could not find the requested resource (get pods dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9)
  Jul  1 13:16:36.839: INFO: Unable to read jessie_tcp@dns-test-service.dns-4427.svc from pod dns-4427/dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9: the server could not find the requested resource (get pods dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9)
  Jul  1 13:16:36.889: INFO: Lookups using dns-4427/dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4427 wheezy_tcp@dns-test-service.dns-4427 wheezy_udp@dns-test-service.dns-4427.svc wheezy_tcp@dns-test-service.dns-4427.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4427 jessie_tcp@dns-test-service.dns-4427 jessie_udp@dns-test-service.dns-4427.svc jessie_tcp@dns-test-service.dns-4427.svc]

  E0701 13:16:37.683571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:38.684646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:39.684746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:40.684997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:41.685722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:16:42.061: INFO: DNS probes using dns-4427/dns-test-a6ce7338-25da-4e11-a689-0bf85ca1ecd9 succeeded

  Jul  1 13:16:42.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/01/23 13:16:42.072
  STEP: deleting the test service @ 07/01/23 13:16:42.096
  STEP: deleting the test headless service @ 07/01/23 13:16:42.162
  STEP: Destroying namespace "dns-4427" for this suite. @ 07/01/23 13:16:42.183
• [7.620 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 07/01/23 13:16:42.207
  Jul  1 13:16:42.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 13:16:42.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:16:42.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:16:42.246
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 13:16:42.259
  E0701 13:16:42.685742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:43.685996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:44.686441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:45.686699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:16:46.309
  Jul  1 13:16:46.314: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-aae9b2c7-98a3-4a7f-b9e5-7e0ff06efd7a container client-container: <nil>
  STEP: delete the pod @ 07/01/23 13:16:46.325
  Jul  1 13:16:46.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6464" for this suite. @ 07/01/23 13:16:46.363
• [4.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 07/01/23 13:16:46.378
  Jul  1 13:16:46.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename disruption @ 07/01/23 13:16:46.379
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:16:46.409
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:16:46.414
  STEP: Waiting for the pdb to be processed @ 07/01/23 13:16:46.428
  STEP: Updating PodDisruptionBudget status @ 07/01/23 13:16:46.438
  STEP: Waiting for all pods to be running @ 07/01/23 13:16:46.454
  Jul  1 13:16:46.460: INFO: running pods: 0 < 1
  E0701 13:16:46.687142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:47.687804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 07/01/23 13:16:48.471
  STEP: Waiting for the pdb to be processed @ 07/01/23 13:16:48.49
  STEP: Patching PodDisruptionBudget status @ 07/01/23 13:16:48.507
  STEP: Waiting for the pdb to be processed @ 07/01/23 13:16:48.525
  Jul  1 13:16:48.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-997" for this suite. @ 07/01/23 13:16:48.538
• [2.171 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 07/01/23 13:16:48.557
  Jul  1 13:16:48.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename configmap @ 07/01/23 13:16:48.558
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:16:48.581
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:16:48.587
  STEP: Creating configMap with name configmap-test-upd-35e1522b-6030-44bb-ae86-1a7f429090e2 @ 07/01/23 13:16:48.602
  STEP: Creating the pod @ 07/01/23 13:16:48.609
  E0701 13:16:48.688168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:49.688298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 07/01/23 13:16:50.642
  STEP: Waiting for pod with binary data @ 07/01/23 13:16:50.652
  Jul  1 13:16:50.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5441" for this suite. @ 07/01/23 13:16:50.671
• [2.126 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 07/01/23 13:16:50.683
  Jul  1 13:16:50.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename replication-controller @ 07/01/23 13:16:50.684
  E0701 13:16:50.689151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:16:50.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:16:50.72
  Jul  1 13:16:50.726: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 07/01/23 13:16:50.75
  STEP: Checking rc "condition-test" has the desired failure condition set @ 07/01/23 13:16:50.759
  E0701 13:16:51.689614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 07/01/23 13:16:51.778
  Jul  1 13:16:51.796: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 07/01/23 13:16:51.796
  Jul  1 13:16:51.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3908" for this suite. @ 07/01/23 13:16:51.815
• [1.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 07/01/23 13:16:51.828
  Jul  1 13:16:51.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename pods @ 07/01/23 13:16:51.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:16:51.855
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:16:51.859
  E0701 13:16:52.690720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:53.690784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:54.691546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:55.692612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:56.692847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:57.693039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:16:57.991
  Jul  1 13:16:57.995: INFO: Trying to get logs from node ip-172-31-12-125 pod client-envvars-7aed4c90-7123-49eb-bd26-e05d5b91eedf container env3cont: <nil>
  STEP: delete the pod @ 07/01/23 13:16:58.02
  Jul  1 13:16:58.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-755" for this suite. @ 07/01/23 13:16:58.05
• [6.233 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 07/01/23 13:16:58.063
  Jul  1 13:16:58.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename secrets @ 07/01/23 13:16:58.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:16:58.087
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:16:58.093
  STEP: creating a secret @ 07/01/23 13:16:58.099
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 07/01/23 13:16:58.106
  STEP: patching the secret @ 07/01/23 13:16:58.113
  STEP: deleting the secret using a LabelSelector @ 07/01/23 13:16:58.125
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 07/01/23 13:16:58.15
  Jul  1 13:16:58.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-222" for this suite. @ 07/01/23 13:16:58.161
• [0.111 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 07/01/23 13:16:58.174
  Jul  1 13:16:58.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename configmap @ 07/01/23 13:16:58.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:16:58.196
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:16:58.2
  STEP: creating a ConfigMap @ 07/01/23 13:16:58.207
  STEP: fetching the ConfigMap @ 07/01/23 13:16:58.216
  STEP: patching the ConfigMap @ 07/01/23 13:16:58.221
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 07/01/23 13:16:58.228
  STEP: deleting the ConfigMap by collection with a label selector @ 07/01/23 13:16:58.234
  STEP: listing all ConfigMaps in test namespace @ 07/01/23 13:16:58.247
  Jul  1 13:16:58.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3017" for this suite. @ 07/01/23 13:16:58.262
• [0.100 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 07/01/23 13:16:58.275
  Jul  1 13:16:58.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 13:16:58.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:16:58.296
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:16:58.303
  STEP: Creating configMap with name projected-configmap-test-volume-map-094e3050-c85c-4fca-9e76-3c18a1617e6a @ 07/01/23 13:16:58.311
  STEP: Creating a pod to test consume configMaps @ 07/01/23 13:16:58.319
  E0701 13:16:58.693970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:16:59.694101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:00.694953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:01.695496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:17:02.357
  Jul  1 13:17:02.363: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-projected-configmaps-f40ccc57-0400-4824-b893-277af6b90956 container agnhost-container: <nil>
  STEP: delete the pod @ 07/01/23 13:17:02.374
  Jul  1 13:17:02.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3888" for this suite. @ 07/01/23 13:17:02.406
• [4.141 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 07/01/23 13:17:02.416
  Jul  1 13:17:02.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir @ 07/01/23 13:17:02.417
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:17:02.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:17:02.46
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 07/01/23 13:17:02.467
  E0701 13:17:02.696353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:03.696374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:04.696496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:05.696600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:17:06.504
  Jul  1 13:17:06.511: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-edd25ba5-6088-498d-b10a-4d34452ad17f container test-container: <nil>
  STEP: delete the pod @ 07/01/23 13:17:06.525
  Jul  1 13:17:06.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4172" for this suite. @ 07/01/23 13:17:06.559
• [4.155 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 07/01/23 13:17:06.572
  Jul  1 13:17:06.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename subjectreview @ 07/01/23 13:17:06.573
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:17:06.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:17:06.661
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-7378" @ 07/01/23 13:17:06.668
  Jul  1 13:17:06.674: INFO: saUsername: "system:serviceaccount:subjectreview-7378:e2e"
  Jul  1 13:17:06.675: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-7378"}
  Jul  1 13:17:06.675: INFO: saUID: "f58b4c25-0a8d-4180-890f-ae6f88f1c83b"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-7378:e2e" @ 07/01/23 13:17:06.675
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-7378:e2e" @ 07/01/23 13:17:06.676
  Jul  1 13:17:06.679: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-7378:e2e" api 'list' configmaps in "subjectreview-7378" namespace @ 07/01/23 13:17:06.679
  Jul  1 13:17:06.681: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-7378:e2e" @ 07/01/23 13:17:06.681
  Jul  1 13:17:06.685: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Jul  1 13:17:06.685: INFO: LocalSubjectAccessReview has been verified
  Jul  1 13:17:06.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-7378" for this suite. @ 07/01/23 13:17:06.695
  E0701 13:17:06.697310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [0.134 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 07/01/23 13:17:06.706
  Jul  1 13:17:06.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename pod-network-test @ 07/01/23 13:17:06.707
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:17:06.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:17:06.737
  STEP: Performing setup for networking test in namespace pod-network-test-4750 @ 07/01/23 13:17:06.743
  STEP: creating a selector @ 07/01/23 13:17:06.743
  STEP: Creating the service pods in kubernetes @ 07/01/23 13:17:06.743
  Jul  1 13:17:06.743: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0701 13:17:07.697622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:08.697589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:09.697855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:10.697999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:11.698078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:12.698179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:13.698307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:14.698400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:15.698682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:16.699067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:17.700134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:18.700663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 07/01/23 13:17:18.904
  E0701 13:17:19.700983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:20.701098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:17:20.977: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul  1 13:17:20.977: INFO: Going to poll 192.168.175.34 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul  1 13:17:20.985: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.175.34 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4750 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:17:20.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:17:20.985: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:17:20.986: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4750/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.175.34+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0701 13:17:21.701735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:17:22.089: INFO: Found all 1 expected endpoints: [netserver-0]
  Jul  1 13:17:22.090: INFO: Going to poll 192.168.85.56 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul  1 13:17:22.096: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.85.56 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4750 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:17:22.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:17:22.096: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:17:22.097: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4750/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.85.56+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0701 13:17:22.701843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:17:23.184: INFO: Found all 1 expected endpoints: [netserver-1]
  Jul  1 13:17:23.184: INFO: Going to poll 192.168.62.97 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul  1 13:17:23.191: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.62.97 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4750 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:17:23.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:17:23.192: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:17:23.192: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4750/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.62.97+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0701 13:17:23.702078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:17:24.283: INFO: Found all 1 expected endpoints: [netserver-2]
  Jul  1 13:17:24.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4750" for this suite. @ 07/01/23 13:17:24.291
• [17.598 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 07/01/23 13:17:24.306
  Jul  1 13:17:24.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 13:17:24.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:17:24.345
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:17:24.357
  STEP: Setting up server cert @ 07/01/23 13:17:24.428
  E0701 13:17:24.702310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 13:17:24.866
  STEP: Deploying the webhook pod @ 07/01/23 13:17:24.88
  STEP: Wait for the deployment to be ready @ 07/01/23 13:17:24.904
  Jul  1 13:17:24.937: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0701 13:17:25.703419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:26.703539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/01/23 13:17:26.955
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 13:17:26.968
  E0701 13:17:27.703574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:17:27.969: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 07/01/23 13:17:27.975
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/01/23 13:17:27.975
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 07/01/23 13:17:27.997
  E0701 13:17:28.704747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 07/01/23 13:17:29.02
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/01/23 13:17:29.02
  E0701 13:17:29.705703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 07/01/23 13:17:30.065
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/01/23 13:17:30.065
  E0701 13:17:30.706574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:31.707131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:32.708034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:33.708161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:34.708292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 07/01/23 13:17:35.139
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/01/23 13:17:35.139
  E0701 13:17:35.708389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:36.708705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:37.709236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:38.709340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:39.709498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:17:40.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1148" for this suite. @ 07/01/23 13:17:40.297
  STEP: Destroying namespace "webhook-markers-9734" for this suite. @ 07/01/23 13:17:40.308
• [16.015 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 07/01/23 13:17:40.322
  Jul  1 13:17:40.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/01/23 13:17:40.324
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:17:40.358
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:17:40.368
  Jul  1 13:17:40.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  E0701 13:17:40.710361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:41.711002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/01/23 13:17:41.962
  Jul  1 13:17:41.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-9118 --namespace=crd-publish-openapi-9118 create -f -'
  E0701 13:17:42.711178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:17:43.000: INFO: stderr: ""
  Jul  1 13:17:43.000: INFO: stdout: "e2e-test-crd-publish-openapi-5556-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jul  1 13:17:43.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-9118 --namespace=crd-publish-openapi-9118 delete e2e-test-crd-publish-openapi-5556-crds test-cr'
  Jul  1 13:17:43.104: INFO: stderr: ""
  Jul  1 13:17:43.104: INFO: stdout: "e2e-test-crd-publish-openapi-5556-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Jul  1 13:17:43.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-9118 --namespace=crd-publish-openapi-9118 apply -f -'
  E0701 13:17:43.711523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:17:44.175: INFO: stderr: ""
  Jul  1 13:17:44.175: INFO: stdout: "e2e-test-crd-publish-openapi-5556-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jul  1 13:17:44.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-9118 --namespace=crd-publish-openapi-9118 delete e2e-test-crd-publish-openapi-5556-crds test-cr'
  Jul  1 13:17:44.274: INFO: stderr: ""
  Jul  1 13:17:44.274: INFO: stdout: "e2e-test-crd-publish-openapi-5556-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 07/01/23 13:17:44.274
  Jul  1 13:17:44.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=crd-publish-openapi-9118 explain e2e-test-crd-publish-openapi-5556-crds'
  E0701 13:17:44.712614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:17:44.763: INFO: stderr: ""
  Jul  1 13:17:44.764: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-5556-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0701 13:17:45.713113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:17:46.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9118" for this suite. @ 07/01/23 13:17:46.346
• [6.040 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 07/01/23 13:17:46.363
  Jul  1 13:17:46.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename svcaccounts @ 07/01/23 13:17:46.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:17:46.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:17:46.399
  Jul  1 13:17:46.412: INFO: Got root ca configmap in namespace "svcaccounts-4441"
  Jul  1 13:17:46.425: INFO: Deleted root ca configmap in namespace "svcaccounts-4441"
  E0701 13:17:46.713675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for a new root ca configmap created @ 07/01/23 13:17:46.926
  Jul  1 13:17:46.932: INFO: Recreated root ca configmap in namespace "svcaccounts-4441"
  Jul  1 13:17:46.938: INFO: Updated root ca configmap in namespace "svcaccounts-4441"
  STEP: waiting for the root ca configmap reconciled @ 07/01/23 13:17:47.438
  Jul  1 13:17:47.444: INFO: Reconciled root ca configmap in namespace "svcaccounts-4441"
  Jul  1 13:17:47.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4441" for this suite. @ 07/01/23 13:17:47.45
• [1.100 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 07/01/23 13:17:47.464
  Jul  1 13:17:47.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename resourcequota @ 07/01/23 13:17:47.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:17:47.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:17:47.498
  STEP: Creating resourceQuota "e2e-rq-status-sfng4" @ 07/01/23 13:17:47.509
  Jul  1 13:17:47.525: INFO: Resource quota "e2e-rq-status-sfng4" reports spec: hard cpu limit of 500m
  Jul  1 13:17:47.525: INFO: Resource quota "e2e-rq-status-sfng4" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-sfng4" /status @ 07/01/23 13:17:47.525
  STEP: Confirm /status for "e2e-rq-status-sfng4" resourceQuota via watch @ 07/01/23 13:17:47.538
  Jul  1 13:17:47.540: INFO: observed resourceQuota "e2e-rq-status-sfng4" in namespace "resourcequota-5043" with hard status: v1.ResourceList(nil)
  Jul  1 13:17:47.540: INFO: Found resourceQuota "e2e-rq-status-sfng4" in namespace "resourcequota-5043" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jul  1 13:17:47.540: INFO: ResourceQuota "e2e-rq-status-sfng4" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 07/01/23 13:17:47.546
  Jul  1 13:17:47.555: INFO: Resource quota "e2e-rq-status-sfng4" reports spec: hard cpu limit of 1
  Jul  1 13:17:47.555: INFO: Resource quota "e2e-rq-status-sfng4" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-sfng4" /status @ 07/01/23 13:17:47.555
  STEP: Confirm /status for "e2e-rq-status-sfng4" resourceQuota via watch @ 07/01/23 13:17:47.566
  Jul  1 13:17:47.574: INFO: observed resourceQuota "e2e-rq-status-sfng4" in namespace "resourcequota-5043" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jul  1 13:17:47.574: INFO: Found resourceQuota "e2e-rq-status-sfng4" in namespace "resourcequota-5043" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jul  1 13:17:47.574: INFO: ResourceQuota "e2e-rq-status-sfng4" /status was patched
  STEP: Get "e2e-rq-status-sfng4" /status @ 07/01/23 13:17:47.574
  Jul  1 13:17:47.581: INFO: Resourcequota "e2e-rq-status-sfng4" reports status: hard cpu of 1
  Jul  1 13:17:47.581: INFO: Resourcequota "e2e-rq-status-sfng4" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-sfng4" /status before checking Spec is unchanged @ 07/01/23 13:17:47.586
  Jul  1 13:17:47.594: INFO: Resourcequota "e2e-rq-status-sfng4" reports status: hard cpu of 2
  Jul  1 13:17:47.594: INFO: Resourcequota "e2e-rq-status-sfng4" reports status: hard memory of 2Gi
  Jul  1 13:17:47.596: INFO: Found resourceQuota "e2e-rq-status-sfng4" in namespace "resourcequota-5043" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0701 13:17:47.714535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:48.714676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:49.715054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:50.715321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:51.715937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:52.716544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:53.718530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:54.718819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:55.719047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:56.719368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:57.720374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:58.721189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:17:59.721462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:00.721688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:01.721927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:02.722849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:03.723078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:04.723324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:05.723516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:06.723913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:07.723969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:08.724152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:09.724270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:10.724417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:11.724509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:12.725135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:13.725399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:14.726348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:15.726437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:16.727278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:17.728083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:18.728788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:19.728922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:20.729148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:21.729736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:18:22.609: INFO: ResourceQuota "e2e-rq-status-sfng4" Spec was unchanged and /status reset
  Jul  1 13:18:22.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5043" for this suite. @ 07/01/23 13:18:22.616
• [35.163 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 07/01/23 13:18:22.631
  Jul  1 13:18:22.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir @ 07/01/23 13:18:22.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:18:22.664
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:18:22.674
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 07/01/23 13:18:22.682
  E0701 13:18:22.730394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:23.730739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:24.731747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:25.732633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:18:26.726
  E0701 13:18:26.733239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:18:26.733: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-0d846fa5-f1d6-45d7-9f78-aa18bed73941 container test-container: <nil>
  STEP: delete the pod @ 07/01/23 13:18:26.744
  Jul  1 13:18:26.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7532" for this suite. @ 07/01/23 13:18:26.777
• [4.158 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 07/01/23 13:18:26.791
  Jul  1 13:18:26.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename configmap @ 07/01/23 13:18:26.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:18:26.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:18:26.826
  STEP: Creating configMap with name configmap-test-volume-map-4d1d91f3-9f01-4b3d-aeef-35efe48d462f @ 07/01/23 13:18:26.83
  STEP: Creating a pod to test consume configMaps @ 07/01/23 13:18:26.841
  E0701 13:18:27.733342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:28.733475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:29.733607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:30.733978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:18:30.883
  Jul  1 13:18:30.888: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-configmaps-226eb65c-5f21-48b8-b4b0-3e109cb9225d container agnhost-container: <nil>
  STEP: delete the pod @ 07/01/23 13:18:30.906
  Jul  1 13:18:30.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5624" for this suite. @ 07/01/23 13:18:30.946
• [4.165 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 07/01/23 13:18:30.956
  Jul  1 13:18:30.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename var-expansion @ 07/01/23 13:18:30.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:18:30.988
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:18:30.994
  E0701 13:18:31.734105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:32.734598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:18:33.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  1 13:18:33.045: INFO: Deleting pod "var-expansion-5934a568-4fbe-4ed6-a863-41eb961b0c44" in namespace "var-expansion-6717"
  Jul  1 13:18:33.058: INFO: Wait up to 5m0s for pod "var-expansion-5934a568-4fbe-4ed6-a863-41eb961b0c44" to be fully deleted
  E0701 13:18:33.734970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:34.735111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-6717" for this suite. @ 07/01/23 13:18:35.07
• [4.125 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 07/01/23 13:18:35.083
  Jul  1 13:18:35.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-runtime @ 07/01/23 13:18:35.084
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:18:35.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:18:35.132
  STEP: create the container @ 07/01/23 13:18:35.139
  W0701 13:18:35.156907      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/01/23 13:18:35.157
  E0701 13:18:35.735208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:36.735566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:37.736251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:38.736651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/01/23 13:18:39.193
  STEP: the container should be terminated @ 07/01/23 13:18:39.197
  STEP: the termination message should be set @ 07/01/23 13:18:39.198
  Jul  1 13:18:39.198: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 07/01/23 13:18:39.198
  Jul  1 13:18:39.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4448" for this suite. @ 07/01/23 13:18:39.234
• [4.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 07/01/23 13:18:39.248
  Jul  1 13:18:39.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename deployment @ 07/01/23 13:18:39.249
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:18:39.275
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:18:39.28
  Jul  1 13:18:39.288: INFO: Creating deployment "test-recreate-deployment"
  Jul  1 13:18:39.297: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Jul  1 13:18:39.315: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  E0701 13:18:39.737107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:40.737461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:18:41.327: INFO: Waiting deployment "test-recreate-deployment" to complete
  Jul  1 13:18:41.332: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Jul  1 13:18:41.348: INFO: Updating deployment test-recreate-deployment
  Jul  1 13:18:41.348: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Jul  1 13:18:41.503: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-2581  bd540cd8-343d-4dda-b7d0-a67f44dceea1 35112 2 2023-07-01 13:18:39 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-01 13:18:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 13:18:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ef00b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-01 13:18:41 +0000 UTC,LastTransitionTime:2023-07-01 13:18:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-07-01 13:18:41 +0000 UTC,LastTransitionTime:2023-07-01 13:18:39 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Jul  1 13:18:41.510: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-2581  a8e54995-66dc-4d27-bf00-900c8a4cd803 35110 1 2023-07-01 13:18:41 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment bd540cd8-343d-4dda-b7d0-a67f44dceea1 0xc004157997 0xc004157998}] [] [{kube-controller-manager Update apps/v1 2023-07-01 13:18:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd540cd8-343d-4dda-b7d0-a67f44dceea1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 13:18:41 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004157a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul  1 13:18:41.510: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Jul  1 13:18:41.510: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-2581  3c4a0586-1d2a-473c-87a2-d208a0eff240 35100 2 2023-07-01 13:18:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment bd540cd8-343d-4dda-b7d0-a67f44dceea1 0xc004157aa7 0xc004157aa8}] [] [{kube-controller-manager Update apps/v1 2023-07-01 13:18:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd540cd8-343d-4dda-b7d0-a67f44dceea1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 13:18:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004157b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul  1 13:18:41.516: INFO: Pod "test-recreate-deployment-54757ffd6c-dxw7q" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-dxw7q test-recreate-deployment-54757ffd6c- deployment-2581  2c0f3fdd-3d24-4668-97b7-5f3e03f51703 35111 0 2023-07-01 13:18:41 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c a8e54995-66dc-4d27-bf00-900c8a4cd803 0xc004157fe7 0xc004157fe8}] [] [{kube-controller-manager Update v1 2023-07-01 13:18:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8e54995-66dc-4d27-bf00-900c8a4cd803\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 13:18:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-79vhq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-79vhq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:18:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:18:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:18:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:18:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:,StartTime:2023-07-01 13:18:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  1 13:18:41.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2581" for this suite. @ 07/01/23 13:18:41.536
• [2.301 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 07/01/23 13:18:41.552
  Jul  1 13:18:41.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 13:18:41.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:18:41.596
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:18:41.616
  STEP: Creating projection with secret that has name projected-secret-test-map-721061f1-4e1f-44b9-9682-3bc601e28e98 @ 07/01/23 13:18:41.623
  STEP: Creating a pod to test consume secrets @ 07/01/23 13:18:41.63
  E0701 13:18:41.737804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:42.737927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:43.738408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:44.738775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:18:45.671
  Jul  1 13:18:45.677: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-projected-secrets-3e1bd5ee-8b0b-4e73-91b7-1be7f7aa1ae9 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/01/23 13:18:45.688
  Jul  1 13:18:45.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5988" for this suite. @ 07/01/23 13:18:45.717
• [4.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 07/01/23 13:18:45.731
  Jul  1 13:18:45.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename deployment @ 07/01/23 13:18:45.732
  E0701 13:18:45.739718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:18:45.754
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:18:45.762
  STEP: creating a Deployment @ 07/01/23 13:18:45.776
  STEP: waiting for Deployment to be created @ 07/01/23 13:18:45.786
  STEP: waiting for all Replicas to be Ready @ 07/01/23 13:18:45.789
  Jul  1 13:18:45.792: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul  1 13:18:45.792: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul  1 13:18:45.802: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul  1 13:18:45.802: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul  1 13:18:45.824: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul  1 13:18:45.824: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul  1 13:18:45.865: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul  1 13:18:45.865: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0701 13:18:46.741700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:18:47.185: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jul  1 13:18:47.185: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jul  1 13:18:47.623: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 07/01/23 13:18:47.623
  W0701 13:18:47.638311      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul  1 13:18:47.640: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 07/01/23 13:18:47.641
  Jul  1 13:18:47.643: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 0
  Jul  1 13:18:47.643: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 0
  Jul  1 13:18:47.643: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 0
  Jul  1 13:18:47.643: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 0
  Jul  1 13:18:47.643: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 0
  Jul  1 13:18:47.643: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 0
  Jul  1 13:18:47.643: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 0
  Jul  1 13:18:47.643: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 0
  Jul  1 13:18:47.643: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1
  Jul  1 13:18:47.643: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1
  Jul  1 13:18:47.643: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2
  Jul  1 13:18:47.643: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2
  Jul  1 13:18:47.644: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2
  Jul  1 13:18:47.644: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2
  Jul  1 13:18:47.654: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2
  Jul  1 13:18:47.654: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2
  Jul  1 13:18:47.677: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2
  Jul  1 13:18:47.677: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2
  Jul  1 13:18:47.694: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1
  Jul  1 13:18:47.694: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1
  Jul  1 13:18:47.720: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1
  Jul  1 13:18:47.720: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1
  E0701 13:18:47.742172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:48.742504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:18:49.203: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2
  Jul  1 13:18:49.203: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2
  Jul  1 13:18:49.245: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1
  STEP: listing Deployments @ 07/01/23 13:18:49.246
  Jul  1 13:18:49.255: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 07/01/23 13:18:49.256
  Jul  1 13:18:49.285: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 07/01/23 13:18:49.285
  Jul  1 13:18:49.311: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul  1 13:18:49.311: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul  1 13:18:49.377: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul  1 13:18:49.408: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul  1 13:18:49.418: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0701 13:18:49.743277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:18:50.212: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul  1 13:18:50.236: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul  1 13:18:50.266: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul  1 13:18:50.287: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul  1 13:18:50.322: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0701 13:18:50.744096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:18:51.651: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 07/01/23 13:18:51.702
  STEP: fetching the DeploymentStatus @ 07/01/23 13:18:51.712
  Jul  1 13:18:51.720: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1
  Jul  1 13:18:51.720: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1
  Jul  1 13:18:51.720: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1
  Jul  1 13:18:51.720: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1
  Jul  1 13:18:51.720: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 1
  Jul  1 13:18:51.720: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2
  Jul  1 13:18:51.720: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2
  Jul  1 13:18:51.721: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2
  Jul  1 13:18:51.721: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2
  Jul  1 13:18:51.721: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 2
  Jul  1 13:18:51.721: INFO: observed Deployment test-deployment in namespace deployment-2339 with ReadyReplicas 3
  STEP: deleting the Deployment @ 07/01/23 13:18:51.721
  Jul  1 13:18:51.738: INFO: observed event type MODIFIED
  Jul  1 13:18:51.739: INFO: observed event type MODIFIED
  Jul  1 13:18:51.739: INFO: observed event type MODIFIED
  Jul  1 13:18:51.739: INFO: observed event type MODIFIED
  Jul  1 13:18:51.739: INFO: observed event type MODIFIED
  Jul  1 13:18:51.739: INFO: observed event type MODIFIED
  Jul  1 13:18:51.739: INFO: observed event type MODIFIED
  Jul  1 13:18:51.739: INFO: observed event type MODIFIED
  Jul  1 13:18:51.739: INFO: observed event type MODIFIED
  Jul  1 13:18:51.739: INFO: observed event type MODIFIED
  Jul  1 13:18:51.739: INFO: observed event type MODIFIED
  Jul  1 13:18:51.739: INFO: observed event type MODIFIED
  E0701 13:18:51.744336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:18:51.747: INFO: Log out all the ReplicaSets if there is no deployment created
  Jul  1 13:18:51.757: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-2339  6db1f831-91c9-48e9-94f1-e6faf040dbf8 35274 3 2023-07-01 13:18:45 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 1778dd48-a360-485c-a084-984fe9a26545 0xc0051f52c7 0xc0051f52c8}] [] [{kube-controller-manager Update apps/v1 2023-07-01 13:18:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1778dd48-a360-485c-a084-984fe9a26545\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 13:18:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051f5360 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jul  1 13:18:51.763: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-2339  8487e34b-2b94-4e60-9a46-a1bf45bff9e2 35378 4 2023-07-01 13:18:47 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 1778dd48-a360-485c-a084-984fe9a26545 0xc0051f53c7 0xc0051f53c8}] [] [{kube-controller-manager Update apps/v1 2023-07-01 13:18:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1778dd48-a360-485c-a084-984fe9a26545\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 13:18:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051f5460 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jul  1 13:18:51.771: INFO: pod: "test-deployment-5b5dcbcd95-kkjpb":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-kkjpb test-deployment-5b5dcbcd95- deployment-2339  f9a0d7ce-d779-46c6-b202-d25b349cc116 35372 0 2023-07-01 13:18:47 +0000 UTC 2023-07-01 13:18:52 +0000 UTC 0xc0051f5818 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 8487e34b-2b94-4e60-9a46-a1bf45bff9e2 0xc0051f5847 0xc0051f5848}] [] [{kube-controller-manager Update v1 2023-07-01 13:18:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8487e34b-2b94-4e60-9a46-a1bf45bff9e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 13:18:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.62.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4wlbf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4wlbf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:18:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:18:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:18:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:18:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:192.168.62.90,StartTime:2023-07-01 13:18:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-01 13:18:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://0fbbda3d083154f1b93f05ccf7447bc46a3cda97da529166b7e943b1096a09b7,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.62.90,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul  1 13:18:51.771: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-2339  c84dfc4d-bb0c-4b24-9382-8333be3b464c 35367 2 2023-07-01 13:18:49 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 1778dd48-a360-485c-a084-984fe9a26545 0xc0051f54c7 0xc0051f54c8}] [] [{kube-controller-manager Update apps/v1 2023-07-01 13:18:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1778dd48-a360-485c-a084-984fe9a26545\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-01 13:18:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051f5550 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Jul  1 13:18:51.776: INFO: pod: "test-deployment-6fc78d85c6-9fsv9":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-9fsv9 test-deployment-6fc78d85c6- deployment-2339  2a6f9890-dd49-4c6f-abb3-69b925944da5 35366 0 2023-07-01 13:18:50 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 c84dfc4d-bb0c-4b24-9382-8333be3b464c 0xc004e58487 0xc004e58488}] [] [{kube-controller-manager Update v1 2023-07-01 13:18:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c84dfc4d-bb0c-4b24-9382-8333be3b464c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 13:18:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.175.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-crmvj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-crmvj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:18:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:18:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:18:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:18:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.125,PodIP:192.168.175.35,StartTime:2023-07-01 13:18:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-01 13:18:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://44ec685fccbacf1abb19d4decd5e0e0f54e4407a4f058951ffe8dc57bea8050d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.175.35,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul  1 13:18:51.776: INFO: pod: "test-deployment-6fc78d85c6-g2gnr":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-g2gnr test-deployment-6fc78d85c6- deployment-2339  3d9cd57c-7aa6-44ea-886a-0cbe705b7738 35316 0 2023-07-01 13:18:49 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 c84dfc4d-bb0c-4b24-9382-8333be3b464c 0xc004e58677 0xc004e58678}] [] [{kube-controller-manager Update v1 2023-07-01 13:18:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c84dfc4d-bb0c-4b24-9382-8333be3b464c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-01 13:18:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.62.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cpnqj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cpnqj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-91-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:18:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:18:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:18:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-01 13:18:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.91.66,PodIP:192.168.62.101,StartTime:2023-07-01 13:18:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-01 13:18:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a47c397e84e1c5174310ca7766944ee171fa96e3b5dbaa14bc6a1b5b03376783,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.62.101,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul  1 13:18:51.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2339" for this suite. @ 07/01/23 13:18:51.793
• [6.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 07/01/23 13:18:51.807
  Jul  1 13:18:51.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename pods @ 07/01/23 13:18:51.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:18:51.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:18:51.836
  Jul  1 13:18:51.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: creating the pod @ 07/01/23 13:18:51.842
  STEP: submitting the pod to kubernetes @ 07/01/23 13:18:51.842
  E0701 13:18:52.744515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:53.745246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:54.745323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:55.745467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:18:55.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7269" for this suite. @ 07/01/23 13:18:55.918
• [4.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 07/01/23 13:18:55.931
  Jul  1 13:18:55.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename services @ 07/01/23 13:18:55.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:18:55.957
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:18:55.963
  STEP: creating service in namespace services-3633 @ 07/01/23 13:18:55.968
  STEP: creating service affinity-nodeport-transition in namespace services-3633 @ 07/01/23 13:18:55.968
  STEP: creating replication controller affinity-nodeport-transition in namespace services-3633 @ 07/01/23 13:18:55.996
  I0701 13:18:56.023230      19 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-3633, replica count: 3
  E0701 13:18:56.746035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:57.746171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:18:58.746283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0701 13:18:59.074861      19 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0701 13:18:59.746725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:00.747550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:01.747874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0701 13:19:02.075462      19 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  1 13:19:02.099: INFO: Creating new exec pod
  E0701 13:19:02.747902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:03.748859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:04.748961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:19:05.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-3633 exec execpod-affinity9tl5t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Jul  1 13:19:05.387: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Jul  1 13:19:05.387: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 13:19:05.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-3633 exec execpod-affinity9tl5t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.18 80'
  Jul  1 13:19:05.577: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.18 80\nConnection to 10.152.183.18 80 port [tcp/http] succeeded!\n"
  Jul  1 13:19:05.577: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 13:19:05.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-3633 exec execpod-affinity9tl5t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.12.125 32077'
  E0701 13:19:05.749666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:19:05.786: INFO: stderr: "+ nc -v -t -w 2 172.31.12.125 32077\n+ echo hostName\nConnection to 172.31.12.125 32077 port [tcp/*] succeeded!\n"
  Jul  1 13:19:05.786: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 13:19:05.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-3633 exec execpod-affinity9tl5t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.91.66 32077'
  Jul  1 13:19:05.987: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.91.66 32077\nConnection to 172.31.91.66 32077 port [tcp/*] succeeded!\n"
  Jul  1 13:19:05.987: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 13:19:06.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-3633 exec execpod-affinity9tl5t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.12.125:32077/ ; done'
  Jul  1 13:19:06.406: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n"
  Jul  1 13:19:06.406: INFO: stdout: "\naffinity-nodeport-transition-lk56d\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-lk56d\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-lk56d\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-548q7\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-lk56d\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-lk56d\naffinity-nodeport-transition-lk56d\naffinity-nodeport-transition-lk56d\naffinity-nodeport-transition-lk56d\naffinity-nodeport-transition-lk56d"
  Jul  1 13:19:06.406: INFO: Received response from host: affinity-nodeport-transition-lk56d
  Jul  1 13:19:06.406: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.406: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.406: INFO: Received response from host: affinity-nodeport-transition-lk56d
  Jul  1 13:19:06.406: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.406: INFO: Received response from host: affinity-nodeport-transition-lk56d
  Jul  1 13:19:06.406: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.406: INFO: Received response from host: affinity-nodeport-transition-548q7
  Jul  1 13:19:06.406: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.406: INFO: Received response from host: affinity-nodeport-transition-lk56d
  Jul  1 13:19:06.406: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.406: INFO: Received response from host: affinity-nodeport-transition-lk56d
  Jul  1 13:19:06.406: INFO: Received response from host: affinity-nodeport-transition-lk56d
  Jul  1 13:19:06.406: INFO: Received response from host: affinity-nodeport-transition-lk56d
  Jul  1 13:19:06.406: INFO: Received response from host: affinity-nodeport-transition-lk56d
  Jul  1 13:19:06.406: INFO: Received response from host: affinity-nodeport-transition-lk56d
  Jul  1 13:19:06.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-3633 exec execpod-affinity9tl5t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.12.125:32077/ ; done'
  Jul  1 13:19:06.682: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.125:32077/\n"
  Jul  1 13:19:06.682: INFO: stdout: "\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-wt9z9\naffinity-nodeport-transition-wt9z9"
  Jul  1 13:19:06.682: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.682: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.682: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.682: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.682: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.682: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.682: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.682: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.682: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.682: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.682: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.682: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.682: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.682: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.682: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.682: INFO: Received response from host: affinity-nodeport-transition-wt9z9
  Jul  1 13:19:06.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  1 13:19:06.689: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3633, will wait for the garbage collector to delete the pods @ 07/01/23 13:19:06.708
  E0701 13:19:06.750376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:19:06.779: INFO: Deleting ReplicationController affinity-nodeport-transition took: 15.889486ms
  Jul  1 13:19:06.880: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.650219ms
  E0701 13:19:07.751271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:08.752082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-3633" for this suite. @ 07/01/23 13:19:09.424
• [13.501 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 07/01/23 13:19:09.433
  Jul  1 13:19:09.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename security-context-test @ 07/01/23 13:19:09.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:19:09.46
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:19:09.468
  E0701 13:19:09.754399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:10.755296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:11.755472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:12.755564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:19:13.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9244" for this suite. @ 07/01/23 13:19:13.53
• [4.109 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 07/01/23 13:19:13.542
  Jul  1 13:19:13.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename configmap @ 07/01/23 13:19:13.543
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:19:13.566
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:19:13.571
  STEP: Creating configMap with name configmap-test-upd-09846f37-7e17-451b-95c2-58364703a1ee @ 07/01/23 13:19:13.586
  STEP: Creating the pod @ 07/01/23 13:19:13.595
  E0701 13:19:13.756082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:14.757088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap configmap-test-upd-09846f37-7e17-451b-95c2-58364703a1ee @ 07/01/23 13:19:15.638
  STEP: waiting to observe update in volume @ 07/01/23 13:19:15.645
  E0701 13:19:15.757867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:16.758004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:17.758430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:18.758713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:19.759127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:20.759238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:21.760153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:22.760478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:23.760843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:24.761094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:25.761292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:26.761660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:27.762503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:28.762503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:29.762515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:30.762668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:31.763326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:32.763614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:33.764128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:34.764682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:35.764920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:36.765643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:37.766115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:38.766223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:39.766338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:40.766449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:41.767153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:42.767191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:43.767568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:44.768653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:45.769016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:46.769751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:47.770271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:48.770443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:49.770629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:50.770765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:51.771033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:52.771576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:53.772614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:54.772730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:55.772965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:56.773998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:57.774230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:58.774624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:19:59.774721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:00.775207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:01.775831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:02.776105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:03.776639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:04.776769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:05.777164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:06.777671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:07.777882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:08.778001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:09.778128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:10.778725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:11.779321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:12.779573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:13.780613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:14.781328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:15.781433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:16.782313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:17.782438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:18.783315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:19.783555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:20.783667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:21.784604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:22.784712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:23.784871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:24.784967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:25.785191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:26.785303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:27.785416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:28.785811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:29.786469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:30.787408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:31.787557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:32.792962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:33.793078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:34.793157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:35.793527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:36.794467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:37.794709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:38.794788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:39.795176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:40.795286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:41.795564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:42.796272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:43.796859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:44.806883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:45.807314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:20:46.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4018" for this suite. @ 07/01/23 13:20:46.274
• [92.743 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 07/01/23 13:20:46.286
  Jul  1 13:20:46.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename security-context @ 07/01/23 13:20:46.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:20:46.362
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:20:46.372
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 07/01/23 13:20:46.377
  E0701 13:20:46.808049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:47.808627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:48.809555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:49.809777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:20:50.415
  Jul  1 13:20:50.421: INFO: Trying to get logs from node ip-172-31-91-66 pod security-context-7434aa12-32e4-458e-9c3b-46d1b637eb0a container test-container: <nil>
  STEP: delete the pod @ 07/01/23 13:20:50.433
  Jul  1 13:20:50.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-6352" for this suite. @ 07/01/23 13:20:50.536
• [4.261 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 07/01/23 13:20:50.551
  Jul  1 13:20:50.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename tables @ 07/01/23 13:20:50.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:20:50.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:20:50.588
  Jul  1 13:20:50.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-5205" for this suite. @ 07/01/23 13:20:50.61
• [0.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 07/01/23 13:20:50.638
  Jul  1 13:20:50.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename services @ 07/01/23 13:20:50.639
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:20:50.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:20:50.689
  STEP: creating service endpoint-test2 in namespace services-8130 @ 07/01/23 13:20:50.697
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8130 to expose endpoints map[] @ 07/01/23 13:20:50.723
  Jul  1 13:20:50.746: INFO: successfully validated that service endpoint-test2 in namespace services-8130 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-8130 @ 07/01/23 13:20:50.746
  E0701 13:20:50.810241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:51.810848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8130 to expose endpoints map[pod1:[80]] @ 07/01/23 13:20:52.801
  E0701 13:20:52.811559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:20:52.818: INFO: successfully validated that service endpoint-test2 in namespace services-8130 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 07/01/23 13:20:52.818
  Jul  1 13:20:52.818: INFO: Creating new exec pod
  E0701 13:20:53.811678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:54.812622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:55.813316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:20:55.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-8130 exec execpodjswqp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul  1 13:20:56.080: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul  1 13:20:56.080: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 13:20:56.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-8130 exec execpodjswqp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.35 80'
  Jul  1 13:20:56.251: INFO: stderr: "+ + nc -v -t -w 2 10.152.183.35 80\nConnection to 10.152.183.35 80 port [tcp/http] succeeded!\necho hostName\n"
  Jul  1 13:20:56.251: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-8130 @ 07/01/23 13:20:56.251
  E0701 13:20:56.814567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:20:57.814674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8130 to expose endpoints map[pod1:[80] pod2:[80]] @ 07/01/23 13:20:58.283
  Jul  1 13:20:58.309: INFO: successfully validated that service endpoint-test2 in namespace services-8130 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 07/01/23 13:20:58.309
  E0701 13:20:58.815084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:20:59.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-8130 exec execpodjswqp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul  1 13:20:59.511: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul  1 13:20:59.511: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 13:20:59.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-8130 exec execpodjswqp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.35 80'
  Jul  1 13:20:59.713: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.35 80\nConnection to 10.152.183.35 80 port [tcp/http] succeeded!\n"
  Jul  1 13:20:59.713: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-8130 @ 07/01/23 13:20:59.713
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8130 to expose endpoints map[pod2:[80]] @ 07/01/23 13:20:59.744
  Jul  1 13:20:59.813: INFO: successfully validated that service endpoint-test2 in namespace services-8130 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 07/01/23 13:20:59.814
  E0701 13:20:59.815137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:21:00.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-8130 exec execpodjswqp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0701 13:21:00.815925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:21:00.996: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul  1 13:21:00.996: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  1 13:21:00.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=services-8130 exec execpodjswqp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.35 80'
  Jul  1 13:21:01.181: INFO: stderr: "+ nc -v -t -w 2 10.152.183.35 80\n+ echo hostName\nConnection to 10.152.183.35 80 port [tcp/http] succeeded!\n"
  Jul  1 13:21:01.181: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-8130 @ 07/01/23 13:21:01.181
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8130 to expose endpoints map[] @ 07/01/23 13:21:01.211
  Jul  1 13:21:01.265: INFO: successfully validated that service endpoint-test2 in namespace services-8130 exposes endpoints map[]
  Jul  1 13:21:01.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8130" for this suite. @ 07/01/23 13:21:01.319
• [10.698 seconds]
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 07/01/23 13:21:01.336
  Jul  1 13:21:01.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 13:21:01.337
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:21:01.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:21:01.403
  STEP: Creating projection with secret that has name projected-secret-test-4fb3f893-1bcf-4cd9-9247-e0f37c5e9fa2 @ 07/01/23 13:21:01.411
  STEP: Creating a pod to test consume secrets @ 07/01/23 13:21:01.424
  E0701 13:21:01.816098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:02.816623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:03.818105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:04.817766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:21:05.463
  Jul  1 13:21:05.470: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-projected-secrets-a102516c-2873-4016-9687-cdaf3a232457 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/01/23 13:21:05.48
  Jul  1 13:21:05.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5615" for this suite. @ 07/01/23 13:21:05.512
• [4.187 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 07/01/23 13:21:05.523
  Jul  1 13:21:05.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/01/23 13:21:05.524
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:21:05.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:21:05.553
  Jul  1 13:21:05.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  E0701 13:21:05.817875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:21:06.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-540" for this suite. @ 07/01/23 13:21:06.14
• [0.629 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 07/01/23 13:21:06.153
  Jul  1 13:21:06.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename sched-preemption @ 07/01/23 13:21:06.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:21:06.182
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:21:06.193
  Jul  1 13:21:06.225: INFO: Waiting up to 1m0s for all nodes to be ready
  E0701 13:21:06.818728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:07.819713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:08.819931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:09.819960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:10.820587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:11.820657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:12.820793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:13.821162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:14.821522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:15.821633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:16.822555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:17.822664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:18.822970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:19.823080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:20.823202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:21.823556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:22.824553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:23.824738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:24.825521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:25.825819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:26.826150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:27.826783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:28.826897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:29.827134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:30.827367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:31.827921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:32.828933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:33.829032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:34.829236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:35.829344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:36.829774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:37.830027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:38.830229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:39.830334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:40.830425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:41.830980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:42.831782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:43.831888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:44.832643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:45.832914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:46.833233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:47.833461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:48.834156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:49.834456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:50.835339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:51.835545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:52.836489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:53.836738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:54.837280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:55.837721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:56.838612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:57.838887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:58.839072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:21:59.839299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:00.839561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:01.840705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:02.841562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:03.841667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:04.841924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:05.842688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:22:06.289: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 07/01/23 13:22:06.306
  Jul  1 13:22:06.353: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jul  1 13:22:06.375: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jul  1 13:22:06.413: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jul  1 13:22:06.429: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jul  1 13:22:06.496: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jul  1 13:22:06.534: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 07/01/23 13:22:06.535
  E0701 13:22:06.843151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:07.844145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:08.844627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:09.845371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 07/01/23 13:22:10.617
  E0701 13:22:10.846460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:11.846574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:12.847434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:13.847850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:22:14.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-7786" for this suite. @ 07/01/23 13:22:14.751
• [68.609 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 07/01/23 13:22:14.764
  Jul  1 13:22:14.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename secrets @ 07/01/23 13:22:14.765
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:22:14.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:22:14.795
  STEP: Creating secret with name secret-test-9ed17575-5734-4871-bb91-50c5169aeca6 @ 07/01/23 13:22:14.803
  STEP: Creating a pod to test consume secrets @ 07/01/23 13:22:14.815
  E0701 13:22:14.848813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:15.848953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:16.849115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:17.849249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:18.849375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:22:18.885
  Jul  1 13:22:18.895: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-secrets-9707da59-d5db-4db0-95d8-5a9f39d74de4 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/01/23 13:22:18.911
  Jul  1 13:22:18.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3555" for this suite. @ 07/01/23 13:22:18.94
• [4.189 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 07/01/23 13:22:18.956
  Jul  1 13:22:18.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl @ 07/01/23 13:22:18.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:22:18.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:22:18.989
  STEP: creating a replication controller @ 07/01/23 13:22:18.996
  Jul  1 13:22:18.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 create -f -'
  Jul  1 13:22:19.556: INFO: stderr: ""
  Jul  1 13:22:19.556: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/01/23 13:22:19.556
  Jul  1 13:22:19.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul  1 13:22:19.655: INFO: stderr: ""
  Jul  1 13:22:19.655: INFO: stdout: "update-demo-nautilus-w6hvj update-demo-nautilus-xzfc5 "
  Jul  1 13:22:19.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods update-demo-nautilus-w6hvj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  1 13:22:19.761: INFO: stderr: ""
  Jul  1 13:22:19.761: INFO: stdout: ""
  Jul  1 13:22:19.761: INFO: update-demo-nautilus-w6hvj is created but not running
  E0701 13:22:19.849680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:20.850036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:21.850735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:22.850933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:23.851000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:22:24.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0701 13:22:24.851861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:22:24.853: INFO: stderr: ""
  Jul  1 13:22:24.853: INFO: stdout: "update-demo-nautilus-w6hvj update-demo-nautilus-xzfc5 "
  Jul  1 13:22:24.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods update-demo-nautilus-w6hvj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  1 13:22:24.923: INFO: stderr: ""
  Jul  1 13:22:24.923: INFO: stdout: "true"
  Jul  1 13:22:24.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods update-demo-nautilus-w6hvj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul  1 13:22:24.991: INFO: stderr: ""
  Jul  1 13:22:24.991: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul  1 13:22:24.991: INFO: validating pod update-demo-nautilus-w6hvj
  Jul  1 13:22:25.003: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul  1 13:22:25.003: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul  1 13:22:25.003: INFO: update-demo-nautilus-w6hvj is verified up and running
  Jul  1 13:22:25.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods update-demo-nautilus-xzfc5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  1 13:22:25.087: INFO: stderr: ""
  Jul  1 13:22:25.087: INFO: stdout: "true"
  Jul  1 13:22:25.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods update-demo-nautilus-xzfc5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul  1 13:22:25.158: INFO: stderr: ""
  Jul  1 13:22:25.158: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul  1 13:22:25.158: INFO: validating pod update-demo-nautilus-xzfc5
  Jul  1 13:22:25.167: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul  1 13:22:25.168: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul  1 13:22:25.168: INFO: update-demo-nautilus-xzfc5 is verified up and running
  STEP: scaling down the replication controller @ 07/01/23 13:22:25.168
  Jul  1 13:22:25.169: INFO: scanned /root for discovery docs: <nil>
  Jul  1 13:22:25.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0701 13:22:25.852613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:22:26.268: INFO: stderr: ""
  Jul  1 13:22:26.268: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/01/23 13:22:26.268
  Jul  1 13:22:26.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul  1 13:22:26.340: INFO: stderr: ""
  Jul  1 13:22:26.340: INFO: stdout: "update-demo-nautilus-w6hvj "
  Jul  1 13:22:26.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods update-demo-nautilus-w6hvj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  1 13:22:26.413: INFO: stderr: ""
  Jul  1 13:22:26.413: INFO: stdout: "true"
  Jul  1 13:22:26.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods update-demo-nautilus-w6hvj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul  1 13:22:26.489: INFO: stderr: ""
  Jul  1 13:22:26.489: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul  1 13:22:26.489: INFO: validating pod update-demo-nautilus-w6hvj
  Jul  1 13:22:26.497: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul  1 13:22:26.497: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul  1 13:22:26.497: INFO: update-demo-nautilus-w6hvj is verified up and running
  STEP: scaling up the replication controller @ 07/01/23 13:22:26.497
  Jul  1 13:22:26.499: INFO: scanned /root for discovery docs: <nil>
  Jul  1 13:22:26.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0701 13:22:26.853179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:22:27.597: INFO: stderr: ""
  Jul  1 13:22:27.597: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/01/23 13:22:27.597
  Jul  1 13:22:27.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul  1 13:22:27.677: INFO: stderr: ""
  Jul  1 13:22:27.677: INFO: stdout: "update-demo-nautilus-65nzm update-demo-nautilus-w6hvj "
  Jul  1 13:22:27.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods update-demo-nautilus-65nzm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  1 13:22:27.750: INFO: stderr: ""
  Jul  1 13:22:27.750: INFO: stdout: ""
  Jul  1 13:22:27.750: INFO: update-demo-nautilus-65nzm is created but not running
  E0701 13:22:27.853252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:28.853530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:29.853772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:30.853853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:31.853979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:22:32.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul  1 13:22:32.840: INFO: stderr: ""
  Jul  1 13:22:32.840: INFO: stdout: "update-demo-nautilus-65nzm update-demo-nautilus-w6hvj "
  Jul  1 13:22:32.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods update-demo-nautilus-65nzm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0701 13:22:32.854865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:22:32.923: INFO: stderr: ""
  Jul  1 13:22:32.923: INFO: stdout: "true"
  Jul  1 13:22:32.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods update-demo-nautilus-65nzm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul  1 13:22:33.009: INFO: stderr: ""
  Jul  1 13:22:33.009: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul  1 13:22:33.009: INFO: validating pod update-demo-nautilus-65nzm
  Jul  1 13:22:33.020: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul  1 13:22:33.020: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul  1 13:22:33.020: INFO: update-demo-nautilus-65nzm is verified up and running
  Jul  1 13:22:33.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods update-demo-nautilus-w6hvj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  1 13:22:33.120: INFO: stderr: ""
  Jul  1 13:22:33.120: INFO: stdout: "true"
  Jul  1 13:22:33.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods update-demo-nautilus-w6hvj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul  1 13:22:33.262: INFO: stderr: ""
  Jul  1 13:22:33.262: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul  1 13:22:33.262: INFO: validating pod update-demo-nautilus-w6hvj
  Jul  1 13:22:33.268: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul  1 13:22:33.268: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul  1 13:22:33.268: INFO: update-demo-nautilus-w6hvj is verified up and running
  STEP: using delete to clean up resources @ 07/01/23 13:22:33.268
  Jul  1 13:22:33.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 delete --grace-period=0 --force -f -'
  Jul  1 13:22:33.342: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  1 13:22:33.342: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jul  1 13:22:33.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get rc,svc -l name=update-demo --no-headers'
  Jul  1 13:22:33.460: INFO: stderr: "No resources found in kubectl-5064 namespace.\n"
  Jul  1 13:22:33.460: INFO: stdout: ""
  Jul  1 13:22:33.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-5064 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul  1 13:22:33.585: INFO: stderr: ""
  Jul  1 13:22:33.585: INFO: stdout: ""
  Jul  1 13:22:33.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5064" for this suite. @ 07/01/23 13:22:33.592
• [14.648 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 07/01/23 13:22:33.605
  Jul  1 13:22:33.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename svcaccounts @ 07/01/23 13:22:33.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:22:33.631
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:22:33.636
  Jul  1 13:22:33.671: INFO: created pod pod-service-account-defaultsa
  Jul  1 13:22:33.671: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Jul  1 13:22:33.680: INFO: created pod pod-service-account-mountsa
  Jul  1 13:22:33.680: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Jul  1 13:22:33.693: INFO: created pod pod-service-account-nomountsa
  Jul  1 13:22:33.693: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Jul  1 13:22:33.707: INFO: created pod pod-service-account-defaultsa-mountspec
  Jul  1 13:22:33.707: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Jul  1 13:22:33.717: INFO: created pod pod-service-account-mountsa-mountspec
  Jul  1 13:22:33.717: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Jul  1 13:22:33.736: INFO: created pod pod-service-account-nomountsa-mountspec
  Jul  1 13:22:33.736: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Jul  1 13:22:33.752: INFO: created pod pod-service-account-defaultsa-nomountspec
  Jul  1 13:22:33.752: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Jul  1 13:22:33.766: INFO: created pod pod-service-account-mountsa-nomountspec
  Jul  1 13:22:33.766: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Jul  1 13:22:33.779: INFO: created pod pod-service-account-nomountsa-nomountspec
  Jul  1 13:22:33.779: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Jul  1 13:22:33.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2628" for this suite. @ 07/01/23 13:22:33.795
• [0.215 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 07/01/23 13:22:33.821
  Jul  1 13:22:33.821: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename resourcequota @ 07/01/23 13:22:33.822
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:22:33.851
  E0701 13:22:33.854916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:22:33.857
  STEP: Counting existing ResourceQuota @ 07/01/23 13:22:33.863
  E0701 13:22:34.855343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:35.855892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:36.856194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:37.856465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:38.856513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/01/23 13:22:38.868
  STEP: Ensuring resource quota status is calculated @ 07/01/23 13:22:38.877
  E0701 13:22:39.858880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:40.862329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:22:40.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1090" for this suite. @ 07/01/23 13:22:40.891
• [7.080 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 07/01/23 13:22:40.902
  Jul  1 13:22:40.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename emptydir-wrapper @ 07/01/23 13:22:40.904
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:22:40.936
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:22:40.945
  STEP: Creating 50 configmaps @ 07/01/23 13:22:40.951
  STEP: Creating RC which spawns configmap-volume pods @ 07/01/23 13:22:41.402
  Jul  1 13:22:41.428: INFO: Pod name wrapped-volume-race-db6e85f0-db38-4197-848f-48aae014393f: Found 0 pods out of 5
  E0701 13:22:41.863001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:42.863549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:43.863686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:44.864076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:45.864166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:22:46.438: INFO: Pod name wrapped-volume-race-db6e85f0-db38-4197-848f-48aae014393f: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/01/23 13:22:46.438
  STEP: Creating RC which spawns configmap-volume pods @ 07/01/23 13:22:46.476
  Jul  1 13:22:46.510: INFO: Pod name wrapped-volume-race-d0b4f276-452c-40c3-b425-4b4a77e52a17: Found 1 pods out of 5
  E0701 13:22:46.865271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:47.865652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:48.866573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:49.866813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:50.867077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:22:51.523: INFO: Pod name wrapped-volume-race-d0b4f276-452c-40c3-b425-4b4a77e52a17: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/01/23 13:22:51.523
  STEP: Creating RC which spawns configmap-volume pods @ 07/01/23 13:22:51.562
  Jul  1 13:22:51.601: INFO: Pod name wrapped-volume-race-99782a7b-2ca0-43b2-b89a-8b07343f9a18: Found 1 pods out of 5
  E0701 13:22:51.867614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:52.867876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:53.868615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:54.869146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:55.868946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:22:56.617: INFO: Pod name wrapped-volume-race-99782a7b-2ca0-43b2-b89a-8b07343f9a18: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/01/23 13:22:56.617
  Jul  1 13:22:56.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-99782a7b-2ca0-43b2-b89a-8b07343f9a18 in namespace emptydir-wrapper-8230, will wait for the garbage collector to delete the pods @ 07/01/23 13:22:56.67
  Jul  1 13:22:56.746: INFO: Deleting ReplicationController wrapped-volume-race-99782a7b-2ca0-43b2-b89a-8b07343f9a18 took: 17.195921ms
  Jul  1 13:22:56.847: INFO: Terminating ReplicationController wrapped-volume-race-99782a7b-2ca0-43b2-b89a-8b07343f9a18 pods took: 100.609661ms
  E0701 13:22:56.869375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:57.870031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:22:58.870642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-d0b4f276-452c-40c3-b425-4b4a77e52a17 in namespace emptydir-wrapper-8230, will wait for the garbage collector to delete the pods @ 07/01/23 13:22:59.347
  Jul  1 13:22:59.417: INFO: Deleting ReplicationController wrapped-volume-race-d0b4f276-452c-40c3-b425-4b4a77e52a17 took: 10.972745ms
  Jul  1 13:22:59.517: INFO: Terminating ReplicationController wrapped-volume-race-d0b4f276-452c-40c3-b425-4b4a77e52a17 pods took: 100.166946ms
  E0701 13:22:59.871755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:00.872369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:01.873020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-db6e85f0-db38-4197-848f-48aae014393f in namespace emptydir-wrapper-8230, will wait for the garbage collector to delete the pods @ 07/01/23 13:23:01.918
  Jul  1 13:23:02.028: INFO: Deleting ReplicationController wrapped-volume-race-db6e85f0-db38-4197-848f-48aae014393f took: 20.23239ms
  Jul  1 13:23:02.129: INFO: Terminating ReplicationController wrapped-volume-race-db6e85f0-db38-4197-848f-48aae014393f pods took: 101.21062ms
  E0701 13:23:02.873475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:03.874227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 07/01/23 13:23:04.33
  STEP: Destroying namespace "emptydir-wrapper-8230" for this suite. @ 07/01/23 13:23:04.83
• [23.939 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 07/01/23 13:23:04.866
  Jul  1 13:23:04.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename ingressclass @ 07/01/23 13:23:04.869
  E0701 13:23:04.874545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:23:04.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:23:04.903
  STEP: getting /apis @ 07/01/23 13:23:04.909
  STEP: getting /apis/networking.k8s.io @ 07/01/23 13:23:04.915
  STEP: getting /apis/networking.k8s.iov1 @ 07/01/23 13:23:04.917
  STEP: creating @ 07/01/23 13:23:04.918
  STEP: getting @ 07/01/23 13:23:04.941
  STEP: listing @ 07/01/23 13:23:04.947
  STEP: watching @ 07/01/23 13:23:04.953
  Jul  1 13:23:04.953: INFO: starting watch
  STEP: patching @ 07/01/23 13:23:04.955
  STEP: updating @ 07/01/23 13:23:04.963
  Jul  1 13:23:04.972: INFO: waiting for watch events with expected annotations
  Jul  1 13:23:04.972: INFO: saw patched and updated annotations
  STEP: deleting @ 07/01/23 13:23:04.973
  STEP: deleting a collection @ 07/01/23 13:23:04.993
  Jul  1 13:23:05.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-4469" for this suite. @ 07/01/23 13:23:05.033
• [0.182 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 07/01/23 13:23:05.048
  Jul  1 13:23:05.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubelet-test @ 07/01/23 13:23:05.049
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:23:05.124
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:23:05.129
  E0701 13:23:05.875448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:06.876048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:07.876179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:08.876318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:23:09.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2927" for this suite. @ 07/01/23 13:23:09.172
• [4.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 07/01/23 13:23:09.186
  Jul  1 13:23:09.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename proxy @ 07/01/23 13:23:09.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:23:09.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:23:09.218
  Jul  1 13:23:09.229: INFO: Creating pod...
  E0701 13:23:09.876451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:10.876702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:23:11.267: INFO: Creating service...
  Jul  1 13:23:11.291: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1313/pods/agnhost/proxy/some/path/with/DELETE
  Jul  1 13:23:11.303: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul  1 13:23:11.303: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1313/pods/agnhost/proxy/some/path/with/GET
  Jul  1 13:23:11.309: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jul  1 13:23:11.309: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1313/pods/agnhost/proxy/some/path/with/HEAD
  Jul  1 13:23:11.315: INFO: http.Client request:HEAD | StatusCode:200
  Jul  1 13:23:11.315: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1313/pods/agnhost/proxy/some/path/with/OPTIONS
  Jul  1 13:23:11.322: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul  1 13:23:11.322: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1313/pods/agnhost/proxy/some/path/with/PATCH
  Jul  1 13:23:11.329: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul  1 13:23:11.329: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1313/pods/agnhost/proxy/some/path/with/POST
  Jul  1 13:23:11.337: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul  1 13:23:11.337: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1313/pods/agnhost/proxy/some/path/with/PUT
  Jul  1 13:23:11.344: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul  1 13:23:11.344: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1313/services/test-service/proxy/some/path/with/DELETE
  Jul  1 13:23:11.354: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul  1 13:23:11.354: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1313/services/test-service/proxy/some/path/with/GET
  Jul  1 13:23:11.363: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jul  1 13:23:11.363: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1313/services/test-service/proxy/some/path/with/HEAD
  Jul  1 13:23:11.373: INFO: http.Client request:HEAD | StatusCode:200
  Jul  1 13:23:11.373: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1313/services/test-service/proxy/some/path/with/OPTIONS
  Jul  1 13:23:11.386: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul  1 13:23:11.386: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1313/services/test-service/proxy/some/path/with/PATCH
  Jul  1 13:23:11.395: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul  1 13:23:11.395: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1313/services/test-service/proxy/some/path/with/POST
  Jul  1 13:23:11.405: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul  1 13:23:11.406: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1313/services/test-service/proxy/some/path/with/PUT
  Jul  1 13:23:11.418: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul  1 13:23:11.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-1313" for this suite. @ 07/01/23 13:23:11.425
• [2.249 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 07/01/23 13:23:11.436
  Jul  1 13:23:11.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename field-validation @ 07/01/23 13:23:11.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:23:11.461
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:23:11.467
  Jul  1 13:23:11.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  E0701 13:23:11.877342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:12.878257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:13.878492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0701 13:23:14.037908      19 warnings.go:70] unknown field "alpha"
  W0701 13:23:14.037931      19 warnings.go:70] unknown field "beta"
  W0701 13:23:14.037938      19 warnings.go:70] unknown field "delta"
  W0701 13:23:14.037946      19 warnings.go:70] unknown field "epsilon"
  W0701 13:23:14.037953      19 warnings.go:70] unknown field "gamma"
  Jul  1 13:23:14.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-844" for this suite. @ 07/01/23 13:23:14.09
• [2.667 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 07/01/23 13:23:14.103
  Jul  1 13:23:14.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename gc @ 07/01/23 13:23:14.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:23:14.152
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:23:14.158
  STEP: create the rc1 @ 07/01/23 13:23:14.17
  STEP: create the rc2 @ 07/01/23 13:23:14.179
  E0701 13:23:14.879564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:15.879705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:16.880627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:17.881182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:18.882876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:19.884037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 07/01/23 13:23:20.202
  E0701 13:23:20.884834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc simpletest-rc-to-be-deleted @ 07/01/23 13:23:21.043
  STEP: wait for the rc to be deleted @ 07/01/23 13:23:21.063
  E0701 13:23:21.885922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:22.886901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:23.888005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:24.888620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:25.888931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:23:26.081: INFO: 69 pods remaining
  Jul  1 13:23:26.081: INFO: 69 pods has nil DeletionTimestamp
  Jul  1 13:23:26.081: INFO: 
  E0701 13:23:26.889400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:27.889831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:28.890199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:29.891290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:30.891904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/01/23 13:23:31.089
  W0701 13:23:31.096369      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul  1 13:23:31.096: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul  1 13:23:31.098: INFO: Deleting pod "simpletest-rc-to-be-deleted-25fl4" in namespace "gc-7376"
  Jul  1 13:23:31.123: INFO: Deleting pod "simpletest-rc-to-be-deleted-26mlh" in namespace "gc-7376"
  Jul  1 13:23:31.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-44qp8" in namespace "gc-7376"
  Jul  1 13:23:31.215: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kwhb" in namespace "gc-7376"
  Jul  1 13:23:31.250: INFO: Deleting pod "simpletest-rc-to-be-deleted-4pxqm" in namespace "gc-7376"
  Jul  1 13:23:31.305: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wq5b" in namespace "gc-7376"
  Jul  1 13:23:31.369: INFO: Deleting pod "simpletest-rc-to-be-deleted-52m46" in namespace "gc-7376"
  Jul  1 13:23:31.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-55lfc" in namespace "gc-7376"
  Jul  1 13:23:31.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-59mxx" in namespace "gc-7376"
  Jul  1 13:23:31.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-5gvbs" in namespace "gc-7376"
  Jul  1 13:23:31.479: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vx2v" in namespace "gc-7376"
  Jul  1 13:23:31.506: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zm6g" in namespace "gc-7376"
  Jul  1 13:23:31.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zq57" in namespace "gc-7376"
  Jul  1 13:23:31.543: INFO: Deleting pod "simpletest-rc-to-be-deleted-69znt" in namespace "gc-7376"
  Jul  1 13:23:31.562: INFO: Deleting pod "simpletest-rc-to-be-deleted-6f2s4" in namespace "gc-7376"
  Jul  1 13:23:31.588: INFO: Deleting pod "simpletest-rc-to-be-deleted-6g26h" in namespace "gc-7376"
  Jul  1 13:23:31.615: INFO: Deleting pod "simpletest-rc-to-be-deleted-6k674" in namespace "gc-7376"
  Jul  1 13:23:31.637: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qbtb" in namespace "gc-7376"
  Jul  1 13:23:31.659: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wdcn" in namespace "gc-7376"
  Jul  1 13:23:31.686: INFO: Deleting pod "simpletest-rc-to-be-deleted-768jf" in namespace "gc-7376"
  Jul  1 13:23:31.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dlgh" in namespace "gc-7376"
  Jul  1 13:23:31.752: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jbcq" in namespace "gc-7376"
  Jul  1 13:23:31.776: INFO: Deleting pod "simpletest-rc-to-be-deleted-95m5m" in namespace "gc-7376"
  Jul  1 13:23:31.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nbnb" in namespace "gc-7376"
  Jul  1 13:23:31.827: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7qzr" in namespace "gc-7376"
  Jul  1 13:23:31.853: INFO: Deleting pod "simpletest-rc-to-be-deleted-b97gw" in namespace "gc-7376"
  Jul  1 13:23:31.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2bhb" in namespace "gc-7376"
  E0701 13:23:31.892187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:23:31.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-cdfv5" in namespace "gc-7376"
  Jul  1 13:23:31.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-chg8v" in namespace "gc-7376"
  Jul  1 13:23:31.954: INFO: Deleting pod "simpletest-rc-to-be-deleted-ck94d" in namespace "gc-7376"
  Jul  1 13:23:31.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-cn7m2" in namespace "gc-7376"
  Jul  1 13:23:32.026: INFO: Deleting pod "simpletest-rc-to-be-deleted-cs9pt" in namespace "gc-7376"
  Jul  1 13:23:32.051: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvk66" in namespace "gc-7376"
  Jul  1 13:23:32.076: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5k8b" in namespace "gc-7376"
  Jul  1 13:23:32.100: INFO: Deleting pod "simpletest-rc-to-be-deleted-d856m" in namespace "gc-7376"
  Jul  1 13:23:32.118: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhv7j" in namespace "gc-7376"
  Jul  1 13:23:32.139: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqchv" in namespace "gc-7376"
  Jul  1 13:23:32.158: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdjkt" in namespace "gc-7376"
  Jul  1 13:23:32.177: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkhnh" in namespace "gc-7376"
  Jul  1 13:23:32.204: INFO: Deleting pod "simpletest-rc-to-be-deleted-gml68" in namespace "gc-7376"
  Jul  1 13:23:32.220: INFO: Deleting pod "simpletest-rc-to-be-deleted-h29f5" in namespace "gc-7376"
  Jul  1 13:23:32.244: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcz86" in namespace "gc-7376"
  Jul  1 13:23:32.265: INFO: Deleting pod "simpletest-rc-to-be-deleted-hj6lv" in namespace "gc-7376"
  Jul  1 13:23:32.289: INFO: Deleting pod "simpletest-rc-to-be-deleted-hp976" in namespace "gc-7376"
  Jul  1 13:23:32.307: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpsjw" in namespace "gc-7376"
  Jul  1 13:23:32.329: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5dlz" in namespace "gc-7376"
  Jul  1 13:23:32.353: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5p87" in namespace "gc-7376"
  Jul  1 13:23:32.381: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9dgr" in namespace "gc-7376"
  Jul  1 13:23:32.403: INFO: Deleting pod "simpletest-rc-to-be-deleted-jgp92" in namespace "gc-7376"
  Jul  1 13:23:32.431: INFO: Deleting pod "simpletest-rc-to-be-deleted-jvfjw" in namespace "gc-7376"
  Jul  1 13:23:32.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7376" for this suite. @ 07/01/23 13:23:32.454
• [18.361 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 07/01/23 13:23:32.478
  Jul  1 13:23:32.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename pods @ 07/01/23 13:23:32.479
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:23:32.51
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:23:32.516
  STEP: creating the pod @ 07/01/23 13:23:32.524
  STEP: submitting the pod to kubernetes @ 07/01/23 13:23:32.525
  W0701 13:23:32.540829      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0701 13:23:32.892893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:33.893150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:34.894002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:35.894260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:36.895262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:37.896357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 07/01/23 13:23:38.578
  STEP: updating the pod @ 07/01/23 13:23:38.585
  E0701 13:23:38.897403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:23:39.101: INFO: Successfully updated pod "pod-update-activedeadlineseconds-4b9ad39d-5702-4272-aa5d-420fd0bd6828"
  E0701 13:23:39.897749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:40.898013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:41.898642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:42.898731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:23:43.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5027" for this suite. @ 07/01/23 13:23:43.129
• [10.660 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 07/01/23 13:23:43.14
  Jul  1 13:23:43.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename downward-api @ 07/01/23 13:23:43.141
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:23:43.163
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:23:43.169
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 13:23:43.174
  E0701 13:23:43.898785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:44.898993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:45.899024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:46.899401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:23:47.206
  Jul  1 13:23:47.210: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-306b6dfc-1d44-4cfa-99b3-fa04eafc5127 container client-container: <nil>
  STEP: delete the pod @ 07/01/23 13:23:47.223
  Jul  1 13:23:47.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2954" for this suite. @ 07/01/23 13:23:47.254
• [4.128 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 07/01/23 13:23:47.269
  Jul  1 13:23:47.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename job @ 07/01/23 13:23:47.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:23:47.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:23:47.305
  STEP: Creating a job @ 07/01/23 13:23:47.318
  STEP: Ensure pods equal to parallelism count is attached to the job @ 07/01/23 13:23:47.331
  E0701 13:23:47.899966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:48.900293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:49.900604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:50.901354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 07/01/23 13:23:51.337
  STEP: updating /status @ 07/01/23 13:23:51.346
  STEP: get /status @ 07/01/23 13:23:51.359
  Jul  1 13:23:51.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2841" for this suite. @ 07/01/23 13:23:51.37
• [4.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 07/01/23 13:23:51.383
  Jul  1 13:23:51.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/01/23 13:23:51.387
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:23:51.409
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:23:51.415
  STEP: create the container to handle the HTTPGet hook request. @ 07/01/23 13:23:51.425
  E0701 13:23:51.902368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:52.902647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 07/01/23 13:23:53.46
  E0701 13:23:53.903744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:54.903862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 07/01/23 13:23:55.489
  STEP: delete the pod with lifecycle hook @ 07/01/23 13:23:55.515
  E0701 13:23:55.904576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:56.905005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:23:57.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3409" for this suite. @ 07/01/23 13:23:57.543
• [6.169 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 07/01/23 13:23:57.556
  Jul  1 13:23:57.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename crd-watch @ 07/01/23 13:23:57.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:23:57.579
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:23:57.584
  Jul  1 13:23:57.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  E0701 13:23:57.905993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:58.906330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:23:59.906502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 07/01/23 13:24:00.162
  Jul  1 13:24:00.169: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-01T13:24:00Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-01T13:24:00Z]] name:name1 resourceVersion:40360 uid:0ccf88d5-3f86-48a6-9eba-a9957c59625f] num:map[num1:9223372036854775807 num2:1000000]]}
  E0701 13:24:00.907402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:01.907550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:02.907939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:03.908120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:04.908426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:05.908515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:06.908878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:07.908993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:08.909111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:09.909236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 07/01/23 13:24:10.17
  Jul  1 13:24:10.180: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-01T13:24:10Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-01T13:24:10Z]] name:name2 resourceVersion:40404 uid:8edd0196-e2ac-4663-be3c-09f8fe2c5a23] num:map[num1:9223372036854775807 num2:1000000]]}
  E0701 13:24:10.909699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:11.910155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:12.911569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:13.912237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:14.912357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:15.912425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:16.912535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:17.912639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:18.912773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:19.913097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 07/01/23 13:24:20.181
  Jul  1 13:24:20.191: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-01T13:24:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-01T13:24:20Z]] name:name1 resourceVersion:40424 uid:0ccf88d5-3f86-48a6-9eba-a9957c59625f] num:map[num1:9223372036854775807 num2:1000000]]}
  E0701 13:24:20.913463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:21.913602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:22.913728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:23.913838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:24.914068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:25.914366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:26.914616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:27.914902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:28.915220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:29.915443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 07/01/23 13:24:30.191
  Jul  1 13:24:30.200: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-01T13:24:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-01T13:24:30Z]] name:name2 resourceVersion:40462 uid:8edd0196-e2ac-4663-be3c-09f8fe2c5a23] num:map[num1:9223372036854775807 num2:1000000]]}
  E0701 13:24:30.916482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:31.916616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:32.916722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:33.916973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:34.917195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:35.917601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:36.918235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:37.918518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:38.919274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:39.920160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 07/01/23 13:24:40.2
  Jul  1 13:24:40.219: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-01T13:24:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-01T13:24:20Z]] name:name1 resourceVersion:40494 uid:0ccf88d5-3f86-48a6-9eba-a9957c59625f] num:map[num1:9223372036854775807 num2:1000000]]}
  E0701 13:24:40.920268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:41.920345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:42.920455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:43.920672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:44.921155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:45.921583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:46.921832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:47.921946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:48.922067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:49.922197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 07/01/23 13:24:50.219
  Jul  1 13:24:50.232: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-01T13:24:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-01T13:24:30Z]] name:name2 resourceVersion:40514 uid:8edd0196-e2ac-4663-be3c-09f8fe2c5a23] num:map[num1:9223372036854775807 num2:1000000]]}
  E0701 13:24:50.922466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:51.922736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:52.923005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:53.923226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:54.923516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:55.923851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:56.923969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:57.924865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:58.929178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:24:59.929271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:25:00.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-3948" for this suite. @ 07/01/23 13:25:00.764
• [63.220 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 07/01/23 13:25:00.777
  Jul  1 13:25:00.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename container-probe @ 07/01/23 13:25:00.778
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:25:00.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:25:00.808
  STEP: Creating pod test-grpc-caf23fa1-2697-4f4b-8eee-d1977b804807 in namespace container-probe-759 @ 07/01/23 13:25:00.814
  E0701 13:25:00.929326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:01.929452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:25:02.858: INFO: Started pod test-grpc-caf23fa1-2697-4f4b-8eee-d1977b804807 in namespace container-probe-759
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/01/23 13:25:02.858
  Jul  1 13:25:02.863: INFO: Initial restart count of pod test-grpc-caf23fa1-2697-4f4b-8eee-d1977b804807 is 0
  E0701 13:25:02.929947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:03.930060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:04.930099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:05.930448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:06.930996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:07.931225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:08.931334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:09.931776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:10.932716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:11.932829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:12.933867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:13.934104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:14.934724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:15.935138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:16.935642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:17.935758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:18.936034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:19.936856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:20.937525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:21.938300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:22.939296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:23.939757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:24.939862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:25.940247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:26.940349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:27.941120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:28.941812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:29.941949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:30.942413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:31.942521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:32.942742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:33.943114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:34.943464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:35.944446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:36.944501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:37.944590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:38.945431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:39.945652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:40.946411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:41.946546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:42.946798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:43.946920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:44.947056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:45.947462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:46.948277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:47.948712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:48.948960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:49.949388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:50.950033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:51.950187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:52.950944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:53.951075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:54.951204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:55.951634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:56.951581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:57.951698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:58.952652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:25:59.952769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:00.953451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:01.954129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:02.954259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:03.954414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:04.955582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:05.955810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:06.955964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:07.956593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:08.956699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:09.956820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:10.957647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:11.958533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:12.958788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:13.959578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:14.959701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:15.959956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:16.960619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:17.960675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:18.960791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:19.961087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:20.961671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:21.961808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:22.961948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:23.962075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:24.962186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:25.963021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:26.963178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:27.963299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:28.963410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:29.963794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:30.964424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:31.964673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:32.964680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:33.964811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:34.964923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:35.965913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:36.966443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:37.966625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:38.966725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:39.967711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:40.968375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:41.968808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:42.969131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:43.969256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:44.969359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:45.970035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:46.970160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:47.970923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:48.971241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:49.971755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:50.972384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:51.972551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:52.972676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:53.972793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:54.972972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:55.973881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:56.974738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:57.974942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:58.975068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:26:59.975713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:00.976330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:01.976440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:02.976563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:03.976673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:04.976869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:05.977317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:06.977524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:07.977658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:08.977977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:09.978107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:10.978333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:11.979044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:12.979182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:13.979515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:14.979861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:15.980917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:16.981004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:17.981936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:18.982042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:19.983205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:20.983722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:21.984288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:22.984411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:23.985222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:24.985518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:25.985896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:26.986571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:27.986897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:28.987503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:29.988389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:30.989357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:31.989462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:32.989805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:33.989882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:34.990131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:35.990555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:36.990688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:37.990932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:38.991156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:39.991482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:40.992470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:41.993136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:42.993251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:43.993379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:44.993528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:45.993887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:46.994469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:47.995373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:48.995519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:49.996613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:50.997091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:51.997944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:52.998188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:53.998316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:54.998821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:55.999343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:57.000378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:58.000520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:27:59.000654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:00.001295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:01.001408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:02.001796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:03.001926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:04.002049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:05.002470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:06.002840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:07.003785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:08.004537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:09.004740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:10.005725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:11.006290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:12.007263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:13.007511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:14.008446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:15.008592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:16.009525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:17.010206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:18.010336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:19.010481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:20.011373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:21.011890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:22.012639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:23.012769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:24.012897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:25.013074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:26.013974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:27.014100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:28.014852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:29.014956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:30.015089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:31.015752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:32.016556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:33.016822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:34.017373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:35.017822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:36.018612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:37.018759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:38.019303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:39.019641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:40.020622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:41.021191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:42.021804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:43.022025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:44.022798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:45.022998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:46.023316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:47.024063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:48.024197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:49.024316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:50.025266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:51.025896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:52.026337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:53.026445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:54.027577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:55.028618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:56.029336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:57.030168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:58.030246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:28:59.030348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:00.031215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:01.032345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:02.031938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:03.032647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:29:03.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/01/23 13:29:03.769
  STEP: Destroying namespace "container-probe-759" for this suite. @ 07/01/23 13:29:03.79
• [243.033 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 07/01/23 13:29:03.811
  Jul  1 13:29:03.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename services @ 07/01/23 13:29:03.812
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:29:03.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:29:03.846
  STEP: creating a Service @ 07/01/23 13:29:03.86
  STEP: watching for the Service to be added @ 07/01/23 13:29:03.876
  Jul  1 13:29:03.880: INFO: Found Service test-service-zb9zn in namespace services-3989 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Jul  1 13:29:03.881: INFO: Service test-service-zb9zn created
  STEP: Getting /status @ 07/01/23 13:29:03.881
  Jul  1 13:29:03.889: INFO: Service test-service-zb9zn has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 07/01/23 13:29:03.89
  STEP: watching for the Service to be patched @ 07/01/23 13:29:03.898
  Jul  1 13:29:03.901: INFO: observed Service test-service-zb9zn in namespace services-3989 with annotations: map[] & LoadBalancer: {[]}
  Jul  1 13:29:03.901: INFO: Found Service test-service-zb9zn in namespace services-3989 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Jul  1 13:29:03.901: INFO: Service test-service-zb9zn has service status patched
  STEP: updating the ServiceStatus @ 07/01/23 13:29:03.901
  Jul  1 13:29:03.920: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 07/01/23 13:29:03.92
  Jul  1 13:29:03.923: INFO: Observed Service test-service-zb9zn in namespace services-3989 with annotations: map[] & Conditions: {[]}
  Jul  1 13:29:03.923: INFO: Observed event: &Service{ObjectMeta:{test-service-zb9zn  services-3989  3687ade6-5010-4ac0-b9dc-fdbfd56d5f70 41038 0 2023-07-01 13:29:03 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-07-01 13:29:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-07-01 13:29:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.71,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.71],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Jul  1 13:29:03.923: INFO: Found Service test-service-zb9zn in namespace services-3989 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul  1 13:29:03.923: INFO: Service test-service-zb9zn has service status updated
  STEP: patching the service @ 07/01/23 13:29:03.923
  STEP: watching for the Service to be patched @ 07/01/23 13:29:03.938
  Jul  1 13:29:03.942: INFO: observed Service test-service-zb9zn in namespace services-3989 with labels: map[test-service-static:true]
  Jul  1 13:29:03.942: INFO: observed Service test-service-zb9zn in namespace services-3989 with labels: map[test-service-static:true]
  Jul  1 13:29:03.942: INFO: observed Service test-service-zb9zn in namespace services-3989 with labels: map[test-service-static:true]
  Jul  1 13:29:03.942: INFO: Found Service test-service-zb9zn in namespace services-3989 with labels: map[test-service:patched test-service-static:true]
  Jul  1 13:29:03.942: INFO: Service test-service-zb9zn patched
  STEP: deleting the service @ 07/01/23 13:29:03.942
  STEP: watching for the Service to be deleted @ 07/01/23 13:29:03.969
  Jul  1 13:29:03.972: INFO: Observed event: ADDED
  Jul  1 13:29:03.972: INFO: Observed event: MODIFIED
  Jul  1 13:29:03.972: INFO: Observed event: MODIFIED
  Jul  1 13:29:03.972: INFO: Observed event: MODIFIED
  Jul  1 13:29:03.972: INFO: Found Service test-service-zb9zn in namespace services-3989 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Jul  1 13:29:03.972: INFO: Service test-service-zb9zn deleted
  Jul  1 13:29:03.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3989" for this suite. @ 07/01/23 13:29:03.981
• [0.183 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 07/01/23 13:29:03.996
  Jul  1 13:29:03.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 13:29:03.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:29:04.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:29:04.03
  E0701 13:29:04.033203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating configMap with name projected-configmap-test-volume-81e82da7-0f2b-4d95-96af-26e61ca1e24d @ 07/01/23 13:29:04.034
  STEP: Creating a pod to test consume configMaps @ 07/01/23 13:29:04.042
  E0701 13:29:05.033390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:06.033458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:07.033643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:08.033982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:29:08.077
  Jul  1 13:29:08.083: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-projected-configmaps-e9d478e7-5059-4399-ba03-0b77c72ca5ef container agnhost-container: <nil>
  STEP: delete the pod @ 07/01/23 13:29:08.115
  Jul  1 13:29:08.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2105" for this suite. @ 07/01/23 13:29:08.149
• [4.163 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 07/01/23 13:29:08.16
  Jul  1 13:29:08.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename csiinlinevolumes @ 07/01/23 13:29:08.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:29:08.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:29:08.195
  STEP: creating @ 07/01/23 13:29:08.202
  STEP: getting @ 07/01/23 13:29:08.238
  STEP: listing in namespace @ 07/01/23 13:29:08.25
  STEP: patching @ 07/01/23 13:29:08.256
  STEP: deleting @ 07/01/23 13:29:08.27
  Jul  1 13:29:08.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-3646" for this suite. @ 07/01/23 13:29:08.296
• [0.146 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 07/01/23 13:29:08.309
  Jul  1 13:29:08.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename taint-single-pod @ 07/01/23 13:29:08.315
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:29:08.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:29:08.344
  Jul  1 13:29:08.350: INFO: Waiting up to 1m0s for all nodes to be ready
  E0701 13:29:09.034856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:10.035024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:11.035141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:12.035422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:13.035575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:14.035714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:15.036561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:16.036682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:17.037651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:18.037906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:19.038606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:20.038854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:21.039506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:22.039614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:23.040326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:24.040353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:25.040679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:26.041007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:27.041517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:28.041677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:29.041805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:30.042059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:31.042146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:32.042631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:33.042666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:34.044857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:35.044965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:36.045798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:37.046561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:38.046687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:39.046818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:40.047066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:41.047659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:42.048640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:43.049153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:44.049407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:45.050251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:46.050977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:47.051115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:48.051223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:49.051351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:50.051548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:51.051723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:52.051817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:53.052717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:54.052782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:55.053670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:56.054471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:57.054860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:58.055115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:29:59.055787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:00.056661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:01.057522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:02.057678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:03.058564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:04.058702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:05.058852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:06.058936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:07.059090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:08.059201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:30:08.371: INFO: Waiting for terminating namespaces to be deleted...
  Jul  1 13:30:08.376: INFO: Starting informer...
  STEP: Starting pod... @ 07/01/23 13:30:08.376
  Jul  1 13:30:08.607: INFO: Pod is running on ip-172-31-91-66. Tainting Node
  STEP: Trying to apply a taint on the Node @ 07/01/23 13:30:08.608
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/01/23 13:30:08.624
  STEP: Waiting short time to make sure Pod is queued for deletion @ 07/01/23 13:30:08.635
  Jul  1 13:30:08.635: INFO: Pod wasn't evicted. Proceeding
  Jul  1 13:30:08.635: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/01/23 13:30:08.687
  STEP: Waiting some time to make sure that toleration time passed. @ 07/01/23 13:30:08.7
  E0701 13:30:09.059992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:10.060204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:11.060519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:12.060760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:13.061344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:14.061647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:15.061776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:16.062134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:17.062385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:18.062698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:19.062900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:20.063345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:21.063927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:22.064915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:23.065047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:24.065198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:25.065295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:26.065860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:27.065973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:28.066585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:29.066703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:30.067039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:31.067635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:32.067739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:33.067874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:34.068656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:35.069040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:36.070001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:37.070252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:38.070503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:39.070620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:40.070764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:41.071527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:42.071802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:43.071972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:44.072128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:45.072253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:46.072344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:47.072546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:48.072805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:49.072998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:50.073091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:51.073520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:52.073946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:53.074316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:54.075546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:55.076764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:56.077018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:57.077152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:58.077292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:30:59.077429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:00.077592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:01.078256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:02.078485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:03.078602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:04.078877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:05.079112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:06.079683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:07.079541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:08.080625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:09.080851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:10.081091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:11.081392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:12.081509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:13.082279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:14.082802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:15.083109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:16.085685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:17.085879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:18.086152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:19.086890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:20.087103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:21.087516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:22.087640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:23.088600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:31:23.701: INFO: Pod wasn't evicted. Test successful
  Jul  1 13:31:23.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-1007" for this suite. @ 07/01/23 13:31:23.709
• [135.410 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 07/01/23 13:31:23.72
  Jul  1 13:31:23.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename endpointslice @ 07/01/23 13:31:23.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:31:23.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:31:23.753
  Jul  1 13:31:23.774: INFO: Endpoints addresses: [172.31.34.114 172.31.68.61] , ports: [6443]
  Jul  1 13:31:23.774: INFO: EndpointSlices addresses: [172.31.34.114 172.31.68.61] , ports: [6443]
  Jul  1 13:31:23.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5253" for this suite. @ 07/01/23 13:31:23.781
• [0.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 07/01/23 13:31:23.793
  Jul  1 13:31:23.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename configmap @ 07/01/23 13:31:23.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:31:23.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:31:23.823
  Jul  1 13:31:23.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6227" for this suite. @ 07/01/23 13:31:23.9
• [0.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 07/01/23 13:31:23.912
  Jul  1 13:31:23.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 13:31:23.913
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:31:23.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:31:23.943
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 13:31:23.947
  E0701 13:31:24.089104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:25.089217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:26.089834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:27.089948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:31:27.985
  Jul  1 13:31:27.991: INFO: Trying to get logs from node ip-172-31-91-66 pod downwardapi-volume-dc1f5fbb-5add-493d-aff6-60ed79f2ba53 container client-container: <nil>
  STEP: delete the pod @ 07/01/23 13:31:28.018
  Jul  1 13:31:28.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6353" for this suite. @ 07/01/23 13:31:28.053
• [4.153 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 07/01/23 13:31:28.066
  Jul  1 13:31:28.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl @ 07/01/23 13:31:28.067
  E0701 13:31:28.090519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:31:28.092
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:31:28.098
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/01/23 13:31:28.106
  Jul  1 13:31:28.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-3085 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Jul  1 13:31:28.227: INFO: stderr: ""
  Jul  1 13:31:28.227: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 07/01/23 13:31:28.227
  Jul  1 13:31:28.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-3085 delete pods e2e-test-httpd-pod'
  E0701 13:31:29.090763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:30.090995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:31:30.590: INFO: stderr: ""
  Jul  1 13:31:30.590: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul  1 13:31:30.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3085" for this suite. @ 07/01/23 13:31:30.6
• [2.545 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 07/01/23 13:31:30.612
  Jul  1 13:31:30.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename secrets @ 07/01/23 13:31:30.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:31:30.636
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:31:30.653
  STEP: Creating secret with name secret-test-72fcde14-6874-4f0b-9555-0c0e324158c0 @ 07/01/23 13:31:30.66
  STEP: Creating a pod to test consume secrets @ 07/01/23 13:31:30.669
  E0701 13:31:31.091838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:32.092739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:33.093343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:34.094034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:31:34.708
  Jul  1 13:31:34.715: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-secrets-fe8aff59-9fec-43c4-97c4-a412a332e37f container secret-env-test: <nil>
  STEP: delete the pod @ 07/01/23 13:31:34.728
  Jul  1 13:31:34.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-873" for this suite. @ 07/01/23 13:31:34.758
• [4.161 seconds]
------------------------------
S
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 07/01/23 13:31:34.773
  Jul  1 13:31:34.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 07/01/23 13:31:34.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:31:34.799
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:31:34.806
  STEP: Setting up the test @ 07/01/23 13:31:34.811
  STEP: Creating hostNetwork=false pod @ 07/01/23 13:31:34.811
  E0701 13:31:35.094142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:36.094321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 07/01/23 13:31:36.85
  E0701 13:31:37.095317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:38.095588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 07/01/23 13:31:38.88
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 07/01/23 13:31:38.88
  Jul  1 13:31:38.880: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6493 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:31:38.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:31:38.881: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:31:38.881: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6493/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul  1 13:31:38.964: INFO: Exec stderr: ""
  Jul  1 13:31:38.964: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6493 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:31:38.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:31:38.964: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:31:38.964: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6493/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul  1 13:31:39.042: INFO: Exec stderr: ""
  Jul  1 13:31:39.042: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6493 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:31:39.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:31:39.043: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:31:39.043: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6493/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  E0701 13:31:39.096005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:31:39.125: INFO: Exec stderr: ""
  Jul  1 13:31:39.125: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6493 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:31:39.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:31:39.126: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:31:39.126: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6493/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul  1 13:31:39.196: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 07/01/23 13:31:39.196
  Jul  1 13:31:39.196: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6493 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:31:39.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:31:39.197: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:31:39.197: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6493/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jul  1 13:31:39.270: INFO: Exec stderr: ""
  Jul  1 13:31:39.270: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6493 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:31:39.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:31:39.271: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:31:39.271: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6493/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jul  1 13:31:39.345: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 07/01/23 13:31:39.345
  Jul  1 13:31:39.345: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6493 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:31:39.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:31:39.346: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:31:39.346: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6493/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul  1 13:31:39.430: INFO: Exec stderr: ""
  Jul  1 13:31:39.430: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6493 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:31:39.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:31:39.432: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:31:39.432: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6493/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul  1 13:31:39.501: INFO: Exec stderr: ""
  Jul  1 13:31:39.501: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6493 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:31:39.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:31:39.502: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:31:39.502: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6493/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul  1 13:31:39.571: INFO: Exec stderr: ""
  Jul  1 13:31:39.571: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6493 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:31:39.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:31:39.572: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:31:39.572: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6493/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul  1 13:31:39.642: INFO: Exec stderr: ""
  Jul  1 13:31:39.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-6493" for this suite. @ 07/01/23 13:31:39.649
• [4.888 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 07/01/23 13:31:39.663
  Jul  1 13:31:39.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename downward-api @ 07/01/23 13:31:39.664
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:31:39.688
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:31:39.696
  STEP: Creating the pod @ 07/01/23 13:31:39.707
  E0701 13:31:40.097407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:41.097564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:42.098289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:31:42.281: INFO: Successfully updated pod "labelsupdate5b345c1b-cda4-44a3-a768-37912dd61f54"
  E0701 13:31:43.099218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:44.099545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:31:44.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4670" for this suite. @ 07/01/23 13:31:44.315
• [4.664 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 07/01/23 13:31:44.328
  Jul  1 13:31:44.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 13:31:44.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:31:44.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:31:44.409
  STEP: Creating a pod to test downward API volume plugin @ 07/01/23 13:31:44.416
  E0701 13:31:45.100059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:46.100384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:47.101331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:48.101459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:31:48.451
  Jul  1 13:31:48.456: INFO: Trying to get logs from node ip-172-31-16-94 pod downwardapi-volume-d05a2e87-7f30-4108-924d-65f59b33504e container client-container: <nil>
  STEP: delete the pod @ 07/01/23 13:31:48.481
  Jul  1 13:31:48.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4393" for this suite. @ 07/01/23 13:31:48.51
• [4.195 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 07/01/23 13:31:48.524
  Jul  1 13:31:48.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename runtimeclass @ 07/01/23 13:31:48.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:31:48.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:31:48.551
  Jul  1 13:31:48.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3025" for this suite. @ 07/01/23 13:31:48.604
• [0.091 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 07/01/23 13:31:48.615
  Jul  1 13:31:48.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename disruption @ 07/01/23 13:31:48.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:31:48.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:31:48.648
  STEP: Waiting for the pdb to be processed @ 07/01/23 13:31:48.661
  STEP: Waiting for all pods to be running @ 07/01/23 13:31:48.712
  Jul  1 13:31:48.722: INFO: running pods: 0 < 3
  E0701 13:31:49.102346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:50.102362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:31:50.728: INFO: running pods: 1 < 3
  E0701 13:31:51.102929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:52.102976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:31:52.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-1237" for this suite. @ 07/01/23 13:31:52.741
• [4.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 07/01/23 13:31:52.756
  Jul  1 13:31:52.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename configmap @ 07/01/23 13:31:52.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:31:52.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:31:52.798
  STEP: Creating configMap configmap-3904/configmap-test-27f8be0c-8ff0-4a1d-adab-ae6c058eebdc @ 07/01/23 13:31:52.805
  STEP: Creating a pod to test consume configMaps @ 07/01/23 13:31:52.812
  E0701 13:31:53.103188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:54.103471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:55.103938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:56.104936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:31:56.848
  Jul  1 13:31:56.854: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-configmaps-7f37d324-9bc7-4494-b195-398804c231d1 container env-test: <nil>
  STEP: delete the pod @ 07/01/23 13:31:56.867
  Jul  1 13:31:56.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3904" for this suite. @ 07/01/23 13:31:56.902
• [4.168 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 07/01/23 13:31:56.925
  Jul  1 13:31:56.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename cronjob @ 07/01/23 13:31:56.926
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:31:56.949
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:31:56.957
  STEP: Creating a ForbidConcurrent cronjob @ 07/01/23 13:31:56.965
  STEP: Ensuring a job is scheduled @ 07/01/23 13:31:56.977
  E0701 13:31:57.105881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:58.106013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:31:59.106886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:00.107357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 07/01/23 13:32:00.984
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 07/01/23 13:32:00.988
  STEP: Ensuring no more jobs are scheduled @ 07/01/23 13:32:00.994
  E0701 13:32:01.107854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:02.107984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:03.109210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:04.109266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:05.109786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:06.110198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:07.110579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:08.110676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:09.111543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:10.111571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:11.112173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:12.112272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:13.113148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:14.113526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:15.114566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:16.115052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:17.115906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:18.116633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:19.117605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:20.117872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:21.118303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:22.118532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:23.118865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:24.119453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:25.119873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:26.120361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:27.121042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:28.121162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:29.121751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:30.122255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:31.123056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:32.123273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:33.123308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:34.123481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:35.124018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:36.124143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:37.125220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:38.125524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:39.126021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:40.126420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:41.126975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:42.127178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:43.127675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:44.128923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:45.129752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:46.130064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:47.131150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:48.131297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:49.131608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:50.132614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:51.133112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:52.133474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:53.134363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:54.134492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:55.134824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:56.135180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:57.136202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:58.136292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:32:59.136791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:00.137141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:01.137713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:02.138011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:03.138056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:04.138388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:05.139061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:06.139755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:07.140580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:08.140715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:09.140826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:10.141230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:11.141627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:12.141574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:13.142146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:14.142441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:15.143322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:16.144078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:17.144220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:18.144643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:19.145405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:20.146468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:21.146815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:22.146981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:23.147493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:24.147597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:25.148252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:26.148348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:27.149395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:28.149481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:29.150333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:30.150556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:31.151482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:32.151550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:33.151669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:34.151772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:35.151873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:36.152691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:37.153823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:38.154077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:39.154839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:40.155104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:41.155695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:42.156626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:43.157248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:44.159512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:45.160378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:46.160519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:47.161312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:48.161693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:49.161748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:50.161853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:51.162852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:52.163019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:53.163977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:54.164106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:55.164629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:56.165096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:57.166151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:58.166657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:33:59.167573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:00.168651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:01.169973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:02.170106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:03.171057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:04.171451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:05.171950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:06.172070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:07.172904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:08.173020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:09.173083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:10.173161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:11.174136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:12.174241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:13.175099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:14.175631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:15.176140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:16.176246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:17.176415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:18.176642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:19.177254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:20.177503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:21.178315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:22.178434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:23.178998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:24.179485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:25.180215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:26.180608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:27.180689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:28.181637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:29.182547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:30.183173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:31.183841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:32.184133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:33.185010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:34.185151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:35.185700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:36.185819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:37.186452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:38.186689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:39.187267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:40.187420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:41.188043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:42.188184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:43.189231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:44.189431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:45.189509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:46.189755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:47.191440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:48.191636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:49.192658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:50.192749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:51.193142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:52.193289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:53.193374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:54.193614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:55.194123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:56.195373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:57.195579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:58.195685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:34:59.196487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:00.196756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:01.196841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:02.197370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:03.198287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:04.198508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:05.199571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:06.200071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:07.200652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:08.200786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:09.201197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:10.201308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:11.202806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:12.203066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:13.203867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:14.204033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:15.204661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:16.205187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:17.205409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:18.205623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:19.206085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:20.206831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:21.206960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:22.207198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:23.207660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:24.207765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:25.208166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:26.209250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:27.209811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:28.209923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:29.210378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:30.210602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:31.211583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:32.211736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:33.212596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:34.212733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:35.212853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:36.213171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:37.214138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:38.214394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:39.214504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:40.215543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:41.215910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:42.216629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:43.217389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:44.217506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:45.217974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:46.218269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:47.219205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:48.219430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:49.219932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:50.220128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:51.220627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:52.220844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:53.221172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:54.221399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:55.222174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:56.223024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:57.223246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:58.224237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:35:59.225040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:00.225176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:01.225300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:02.225401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:03.226117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:04.226245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:05.226762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:06.227085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:07.227707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:08.227832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:09.228592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:10.229329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:11.229760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:12.230012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:13.231092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:14.231228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:15.231956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:16.232101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:17.232648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:18.232765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:19.233206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:20.233245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:21.233903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:22.234045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:23.234519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:24.234727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:25.234930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:26.235455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:27.235931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:28.236060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:29.236973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:30.237096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:31.237918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:32.238037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:33.238556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:34.238787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:35.239412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:36.239886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:37.240174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:38.240295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:39.240902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:40.241917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:41.242971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:42.243218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:43.244134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:44.244372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:45.244653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:46.245642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:47.246600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:48.247402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:49.248043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:50.248651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:51.249743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:52.249987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:53.250181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:54.250417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:55.250848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:56.251136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:57.251926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:58.252904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:36:59.253823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:00.253942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 07/01/23 13:37:01.007
  Jul  1 13:37:01.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4186" for this suite. @ 07/01/23 13:37:01.024
• [304.112 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 07/01/23 13:37:01.038
  Jul  1 13:37:01.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename pods @ 07/01/23 13:37:01.038
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:37:01.062
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:37:01.075
  STEP: creating the pod @ 07/01/23 13:37:01.081
  STEP: submitting the pod to kubernetes @ 07/01/23 13:37:01.081
  E0701 13:37:01.255013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:02.255280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 07/01/23 13:37:03.118
  STEP: updating the pod @ 07/01/23 13:37:03.124
  E0701 13:37:03.256087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:37:03.641: INFO: Successfully updated pod "pod-update-f70af7a5-08a4-4060-ae41-8f2813bd972f"
  STEP: verifying the updated pod is in kubernetes @ 07/01/23 13:37:03.647
  Jul  1 13:37:03.652: INFO: Pod update OK
  Jul  1 13:37:03.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7372" for this suite. @ 07/01/23 13:37:03.658
• [2.632 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 07/01/23 13:37:03.68
  Jul  1 13:37:03.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename webhook @ 07/01/23 13:37:03.682
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:37:03.71
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:37:03.715
  STEP: Setting up server cert @ 07/01/23 13:37:03.753
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/01/23 13:37:04.072
  STEP: Deploying the webhook pod @ 07/01/23 13:37:04.085
  STEP: Wait for the deployment to be ready @ 07/01/23 13:37:04.106
  Jul  1 13:37:04.126: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0701 13:37:04.256536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:05.256640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/01/23 13:37:06.152
  STEP: Verifying the service has paired with the endpoint @ 07/01/23 13:37:06.183
  E0701 13:37:06.258186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:37:07.184: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 07/01/23 13:37:07.192
  STEP: Creating a custom resource definition that should be denied by the webhook @ 07/01/23 13:37:07.216
  Jul  1 13:37:07.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:37:07.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0701 13:37:07.258381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-1213" for this suite. @ 07/01/23 13:37:07.319
  STEP: Destroying namespace "webhook-markers-2013" for this suite. @ 07/01/23 13:37:07.334
• [3.667 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 07/01/23 13:37:07.348
  Jul  1 13:37:07.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename sched-preemption @ 07/01/23 13:37:07.349
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:37:07.382
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:37:07.386
  Jul  1 13:37:07.411: INFO: Waiting up to 1m0s for all nodes to be ready
  E0701 13:37:08.258395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:09.258537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:10.258683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:11.259299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:12.259649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:13.260648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:14.261172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:15.261391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:16.261700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:17.261949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:18.262088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:19.262894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:20.263050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:21.263791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:22.264044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:23.264152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:24.264475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:25.264789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:26.265110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:27.265199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:28.265312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:29.265574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:30.265695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:31.266429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:32.266894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:33.267025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:34.267524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:35.267580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:36.268314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:37.268575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:38.268694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:39.268949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:40.269289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:41.269874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:42.271141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:43.271524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:44.271573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:45.271653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:46.272615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:47.272743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:48.273527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:49.273793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:50.273941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:51.274688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:52.275373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:53.275563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:54.275686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:55.275804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:56.275932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:57.276205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:58.276323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:37:59.276442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:00.276483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:01.276709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:02.276857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:03.277140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:04.277257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:05.277376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:06.277875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:07.278483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:38:07.440: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 07/01/23 13:38:07.444
  Jul  1 13:38:07.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename sched-preemption-path @ 07/01/23 13:38:07.445
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:38:07.468
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:38:07.475
  Jul  1 13:38:07.504: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Jul  1 13:38:07.509: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Jul  1 13:38:07.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  1 13:38:07.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-2500" for this suite. @ 07/01/23 13:38:07.677
  STEP: Destroying namespace "sched-preemption-5263" for this suite. @ 07/01/23 13:38:07.687
• [60.352 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 07/01/23 13:38:07.702
  Jul  1 13:38:07.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename projected @ 07/01/23 13:38:07.703
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:38:07.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:38:07.773
  STEP: Creating secret with name projected-secret-test-bbb31c3d-817b-4e1f-9e29-da2042a503a5 @ 07/01/23 13:38:07.778
  STEP: Creating a pod to test consume secrets @ 07/01/23 13:38:07.79
  E0701 13:38:08.278599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:09.278835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:10.279677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:11.280662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:38:11.844
  Jul  1 13:38:11.850: INFO: Trying to get logs from node ip-172-31-91-66 pod pod-projected-secrets-9a5d85cf-82c1-4a37-b1b7-a53611417e54 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/01/23 13:38:11.879
  Jul  1 13:38:11.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9453" for this suite. @ 07/01/23 13:38:11.93
• [4.238 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 07/01/23 13:38:11.941
  Jul  1 13:38:11.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename dns @ 07/01/23 13:38:11.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:38:11.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:38:11.977
  STEP: Creating a test headless service @ 07/01/23 13:38:11.982
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7148.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7148.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7148.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7148.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7148.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7148.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7148.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7148.svc.cluster.local;sleep 1; done
   @ 07/01/23 13:38:12
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7148.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7148.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7148.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7148.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7148.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7148.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7148.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7148.svc.cluster.local;sleep 1; done
   @ 07/01/23 13:38:12
  STEP: creating a pod to probe DNS @ 07/01/23 13:38:12
  STEP: submitting the pod to kubernetes @ 07/01/23 13:38:12
  E0701 13:38:12.281136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:13.281424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/01/23 13:38:14.043
  STEP: looking for the results for each expected name from probers @ 07/01/23 13:38:14.049
  Jul  1 13:38:14.058: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7148.svc.cluster.local from pod dns-7148/dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926: the server could not find the requested resource (get pods dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926)
  Jul  1 13:38:14.065: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7148.svc.cluster.local from pod dns-7148/dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926: the server could not find the requested resource (get pods dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926)
  Jul  1 13:38:14.071: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7148.svc.cluster.local from pod dns-7148/dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926: the server could not find the requested resource (get pods dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926)
  Jul  1 13:38:14.081: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7148.svc.cluster.local from pod dns-7148/dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926: the server could not find the requested resource (get pods dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926)
  Jul  1 13:38:14.089: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7148.svc.cluster.local from pod dns-7148/dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926: the server could not find the requested resource (get pods dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926)
  Jul  1 13:38:14.099: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7148.svc.cluster.local from pod dns-7148/dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926: the server could not find the requested resource (get pods dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926)
  Jul  1 13:38:14.112: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7148.svc.cluster.local from pod dns-7148/dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926: the server could not find the requested resource (get pods dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926)
  Jul  1 13:38:14.119: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7148.svc.cluster.local from pod dns-7148/dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926: the server could not find the requested resource (get pods dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926)
  Jul  1 13:38:14.119: INFO: Lookups using dns-7148/dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7148.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7148.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7148.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7148.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7148.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7148.svc.cluster.local jessie_udp@dns-test-service-2.dns-7148.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7148.svc.cluster.local]

  E0701 13:38:14.282232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:15.282355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:16.282463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:17.282595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:18.282843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:38:19.174: INFO: DNS probes using dns-7148/dns-test-d5ee1718-7223-4a9f-941e-5ba2f768f926 succeeded

  Jul  1 13:38:19.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/01/23 13:38:19.18
  STEP: deleting the test headless service @ 07/01/23 13:38:19.197
  STEP: Destroying namespace "dns-7148" for this suite. @ 07/01/23 13:38:19.221
• [7.288 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 07/01/23 13:38:19.235
  Jul  1 13:38:19.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename var-expansion @ 07/01/23 13:38:19.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:38:19.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:38:19.263
  STEP: Creating a pod to test env composition @ 07/01/23 13:38:19.269
  E0701 13:38:19.283309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:20.283569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:21.283694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:22.283803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:23.283979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/01/23 13:38:23.306
  Jul  1 13:38:23.310: INFO: Trying to get logs from node ip-172-31-91-66 pod var-expansion-39581da9-f855-4b07-9b08-b4317ead927f container dapi-container: <nil>
  STEP: delete the pod @ 07/01/23 13:38:23.322
  Jul  1 13:38:23.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9968" for this suite. @ 07/01/23 13:38:23.352
• [4.131 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 07/01/23 13:38:23.365
  Jul  1 13:38:23.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename pod-network-test @ 07/01/23 13:38:23.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:38:23.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:38:23.397
  STEP: Performing setup for networking test in namespace pod-network-test-8072 @ 07/01/23 13:38:23.402
  STEP: creating a selector @ 07/01/23 13:38:23.402
  STEP: Creating the service pods in kubernetes @ 07/01/23 13:38:23.402
  Jul  1 13:38:23.402: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0701 13:38:24.284319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:25.284442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:26.285205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:27.285378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:28.286216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:29.286820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:30.287559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:31.288651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:32.288876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:33.288996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:34.289097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:35.289219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:36.290256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:37.290403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:38.291478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:39.291598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:40.291708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:41.292137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:42.292629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:43.292769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:44.293139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:45.293315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 07/01/23 13:38:45.565
  E0701 13:38:46.294143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0701 13:38:47.294302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:38:47.601: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul  1 13:38:47.601: INFO: Breadth first check of 192.168.175.30 on host 172.31.12.125...
  Jul  1 13:38:47.608: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.62.107:9080/dial?request=hostname&protocol=udp&host=192.168.175.30&port=8081&tries=1'] Namespace:pod-network-test-8072 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:38:47.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:38:47.609: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:38:47.610: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8072/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.62.107%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.175.30%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul  1 13:38:47.705: INFO: Waiting for responses: map[]
  Jul  1 13:38:47.705: INFO: reached 192.168.175.30 after 0/1 tries
  Jul  1 13:38:47.705: INFO: Breadth first check of 192.168.85.36 on host 172.31.16.94...
  Jul  1 13:38:47.711: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.62.107:9080/dial?request=hostname&protocol=udp&host=192.168.85.36&port=8081&tries=1'] Namespace:pod-network-test-8072 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:38:47.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:38:47.712: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:38:47.712: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8072/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.62.107%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.85.36%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul  1 13:38:47.797: INFO: Waiting for responses: map[]
  Jul  1 13:38:47.797: INFO: reached 192.168.85.36 after 0/1 tries
  Jul  1 13:38:47.797: INFO: Breadth first check of 192.168.62.117 on host 172.31.91.66...
  Jul  1 13:38:47.805: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.62.107:9080/dial?request=hostname&protocol=udp&host=192.168.62.117&port=8081&tries=1'] Namespace:pod-network-test-8072 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  1 13:38:47.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  Jul  1 13:38:47.806: INFO: ExecWithOptions: Clientset creation
  Jul  1 13:38:47.806: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8072/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.62.107%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.62.117%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul  1 13:38:47.881: INFO: Waiting for responses: map[]
  Jul  1 13:38:47.881: INFO: reached 192.168.62.117 after 0/1 tries
  Jul  1 13:38:47.881: INFO: Going to retry 0 out of 3 pods....
  Jul  1 13:38:47.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-8072" for this suite. @ 07/01/23 13:38:47.89
• [24.535 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 07/01/23 13:38:47.903
  Jul  1 13:38:47.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename services @ 07/01/23 13:38:47.905
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:38:47.934
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:38:47.945
  STEP: fetching services @ 07/01/23 13:38:47.95
  Jul  1 13:38:47.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9568" for this suite. @ 07/01/23 13:38:47.966
• [0.073 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 07/01/23 13:38:47.977
  Jul  1 13:38:47.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2735291248
  STEP: Building a namespace api object, basename kubectl @ 07/01/23 13:38:47.978
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/01/23 13:38:48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/01/23 13:38:48.007
  Jul  1 13:38:48.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-4505 create -f -'
  E0701 13:38:48.294828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:38:49.094: INFO: stderr: ""
  Jul  1 13:38:49.094: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Jul  1 13:38:49.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-4505 create -f -'
  E0701 13:38:49.295572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:38:49.785: INFO: stderr: ""
  Jul  1 13:38:49.785: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/01/23 13:38:49.785
  E0701 13:38:50.295655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:38:50.794: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  1 13:38:50.794: INFO: Found 1 / 1
  Jul  1 13:38:50.794: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jul  1 13:38:50.798: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  1 13:38:50.798: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul  1 13:38:50.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-4505 describe pod agnhost-primary-9gthb'
  Jul  1 13:38:50.937: INFO: stderr: ""
  Jul  1 13:38:50.937: INFO: stdout: "Name:             agnhost-primary-9gthb\nNamespace:        kubectl-4505\nPriority:         0\nService Account:  default\nNode:             ip-172-31-91-66/172.31.91.66\nStart Time:       Sat, 01 Jul 2023 13:38:49 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.62.108\nIPs:\n  IP:           192.168.62.108\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://f6d1a18d8b48e01cfb0e48e717c0f5dedc2d2ad974615a8dff3ef88d707a632a\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 01 Jul 2023 13:38:50 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mrkjz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-mrkjz:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-4505/agnhost-primary-9gthb to ip-172-31-91-66\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
  Jul  1 13:38:50.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-4505 describe rc agnhost-primary'
  Jul  1 13:38:51.063: INFO: stderr: ""
  Jul  1 13:38:51.063: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4505\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-9gthb\n"
  Jul  1 13:38:51.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-4505 describe service agnhost-primary'
  Jul  1 13:38:51.148: INFO: stderr: ""
  Jul  1 13:38:51.148: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4505\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.37\nIPs:               10.152.183.37\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.62.108:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Jul  1 13:38:51.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-4505 describe node ip-172-31-12-125'
  Jul  1 13:38:51.258: INFO: stderr: ""
  Jul  1 13:38:51.258: INFO: stdout: "Name:               ip-172-31-12-125\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-worker\n                    juju-charm=kubernetes-worker\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-12-125\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 01 Jul 2023 11:52:11 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-12-125\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 01 Jul 2023 13:38:50 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 01 Jul 2023 13:37:17 +0000   Sat, 01 Jul 2023 11:52:11 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 01 Jul 2023 13:37:17 +0000   Sat, 01 Jul 2023 11:52:11 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 01 Jul 2023 13:37:17 +0000   Sat, 01 Jul 2023 11:52:11 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 01 Jul 2023 13:37:17 +0000   Sat, 01 Jul 2023 11:52:57 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.12.125\n  Hostname:    ip-172-31-12-125\nCapacity:\n  cpu:                2\n  ephemeral-storage:  16069568Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8023592Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  14809713845\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7921192Ki\n  pods:               110\nSystem Info:\n  Machine ID:                      ec2dfe9466d01a98d827e49c67be11e7\n  System UUID:                     ec2dfe94-66d0-1a98-d827-e49c67be11e7\n  Boot ID:                         323e03d5-97cd-4c20-9664-007b6d1b97ac\n  Kernel Version:                  5.19.0-1028-aws\n  OS Image:                        Ubuntu 22.04.2 LTS\n  Operating System:                linux\n  Architecture:                    amd64\n  Container Runtime Version:       containerd://1.6.8\n  Kubelet Version:                 v1.27.3\n  Kube-Proxy Version:              v1.27.3\nNon-terminated Pods:               (6 in total)\n  Namespace                        Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                        ----                                                       ------------  ----------  ---------------  -------------  ---\n  ingress-nginx-kubernetes-worker  default-http-backend-kubernetes-worker-65fc475d49-lbfkp    10m (0%)      10m (0%)    20Mi (0%)        20Mi (0%)      26m\n  ingress-nginx-kubernetes-worker  nginx-ingress-controller-kubernetes-worker-tjncz           0 (0%)        0 (0%)      0 (0%)           0 (0%)         106m\n  kube-system                      calico-kube-controllers-9c5cff4fb-6hfws                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         106m\n  pod-network-test-8072            netserver-0                                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         28s\n  sonobuoy                         sonobuoy-e2e-job-e4b68f70fcf04452                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         100m\n  sonobuoy                         sonobuoy-systemd-logs-daemon-set-0aed011ee39b4f14-g2zvl    0 (0%)        0 (0%)      0 (0%)           0 (0%)         100m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                10m (0%)   10m (0%)\n  memory             20Mi (0%)  20Mi (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\n  hugepages-1Gi      0 (0%)     0 (0%)\n  hugepages-2Mi      0 (0%)     0 (0%)\nEvents:              <none>\n"
  Jul  1 13:38:51.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2735291248 --namespace=kubectl-4505 describe namespace kubectl-4505'
  E0701 13:38:51.295717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  1 13:38:51.354: INFO: stderr: ""
  Jul  1 13:38:51.354: INFO: stdout: "Name:         kubectl-4505\nLabels:       e2e-framework=kubectl\n              e2e-run=a65dfce2-a914-45b4-bc26-49557909761b\n              kubernetes.io/metadata.name=kubectl-4505\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Jul  1 13:38:51.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4505" for this suite. @ 07/01/23 13:38:51.363
• [3.403 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Jul  1 13:38:51.383: INFO: Running AfterSuite actions on node 1
  Jul  1 13:38:51.383: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.079 seconds]
------------------------------

Ran 378 of 7207 Specs in 6040.096 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h40m40.660618385s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

