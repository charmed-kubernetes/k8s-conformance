  I0715 11:58:34.956393      23 e2e.go:117] Starting e2e run "18cdf2d3-4826-4237-a5b2-e9decef30f4d" on Ginkgo node 1
  Jul 15 11:58:34.974: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1689422314 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Jul 15 11:58:35.073: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 11:58:35.073: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Jul 15 11:58:35.101: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Jul 15 11:58:35.105: INFO: e2e test version: v1.27.3
  Jul 15 11:58:35.106: INFO: kube-apiserver version: v1.27.3
  Jul 15 11:58:35.106: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 11:58:35.110: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.037 seconds]
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 07/15/23 11:58:35.265
  Jul 15 11:58:35.266: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename runtimeclass @ 07/15/23 11:58:35.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 11:58:35.283
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 11:58:35.286
  Jul 15 11:58:35.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-825" for this suite. @ 07/15/23 11:58:35.3
• [0.040 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 07/15/23 11:58:35.306
  Jul 15 11:58:35.306: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename resourcequota @ 07/15/23 11:58:35.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 11:58:35.324
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 11:58:35.327
  STEP: Creating a ResourceQuota with terminating scope @ 07/15/23 11:58:35.33
  STEP: Ensuring ResourceQuota status is calculated @ 07/15/23 11:58:35.334
  STEP: Creating a ResourceQuota with not terminating scope @ 07/15/23 11:58:37.338
  STEP: Ensuring ResourceQuota status is calculated @ 07/15/23 11:58:37.345
  STEP: Creating a long running pod @ 07/15/23 11:58:39.35
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 07/15/23 11:58:39.363
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 07/15/23 11:58:41.368
  STEP: Deleting the pod @ 07/15/23 11:58:43.373
  STEP: Ensuring resource quota status released the pod usage @ 07/15/23 11:58:43.387
  STEP: Creating a terminating pod @ 07/15/23 11:58:45.393
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 07/15/23 11:58:45.403
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 07/15/23 11:58:47.408
  STEP: Deleting the pod @ 07/15/23 11:58:49.414
  STEP: Ensuring resource quota status released the pod usage @ 07/15/23 11:58:49.426
  Jul 15 11:58:51.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4050" for this suite. @ 07/15/23 11:58:51.436
• [16.136 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 07/15/23 11:58:51.442
  Jul 15 11:58:51.442: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/15/23 11:58:51.443
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 11:58:51.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 11:58:51.462
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 07/15/23 11:58:51.466
  Jul 15 11:58:51.466: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 11:58:52.730: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 11:58:57.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1340" for this suite. @ 07/15/23 11:58:57.777
• [6.340 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 07/15/23 11:58:57.783
  Jul 15 11:58:57.783: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl @ 07/15/23 11:58:57.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 11:58:57.8
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 11:58:57.803
  STEP: creating all guestbook components @ 07/15/23 11:58:57.806
  Jul 15 11:58:57.806: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Jul 15 11:58:57.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-7804 create -f -'
  Jul 15 11:58:58.182: INFO: stderr: ""
  Jul 15 11:58:58.182: INFO: stdout: "service/agnhost-replica created\n"
  Jul 15 11:58:58.182: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Jul 15 11:58:58.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-7804 create -f -'
  Jul 15 11:58:58.332: INFO: stderr: ""
  Jul 15 11:58:58.332: INFO: stdout: "service/agnhost-primary created\n"
  Jul 15 11:58:58.332: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Jul 15 11:58:58.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-7804 create -f -'
  Jul 15 11:58:58.487: INFO: stderr: ""
  Jul 15 11:58:58.487: INFO: stdout: "service/frontend created\n"
  Jul 15 11:58:58.487: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Jul 15 11:58:58.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-7804 create -f -'
  Jul 15 11:58:58.620: INFO: stderr: ""
  Jul 15 11:58:58.620: INFO: stdout: "deployment.apps/frontend created\n"
  Jul 15 11:58:58.620: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jul 15 11:58:58.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-7804 create -f -'
  Jul 15 11:58:58.751: INFO: stderr: ""
  Jul 15 11:58:58.751: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Jul 15 11:58:58.751: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jul 15 11:58:58.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-7804 create -f -'
  Jul 15 11:58:58.886: INFO: stderr: ""
  Jul 15 11:58:58.886: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 07/15/23 11:58:58.886
  Jul 15 11:58:58.886: INFO: Waiting for all frontend pods to be Running.
  Jul 15 11:59:03.937: INFO: Waiting for frontend to serve content.
  Jul 15 11:59:03.946: INFO: Trying to add a new entry to the guestbook.
  Jul 15 11:59:03.955: INFO: Verifying that added entry can be retrieved.
  Jul 15 11:59:03.965: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
  STEP: using delete to clean up resources @ 07/15/23 11:59:08.979
  Jul 15 11:59:08.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-7804 delete --grace-period=0 --force -f -'
  Jul 15 11:59:09.036: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 15 11:59:09.036: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 07/15/23 11:59:09.036
  Jul 15 11:59:09.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-7804 delete --grace-period=0 --force -f -'
  Jul 15 11:59:09.093: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 15 11:59:09.093: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 07/15/23 11:59:09.093
  Jul 15 11:59:09.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-7804 delete --grace-period=0 --force -f -'
  Jul 15 11:59:09.150: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 15 11:59:09.150: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 07/15/23 11:59:09.15
  Jul 15 11:59:09.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-7804 delete --grace-period=0 --force -f -'
  Jul 15 11:59:09.197: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 15 11:59:09.197: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 07/15/23 11:59:09.197
  Jul 15 11:59:09.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-7804 delete --grace-period=0 --force -f -'
  Jul 15 11:59:09.246: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 15 11:59:09.246: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 07/15/23 11:59:09.246
  Jul 15 11:59:09.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-7804 delete --grace-period=0 --force -f -'
  Jul 15 11:59:09.294: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 15 11:59:09.294: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Jul 15 11:59:09.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7804" for this suite. @ 07/15/23 11:59:09.299
• [11.521 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 07/15/23 11:59:09.305
  Jul 15 11:59:09.305: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename secrets @ 07/15/23 11:59:09.305
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 11:59:09.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 11:59:09.325
  STEP: Creating secret with name secret-test-a9e72c5b-6f50-48b3-9080-c180988e8c45 @ 07/15/23 11:59:09.346
  STEP: Creating a pod to test consume secrets @ 07/15/23 11:59:09.35
  STEP: Saw pod success @ 07/15/23 11:59:13.373
  Jul 15 11:59:13.377: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-secrets-69e5bb88-b95c-434c-adff-33d99298c72d container secret-volume-test: <nil>
  STEP: delete the pod @ 07/15/23 11:59:13.395
  Jul 15 11:59:13.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-580" for this suite. @ 07/15/23 11:59:13.416
  STEP: Destroying namespace "secret-namespace-6701" for this suite. @ 07/15/23 11:59:13.423
• [4.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 07/15/23 11:59:13.43
  Jul 15 11:59:13.430: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 11:59:13.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 11:59:13.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 11:59:13.458
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 11:59:13.462
  STEP: Saw pod success @ 07/15/23 11:59:17.489
  Jul 15 11:59:17.492: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-c883a69b-6d53-459d-be20-b79bcc7df193 container client-container: <nil>
  STEP: delete the pod @ 07/15/23 11:59:17.5
  Jul 15 11:59:17.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8576" for this suite. @ 07/15/23 11:59:17.52
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 07/15/23 11:59:17.527
  Jul 15 11:59:17.527: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename gc @ 07/15/23 11:59:17.528
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 11:59:17.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 11:59:17.546
  STEP: create the deployment @ 07/15/23 11:59:17.549
  W0715 11:59:17.554605      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 07/15/23 11:59:17.554
  STEP: delete the deployment @ 07/15/23 11:59:18.066
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 07/15/23 11:59:18.072
  STEP: Gathering metrics @ 07/15/23 11:59:18.592
  W0715 11:59:18.596831      23 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 15 11:59:18.596: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 15 11:59:18.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1487" for this suite. @ 07/15/23 11:59:18.602
• [1.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 07/15/23 11:59:18.611
  Jul 15 11:59:18.611: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 11:59:18.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 11:59:18.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 11:59:18.63
  STEP: Creating projection with secret that has name projected-secret-test-0d2375cd-4028-4e1d-ac6e-923437275466 @ 07/15/23 11:59:18.633
  STEP: Creating a pod to test consume secrets @ 07/15/23 11:59:18.639
  STEP: Saw pod success @ 07/15/23 11:59:22.664
  Jul 15 11:59:22.668: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-projected-secrets-f87e6f4f-768c-4250-b1a6-749132fa556d container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/15/23 11:59:22.674
  Jul 15 11:59:22.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6521" for this suite. @ 07/15/23 11:59:22.693
• [4.088 seconds]
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 07/15/23 11:59:22.699
  Jul 15 11:59:22.699: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-probe @ 07/15/23 11:59:22.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 11:59:22.714
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 11:59:22.717
  STEP: Creating pod liveness-5dafc8d7-11af-4a1a-821c-25f420d80b73 in namespace container-probe-7687 @ 07/15/23 11:59:22.72
  Jul 15 11:59:24.735: INFO: Started pod liveness-5dafc8d7-11af-4a1a-821c-25f420d80b73 in namespace container-probe-7687
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/15/23 11:59:24.735
  Jul 15 11:59:24.740: INFO: Initial restart count of pod liveness-5dafc8d7-11af-4a1a-821c-25f420d80b73 is 0
  Jul 15 11:59:44.798: INFO: Restart count of pod container-probe-7687/liveness-5dafc8d7-11af-4a1a-821c-25f420d80b73 is now 1 (20.058038865s elapsed)
  Jul 15 11:59:44.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/15/23 11:59:44.803
  STEP: Destroying namespace "container-probe-7687" for this suite. @ 07/15/23 11:59:44.814
• [22.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 07/15/23 11:59:44.822
  Jul 15 11:59:44.822: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename server-version @ 07/15/23 11:59:44.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 11:59:44.839
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 11:59:44.842
  STEP: Request ServerVersion @ 07/15/23 11:59:44.845
  STEP: Confirm major version @ 07/15/23 11:59:44.847
  Jul 15 11:59:44.847: INFO: Major version: 1
  STEP: Confirm minor version @ 07/15/23 11:59:44.847
  Jul 15 11:59:44.847: INFO: cleanMinorVersion: 27
  Jul 15 11:59:44.847: INFO: Minor version: 27
  Jul 15 11:59:44.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-4890" for this suite. @ 07/15/23 11:59:44.851
• [0.035 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 07/15/23 11:59:44.858
  Jul 15 11:59:44.858: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-probe @ 07/15/23 11:59:44.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 11:59:44.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 11:59:44.875
  Jul 15 12:00:44.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-4613" for this suite. @ 07/15/23 12:00:44.895
• [60.046 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 07/15/23 12:00:44.904
  Jul 15 12:00:44.904: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir @ 07/15/23 12:00:44.905
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:00:44.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:00:44.925
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 07/15/23 12:00:44.928
  STEP: Saw pod success @ 07/15/23 12:00:48.951
  Jul 15 12:00:48.955: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-40d6452c-e107-4a3c-b74d-ff068a77c717 container test-container: <nil>
  STEP: delete the pod @ 07/15/23 12:00:48.962
  Jul 15 12:00:48.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1558" for this suite. @ 07/15/23 12:00:48.978
• [4.082 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 07/15/23 12:00:48.986
  Jul 15 12:00:48.986: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename watch @ 07/15/23 12:00:48.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:00:49
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:00:49.003
  STEP: creating a watch on configmaps with label A @ 07/15/23 12:00:49.006
  STEP: creating a watch on configmaps with label B @ 07/15/23 12:00:49.007
  STEP: creating a watch on configmaps with label A or B @ 07/15/23 12:00:49.009
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 07/15/23 12:00:49.01
  Jul 15 12:00:49.014: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8012  a7b4f14a-83c3-4e5e-b34c-699fe33d546e 3252 0 2023-07-15 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-15 12:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 12:00:49.014: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8012  a7b4f14a-83c3-4e5e-b34c-699fe33d546e 3252 0 2023-07-15 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-15 12:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 07/15/23 12:00:49.014
  Jul 15 12:00:49.022: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8012  a7b4f14a-83c3-4e5e-b34c-699fe33d546e 3253 0 2023-07-15 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-15 12:00:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 12:00:49.022: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8012  a7b4f14a-83c3-4e5e-b34c-699fe33d546e 3253 0 2023-07-15 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-15 12:00:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 07/15/23 12:00:49.022
  Jul 15 12:00:49.033: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8012  a7b4f14a-83c3-4e5e-b34c-699fe33d546e 3254 0 2023-07-15 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-15 12:00:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 12:00:49.033: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8012  a7b4f14a-83c3-4e5e-b34c-699fe33d546e 3254 0 2023-07-15 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-15 12:00:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 07/15/23 12:00:49.033
  Jul 15 12:00:49.039: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8012  a7b4f14a-83c3-4e5e-b34c-699fe33d546e 3255 0 2023-07-15 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-15 12:00:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 12:00:49.039: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8012  a7b4f14a-83c3-4e5e-b34c-699fe33d546e 3255 0 2023-07-15 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-15 12:00:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 07/15/23 12:00:49.039
  Jul 15 12:00:49.044: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8012  7576d70a-3684-435c-b3f0-08813eb4d998 3256 0 2023-07-15 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-15 12:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 12:00:49.044: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8012  7576d70a-3684-435c-b3f0-08813eb4d998 3256 0 2023-07-15 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-15 12:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 07/15/23 12:00:59.044
  Jul 15 12:00:59.052: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8012  7576d70a-3684-435c-b3f0-08813eb4d998 3306 0 2023-07-15 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-15 12:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 12:00:59.053: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8012  7576d70a-3684-435c-b3f0-08813eb4d998 3306 0 2023-07-15 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-15 12:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 12:01:09.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8012" for this suite. @ 07/15/23 12:01:09.063
• [20.083 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 07/15/23 12:01:09.07
  Jul 15 12:01:09.070: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename svcaccounts @ 07/15/23 12:01:09.07
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:01:09.087
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:01:09.09
  Jul 15 12:01:09.112: INFO: created pod pod-service-account-defaultsa
  Jul 15 12:01:09.112: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Jul 15 12:01:09.119: INFO: created pod pod-service-account-mountsa
  Jul 15 12:01:09.119: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Jul 15 12:01:09.125: INFO: created pod pod-service-account-nomountsa
  Jul 15 12:01:09.125: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Jul 15 12:01:09.132: INFO: created pod pod-service-account-defaultsa-mountspec
  Jul 15 12:01:09.132: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Jul 15 12:01:09.141: INFO: created pod pod-service-account-mountsa-mountspec
  Jul 15 12:01:09.141: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Jul 15 12:01:09.146: INFO: created pod pod-service-account-nomountsa-mountspec
  Jul 15 12:01:09.146: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Jul 15 12:01:09.153: INFO: created pod pod-service-account-defaultsa-nomountspec
  Jul 15 12:01:09.153: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Jul 15 12:01:09.254: INFO: created pod pod-service-account-mountsa-nomountspec
  Jul 15 12:01:09.254: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Jul 15 12:01:09.259: INFO: created pod pod-service-account-nomountsa-nomountspec
  Jul 15 12:01:09.259: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Jul 15 12:01:09.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7783" for this suite. @ 07/15/23 12:01:09.265
• [0.204 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 07/15/23 12:01:09.274
  Jul 15 12:01:09.274: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename resourcequota @ 07/15/23 12:01:09.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:01:09.29
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:01:09.293
  STEP: Creating a ResourceQuota @ 07/15/23 12:01:09.296
  STEP: Getting a ResourceQuota @ 07/15/23 12:01:09.301
  STEP: Listing all ResourceQuotas with LabelSelector @ 07/15/23 12:01:09.305
  STEP: Patching the ResourceQuota @ 07/15/23 12:01:09.308
  STEP: Deleting a Collection of ResourceQuotas @ 07/15/23 12:01:09.312
  STEP: Verifying the deleted ResourceQuota @ 07/15/23 12:01:09.322
  Jul 15 12:01:09.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2103" for this suite. @ 07/15/23 12:01:09.329
• [0.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 07/15/23 12:01:09.338
  Jul 15 12:01:09.338: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename field-validation @ 07/15/23 12:01:09.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:01:09.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:01:09.36
  Jul 15 12:01:09.363: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  W0715 12:01:11.901640      23 warnings.go:70] unknown field "alpha"
  W0715 12:01:11.901669      23 warnings.go:70] unknown field "beta"
  W0715 12:01:11.901674      23 warnings.go:70] unknown field "delta"
  W0715 12:01:11.901681      23 warnings.go:70] unknown field "epsilon"
  W0715 12:01:11.901689      23 warnings.go:70] unknown field "gamma"
  Jul 15 12:01:11.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-269" for this suite. @ 07/15/23 12:01:11.935
• [2.604 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 07/15/23 12:01:11.943
  Jul 15 12:01:11.943: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename dns @ 07/15/23 12:01:11.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:01:11.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:01:11.958
  STEP: Creating a test headless service @ 07/15/23 12:01:11.961
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3147 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3147;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3147 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3147;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3147.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3147.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3147.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3147.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3147.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3147.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3147.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3147.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3147.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3147.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3147.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3147.svc;check="$$(dig +notcp +noall +answer +search 58.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.58_udp@PTR;check="$$(dig +tcp +noall +answer +search 58.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.58_tcp@PTR;sleep 1; done
   @ 07/15/23 12:01:11.976
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3147 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3147;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3147 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3147;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3147.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3147.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3147.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3147.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3147.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3147.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3147.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3147.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3147.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3147.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3147.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3147.svc;check="$$(dig +notcp +noall +answer +search 58.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.58_udp@PTR;check="$$(dig +tcp +noall +answer +search 58.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.58_tcp@PTR;sleep 1; done
   @ 07/15/23 12:01:11.976
  STEP: creating a pod to probe DNS @ 07/15/23 12:01:11.976
  STEP: submitting the pod to kubernetes @ 07/15/23 12:01:11.976
  STEP: retrieving the pod @ 07/15/23 12:01:20.013
  STEP: looking for the results for each expected name from probers @ 07/15/23 12:01:20.017
  Jul 15 12:01:20.114: INFO: DNS probes using dns-3147/dns-test-4d0b223c-e7bf-4960-8328-e6792511c32c succeeded

  Jul 15 12:01:20.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/15/23 12:01:20.118
  STEP: deleting the test service @ 07/15/23 12:01:20.131
  STEP: deleting the test headless service @ 07/15/23 12:01:20.149
  STEP: Destroying namespace "dns-3147" for this suite. @ 07/15/23 12:01:20.158
• [8.221 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 07/15/23 12:01:20.165
  Jul 15 12:01:20.165: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename pods @ 07/15/23 12:01:20.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:01:20.18
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:01:20.183
  Jul 15 12:01:20.186: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: creating the pod @ 07/15/23 12:01:20.186
  STEP: submitting the pod to kubernetes @ 07/15/23 12:01:20.186
  Jul 15 12:01:22.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4927" for this suite. @ 07/15/23 12:01:22.234
• [2.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 07/15/23 12:01:22.241
  Jul 15 12:01:22.241: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:01:22.242
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:01:22.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:01:22.258
  STEP: Creating configMap with name projected-configmap-test-volume-6300a7f5-3e18-465b-9d76-d9ff1247fc17 @ 07/15/23 12:01:22.262
  STEP: Creating a pod to test consume configMaps @ 07/15/23 12:01:22.266
  STEP: Saw pod success @ 07/15/23 12:01:26.288
  Jul 15 12:01:26.292: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-projected-configmaps-d3483baa-ff65-41aa-ba4d-52911ca826fe container agnhost-container: <nil>
  STEP: delete the pod @ 07/15/23 12:01:26.3
  Jul 15 12:01:26.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5655" for this suite. @ 07/15/23 12:01:26.32
• [4.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 07/15/23 12:01:26.327
  Jul 15 12:01:26.327: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/15/23 12:01:26.328
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:01:26.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:01:26.347
  STEP: create the container to handle the HTTPGet hook request. @ 07/15/23 12:01:26.353
  STEP: create the pod with lifecycle hook @ 07/15/23 12:01:28.376
  STEP: delete the pod with lifecycle hook @ 07/15/23 12:01:30.397
  STEP: check prestop hook @ 07/15/23 12:01:32.415
  Jul 15 12:01:32.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-4234" for this suite. @ 07/15/23 12:01:32.427
• [6.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 07/15/23 12:01:32.434
  Jul 15 12:01:32.434: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:01:32.435
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:01:32.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:01:32.455
  STEP: Creating secret with name s-test-opt-del-4f1b9ffd-9a25-4679-9ac6-60674617a584 @ 07/15/23 12:01:32.462
  STEP: Creating secret with name s-test-opt-upd-d72b67a0-1882-41fc-9461-eecc22eeddfb @ 07/15/23 12:01:32.467
  STEP: Creating the pod @ 07/15/23 12:01:32.472
  STEP: Deleting secret s-test-opt-del-4f1b9ffd-9a25-4679-9ac6-60674617a584 @ 07/15/23 12:01:34.516
  STEP: Updating secret s-test-opt-upd-d72b67a0-1882-41fc-9461-eecc22eeddfb @ 07/15/23 12:01:34.523
  STEP: Creating secret with name s-test-opt-create-d8bdbc28-b093-407d-b395-947ab0d73c93 @ 07/15/23 12:01:34.527
  STEP: waiting to observe update in volume @ 07/15/23 12:01:34.532
  Jul 15 12:02:50.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7574" for this suite. @ 07/15/23 12:02:50.914
• [78.485 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 07/15/23 12:02:50.92
  Jul 15 12:02:50.920: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubelet-test @ 07/15/23 12:02:50.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:02:50.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:02:50.938
  Jul 15 12:02:50.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1960" for this suite. @ 07/15/23 12:02:50.962
• [0.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 07/15/23 12:02:50.97
  Jul 15 12:02:50.970: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename downward-api @ 07/15/23 12:02:50.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:02:50.984
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:02:50.987
  STEP: Creating the pod @ 07/15/23 12:02:50.99
  Jul 15 12:02:53.530: INFO: Successfully updated pod "labelsupdatefb8a3c02-c241-4ae9-bce0-23aa655616be"
  Jul 15 12:02:57.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2213" for this suite. @ 07/15/23 12:02:57.558
• [6.594 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 07/15/23 12:02:57.565
  Jul 15 12:02:57.565: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename discovery @ 07/15/23 12:02:57.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:02:57.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:02:57.586
  STEP: Setting up server cert @ 07/15/23 12:02:57.59
  Jul 15 12:02:57.868: INFO: Checking APIGroup: apiregistration.k8s.io
  Jul 15 12:02:57.870: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Jul 15 12:02:57.870: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Jul 15 12:02:57.870: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Jul 15 12:02:57.870: INFO: Checking APIGroup: apps
  Jul 15 12:02:57.871: INFO: PreferredVersion.GroupVersion: apps/v1
  Jul 15 12:02:57.871: INFO: Versions found [{apps/v1 v1}]
  Jul 15 12:02:57.871: INFO: apps/v1 matches apps/v1
  Jul 15 12:02:57.871: INFO: Checking APIGroup: events.k8s.io
  Jul 15 12:02:57.872: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Jul 15 12:02:57.872: INFO: Versions found [{events.k8s.io/v1 v1}]
  Jul 15 12:02:57.872: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Jul 15 12:02:57.872: INFO: Checking APIGroup: authentication.k8s.io
  Jul 15 12:02:57.873: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Jul 15 12:02:57.873: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Jul 15 12:02:57.873: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Jul 15 12:02:57.873: INFO: Checking APIGroup: authorization.k8s.io
  Jul 15 12:02:57.875: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Jul 15 12:02:57.875: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Jul 15 12:02:57.875: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Jul 15 12:02:57.875: INFO: Checking APIGroup: autoscaling
  Jul 15 12:02:57.876: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Jul 15 12:02:57.876: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Jul 15 12:02:57.876: INFO: autoscaling/v2 matches autoscaling/v2
  Jul 15 12:02:57.876: INFO: Checking APIGroup: batch
  Jul 15 12:02:57.877: INFO: PreferredVersion.GroupVersion: batch/v1
  Jul 15 12:02:57.877: INFO: Versions found [{batch/v1 v1}]
  Jul 15 12:02:57.877: INFO: batch/v1 matches batch/v1
  Jul 15 12:02:57.877: INFO: Checking APIGroup: certificates.k8s.io
  Jul 15 12:02:57.878: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Jul 15 12:02:57.878: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Jul 15 12:02:57.878: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Jul 15 12:02:57.878: INFO: Checking APIGroup: networking.k8s.io
  Jul 15 12:02:57.879: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Jul 15 12:02:57.879: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Jul 15 12:02:57.879: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Jul 15 12:02:57.879: INFO: Checking APIGroup: policy
  Jul 15 12:02:57.881: INFO: PreferredVersion.GroupVersion: policy/v1
  Jul 15 12:02:57.881: INFO: Versions found [{policy/v1 v1}]
  Jul 15 12:02:57.881: INFO: policy/v1 matches policy/v1
  Jul 15 12:02:57.881: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Jul 15 12:02:57.882: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Jul 15 12:02:57.882: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Jul 15 12:02:57.882: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Jul 15 12:02:57.882: INFO: Checking APIGroup: storage.k8s.io
  Jul 15 12:02:57.883: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Jul 15 12:02:57.883: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Jul 15 12:02:57.883: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Jul 15 12:02:57.883: INFO: Checking APIGroup: admissionregistration.k8s.io
  Jul 15 12:02:57.884: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Jul 15 12:02:57.884: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Jul 15 12:02:57.884: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Jul 15 12:02:57.884: INFO: Checking APIGroup: apiextensions.k8s.io
  Jul 15 12:02:57.885: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Jul 15 12:02:57.885: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Jul 15 12:02:57.885: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Jul 15 12:02:57.885: INFO: Checking APIGroup: scheduling.k8s.io
  Jul 15 12:02:57.887: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Jul 15 12:02:57.887: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Jul 15 12:02:57.887: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Jul 15 12:02:57.887: INFO: Checking APIGroup: coordination.k8s.io
  Jul 15 12:02:57.888: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Jul 15 12:02:57.888: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Jul 15 12:02:57.888: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Jul 15 12:02:57.888: INFO: Checking APIGroup: node.k8s.io
  Jul 15 12:02:57.889: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Jul 15 12:02:57.889: INFO: Versions found [{node.k8s.io/v1 v1}]
  Jul 15 12:02:57.889: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Jul 15 12:02:57.889: INFO: Checking APIGroup: discovery.k8s.io
  Jul 15 12:02:57.890: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Jul 15 12:02:57.890: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Jul 15 12:02:57.890: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Jul 15 12:02:57.890: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Jul 15 12:02:57.891: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Jul 15 12:02:57.891: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Jul 15 12:02:57.891: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Jul 15 12:02:57.891: INFO: Checking APIGroup: metrics.k8s.io
  Jul 15 12:02:57.893: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Jul 15 12:02:57.893: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Jul 15 12:02:57.893: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Jul 15 12:02:57.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-1876" for this suite. @ 07/15/23 12:02:57.896
• [0.338 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 07/15/23 12:02:57.904
  Jul 15 12:02:57.904: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 12:02:57.904
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:02:57.921
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:02:57.924
  STEP: Setting up server cert @ 07/15/23 12:02:57.947
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 12:02:58.294
  STEP: Deploying the webhook pod @ 07/15/23 12:02:58.303
  STEP: Wait for the deployment to be ready @ 07/15/23 12:02:58.313
  Jul 15 12:02:58.320: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/15/23 12:03:00.332
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 12:03:00.342
  Jul 15 12:03:01.342: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 07/15/23 12:03:01.347
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 07/15/23 12:03:01.365
  STEP: Creating a configMap that should not be mutated @ 07/15/23 12:03:01.372
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 07/15/23 12:03:01.382
  STEP: Creating a configMap that should be mutated @ 07/15/23 12:03:01.388
  Jul 15 12:03:01.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2901" for this suite. @ 07/15/23 12:03:01.458
  STEP: Destroying namespace "webhook-markers-9060" for this suite. @ 07/15/23 12:03:01.464
• [3.566 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 07/15/23 12:03:01.47
  Jul 15 12:03:01.470: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename resourcequota @ 07/15/23 12:03:01.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:03:01.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:03:01.49
  STEP: Counting existing ResourceQuota @ 07/15/23 12:03:01.493
  STEP: Creating a ResourceQuota @ 07/15/23 12:03:06.498
  STEP: Ensuring resource quota status is calculated @ 07/15/23 12:03:06.503
  STEP: Creating a Service @ 07/15/23 12:03:08.509
  STEP: Creating a NodePort Service @ 07/15/23 12:03:08.523
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 07/15/23 12:03:08.54
  STEP: Ensuring resource quota status captures service creation @ 07/15/23 12:03:08.559
  STEP: Deleting Services @ 07/15/23 12:03:10.565
  STEP: Ensuring resource quota status released usage @ 07/15/23 12:03:10.6
  Jul 15 12:03:12.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2378" for this suite. @ 07/15/23 12:03:12.61
• [11.146 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 07/15/23 12:03:12.617
  Jul 15 12:03:12.617: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir @ 07/15/23 12:03:12.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:03:12.634
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:03:12.637
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 07/15/23 12:03:12.642
  STEP: Saw pod success @ 07/15/23 12:03:16.664
  Jul 15 12:03:16.668: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-8853b318-ccd9-4d84-8fc4-ee765c2a8ad4 container test-container: <nil>
  STEP: delete the pod @ 07/15/23 12:03:16.675
  Jul 15 12:03:16.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3115" for this suite. @ 07/15/23 12:03:16.695
• [4.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 07/15/23 12:03:16.704
  Jul 15 12:03:16.704: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename sched-preemption @ 07/15/23 12:03:16.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:03:16.721
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:03:16.724
  Jul 15 12:03:16.740: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 15 12:04:16.756: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 07/15/23 12:04:16.76
  Jul 15 12:04:16.760: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename sched-preemption-path @ 07/15/23 12:04:16.761
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:04:16.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:04:16.782
  STEP: Finding an available node @ 07/15/23 12:04:16.785
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/15/23 12:04:16.785
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/15/23 12:04:18.805
  Jul 15 12:04:18.817: INFO: found a healthy node: ip-172-31-16-190
  Jul 15 12:04:24.891: INFO: pods created so far: [1 1 1]
  Jul 15 12:04:24.891: INFO: length of pods created so far: 3
  Jul 15 12:04:26.903: INFO: pods created so far: [2 2 1]
  Jul 15 12:04:33.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 15 12:04:33.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-2875" for this suite. @ 07/15/23 12:04:33.978
  STEP: Destroying namespace "sched-preemption-5649" for this suite. @ 07/15/23 12:04:33.984
• [77.286 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 07/15/23 12:04:33.991
  Jul 15 12:04:33.991: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename init-container @ 07/15/23 12:04:33.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:04:34.009
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:04:34.011
  STEP: creating the pod @ 07/15/23 12:04:34.016
  Jul 15 12:04:34.016: INFO: PodSpec: initContainers in spec.initContainers
  Jul 15 12:04:38.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-589" for this suite. @ 07/15/23 12:04:38.291
• [4.308 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 07/15/23 12:04:38.299
  Jul 15 12:04:38.299: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir @ 07/15/23 12:04:38.3
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:04:38.319
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:04:38.322
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 07/15/23 12:04:38.325
  STEP: Saw pod success @ 07/15/23 12:04:42.346
  Jul 15 12:04:42.351: INFO: Trying to get logs from node ip-172-31-42-138 pod pod-f201c6d9-8525-4475-8e91-c38318c115f7 container test-container: <nil>
  STEP: delete the pod @ 07/15/23 12:04:42.368
  Jul 15 12:04:42.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7703" for this suite. @ 07/15/23 12:04:42.389
• [4.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 07/15/23 12:04:42.398
  Jul 15 12:04:42.398: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename prestop @ 07/15/23 12:04:42.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:04:42.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:04:42.417
  STEP: Creating server pod server in namespace prestop-6253 @ 07/15/23 12:04:42.422
  STEP: Waiting for pods to come up. @ 07/15/23 12:04:42.431
  STEP: Creating tester pod tester in namespace prestop-6253 @ 07/15/23 12:04:44.444
  STEP: Deleting pre-stop pod @ 07/15/23 12:04:46.461
  Jul 15 12:04:51.475: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Jul 15 12:04:51.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 07/15/23 12:04:51.479
  STEP: Destroying namespace "prestop-6253" for this suite. @ 07/15/23 12:04:51.492
• [9.100 seconds]
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 07/15/23 12:04:51.499
  Jul 15 12:04:51.499: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename svcaccounts @ 07/15/23 12:04:51.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:04:51.6
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:04:51.603
  Jul 15 12:04:51.610: INFO: Got root ca configmap in namespace "svcaccounts-4349"
  Jul 15 12:04:51.615: INFO: Deleted root ca configmap in namespace "svcaccounts-4349"
  STEP: waiting for a new root ca configmap created @ 07/15/23 12:04:52.116
  Jul 15 12:04:52.120: INFO: Recreated root ca configmap in namespace "svcaccounts-4349"
  Jul 15 12:04:52.126: INFO: Updated root ca configmap in namespace "svcaccounts-4349"
  STEP: waiting for the root ca configmap reconciled @ 07/15/23 12:04:52.627
  Jul 15 12:04:52.630: INFO: Reconciled root ca configmap in namespace "svcaccounts-4349"
  Jul 15 12:04:52.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4349" for this suite. @ 07/15/23 12:04:52.634
• [1.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 07/15/23 12:04:52.646
  Jul 15 12:04:52.646: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 12:04:52.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:04:52.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:04:52.664
  STEP: Setting up server cert @ 07/15/23 12:04:52.694
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 12:04:52.945
  STEP: Deploying the webhook pod @ 07/15/23 12:04:52.954
  STEP: Wait for the deployment to be ready @ 07/15/23 12:04:52.969
  Jul 15 12:04:52.977: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/15/23 12:04:54.988
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 12:04:54.996
  Jul 15 12:04:55.996: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 07/15/23 12:04:56.002
  STEP: create a namespace for the webhook @ 07/15/23 12:04:56.016
  STEP: create a configmap should be unconditionally rejected by the webhook @ 07/15/23 12:04:56.028
  Jul 15 12:04:56.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1667" for this suite. @ 07/15/23 12:04:56.106
  STEP: Destroying namespace "webhook-markers-7519" for this suite. @ 07/15/23 12:04:56.113
  STEP: Destroying namespace "fail-closed-namespace-4921" for this suite. @ 07/15/23 12:04:56.119
• [3.479 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 07/15/23 12:04:56.126
  Jul 15 12:04:56.126: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename sched-pred @ 07/15/23 12:04:56.126
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:04:56.141
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:04:56.144
  Jul 15 12:04:56.147: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 15 12:04:56.155: INFO: Waiting for terminating namespaces to be deleted...
  Jul 15 12:04:56.158: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-16-190 before test
  Jul 15 12:04:56.162: INFO: nginx-ingress-controller-kubernetes-worker-zr45d from ingress-nginx-kubernetes-worker started at 2023-07-15 11:51:56 +0000 UTC (1 container statuses recorded)
  Jul 15 12:04:56.162: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 15 12:04:56.162: INFO: calico-kube-controllers-7d466d5f7-kh5vx from kube-system started at 2023-07-15 11:51:59 +0000 UTC (1 container statuses recorded)
  Jul 15 12:04:56.162: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul 15 12:04:56.162: INFO: tester from prestop-6253 started at 2023-07-15 12:04:44 +0000 UTC (1 container statuses recorded)
  Jul 15 12:04:56.162: INFO: 	Container tester ready: true, restart count 0
  Jul 15 12:04:56.162: INFO: sonobuoy-systemd-logs-daemon-set-ff6bac3670ee4930-cffz4 from sonobuoy started at 2023-07-15 11:58:26 +0000 UTC (2 container statuses recorded)
  Jul 15 12:04:56.162: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 15 12:04:56.162: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 15 12:04:56.162: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-42-138 before test
  Jul 15 12:04:56.167: INFO: nginx-ingress-controller-kubernetes-worker-7nm9g from ingress-nginx-kubernetes-worker started at 2023-07-15 11:50:47 +0000 UTC (1 container statuses recorded)
  Jul 15 12:04:56.167: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 15 12:04:56.167: INFO: sonobuoy from sonobuoy started at 2023-07-15 11:58:24 +0000 UTC (1 container statuses recorded)
  Jul 15 12:04:56.167: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 15 12:04:56.167: INFO: sonobuoy-e2e-job-7bfe7aae7c0f4f05 from sonobuoy started at 2023-07-15 11:58:25 +0000 UTC (2 container statuses recorded)
  Jul 15 12:04:56.167: INFO: 	Container e2e ready: true, restart count 0
  Jul 15 12:04:56.167: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 15 12:04:56.168: INFO: sonobuoy-systemd-logs-daemon-set-ff6bac3670ee4930-rmnwj from sonobuoy started at 2023-07-15 11:58:26 +0000 UTC (2 container statuses recorded)
  Jul 15 12:04:56.168: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 15 12:04:56.168: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 15 12:04:56.168: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-84-236 before test
  Jul 15 12:04:56.173: INFO: default-http-backend-kubernetes-worker-65fc475d49-z5npc from ingress-nginx-kubernetes-worker started at 2023-07-15 11:48:26 +0000 UTC (1 container statuses recorded)
  Jul 15 12:04:56.173: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul 15 12:04:56.173: INFO: nginx-ingress-controller-kubernetes-worker-r7qgp from ingress-nginx-kubernetes-worker started at 2023-07-15 11:48:26 +0000 UTC (1 container statuses recorded)
  Jul 15 12:04:56.173: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 15 12:04:56.173: INFO: coredns-5c7f76ccb8-cn56l from kube-system started at 2023-07-15 11:48:22 +0000 UTC (1 container statuses recorded)
  Jul 15 12:04:56.173: INFO: 	Container coredns ready: true, restart count 0
  Jul 15 12:04:56.173: INFO: kube-state-metrics-5b95b4459c-dcf9f from kube-system started at 2023-07-15 11:48:22 +0000 UTC (1 container statuses recorded)
  Jul 15 12:04:56.173: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul 15 12:04:56.173: INFO: metrics-server-v0.5.2-6cf8c8b69c-kdwqb from kube-system started at 2023-07-15 11:48:22 +0000 UTC (2 container statuses recorded)
  Jul 15 12:04:56.173: INFO: 	Container metrics-server ready: true, restart count 0
  Jul 15 12:04:56.173: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul 15 12:04:56.173: INFO: dashboard-metrics-scraper-6b8586b5c9-h8plq from kubernetes-dashboard started at 2023-07-15 11:48:22 +0000 UTC (1 container statuses recorded)
  Jul 15 12:04:56.173: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul 15 12:04:56.173: INFO: kubernetes-dashboard-6869f4cd5f-2knpl from kubernetes-dashboard started at 2023-07-15 11:48:22 +0000 UTC (1 container statuses recorded)
  Jul 15 12:04:56.173: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul 15 12:04:56.173: INFO: sonobuoy-systemd-logs-daemon-set-ff6bac3670ee4930-k229l from sonobuoy started at 2023-07-15 11:58:26 +0000 UTC (2 container statuses recorded)
  Jul 15 12:04:56.173: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 15 12:04:56.173: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/15/23 12:04:56.173
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/15/23 12:04:58.192
  STEP: Trying to apply a random label on the found node. @ 07/15/23 12:04:58.208
  STEP: verifying the node has the label kubernetes.io/e2e-7b5b610b-6e47-433b-ab44-f32a1cb4556f 42 @ 07/15/23 12:04:58.216
  STEP: Trying to relaunch the pod, now with labels. @ 07/15/23 12:04:58.219
  STEP: removing the label kubernetes.io/e2e-7b5b610b-6e47-433b-ab44-f32a1cb4556f off the node ip-172-31-16-190 @ 07/15/23 12:05:00.236
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-7b5b610b-6e47-433b-ab44-f32a1cb4556f @ 07/15/23 12:05:00.247
  Jul 15 12:05:00.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3447" for this suite. @ 07/15/23 12:05:00.253
• [4.134 seconds]
------------------------------
SSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 07/15/23 12:05:00.26
  Jul 15 12:05:00.260: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename podtemplate @ 07/15/23 12:05:00.261
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:05:00.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:05:00.279
  STEP: Create a pod template @ 07/15/23 12:05:00.282
  STEP: Replace a pod template @ 07/15/23 12:05:00.287
  Jul 15 12:05:00.295: INFO: Found updated podtemplate annotation: "true"

  Jul 15 12:05:00.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-2028" for this suite. @ 07/15/23 12:05:00.3
• [0.046 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 07/15/23 12:05:00.306
  Jul 15 12:05:00.306: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl @ 07/15/23 12:05:00.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:05:00.32
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:05:00.323
  STEP: create deployment with httpd image @ 07/15/23 12:05:00.325
  Jul 15 12:05:00.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-3478 create -f -'
  Jul 15 12:05:00.498: INFO: stderr: ""
  Jul 15 12:05:00.498: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 07/15/23 12:05:00.498
  Jul 15 12:05:00.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-3478 diff -f -'
  Jul 15 12:05:00.887: INFO: rc: 1
  Jul 15 12:05:00.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-3478 delete -f -'
  Jul 15 12:05:00.936: INFO: stderr: ""
  Jul 15 12:05:00.936: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Jul 15 12:05:00.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3478" for this suite. @ 07/15/23 12:05:00.94
• [0.640 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 07/15/23 12:05:00.947
  Jul 15 12:05:00.947: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename lease-test @ 07/15/23 12:05:00.947
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:05:00.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:05:00.969
  Jul 15 12:05:01.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-3709" for this suite. @ 07/15/23 12:05:01.029
• [0.087 seconds]
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 07/15/23 12:05:01.034
  Jul 15 12:05:01.034: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename statefulset @ 07/15/23 12:05:01.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:05:01.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:05:01.052
  STEP: Creating service test in namespace statefulset-1714 @ 07/15/23 12:05:01.055
  STEP: Creating statefulset ss in namespace statefulset-1714 @ 07/15/23 12:05:01.066
  Jul 15 12:05:01.075: INFO: Found 0 stateful pods, waiting for 1
  Jul 15 12:05:11.081: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 07/15/23 12:05:11.089
  STEP: Getting /status @ 07/15/23 12:05:11.097
  Jul 15 12:05:11.102: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 07/15/23 12:05:11.102
  Jul 15 12:05:11.110: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 07/15/23 12:05:11.11
  Jul 15 12:05:11.112: INFO: Observed &StatefulSet event: ADDED
  Jul 15 12:05:11.112: INFO: Found Statefulset ss in namespace statefulset-1714 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 15 12:05:11.112: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 07/15/23 12:05:11.112
  Jul 15 12:05:11.112: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul 15 12:05:11.119: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 07/15/23 12:05:11.119
  Jul 15 12:05:11.121: INFO: Observed &StatefulSet event: ADDED
  Jul 15 12:05:11.121: INFO: Deleting all statefulset in ns statefulset-1714
  Jul 15 12:05:11.125: INFO: Scaling statefulset ss to 0
  Jul 15 12:05:21.145: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 15 12:05:21.148: INFO: Deleting statefulset ss
  Jul 15 12:05:21.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1714" for this suite. @ 07/15/23 12:05:21.166
• [20.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 07/15/23 12:05:21.173
  Jul 15 12:05:21.173: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename var-expansion @ 07/15/23 12:05:21.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:05:21.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:05:21.205
  STEP: Creating a pod to test env composition @ 07/15/23 12:05:21.207
  STEP: Saw pod success @ 07/15/23 12:05:25.229
  Jul 15 12:05:25.232: INFO: Trying to get logs from node ip-172-31-16-190 pod var-expansion-134ace85-fba0-4c06-84be-4cf6943b9d31 container dapi-container: <nil>
  STEP: delete the pod @ 07/15/23 12:05:25.25
  Jul 15 12:05:25.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1513" for this suite. @ 07/15/23 12:05:25.27
• [4.102 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 07/15/23 12:05:25.275
  Jul 15 12:05:25.276: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename cronjob @ 07/15/23 12:05:25.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:05:25.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:05:25.295
  STEP: Creating a ForbidConcurrent cronjob @ 07/15/23 12:05:25.297
  STEP: Ensuring a job is scheduled @ 07/15/23 12:05:25.304
  STEP: Ensuring exactly one is scheduled @ 07/15/23 12:06:01.31
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 07/15/23 12:06:01.314
  STEP: Ensuring no more jobs are scheduled @ 07/15/23 12:06:01.318
  STEP: Removing cronjob @ 07/15/23 12:11:01.326
  Jul 15 12:11:01.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1300" for this suite. @ 07/15/23 12:11:01.337
• [336.067 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 07/15/23 12:11:01.343
  Jul 15 12:11:01.343: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/15/23 12:11:01.343
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:11:01.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:11:01.366
  Jul 15 12:11:01.369: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/15/23 12:11:02.659
  Jul 15 12:11:02.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-687 --namespace=crd-publish-openapi-687 create -f -'
  Jul 15 12:11:03.027: INFO: stderr: ""
  Jul 15 12:11:03.027: INFO: stdout: "e2e-test-crd-publish-openapi-6063-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jul 15 12:11:03.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-687 --namespace=crd-publish-openapi-687 delete e2e-test-crd-publish-openapi-6063-crds test-cr'
  Jul 15 12:11:03.090: INFO: stderr: ""
  Jul 15 12:11:03.090: INFO: stdout: "e2e-test-crd-publish-openapi-6063-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Jul 15 12:11:03.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-687 --namespace=crd-publish-openapi-687 apply -f -'
  Jul 15 12:11:03.213: INFO: stderr: ""
  Jul 15 12:11:03.213: INFO: stdout: "e2e-test-crd-publish-openapi-6063-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jul 15 12:11:03.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-687 --namespace=crd-publish-openapi-687 delete e2e-test-crd-publish-openapi-6063-crds test-cr'
  Jul 15 12:11:03.262: INFO: stderr: ""
  Jul 15 12:11:03.262: INFO: stdout: "e2e-test-crd-publish-openapi-6063-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 07/15/23 12:11:03.262
  Jul 15 12:11:03.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-687 explain e2e-test-crd-publish-openapi-6063-crds'
  Jul 15 12:11:03.577: INFO: stderr: ""
  Jul 15 12:11:03.577: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-6063-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Jul 15 12:11:04.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-687" for this suite. @ 07/15/23 12:11:04.851
• [3.515 seconds]
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 07/15/23 12:11:04.858
  Jul 15 12:11:04.858: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename var-expansion @ 07/15/23 12:11:04.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:11:04.874
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:11:04.877
  Jul 15 12:11:06.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 15 12:11:06.903: INFO: Deleting pod "var-expansion-4cf6e44f-2100-407c-b8dd-6b0ffc4b09f3" in namespace "var-expansion-1844"
  Jul 15 12:11:06.911: INFO: Wait up to 5m0s for pod "var-expansion-4cf6e44f-2100-407c-b8dd-6b0ffc4b09f3" to be fully deleted
  STEP: Destroying namespace "var-expansion-1844" for this suite. @ 07/15/23 12:11:08.92
• [4.070 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 07/15/23 12:11:08.928
  Jul 15 12:11:08.928: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename resourcequota @ 07/15/23 12:11:08.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:11:08.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:11:08.947
  STEP: Counting existing ResourceQuota @ 07/15/23 12:11:08.951
  STEP: Creating a ResourceQuota @ 07/15/23 12:11:13.955
  STEP: Ensuring resource quota status is calculated @ 07/15/23 12:11:13.962
  STEP: Creating a Pod that fits quota @ 07/15/23 12:11:15.967
  STEP: Ensuring ResourceQuota status captures the pod usage @ 07/15/23 12:11:15.982
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 07/15/23 12:11:17.988
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 07/15/23 12:11:17.99
  STEP: Ensuring a pod cannot update its resource requirements @ 07/15/23 12:11:17.993
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 07/15/23 12:11:17.997
  STEP: Deleting the pod @ 07/15/23 12:11:20.002
  STEP: Ensuring resource quota status released the pod usage @ 07/15/23 12:11:20.016
  Jul 15 12:11:22.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1280" for this suite. @ 07/15/23 12:11:22.025
• [13.103 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 07/15/23 12:11:22.031
  Jul 15 12:11:22.031: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename pods @ 07/15/23 12:11:22.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:11:22.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:11:22.051
  Jul 15 12:11:22.054: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: creating the pod @ 07/15/23 12:11:22.054
  STEP: submitting the pod to kubernetes @ 07/15/23 12:11:22.054
  Jul 15 12:11:24.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5679" for this suite. @ 07/15/23 12:11:24.122
• [2.098 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 07/15/23 12:11:24.13
  Jul 15 12:11:24.130: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename dns @ 07/15/23 12:11:24.13
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:11:24.146
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:11:24.149
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2207.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2207.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 07/15/23 12:11:24.153
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2207.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2207.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 07/15/23 12:11:24.153
  STEP: creating a pod to probe /etc/hosts @ 07/15/23 12:11:24.153
  STEP: submitting the pod to kubernetes @ 07/15/23 12:11:24.153
  STEP: retrieving the pod @ 07/15/23 12:11:26.17
  STEP: looking for the results for each expected name from probers @ 07/15/23 12:11:26.173
  Jul 15 12:11:26.191: INFO: DNS probes using dns-2207/dns-test-afcd253c-98cc-42cd-ad33-b36d7c721da2 succeeded

  Jul 15 12:11:26.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/15/23 12:11:26.196
  STEP: Destroying namespace "dns-2207" for this suite. @ 07/15/23 12:11:26.209
• [2.084 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 07/15/23 12:11:26.214
  Jul 15 12:11:26.214: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:11:26.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:11:26.232
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:11:26.235
  STEP: Creating secret with name projected-secret-test-cdccbde3-16c1-48dd-b483-ef2f1edc2bd1 @ 07/15/23 12:11:26.238
  STEP: Creating a pod to test consume secrets @ 07/15/23 12:11:26.242
  STEP: Saw pod success @ 07/15/23 12:11:30.264
  Jul 15 12:11:30.268: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-projected-secrets-fc05a173-c1c1-4ecf-96c8-38c41981cff1 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/15/23 12:11:30.282
  Jul 15 12:11:30.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7551" for this suite. @ 07/15/23 12:11:30.303
• [4.094 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 07/15/23 12:11:30.309
  Jul 15 12:11:30.309: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename events @ 07/15/23 12:11:30.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:11:30.327
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:11:30.33
  STEP: Create set of events @ 07/15/23 12:11:30.333
  STEP: get a list of Events with a label in the current namespace @ 07/15/23 12:11:30.35
  STEP: delete a list of events @ 07/15/23 12:11:30.354
  Jul 15 12:11:30.354: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 07/15/23 12:11:30.375
  Jul 15 12:11:30.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8948" for this suite. @ 07/15/23 12:11:30.384
• [0.080 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 07/15/23 12:11:30.389
  Jul 15 12:11:30.389: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename namespaces @ 07/15/23 12:11:30.39
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:11:30.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:11:30.406
  STEP: Read namespace status @ 07/15/23 12:11:30.409
  Jul 15 12:11:30.413: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 07/15/23 12:11:30.413
  Jul 15 12:11:30.417: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 07/15/23 12:11:30.417
  Jul 15 12:11:30.426: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Jul 15 12:11:30.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1676" for this suite. @ 07/15/23 12:11:30.429
• [0.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 07/15/23 12:11:30.438
  Jul 15 12:11:30.438: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename statefulset @ 07/15/23 12:11:30.438
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:11:30.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:11:30.455
  STEP: Creating service test in namespace statefulset-8803 @ 07/15/23 12:11:30.458
  STEP: Looking for a node to schedule stateful set and pod @ 07/15/23 12:11:30.463
  STEP: Creating pod with conflicting port in namespace statefulset-8803 @ 07/15/23 12:11:30.467
  STEP: Waiting until pod test-pod will start running in namespace statefulset-8803 @ 07/15/23 12:11:30.475
  STEP: Creating statefulset with conflicting port in namespace statefulset-8803 @ 07/15/23 12:11:32.483
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8803 @ 07/15/23 12:11:32.49
  Jul 15 12:11:32.502: INFO: Observed stateful pod in namespace: statefulset-8803, name: ss-0, uid: c04d760e-b4d3-419f-8dea-4414f0da0709, status phase: Pending. Waiting for statefulset controller to delete.
  Jul 15 12:11:32.514: INFO: Observed stateful pod in namespace: statefulset-8803, name: ss-0, uid: c04d760e-b4d3-419f-8dea-4414f0da0709, status phase: Failed. Waiting for statefulset controller to delete.
  Jul 15 12:11:32.521: INFO: Observed stateful pod in namespace: statefulset-8803, name: ss-0, uid: c04d760e-b4d3-419f-8dea-4414f0da0709, status phase: Failed. Waiting for statefulset controller to delete.
  Jul 15 12:11:32.526: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8803
  STEP: Removing pod with conflicting port in namespace statefulset-8803 @ 07/15/23 12:11:32.526
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8803 and will be in running state @ 07/15/23 12:11:32.54
  Jul 15 12:11:34.549: INFO: Deleting all statefulset in ns statefulset-8803
  Jul 15 12:11:34.552: INFO: Scaling statefulset ss to 0
  Jul 15 12:11:44.576: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 15 12:11:44.579: INFO: Deleting statefulset ss
  Jul 15 12:11:44.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8803" for this suite. @ 07/15/23 12:11:44.597
• [14.166 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 07/15/23 12:11:44.604
  Jul 15 12:11:44.604: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 12:11:44.605
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:11:44.619
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:11:44.622
  STEP: Setting up server cert @ 07/15/23 12:11:44.642
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 12:11:44.807
  STEP: Deploying the webhook pod @ 07/15/23 12:11:44.817
  STEP: Wait for the deployment to be ready @ 07/15/23 12:11:44.827
  Jul 15 12:11:44.834: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/15/23 12:11:46.847
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 12:11:46.857
  Jul 15 12:11:47.858: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 15 12:11:47.862: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 07/15/23 12:11:48.372
  STEP: Creating a custom resource that should be denied by the webhook @ 07/15/23 12:11:48.387
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 07/15/23 12:11:50.408
  STEP: Updating the custom resource with disallowed data should be denied @ 07/15/23 12:11:50.415
  STEP: Deleting the custom resource should be denied @ 07/15/23 12:11:50.423
  STEP: Remove the offending key and value from the custom resource data @ 07/15/23 12:11:50.43
  STEP: Deleting the updated custom resource should be successful @ 07/15/23 12:11:50.44
  Jul 15 12:11:50.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3518" for this suite. @ 07/15/23 12:11:51.01
  STEP: Destroying namespace "webhook-markers-6985" for this suite. @ 07/15/23 12:11:51.017
• [6.422 seconds]
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 07/15/23 12:11:51.026
  Jul 15 12:11:51.026: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename daemonsets @ 07/15/23 12:11:51.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:11:51.041
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:11:51.044
  Jul 15 12:11:51.070: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 07/15/23 12:11:51.075
  Jul 15 12:11:51.082: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:11:51.082: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 07/15/23 12:11:51.082
  Jul 15 12:11:51.102: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:11:51.102: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  Jul 15 12:11:52.107: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 15 12:11:52.107: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 07/15/23 12:11:52.11
  Jul 15 12:11:52.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 15 12:11:52.123: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  Jul 15 12:11:53.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:11:53.129: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 07/15/23 12:11:53.129
  Jul 15 12:11:53.142: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:11:53.142: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  Jul 15 12:11:54.147: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:11:54.147: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  Jul 15 12:11:55.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 15 12:11:55.146: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/15/23 12:11:55.152
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1101, will wait for the garbage collector to delete the pods @ 07/15/23 12:11:55.152
  Jul 15 12:11:55.213: INFO: Deleting DaemonSet.extensions daemon-set took: 7.31455ms
  Jul 15 12:11:55.314: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.984997ms
  Jul 15 12:11:57.121: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:11:57.121: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 15 12:11:57.125: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6491"},"items":null}

  Jul 15 12:11:57.128: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6491"},"items":null}

  Jul 15 12:11:57.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1101" for this suite. @ 07/15/23 12:11:57.153
• [6.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 07/15/23 12:11:57.16
  Jul 15 12:11:57.160: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 12:11:57.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:11:57.175
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:11:57.178
  STEP: Setting up server cert @ 07/15/23 12:11:57.202
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 12:11:57.407
  STEP: Deploying the webhook pod @ 07/15/23 12:11:57.413
  STEP: Wait for the deployment to be ready @ 07/15/23 12:11:57.424
  Jul 15 12:11:57.429: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/15/23 12:11:59.442
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 12:11:59.452
  Jul 15 12:12:00.453: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 07/15/23 12:12:00.458
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/15/23 12:12:00.474
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 07/15/23 12:12:00.481
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/15/23 12:12:00.49
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 07/15/23 12:12:00.5
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/15/23 12:12:00.517
  Jul 15 12:12:00.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3293" for this suite. @ 07/15/23 12:12:00.565
  STEP: Destroying namespace "webhook-markers-6939" for this suite. @ 07/15/23 12:12:00.573
• [3.418 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 07/15/23 12:12:00.579
  Jul 15 12:12:00.579: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename pods @ 07/15/23 12:12:00.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:12:00.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:12:00.598
  STEP: creating pod @ 07/15/23 12:12:00.603
  Jul 15 12:12:02.628: INFO: Pod pod-hostip-e28248db-278a-4d27-a10a-0b606bf3b587 has hostIP: 172.31.16.190
  Jul 15 12:12:02.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7822" for this suite. @ 07/15/23 12:12:02.632
• [2.059 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 07/15/23 12:12:02.639
  Jul 15 12:12:02.639: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename secrets @ 07/15/23 12:12:02.64
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:12:02.654
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:12:02.657
  STEP: Creating projection with secret that has name secret-emptykey-test-ad76a84c-c4ce-4140-b250-4f90660dd2f3 @ 07/15/23 12:12:02.66
  Jul 15 12:12:02.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3849" for this suite. @ 07/15/23 12:12:02.665
• [0.032 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 07/15/23 12:12:02.672
  Jul 15 12:12:02.672: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/15/23 12:12:02.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:12:02.688
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:12:02.691
  Jul 15 12:12:02.694: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/15/23 12:12:03.942
  Jul 15 12:12:03.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-1574 --namespace=crd-publish-openapi-1574 create -f -'
  Jul 15 12:12:04.333: INFO: stderr: ""
  Jul 15 12:12:04.333: INFO: stdout: "e2e-test-crd-publish-openapi-6727-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jul 15 12:12:04.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-1574 --namespace=crd-publish-openapi-1574 delete e2e-test-crd-publish-openapi-6727-crds test-cr'
  Jul 15 12:12:04.391: INFO: stderr: ""
  Jul 15 12:12:04.391: INFO: stdout: "e2e-test-crd-publish-openapi-6727-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Jul 15 12:12:04.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-1574 --namespace=crd-publish-openapi-1574 apply -f -'
  Jul 15 12:12:04.522: INFO: stderr: ""
  Jul 15 12:12:04.522: INFO: stdout: "e2e-test-crd-publish-openapi-6727-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jul 15 12:12:04.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-1574 --namespace=crd-publish-openapi-1574 delete e2e-test-crd-publish-openapi-6727-crds test-cr'
  Jul 15 12:12:04.572: INFO: stderr: ""
  Jul 15 12:12:04.572: INFO: stdout: "e2e-test-crd-publish-openapi-6727-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 07/15/23 12:12:04.572
  Jul 15 12:12:04.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-1574 explain e2e-test-crd-publish-openapi-6727-crds'
  Jul 15 12:12:04.862: INFO: stderr: ""
  Jul 15 12:12:04.862: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-6727-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Jul 15 12:12:06.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1574" for this suite. @ 07/15/23 12:12:06.111
• [3.445 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 07/15/23 12:12:06.118
  Jul 15 12:12:06.118: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-runtime @ 07/15/23 12:12:06.119
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:12:06.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:12:06.136
  STEP: create the container @ 07/15/23 12:12:06.139
  W0715 12:12:06.147269      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/15/23 12:12:06.147
  STEP: get the container status @ 07/15/23 12:12:09.165
  STEP: the container should be terminated @ 07/15/23 12:12:09.168
  STEP: the termination message should be set @ 07/15/23 12:12:09.168
  Jul 15 12:12:09.168: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 07/15/23 12:12:09.168
  Jul 15 12:12:09.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4295" for this suite. @ 07/15/23 12:12:09.188
• [3.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 07/15/23 12:12:09.196
  Jul 15 12:12:09.196: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename deployment @ 07/15/23 12:12:09.196
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:12:09.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:12:09.215
  Jul 15 12:12:09.226: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  Jul 15 12:12:14.231: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/15/23 12:12:14.231
  Jul 15 12:12:14.231: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 07/15/23 12:12:14.241
  Jul 15 12:12:14.253: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5099  2989823e-0ea3-4065-b3a9-a8ee5a3fb686 6752 1 2023-07-15 12:12:14 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-07-15 12:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000b27a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Jul 15 12:12:14.260: INFO: New ReplicaSet "test-cleanup-deployment-68b75d69f8" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-68b75d69f8  deployment-5099  02386bbe-162b-4afc-b768-4e8a10cd32c7 6754 1 2023-07-15 12:12:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 2989823e-0ea3-4065-b3a9-a8ee5a3fb686 0xc00301ab07 0xc00301ab08}] [] [{kube-controller-manager Update apps/v1 2023-07-15 12:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2989823e-0ea3-4065-b3a9-a8ee5a3fb686\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 68b75d69f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00301ab98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 15 12:12:14.260: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Jul 15 12:12:14.260: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5099  c93ca727-ca3a-456f-b7fb-1ced34ee48bd 6753 1 2023-07-15 12:12:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 2989823e-0ea3-4065-b3a9-a8ee5a3fb686 0xc00301a9df 0xc00301a9f0}] [] [{e2e.test Update apps/v1 2023-07-15 12:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-15 12:12:10 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-15 12:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"2989823e-0ea3-4065-b3a9-a8ee5a3fb686\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00301aaa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 15 12:12:14.267: INFO: Pod "test-cleanup-controller-rnfnn" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-rnfnn test-cleanup-controller- deployment-5099  0cd7f83b-4df4-4001-8f3a-c0c831b15983 6733 0 2023-07-15 12:12:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller c93ca727-ca3a-456f-b7fb-1ced34ee48bd 0xc00467fdff 0xc00467fe30}] [] [{kube-controller-manager Update v1 2023-07-15 12:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c93ca727-ca3a-456f-b7fb-1ced34ee48bd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:12:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.33.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tjphj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tjphj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:12:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:12:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:12:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:12:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.190,PodIP:192.168.33.108,StartTime:2023-07-15 12:12:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-15 12:12:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5995158a7e2d22416e1864f900ae273a08bee744012a8180496c9b3662719571,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.33.108,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:12:14.267: INFO: Pod "test-cleanup-deployment-68b75d69f8-74qcr" is not available:
  &Pod{ObjectMeta:{test-cleanup-deployment-68b75d69f8-74qcr test-cleanup-deployment-68b75d69f8- deployment-5099  746c63a5-4cb2-4c3c-b34a-6a93cbfcefd8 6758 0 2023-07-15 12:12:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-68b75d69f8 02386bbe-162b-4afc-b768-4e8a10cd32c7 0xc0019b023f 0xc0019b0290}] [] [{kube-controller-manager Update v1 2023-07-15 12:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02386bbe-162b-4afc-b768-4e8a10cd32c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2lw9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2lw9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:12:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:12:14.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5099" for this suite. @ 07/15/23 12:12:14.273
• [5.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 07/15/23 12:12:14.283
  Jul 15 12:12:14.283: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename services @ 07/15/23 12:12:14.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:12:14.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:12:14.3
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-6205 @ 07/15/23 12:12:14.303
  STEP: changing the ExternalName service to type=ClusterIP @ 07/15/23 12:12:14.307
  STEP: creating replication controller externalname-service in namespace services-6205 @ 07/15/23 12:12:14.321
  I0715 12:12:14.327253      23 runners.go:194] Created replication controller with name: externalname-service, namespace: services-6205, replica count: 2
  I0715 12:12:17.378704      23 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 15 12:12:17.378: INFO: Creating new exec pod
  Jul 15 12:12:20.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-6205 exec execpod8mhwz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul 15 12:12:20.505: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul 15 12:12:20.505: INFO: stdout: ""
  Jul 15 12:12:21.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-6205 exec execpod8mhwz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul 15 12:12:21.619: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul 15 12:12:21.619: INFO: stdout: "externalname-service-cmxkw"
  Jul 15 12:12:21.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-6205 exec execpod8mhwz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.84 80'
  Jul 15 12:12:21.721: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.84 80\nConnection to 10.152.183.84 80 port [tcp/http] succeeded!\n"
  Jul 15 12:12:21.721: INFO: stdout: "externalname-service-cmxkw"
  Jul 15 12:12:21.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 15 12:12:21.726: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-6205" for this suite. @ 07/15/23 12:12:21.741
• [7.465 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 07/15/23 12:12:21.749
  Jul 15 12:12:21.749: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl @ 07/15/23 12:12:21.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:12:21.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:12:21.766
  STEP: creating a replication controller @ 07/15/23 12:12:21.769
  Jul 15 12:12:21.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 create -f -'
  Jul 15 12:12:22.161: INFO: stderr: ""
  Jul 15 12:12:22.161: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/15/23 12:12:22.161
  Jul 15 12:12:22.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 15 12:12:22.205: INFO: stderr: ""
  Jul 15 12:12:22.206: INFO: stdout: "update-demo-nautilus-7n9cr update-demo-nautilus-pm8kl "
  Jul 15 12:12:22.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods update-demo-nautilus-7n9cr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 15 12:12:22.250: INFO: stderr: ""
  Jul 15 12:12:22.250: INFO: stdout: ""
  Jul 15 12:12:22.250: INFO: update-demo-nautilus-7n9cr is created but not running
  Jul 15 12:12:27.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 15 12:12:27.294: INFO: stderr: ""
  Jul 15 12:12:27.294: INFO: stdout: "update-demo-nautilus-7n9cr update-demo-nautilus-pm8kl "
  Jul 15 12:12:27.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods update-demo-nautilus-7n9cr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 15 12:12:27.335: INFO: stderr: ""
  Jul 15 12:12:27.335: INFO: stdout: "true"
  Jul 15 12:12:27.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods update-demo-nautilus-7n9cr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 15 12:12:27.376: INFO: stderr: ""
  Jul 15 12:12:27.376: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 15 12:12:27.376: INFO: validating pod update-demo-nautilus-7n9cr
  Jul 15 12:12:27.383: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 15 12:12:27.383: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 15 12:12:27.383: INFO: update-demo-nautilus-7n9cr is verified up and running
  Jul 15 12:12:27.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods update-demo-nautilus-pm8kl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 15 12:12:27.425: INFO: stderr: ""
  Jul 15 12:12:27.425: INFO: stdout: "true"
  Jul 15 12:12:27.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods update-demo-nautilus-pm8kl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 15 12:12:27.465: INFO: stderr: ""
  Jul 15 12:12:27.465: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 15 12:12:27.465: INFO: validating pod update-demo-nautilus-pm8kl
  Jul 15 12:12:27.470: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 15 12:12:27.470: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 15 12:12:27.470: INFO: update-demo-nautilus-pm8kl is verified up and running
  STEP: scaling down the replication controller @ 07/15/23 12:12:27.47
  Jul 15 12:12:27.471: INFO: scanned /root for discovery docs: <nil>
  Jul 15 12:12:27.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  Jul 15 12:12:28.531: INFO: stderr: ""
  Jul 15 12:12:28.531: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/15/23 12:12:28.531
  Jul 15 12:12:28.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 15 12:12:28.575: INFO: stderr: ""
  Jul 15 12:12:28.575: INFO: stdout: "update-demo-nautilus-7n9cr update-demo-nautilus-pm8kl "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 07/15/23 12:12:28.575
  Jul 15 12:12:33.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 15 12:12:33.621: INFO: stderr: ""
  Jul 15 12:12:33.621: INFO: stdout: "update-demo-nautilus-7n9cr "
  Jul 15 12:12:33.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods update-demo-nautilus-7n9cr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 15 12:12:33.664: INFO: stderr: ""
  Jul 15 12:12:33.664: INFO: stdout: "true"
  Jul 15 12:12:33.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods update-demo-nautilus-7n9cr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 15 12:12:33.705: INFO: stderr: ""
  Jul 15 12:12:33.705: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 15 12:12:33.705: INFO: validating pod update-demo-nautilus-7n9cr
  Jul 15 12:12:33.710: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 15 12:12:33.710: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 15 12:12:33.710: INFO: update-demo-nautilus-7n9cr is verified up and running
  STEP: scaling up the replication controller @ 07/15/23 12:12:33.71
  Jul 15 12:12:33.711: INFO: scanned /root for discovery docs: <nil>
  Jul 15 12:12:33.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  Jul 15 12:12:34.766: INFO: stderr: ""
  Jul 15 12:12:34.766: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/15/23 12:12:34.766
  Jul 15 12:12:34.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 15 12:12:34.811: INFO: stderr: ""
  Jul 15 12:12:34.811: INFO: stdout: "update-demo-nautilus-28mld update-demo-nautilus-7n9cr "
  Jul 15 12:12:34.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods update-demo-nautilus-28mld -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 15 12:12:34.852: INFO: stderr: ""
  Jul 15 12:12:34.852: INFO: stdout: ""
  Jul 15 12:12:34.852: INFO: update-demo-nautilus-28mld is created but not running
  Jul 15 12:12:39.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 15 12:12:39.898: INFO: stderr: ""
  Jul 15 12:12:39.898: INFO: stdout: "update-demo-nautilus-28mld update-demo-nautilus-7n9cr "
  Jul 15 12:12:39.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods update-demo-nautilus-28mld -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 15 12:12:39.941: INFO: stderr: ""
  Jul 15 12:12:39.941: INFO: stdout: "true"
  Jul 15 12:12:39.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods update-demo-nautilus-28mld -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 15 12:12:39.982: INFO: stderr: ""
  Jul 15 12:12:39.982: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 15 12:12:39.982: INFO: validating pod update-demo-nautilus-28mld
  Jul 15 12:12:39.988: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 15 12:12:39.989: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 15 12:12:39.989: INFO: update-demo-nautilus-28mld is verified up and running
  Jul 15 12:12:39.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods update-demo-nautilus-7n9cr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 15 12:12:40.031: INFO: stderr: ""
  Jul 15 12:12:40.031: INFO: stdout: "true"
  Jul 15 12:12:40.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods update-demo-nautilus-7n9cr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 15 12:12:40.073: INFO: stderr: ""
  Jul 15 12:12:40.073: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 15 12:12:40.073: INFO: validating pod update-demo-nautilus-7n9cr
  Jul 15 12:12:40.078: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 15 12:12:40.078: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 15 12:12:40.078: INFO: update-demo-nautilus-7n9cr is verified up and running
  STEP: using delete to clean up resources @ 07/15/23 12:12:40.078
  Jul 15 12:12:40.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 delete --grace-period=0 --force -f -'
  Jul 15 12:12:40.126: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 15 12:12:40.126: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jul 15 12:12:40.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get rc,svc -l name=update-demo --no-headers'
  Jul 15 12:12:40.176: INFO: stderr: "No resources found in kubectl-1497 namespace.\n"
  Jul 15 12:12:40.176: INFO: stdout: ""
  Jul 15 12:12:40.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-1497 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul 15 12:12:40.225: INFO: stderr: ""
  Jul 15 12:12:40.225: INFO: stdout: ""
  Jul 15 12:12:40.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1497" for this suite. @ 07/15/23 12:12:40.229
• [18.487 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 07/15/23 12:12:40.237
  Jul 15 12:12:40.237: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename gc @ 07/15/23 12:12:40.237
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:12:40.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:12:40.257
  STEP: create the deployment @ 07/15/23 12:12:40.26
  W0715 12:12:40.264477      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 07/15/23 12:12:40.264
  STEP: delete the deployment @ 07/15/23 12:12:40.774
  STEP: wait for all rs to be garbage collected @ 07/15/23 12:12:40.781
  STEP: expected 0 rs, got 1 rs @ 07/15/23 12:12:40.787
  STEP: expected 0 pods, got 2 pods @ 07/15/23 12:12:40.789
  STEP: Gathering metrics @ 07/15/23 12:12:41.3
  W0715 12:12:41.303762      23 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 15 12:12:41.303: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 15 12:12:41.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1000" for this suite. @ 07/15/23 12:12:41.307
• [1.077 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 07/15/23 12:12:41.314
  Jul 15 12:12:41.314: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename pods @ 07/15/23 12:12:41.315
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:12:41.328
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:12:41.331
  STEP: Create a pod @ 07/15/23 12:12:41.334
  STEP: patching /status @ 07/15/23 12:12:43.35
  Jul 15 12:12:43.358: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Jul 15 12:12:43.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3591" for this suite. @ 07/15/23 12:12:43.362
• [2.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 07/15/23 12:12:43.372
  Jul 15 12:12:43.372: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename watch @ 07/15/23 12:12:43.373
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:12:43.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:12:43.39
  STEP: creating a watch on configmaps with a certain label @ 07/15/23 12:12:43.394
  STEP: creating a new configmap @ 07/15/23 12:12:43.396
  STEP: modifying the configmap once @ 07/15/23 12:12:43.402
  STEP: changing the label value of the configmap @ 07/15/23 12:12:43.41
  STEP: Expecting to observe a delete notification for the watched object @ 07/15/23 12:12:43.421
  Jul 15 12:12:43.421: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2329  88ccf5c3-2a32-40f1-b887-d6e653fb02ad 7118 0 2023-07-15 12:12:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-15 12:12:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 12:12:43.421: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2329  88ccf5c3-2a32-40f1-b887-d6e653fb02ad 7119 0 2023-07-15 12:12:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-15 12:12:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 12:12:43.421: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2329  88ccf5c3-2a32-40f1-b887-d6e653fb02ad 7120 0 2023-07-15 12:12:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-15 12:12:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 07/15/23 12:12:43.421
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 07/15/23 12:12:43.433
  STEP: changing the label value of the configmap back @ 07/15/23 12:12:53.436
  STEP: modifying the configmap a third time @ 07/15/23 12:12:53.446
  STEP: deleting the configmap @ 07/15/23 12:12:53.453
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 07/15/23 12:12:53.459
  Jul 15 12:12:53.459: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2329  88ccf5c3-2a32-40f1-b887-d6e653fb02ad 7205 0 2023-07-15 12:12:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-15 12:12:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 12:12:53.460: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2329  88ccf5c3-2a32-40f1-b887-d6e653fb02ad 7206 0 2023-07-15 12:12:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-15 12:12:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 12:12:53.460: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2329  88ccf5c3-2a32-40f1-b887-d6e653fb02ad 7207 0 2023-07-15 12:12:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-15 12:12:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 12:12:53.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2329" for this suite. @ 07/15/23 12:12:53.464
• [10.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 07/15/23 12:12:53.471
  Jul 15 12:12:53.471: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename replicaset @ 07/15/23 12:12:53.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:12:53.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:12:53.49
  STEP: Create a Replicaset @ 07/15/23 12:12:53.497
  STEP: Verify that the required pods have come up. @ 07/15/23 12:12:53.502
  Jul 15 12:12:53.506: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul 15 12:12:58.513: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/15/23 12:12:58.513
  STEP: Getting /status @ 07/15/23 12:12:58.513
  Jul 15 12:12:58.517: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 07/15/23 12:12:58.517
  Jul 15 12:12:58.526: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 07/15/23 12:12:58.526
  Jul 15 12:12:58.528: INFO: Observed &ReplicaSet event: ADDED
  Jul 15 12:12:58.528: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 15 12:12:58.528: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 15 12:12:58.528: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 15 12:12:58.528: INFO: Found replicaset test-rs in namespace replicaset-5124 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 15 12:12:58.528: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 07/15/23 12:12:58.528
  Jul 15 12:12:58.528: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul 15 12:12:58.534: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 07/15/23 12:12:58.534
  Jul 15 12:12:58.535: INFO: Observed &ReplicaSet event: ADDED
  Jul 15 12:12:58.535: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 15 12:12:58.535: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 15 12:12:58.536: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 15 12:12:58.536: INFO: Observed replicaset test-rs in namespace replicaset-5124 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 15 12:12:58.536: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 15 12:12:58.536: INFO: Found replicaset test-rs in namespace replicaset-5124 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Jul 15 12:12:58.536: INFO: Replicaset test-rs has a patched status
  Jul 15 12:12:58.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5124" for this suite. @ 07/15/23 12:12:58.539
• [5.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 07/15/23 12:12:58.548
  Jul 15 12:12:58.548: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename cronjob @ 07/15/23 12:12:58.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:12:58.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:12:58.567
  STEP: Creating a cronjob @ 07/15/23 12:12:58.57
  STEP: Ensuring more than one job is running at a time @ 07/15/23 12:12:58.575
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 07/15/23 12:14:00.58
  STEP: Removing cronjob @ 07/15/23 12:14:00.584
  Jul 15 12:14:00.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9088" for this suite. @ 07/15/23 12:14:00.595
• [62.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 07/15/23 12:14:00.602
  Jul 15 12:14:00.602: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-probe @ 07/15/23 12:14:00.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:14:00.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:14:00.628
  STEP: Creating pod test-webserver-e0a770c2-3ce7-41e0-8ce1-7e83a6a0f4d3 in namespace container-probe-7027 @ 07/15/23 12:14:00.631
  Jul 15 12:14:02.647: INFO: Started pod test-webserver-e0a770c2-3ce7-41e0-8ce1-7e83a6a0f4d3 in namespace container-probe-7027
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/15/23 12:14:02.647
  Jul 15 12:14:02.651: INFO: Initial restart count of pod test-webserver-e0a770c2-3ce7-41e0-8ce1-7e83a6a0f4d3 is 0
  Jul 15 12:18:03.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/15/23 12:18:03.292
  STEP: Destroying namespace "container-probe-7027" for this suite. @ 07/15/23 12:18:03.402
• [242.806 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 07/15/23 12:18:03.408
  Jul 15 12:18:03.408: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename configmap @ 07/15/23 12:18:03.409
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:18:03.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:18:03.428
  STEP: Creating configMap with name configmap-test-upd-fba8cbb2-686b-4213-9028-bb772a3372ea @ 07/15/23 12:18:03.435
  STEP: Creating the pod @ 07/15/23 12:18:03.438
  STEP: Waiting for pod with text data @ 07/15/23 12:18:05.456
  STEP: Waiting for pod with binary data @ 07/15/23 12:18:05.473
  Jul 15 12:18:05.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9346" for this suite. @ 07/15/23 12:18:05.483
• [2.082 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 07/15/23 12:18:05.491
  Jul 15 12:18:05.491: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:18:05.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:18:05.509
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:18:05.511
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-4de3a99c-3f9c-4099-8047-f39c5191c177 @ 07/15/23 12:18:05.519
  STEP: Creating the pod @ 07/15/23 12:18:05.523
  STEP: Updating configmap projected-configmap-test-upd-4de3a99c-3f9c-4099-8047-f39c5191c177 @ 07/15/23 12:18:07.554
  STEP: waiting to observe update in volume @ 07/15/23 12:18:07.559
  Jul 15 12:19:21.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4316" for this suite. @ 07/15/23 12:19:21.901
• [76.416 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 07/15/23 12:19:21.907
  Jul 15 12:19:21.907: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/15/23 12:19:21.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:19:21.923
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:19:21.926
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 07/15/23 12:19:21.929
  Jul 15 12:19:21.929: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 07/15/23 12:19:27.598
  Jul 15 12:19:27.598: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 12:19:28.845: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 12:19:34.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-281" for this suite. @ 07/15/23 12:19:34.15
• [12.249 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 07/15/23 12:19:34.157
  Jul 15 12:19:34.157: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename csiinlinevolumes @ 07/15/23 12:19:34.158
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:19:34.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:19:34.172
  STEP: creating @ 07/15/23 12:19:34.174
  STEP: getting @ 07/15/23 12:19:34.188
  STEP: listing in namespace @ 07/15/23 12:19:34.191
  STEP: patching @ 07/15/23 12:19:34.194
  STEP: deleting @ 07/15/23 12:19:34.199
  Jul 15 12:19:34.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-2866" for this suite. @ 07/15/23 12:19:34.217
• [0.067 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 07/15/23 12:19:34.225
  Jul 15 12:19:34.225: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename downward-api @ 07/15/23 12:19:34.226
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:19:34.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:19:34.237
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 12:19:34.24
  STEP: Saw pod success @ 07/15/23 12:19:38.263
  Jul 15 12:19:38.266: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-b520aa14-f80b-4433-982e-0a6f141c5763 container client-container: <nil>
  STEP: delete the pod @ 07/15/23 12:19:38.28
  Jul 15 12:19:38.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-558" for this suite. @ 07/15/23 12:19:38.297
• [4.077 seconds]
------------------------------
SS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 07/15/23 12:19:38.302
  Jul 15 12:19:38.302: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename svc-latency @ 07/15/23 12:19:38.302
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:19:38.315
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:19:38.318
  Jul 15 12:19:38.321: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-7493 @ 07/15/23 12:19:38.321
  I0715 12:19:38.325679      23 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7493, replica count: 1
  I0715 12:19:39.376647      23 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0715 12:19:40.377057      23 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 15 12:19:40.488: INFO: Created: latency-svc-vw82t
  Jul 15 12:19:40.493: INFO: Got endpoints: latency-svc-vw82t [16.352967ms]
  Jul 15 12:19:40.502: INFO: Created: latency-svc-875db
  Jul 15 12:19:40.506: INFO: Got endpoints: latency-svc-875db [12.270168ms]
  Jul 15 12:19:40.509: INFO: Created: latency-svc-l224v
  Jul 15 12:19:40.512: INFO: Got endpoints: latency-svc-l224v [17.81368ms]
  Jul 15 12:19:40.515: INFO: Created: latency-svc-597lh
  Jul 15 12:19:40.520: INFO: Got endpoints: latency-svc-597lh [26.02374ms]
  Jul 15 12:19:40.522: INFO: Created: latency-svc-5bkh9
  Jul 15 12:19:40.526: INFO: Got endpoints: latency-svc-5bkh9 [32.222468ms]
  Jul 15 12:19:40.529: INFO: Created: latency-svc-m56m9
  Jul 15 12:19:40.533: INFO: Got endpoints: latency-svc-m56m9 [39.625803ms]
  Jul 15 12:19:40.536: INFO: Created: latency-svc-lz6hn
  Jul 15 12:19:40.539: INFO: Got endpoints: latency-svc-lz6hn [44.767682ms]
  Jul 15 12:19:40.542: INFO: Created: latency-svc-gr44q
  Jul 15 12:19:40.546: INFO: Got endpoints: latency-svc-gr44q [51.86132ms]
  Jul 15 12:19:40.549: INFO: Created: latency-svc-7gwkf
  Jul 15 12:19:40.552: INFO: Got endpoints: latency-svc-7gwkf [58.489901ms]
  Jul 15 12:19:40.557: INFO: Created: latency-svc-d9vvk
  Jul 15 12:19:40.560: INFO: Created: latency-svc-rz5fr
  Jul 15 12:19:40.562: INFO: Got endpoints: latency-svc-d9vvk [68.429669ms]
  Jul 15 12:19:40.566: INFO: Got endpoints: latency-svc-rz5fr [72.357469ms]
  Jul 15 12:19:40.567: INFO: Created: latency-svc-4p5gx
  Jul 15 12:19:40.570: INFO: Got endpoints: latency-svc-4p5gx [76.203706ms]
  Jul 15 12:19:40.572: INFO: Created: latency-svc-8vt6p
  Jul 15 12:19:40.578: INFO: Got endpoints: latency-svc-8vt6p [84.056876ms]
  Jul 15 12:19:40.581: INFO: Created: latency-svc-scst8
  Jul 15 12:19:40.585: INFO: Got endpoints: latency-svc-scst8 [90.986375ms]
  Jul 15 12:19:40.586: INFO: Created: latency-svc-2kkf2
  Jul 15 12:19:40.591: INFO: Got endpoints: latency-svc-2kkf2 [96.575499ms]
  Jul 15 12:19:40.593: INFO: Created: latency-svc-8l8jf
  Jul 15 12:19:40.596: INFO: Got endpoints: latency-svc-8l8jf [101.481404ms]
  Jul 15 12:19:40.598: INFO: Created: latency-svc-q2b5m
  Jul 15 12:19:40.601: INFO: Got endpoints: latency-svc-q2b5m [95.515708ms]
  Jul 15 12:19:40.605: INFO: Created: latency-svc-b796k
  Jul 15 12:19:40.608: INFO: Got endpoints: latency-svc-b796k [96.549116ms]
  Jul 15 12:19:40.610: INFO: Created: latency-svc-2qw26
  Jul 15 12:19:40.614: INFO: Got endpoints: latency-svc-2qw26 [94.594068ms]
  Jul 15 12:19:40.616: INFO: Created: latency-svc-48wgk
  Jul 15 12:19:40.620: INFO: Got endpoints: latency-svc-48wgk [94.344623ms]
  Jul 15 12:19:40.624: INFO: Created: latency-svc-vvp8d
  Jul 15 12:19:40.626: INFO: Got endpoints: latency-svc-vvp8d [92.434816ms]
  Jul 15 12:19:40.634: INFO: Created: latency-svc-qjvnq
  Jul 15 12:19:40.637: INFO: Got endpoints: latency-svc-qjvnq [98.067381ms]
  Jul 15 12:19:40.641: INFO: Created: latency-svc-n2chg
  Jul 15 12:19:40.646: INFO: Got endpoints: latency-svc-n2chg [99.863563ms]
  Jul 15 12:19:40.648: INFO: Created: latency-svc-pk6tj
  Jul 15 12:19:40.652: INFO: Got endpoints: latency-svc-pk6tj [100.066895ms]
  Jul 15 12:19:40.656: INFO: Created: latency-svc-xvk5m
  Jul 15 12:19:40.661: INFO: Got endpoints: latency-svc-xvk5m [98.469415ms]
  Jul 15 12:19:40.662: INFO: Created: latency-svc-9d6ps
  Jul 15 12:19:40.665: INFO: Got endpoints: latency-svc-9d6ps [98.594382ms]
  Jul 15 12:19:40.668: INFO: Created: latency-svc-62kl2
  Jul 15 12:19:40.670: INFO: Got endpoints: latency-svc-62kl2 [99.925956ms]
  Jul 15 12:19:40.673: INFO: Created: latency-svc-t6v4k
  Jul 15 12:19:40.677: INFO: Got endpoints: latency-svc-t6v4k [98.556279ms]
  Jul 15 12:19:40.684: INFO: Created: latency-svc-xscvn
  Jul 15 12:19:40.690: INFO: Got endpoints: latency-svc-xscvn [104.695794ms]
  Jul 15 12:19:40.691: INFO: Created: latency-svc-jm2l6
  Jul 15 12:19:40.694: INFO: Got endpoints: latency-svc-jm2l6 [103.669206ms]
  Jul 15 12:19:40.697: INFO: Created: latency-svc-4qcwm
  Jul 15 12:19:40.701: INFO: Got endpoints: latency-svc-4qcwm [105.018612ms]
  Jul 15 12:19:40.702: INFO: Created: latency-svc-vskt4
  Jul 15 12:19:40.707: INFO: Got endpoints: latency-svc-vskt4 [105.029152ms]
  Jul 15 12:19:40.712: INFO: Created: latency-svc-5qzl4
  Jul 15 12:19:40.731: INFO: Got endpoints: latency-svc-5qzl4 [122.596548ms]
  Jul 15 12:19:40.733: INFO: Created: latency-svc-t8mgr
  Jul 15 12:19:40.737: INFO: Got endpoints: latency-svc-t8mgr [122.948028ms]
  Jul 15 12:19:40.738: INFO: Created: latency-svc-pmhsc
  Jul 15 12:19:40.744: INFO: Got endpoints: latency-svc-pmhsc [123.811036ms]
  Jul 15 12:19:40.745: INFO: Created: latency-svc-xv5kc
  Jul 15 12:19:40.750: INFO: Created: latency-svc-lh89b
  Jul 15 12:19:40.754: INFO: Created: latency-svc-sgr98
  Jul 15 12:19:40.759: INFO: Created: latency-svc-d2nhm
  Jul 15 12:19:40.765: INFO: Created: latency-svc-tcdmd
  Jul 15 12:19:40.769: INFO: Created: latency-svc-p4lnh
  Jul 15 12:19:40.774: INFO: Created: latency-svc-p4jcm
  Jul 15 12:19:40.779: INFO: Created: latency-svc-nj4qt
  Jul 15 12:19:40.786: INFO: Created: latency-svc-mjsgf
  Jul 15 12:19:40.791: INFO: Created: latency-svc-j6qjh
  Jul 15 12:19:40.794: INFO: Got endpoints: latency-svc-xv5kc [167.726769ms]
  Jul 15 12:19:40.798: INFO: Created: latency-svc-4vwtg
  Jul 15 12:19:40.803: INFO: Created: latency-svc-wzvgv
  Jul 15 12:19:40.808: INFO: Created: latency-svc-5cfkx
  Jul 15 12:19:40.812: INFO: Created: latency-svc-xccqx
  Jul 15 12:19:40.816: INFO: Created: latency-svc-rrtj5
  Jul 15 12:19:40.821: INFO: Created: latency-svc-kmcdp
  Jul 15 12:19:40.843: INFO: Got endpoints: latency-svc-lh89b [206.463423ms]
  Jul 15 12:19:40.852: INFO: Created: latency-svc-ff5j5
  Jul 15 12:19:40.895: INFO: Got endpoints: latency-svc-sgr98 [249.179989ms]
  Jul 15 12:19:40.903: INFO: Created: latency-svc-7xxdp
  Jul 15 12:19:40.944: INFO: Got endpoints: latency-svc-d2nhm [290.973934ms]
  Jul 15 12:19:40.954: INFO: Created: latency-svc-44bsr
  Jul 15 12:19:40.992: INFO: Got endpoints: latency-svc-tcdmd [331.325838ms]
  Jul 15 12:19:41.002: INFO: Created: latency-svc-t4vns
  Jul 15 12:19:41.044: INFO: Got endpoints: latency-svc-p4lnh [379.012503ms]
  Jul 15 12:19:41.052: INFO: Created: latency-svc-vzn2q
  Jul 15 12:19:41.094: INFO: Got endpoints: latency-svc-p4jcm [423.68473ms]
  Jul 15 12:19:41.102: INFO: Created: latency-svc-9s4fj
  Jul 15 12:19:41.143: INFO: Got endpoints: latency-svc-nj4qt [466.675562ms]
  Jul 15 12:19:41.153: INFO: Created: latency-svc-qtgpv
  Jul 15 12:19:41.194: INFO: Got endpoints: latency-svc-mjsgf [504.179255ms]
  Jul 15 12:19:41.203: INFO: Created: latency-svc-z7jxn
  Jul 15 12:19:41.243: INFO: Got endpoints: latency-svc-j6qjh [548.701043ms]
  Jul 15 12:19:41.252: INFO: Created: latency-svc-jpnql
  Jul 15 12:19:41.293: INFO: Got endpoints: latency-svc-4vwtg [592.588965ms]
  Jul 15 12:19:41.302: INFO: Created: latency-svc-px67h
  Jul 15 12:19:41.343: INFO: Got endpoints: latency-svc-wzvgv [636.926793ms]
  Jul 15 12:19:41.352: INFO: Created: latency-svc-j4jqn
  Jul 15 12:19:41.394: INFO: Got endpoints: latency-svc-5cfkx [663.552286ms]
  Jul 15 12:19:41.403: INFO: Created: latency-svc-cx5cb
  Jul 15 12:19:41.443: INFO: Got endpoints: latency-svc-xccqx [705.495859ms]
  Jul 15 12:19:41.454: INFO: Created: latency-svc-wd8r5
  Jul 15 12:19:41.493: INFO: Got endpoints: latency-svc-rrtj5 [748.768618ms]
  Jul 15 12:19:41.501: INFO: Created: latency-svc-kn4dh
  Jul 15 12:19:41.544: INFO: Got endpoints: latency-svc-kmcdp [750.603511ms]
  Jul 15 12:19:41.553: INFO: Created: latency-svc-t6659
  Jul 15 12:19:41.594: INFO: Got endpoints: latency-svc-ff5j5 [750.871865ms]
  Jul 15 12:19:41.603: INFO: Created: latency-svc-5bvpl
  Jul 15 12:19:41.644: INFO: Got endpoints: latency-svc-7xxdp [749.327589ms]
  Jul 15 12:19:41.653: INFO: Created: latency-svc-mtjbz
  Jul 15 12:19:41.693: INFO: Got endpoints: latency-svc-44bsr [749.71226ms]
  Jul 15 12:19:41.703: INFO: Created: latency-svc-xrbvc
  Jul 15 12:19:41.743: INFO: Got endpoints: latency-svc-t4vns [750.519036ms]
  Jul 15 12:19:41.753: INFO: Created: latency-svc-clk67
  Jul 15 12:19:41.794: INFO: Got endpoints: latency-svc-vzn2q [749.810387ms]
  Jul 15 12:19:41.803: INFO: Created: latency-svc-jnwfl
  Jul 15 12:19:41.843: INFO: Got endpoints: latency-svc-9s4fj [749.595903ms]
  Jul 15 12:19:41.852: INFO: Created: latency-svc-456jr
  Jul 15 12:19:41.894: INFO: Got endpoints: latency-svc-qtgpv [750.271711ms]
  Jul 15 12:19:41.903: INFO: Created: latency-svc-rvssm
  Jul 15 12:19:41.943: INFO: Got endpoints: latency-svc-z7jxn [749.35152ms]
  Jul 15 12:19:41.953: INFO: Created: latency-svc-gb46n
  Jul 15 12:19:41.993: INFO: Got endpoints: latency-svc-jpnql [749.742942ms]
  Jul 15 12:19:42.002: INFO: Created: latency-svc-9nlv4
  Jul 15 12:19:42.044: INFO: Got endpoints: latency-svc-px67h [750.811702ms]
  Jul 15 12:19:42.054: INFO: Created: latency-svc-v9jwd
  Jul 15 12:19:42.094: INFO: Got endpoints: latency-svc-j4jqn [750.76689ms]
  Jul 15 12:19:42.103: INFO: Created: latency-svc-67d58
  Jul 15 12:19:42.145: INFO: Got endpoints: latency-svc-cx5cb [750.577958ms]
  Jul 15 12:19:42.155: INFO: Created: latency-svc-bgntt
  Jul 15 12:19:42.193: INFO: Got endpoints: latency-svc-wd8r5 [750.174566ms]
  Jul 15 12:19:42.203: INFO: Created: latency-svc-2gs5v
  Jul 15 12:19:42.244: INFO: Got endpoints: latency-svc-kn4dh [751.050606ms]
  Jul 15 12:19:42.252: INFO: Created: latency-svc-62687
  Jul 15 12:19:42.294: INFO: Got endpoints: latency-svc-t6659 [749.324978ms]
  Jul 15 12:19:42.303: INFO: Created: latency-svc-8s8fz
  Jul 15 12:19:42.345: INFO: Got endpoints: latency-svc-5bvpl [750.875536ms]
  Jul 15 12:19:42.354: INFO: Created: latency-svc-5t7th
  Jul 15 12:19:42.395: INFO: Got endpoints: latency-svc-mtjbz [750.989802ms]
  Jul 15 12:19:42.403: INFO: Created: latency-svc-p65xm
  Jul 15 12:19:42.443: INFO: Got endpoints: latency-svc-xrbvc [749.72173ms]
  Jul 15 12:19:42.452: INFO: Created: latency-svc-cmtvd
  Jul 15 12:19:42.493: INFO: Got endpoints: latency-svc-clk67 [749.844618ms]
  Jul 15 12:19:42.501: INFO: Created: latency-svc-mmmb2
  Jul 15 12:19:42.545: INFO: Got endpoints: latency-svc-jnwfl [751.406785ms]
  Jul 15 12:19:42.554: INFO: Created: latency-svc-jjfmv
  Jul 15 12:19:42.594: INFO: Got endpoints: latency-svc-456jr [750.765859ms]
  Jul 15 12:19:42.606: INFO: Created: latency-svc-kjds6
  Jul 15 12:19:42.642: INFO: Got endpoints: latency-svc-rvssm [748.462481ms]
  Jul 15 12:19:42.652: INFO: Created: latency-svc-ffd22
  Jul 15 12:19:42.694: INFO: Got endpoints: latency-svc-gb46n [750.361346ms]
  Jul 15 12:19:42.703: INFO: Created: latency-svc-xw4sm
  Jul 15 12:19:42.744: INFO: Got endpoints: latency-svc-9nlv4 [751.380444ms]
  Jul 15 12:19:42.753: INFO: Created: latency-svc-45z7w
  Jul 15 12:19:42.794: INFO: Got endpoints: latency-svc-v9jwd [750.168896ms]
  Jul 15 12:19:42.803: INFO: Created: latency-svc-rqnhf
  Jul 15 12:19:42.844: INFO: Got endpoints: latency-svc-67d58 [749.844388ms]
  Jul 15 12:19:42.853: INFO: Created: latency-svc-shf7x
  Jul 15 12:19:42.892: INFO: Got endpoints: latency-svc-bgntt [747.280334ms]
  Jul 15 12:19:42.902: INFO: Created: latency-svc-4sjxh
  Jul 15 12:19:42.943: INFO: Got endpoints: latency-svc-2gs5v [749.971356ms]
  Jul 15 12:19:42.952: INFO: Created: latency-svc-8s9v6
  Jul 15 12:19:42.995: INFO: Got endpoints: latency-svc-62687 [751.084037ms]
  Jul 15 12:19:43.004: INFO: Created: latency-svc-dchd6
  Jul 15 12:19:43.043: INFO: Got endpoints: latency-svc-8s8fz [749.612455ms]
  Jul 15 12:19:43.052: INFO: Created: latency-svc-rzzlj
  Jul 15 12:19:43.093: INFO: Got endpoints: latency-svc-5t7th [747.797993ms]
  Jul 15 12:19:43.103: INFO: Created: latency-svc-nlx2m
  Jul 15 12:19:43.143: INFO: Got endpoints: latency-svc-p65xm [748.030356ms]
  Jul 15 12:19:43.154: INFO: Created: latency-svc-5xxts
  Jul 15 12:19:43.193: INFO: Got endpoints: latency-svc-cmtvd [749.458857ms]
  Jul 15 12:19:43.202: INFO: Created: latency-svc-9pbns
  Jul 15 12:19:43.244: INFO: Got endpoints: latency-svc-mmmb2 [750.943709ms]
  Jul 15 12:19:43.253: INFO: Created: latency-svc-kr6nq
  Jul 15 12:19:43.294: INFO: Got endpoints: latency-svc-jjfmv [749.187681ms]
  Jul 15 12:19:43.303: INFO: Created: latency-svc-qwq7x
  Jul 15 12:19:43.344: INFO: Got endpoints: latency-svc-kjds6 [749.355131ms]
  Jul 15 12:19:43.351: INFO: Created: latency-svc-zwvhv
  Jul 15 12:19:43.394: INFO: Got endpoints: latency-svc-ffd22 [751.591696ms]
  Jul 15 12:19:43.408: INFO: Created: latency-svc-9f9xj
  Jul 15 12:19:43.445: INFO: Got endpoints: latency-svc-xw4sm [751.038085ms]
  Jul 15 12:19:43.453: INFO: Created: latency-svc-895s2
  Jul 15 12:19:43.494: INFO: Got endpoints: latency-svc-45z7w [749.583614ms]
  Jul 15 12:19:43.502: INFO: Created: latency-svc-gjdjq
  Jul 15 12:19:43.544: INFO: Got endpoints: latency-svc-rqnhf [749.063253ms]
  Jul 15 12:19:43.553: INFO: Created: latency-svc-g8wfk
  Jul 15 12:19:43.594: INFO: Got endpoints: latency-svc-shf7x [750.088792ms]
  Jul 15 12:19:43.604: INFO: Created: latency-svc-frck9
  Jul 15 12:19:43.645: INFO: Got endpoints: latency-svc-4sjxh [752.405362ms]
  Jul 15 12:19:43.652: INFO: Created: latency-svc-bjkzf
  Jul 15 12:19:43.693: INFO: Got endpoints: latency-svc-8s9v6 [750.166666ms]
  Jul 15 12:19:43.705: INFO: Created: latency-svc-4zxwr
  Jul 15 12:19:43.743: INFO: Got endpoints: latency-svc-dchd6 [747.961923ms]
  Jul 15 12:19:43.753: INFO: Created: latency-svc-66zsf
  Jul 15 12:19:43.793: INFO: Got endpoints: latency-svc-rzzlj [749.775984ms]
  Jul 15 12:19:43.800: INFO: Created: latency-svc-cgrzh
  Jul 15 12:19:43.843: INFO: Got endpoints: latency-svc-nlx2m [750.518735ms]
  Jul 15 12:19:43.853: INFO: Created: latency-svc-7955z
  Jul 15 12:19:43.894: INFO: Got endpoints: latency-svc-5xxts [750.640233ms]
  Jul 15 12:19:43.903: INFO: Created: latency-svc-qds4v
  Jul 15 12:19:43.943: INFO: Got endpoints: latency-svc-9pbns [750.364037ms]
  Jul 15 12:19:43.952: INFO: Created: latency-svc-5bs7t
  Jul 15 12:19:43.993: INFO: Got endpoints: latency-svc-kr6nq [749.489788ms]
  Jul 15 12:19:44.002: INFO: Created: latency-svc-vzzg4
  Jul 15 12:19:44.044: INFO: Got endpoints: latency-svc-qwq7x [749.637547ms]
  Jul 15 12:19:44.053: INFO: Created: latency-svc-vcgp7
  Jul 15 12:19:44.094: INFO: Got endpoints: latency-svc-zwvhv [750.593009ms]
  Jul 15 12:19:44.102: INFO: Created: latency-svc-2gm85
  Jul 15 12:19:44.144: INFO: Got endpoints: latency-svc-9f9xj [749.909201ms]
  Jul 15 12:19:44.156: INFO: Created: latency-svc-p8hrs
  Jul 15 12:19:44.192: INFO: Got endpoints: latency-svc-895s2 [747.55304ms]
  Jul 15 12:19:44.202: INFO: Created: latency-svc-cbfbn
  Jul 15 12:19:44.243: INFO: Got endpoints: latency-svc-gjdjq [749.615335ms]
  Jul 15 12:19:44.251: INFO: Created: latency-svc-6cqtt
  Jul 15 12:19:44.294: INFO: Got endpoints: latency-svc-g8wfk [750.107603ms]
  Jul 15 12:19:44.303: INFO: Created: latency-svc-5lmzd
  Jul 15 12:19:44.344: INFO: Got endpoints: latency-svc-frck9 [749.098486ms]
  Jul 15 12:19:44.352: INFO: Created: latency-svc-kh5nd
  Jul 15 12:19:44.394: INFO: Got endpoints: latency-svc-bjkzf [749.671878ms]
  Jul 15 12:19:44.401: INFO: Created: latency-svc-jbfrd
  Jul 15 12:19:44.443: INFO: Got endpoints: latency-svc-4zxwr [750.074061ms]
  Jul 15 12:19:44.454: INFO: Created: latency-svc-brk9z
  Jul 15 12:19:44.493: INFO: Got endpoints: latency-svc-66zsf [749.52603ms]
  Jul 15 12:19:44.502: INFO: Created: latency-svc-wbmmm
  Jul 15 12:19:44.545: INFO: Got endpoints: latency-svc-cgrzh [751.717272ms]
  Jul 15 12:19:44.552: INFO: Created: latency-svc-ktb6j
  Jul 15 12:19:44.595: INFO: Got endpoints: latency-svc-7955z [751.252937ms]
  Jul 15 12:19:44.606: INFO: Created: latency-svc-nsfg8
  Jul 15 12:19:44.643: INFO: Got endpoints: latency-svc-qds4v [749.52584ms]
  Jul 15 12:19:44.654: INFO: Created: latency-svc-r9kws
  Jul 15 12:19:44.694: INFO: Got endpoints: latency-svc-5bs7t [750.819102ms]
  Jul 15 12:19:44.702: INFO: Created: latency-svc-9whrh
  Jul 15 12:19:44.743: INFO: Got endpoints: latency-svc-vzzg4 [750.274982ms]
  Jul 15 12:19:44.753: INFO: Created: latency-svc-8hrzw
  Jul 15 12:19:44.794: INFO: Got endpoints: latency-svc-vcgp7 [749.515189ms]
  Jul 15 12:19:44.803: INFO: Created: latency-svc-nt8xj
  Jul 15 12:19:44.844: INFO: Got endpoints: latency-svc-2gm85 [749.500228ms]
  Jul 15 12:19:44.851: INFO: Created: latency-svc-9s7dj
  Jul 15 12:19:44.893: INFO: Got endpoints: latency-svc-p8hrs [748.704424ms]
  Jul 15 12:19:44.905: INFO: Created: latency-svc-x5ttf
  Jul 15 12:19:44.943: INFO: Got endpoints: latency-svc-cbfbn [750.129734ms]
  Jul 15 12:19:44.951: INFO: Created: latency-svc-rj5qw
  Jul 15 12:19:44.994: INFO: Got endpoints: latency-svc-6cqtt [750.685274ms]
  Jul 15 12:19:45.002: INFO: Created: latency-svc-pjr85
  Jul 15 12:19:45.044: INFO: Got endpoints: latency-svc-5lmzd [750.618901ms]
  Jul 15 12:19:45.055: INFO: Created: latency-svc-whnd6
  Jul 15 12:19:45.093: INFO: Got endpoints: latency-svc-kh5nd [749.082785ms]
  Jul 15 12:19:45.102: INFO: Created: latency-svc-f84n9
  Jul 15 12:19:45.144: INFO: Got endpoints: latency-svc-jbfrd [748.99763ms]
  Jul 15 12:19:45.152: INFO: Created: latency-svc-s55nv
  Jul 15 12:19:45.193: INFO: Got endpoints: latency-svc-brk9z [749.5205ms]
  Jul 15 12:19:45.203: INFO: Created: latency-svc-7zv68
  Jul 15 12:19:45.244: INFO: Got endpoints: latency-svc-wbmmm [751.212814ms]
  Jul 15 12:19:45.253: INFO: Created: latency-svc-m5kfb
  Jul 15 12:19:45.295: INFO: Got endpoints: latency-svc-ktb6j [749.753763ms]
  Jul 15 12:19:45.302: INFO: Created: latency-svc-8j9qf
  Jul 15 12:19:45.345: INFO: Got endpoints: latency-svc-nsfg8 [750.06686ms]
  Jul 15 12:19:45.355: INFO: Created: latency-svc-m2ms2
  Jul 15 12:19:45.394: INFO: Got endpoints: latency-svc-r9kws [750.381527ms]
  Jul 15 12:19:45.403: INFO: Created: latency-svc-n9ws8
  Jul 15 12:19:45.443: INFO: Got endpoints: latency-svc-9whrh [749.328149ms]
  Jul 15 12:19:45.452: INFO: Created: latency-svc-d59bq
  Jul 15 12:19:45.494: INFO: Got endpoints: latency-svc-8hrzw [750.571318ms]
  Jul 15 12:19:45.504: INFO: Created: latency-svc-pdgtt
  Jul 15 12:19:45.545: INFO: Got endpoints: latency-svc-nt8xj [750.979432ms]
  Jul 15 12:19:45.554: INFO: Created: latency-svc-jsn4z
  Jul 15 12:19:45.594: INFO: Got endpoints: latency-svc-9s7dj [749.780375ms]
  Jul 15 12:19:45.602: INFO: Created: latency-svc-wz86p
  Jul 15 12:19:45.644: INFO: Got endpoints: latency-svc-x5ttf [751.278148ms]
  Jul 15 12:19:45.653: INFO: Created: latency-svc-df4lp
  Jul 15 12:19:45.694: INFO: Got endpoints: latency-svc-rj5qw [751.510431ms]
  Jul 15 12:19:45.703: INFO: Created: latency-svc-mnlxv
  Jul 15 12:19:45.743: INFO: Got endpoints: latency-svc-pjr85 [748.99408ms]
  Jul 15 12:19:45.753: INFO: Created: latency-svc-gsnpv
  Jul 15 12:19:45.793: INFO: Got endpoints: latency-svc-whnd6 [748.328723ms]
  Jul 15 12:19:45.801: INFO: Created: latency-svc-fzffm
  Jul 15 12:19:45.845: INFO: Got endpoints: latency-svc-f84n9 [752.533659ms]
  Jul 15 12:19:45.855: INFO: Created: latency-svc-jsdxf
  Jul 15 12:19:45.894: INFO: Got endpoints: latency-svc-s55nv [750.77251ms]
  Jul 15 12:19:45.904: INFO: Created: latency-svc-c7wjd
  Jul 15 12:19:45.943: INFO: Got endpoints: latency-svc-7zv68 [749.614464ms]
  Jul 15 12:19:45.952: INFO: Created: latency-svc-dqp7m
  Jul 15 12:19:45.993: INFO: Got endpoints: latency-svc-m5kfb [749.197282ms]
  Jul 15 12:19:46.003: INFO: Created: latency-svc-4kwgv
  Jul 15 12:19:46.043: INFO: Got endpoints: latency-svc-8j9qf [748.64262ms]
  Jul 15 12:19:46.052: INFO: Created: latency-svc-8hjxk
  Jul 15 12:19:46.094: INFO: Got endpoints: latency-svc-m2ms2 [749.066783ms]
  Jul 15 12:19:46.103: INFO: Created: latency-svc-98r75
  Jul 15 12:19:46.145: INFO: Got endpoints: latency-svc-n9ws8 [750.458383ms]
  Jul 15 12:19:46.153: INFO: Created: latency-svc-cv8z9
  Jul 15 12:19:46.194: INFO: Got endpoints: latency-svc-d59bq [750.848375ms]
  Jul 15 12:19:46.202: INFO: Created: latency-svc-wslgl
  Jul 15 12:19:46.242: INFO: Got endpoints: latency-svc-pdgtt [748.422008ms]
  Jul 15 12:19:46.252: INFO: Created: latency-svc-7n8mc
  Jul 15 12:19:46.293: INFO: Got endpoints: latency-svc-jsn4z [748.391626ms]
  Jul 15 12:19:46.302: INFO: Created: latency-svc-tpfqt
  Jul 15 12:19:46.342: INFO: Got endpoints: latency-svc-wz86p [748.518353ms]
  Jul 15 12:19:46.351: INFO: Created: latency-svc-bsbh9
  Jul 15 12:19:46.394: INFO: Got endpoints: latency-svc-df4lp [750.154165ms]
  Jul 15 12:19:46.403: INFO: Created: latency-svc-cz724
  Jul 15 12:19:46.444: INFO: Got endpoints: latency-svc-mnlxv [749.846008ms]
  Jul 15 12:19:46.453: INFO: Created: latency-svc-szp8w
  Jul 15 12:19:46.493: INFO: Got endpoints: latency-svc-gsnpv [749.816106ms]
  Jul 15 12:19:46.508: INFO: Created: latency-svc-z8bc2
  Jul 15 12:19:46.543: INFO: Got endpoints: latency-svc-fzffm [750.314524ms]
  Jul 15 12:19:46.552: INFO: Created: latency-svc-w9h4j
  Jul 15 12:19:46.594: INFO: Got endpoints: latency-svc-jsdxf [748.960209ms]
  Jul 15 12:19:46.603: INFO: Created: latency-svc-7xscq
  Jul 15 12:19:46.643: INFO: Got endpoints: latency-svc-c7wjd [748.076989ms]
  Jul 15 12:19:46.652: INFO: Created: latency-svc-hnqng
  Jul 15 12:19:46.695: INFO: Got endpoints: latency-svc-dqp7m [752.042022ms]
  Jul 15 12:19:46.703: INFO: Created: latency-svc-zjczp
  Jul 15 12:19:46.743: INFO: Got endpoints: latency-svc-4kwgv [750.127743ms]
  Jul 15 12:19:46.753: INFO: Created: latency-svc-rvsw9
  Jul 15 12:19:46.794: INFO: Got endpoints: latency-svc-8hjxk [750.107933ms]
  Jul 15 12:19:46.802: INFO: Created: latency-svc-phkzg
  Jul 15 12:19:46.844: INFO: Got endpoints: latency-svc-98r75 [749.722161ms]
  Jul 15 12:19:46.853: INFO: Created: latency-svc-cxgnr
  Jul 15 12:19:46.894: INFO: Got endpoints: latency-svc-cv8z9 [749.071024ms]
  Jul 15 12:19:46.902: INFO: Created: latency-svc-ncbwm
  Jul 15 12:19:46.943: INFO: Got endpoints: latency-svc-wslgl [748.896284ms]
  Jul 15 12:19:46.952: INFO: Created: latency-svc-qq2gd
  Jul 15 12:19:46.992: INFO: Got endpoints: latency-svc-7n8mc [749.933974ms]
  Jul 15 12:19:47.001: INFO: Created: latency-svc-lbvmd
  Jul 15 12:19:47.045: INFO: Got endpoints: latency-svc-tpfqt [751.437047ms]
  Jul 15 12:19:47.054: INFO: Created: latency-svc-9f4pj
  Jul 15 12:19:47.094: INFO: Got endpoints: latency-svc-bsbh9 [751.83777ms]
  Jul 15 12:19:47.104: INFO: Created: latency-svc-p25bp
  Jul 15 12:19:47.143: INFO: Got endpoints: latency-svc-cz724 [748.97888ms]
  Jul 15 12:19:47.152: INFO: Created: latency-svc-hgvj5
  Jul 15 12:19:47.194: INFO: Got endpoints: latency-svc-szp8w [749.900891ms]
  Jul 15 12:19:47.202: INFO: Created: latency-svc-dbm2t
  Jul 15 12:19:47.243: INFO: Got endpoints: latency-svc-z8bc2 [749.598214ms]
  Jul 15 12:19:47.252: INFO: Created: latency-svc-pvnlg
  Jul 15 12:19:47.293: INFO: Got endpoints: latency-svc-w9h4j [750.168797ms]
  Jul 15 12:19:47.302: INFO: Created: latency-svc-gll2c
  Jul 15 12:19:47.344: INFO: Got endpoints: latency-svc-7xscq [750.05456ms]
  Jul 15 12:19:47.353: INFO: Created: latency-svc-rb5jv
  Jul 15 12:19:47.394: INFO: Got endpoints: latency-svc-hnqng [751.468199ms]
  Jul 15 12:19:47.403: INFO: Created: latency-svc-l9wmc
  Jul 15 12:19:47.443: INFO: Got endpoints: latency-svc-zjczp [747.950891ms]
  Jul 15 12:19:47.452: INFO: Created: latency-svc-r4w5c
  Jul 15 12:19:47.493: INFO: Got endpoints: latency-svc-rvsw9 [750.040968ms]
  Jul 15 12:19:47.502: INFO: Created: latency-svc-75nqs
  Jul 15 12:19:47.544: INFO: Got endpoints: latency-svc-phkzg [750.721598ms]
  Jul 15 12:19:47.555: INFO: Created: latency-svc-m2k76
  Jul 15 12:19:47.594: INFO: Got endpoints: latency-svc-cxgnr [750.452351ms]
  Jul 15 12:19:47.604: INFO: Created: latency-svc-l2l6b
  Jul 15 12:19:47.644: INFO: Got endpoints: latency-svc-ncbwm [750.197678ms]
  Jul 15 12:19:47.653: INFO: Created: latency-svc-jhjz2
  Jul 15 12:19:47.693: INFO: Got endpoints: latency-svc-qq2gd [749.181271ms]
  Jul 15 12:19:47.813: INFO: Created: latency-svc-xmg7z
  Jul 15 12:19:47.813: INFO: Got endpoints: latency-svc-lbvmd [820.803058ms]
  Jul 15 12:19:47.814: INFO: Got endpoints: latency-svc-9f4pj [769.636748ms]
  Jul 15 12:19:47.823: INFO: Created: latency-svc-lw9w6
  Jul 15 12:19:47.827: INFO: Created: latency-svc-xb768
  Jul 15 12:19:47.842: INFO: Got endpoints: latency-svc-p25bp [747.642784ms]
  Jul 15 12:19:47.850: INFO: Created: latency-svc-zr8pt
  Jul 15 12:19:47.895: INFO: Got endpoints: latency-svc-hgvj5 [751.12557ms]
  Jul 15 12:19:47.904: INFO: Created: latency-svc-z26l8
  Jul 15 12:19:47.944: INFO: Got endpoints: latency-svc-dbm2t [749.378342ms]
  Jul 15 12:19:47.953: INFO: Created: latency-svc-85f6h
  Jul 15 12:19:47.993: INFO: Got endpoints: latency-svc-pvnlg [750.090042ms]
  Jul 15 12:19:48.001: INFO: Created: latency-svc-5v4pd
  Jul 15 12:19:48.044: INFO: Got endpoints: latency-svc-gll2c [750.657883ms]
  Jul 15 12:19:48.053: INFO: Created: latency-svc-dkqs9
  Jul 15 12:19:48.094: INFO: Got endpoints: latency-svc-rb5jv [749.643946ms]
  Jul 15 12:19:48.102: INFO: Created: latency-svc-ptgjz
  Jul 15 12:19:48.145: INFO: Got endpoints: latency-svc-l9wmc [750.218518ms]
  Jul 15 12:19:48.154: INFO: Created: latency-svc-5g9kz
  Jul 15 12:19:48.192: INFO: Got endpoints: latency-svc-r4w5c [749.485978ms]
  Jul 15 12:19:48.202: INFO: Created: latency-svc-mvm9j
  Jul 15 12:19:48.243: INFO: Got endpoints: latency-svc-75nqs [749.921022ms]
  Jul 15 12:19:48.254: INFO: Created: latency-svc-rpvv7
  Jul 15 12:19:48.293: INFO: Got endpoints: latency-svc-m2k76 [748.693083ms]
  Jul 15 12:19:48.303: INFO: Created: latency-svc-tfgv4
  Jul 15 12:19:48.344: INFO: Got endpoints: latency-svc-l2l6b [749.35524ms]
  Jul 15 12:19:48.393: INFO: Got endpoints: latency-svc-jhjz2 [748.126922ms]
  Jul 15 12:19:48.444: INFO: Got endpoints: latency-svc-xmg7z [751.891233ms]
  Jul 15 12:19:48.494: INFO: Got endpoints: latency-svc-lw9w6 [680.859287ms]
  Jul 15 12:19:48.545: INFO: Got endpoints: latency-svc-xb768 [730.147403ms]
  Jul 15 12:19:48.593: INFO: Got endpoints: latency-svc-zr8pt [750.484314ms]
  Jul 15 12:19:48.644: INFO: Got endpoints: latency-svc-z26l8 [749.515209ms]
  Jul 15 12:19:48.694: INFO: Got endpoints: latency-svc-85f6h [749.997687ms]
  Jul 15 12:19:48.744: INFO: Got endpoints: latency-svc-5v4pd [751.13738ms]
  Jul 15 12:19:48.792: INFO: Got endpoints: latency-svc-dkqs9 [748.296631ms]
  Jul 15 12:19:48.844: INFO: Got endpoints: latency-svc-ptgjz [750.155246ms]
  Jul 15 12:19:48.893: INFO: Got endpoints: latency-svc-5g9kz [748.235127ms]
  Jul 15 12:19:48.943: INFO: Got endpoints: latency-svc-mvm9j [751.141841ms]
  Jul 15 12:19:48.993: INFO: Got endpoints: latency-svc-rpvv7 [749.758583ms]
  Jul 15 12:19:49.044: INFO: Got endpoints: latency-svc-tfgv4 [750.622331ms]
  Jul 15 12:19:49.044: INFO: Latencies: [12.270168ms 17.81368ms 26.02374ms 32.222468ms 39.625803ms 44.767682ms 51.86132ms 58.489901ms 68.429669ms 72.357469ms 76.203706ms 84.056876ms 90.986375ms 92.434816ms 94.344623ms 94.594068ms 95.515708ms 96.549116ms 96.575499ms 98.067381ms 98.469415ms 98.556279ms 98.594382ms 99.863563ms 99.925956ms 100.066895ms 101.481404ms 103.669206ms 104.695794ms 105.018612ms 105.029152ms 122.596548ms 122.948028ms 123.811036ms 167.726769ms 206.463423ms 249.179989ms 290.973934ms 331.325838ms 379.012503ms 423.68473ms 466.675562ms 504.179255ms 548.701043ms 592.588965ms 636.926793ms 663.552286ms 680.859287ms 705.495859ms 730.147403ms 747.280334ms 747.55304ms 747.642784ms 747.797993ms 747.950891ms 747.961923ms 748.030356ms 748.076989ms 748.126922ms 748.235127ms 748.296631ms 748.328723ms 748.391626ms 748.422008ms 748.462481ms 748.518353ms 748.64262ms 748.693083ms 748.704424ms 748.768618ms 748.896284ms 748.960209ms 748.97888ms 748.99408ms 748.99763ms 749.063253ms 749.066783ms 749.071024ms 749.082785ms 749.098486ms 749.181271ms 749.187681ms 749.197282ms 749.324978ms 749.327589ms 749.328149ms 749.35152ms 749.355131ms 749.35524ms 749.378342ms 749.458857ms 749.485978ms 749.489788ms 749.500228ms 749.515189ms 749.515209ms 749.5205ms 749.52584ms 749.52603ms 749.583614ms 749.595903ms 749.598214ms 749.612455ms 749.614464ms 749.615335ms 749.637547ms 749.643946ms 749.671878ms 749.71226ms 749.72173ms 749.722161ms 749.742942ms 749.753763ms 749.758583ms 749.775984ms 749.780375ms 749.810387ms 749.816106ms 749.844388ms 749.844618ms 749.846008ms 749.900891ms 749.909201ms 749.921022ms 749.933974ms 749.971356ms 749.997687ms 750.040968ms 750.05456ms 750.06686ms 750.074061ms 750.088792ms 750.090042ms 750.107603ms 750.107933ms 750.127743ms 750.129734ms 750.154165ms 750.155246ms 750.166666ms 750.168797ms 750.168896ms 750.174566ms 750.197678ms 750.218518ms 750.271711ms 750.274982ms 750.314524ms 750.361346ms 750.364037ms 750.381527ms 750.452351ms 750.458383ms 750.484314ms 750.518735ms 750.519036ms 750.571318ms 750.577958ms 750.593009ms 750.603511ms 750.618901ms 750.622331ms 750.640233ms 750.657883ms 750.685274ms 750.721598ms 750.765859ms 750.76689ms 750.77251ms 750.811702ms 750.819102ms 750.848375ms 750.871865ms 750.875536ms 750.943709ms 750.979432ms 750.989802ms 751.038085ms 751.050606ms 751.084037ms 751.12557ms 751.13738ms 751.141841ms 751.212814ms 751.252937ms 751.278148ms 751.380444ms 751.406785ms 751.437047ms 751.468199ms 751.510431ms 751.591696ms 751.717272ms 751.83777ms 751.891233ms 752.042022ms 752.405362ms 752.533659ms 769.636748ms 820.803058ms]
  Jul 15 12:19:49.044: INFO: 50 %ile: 749.595903ms
  Jul 15 12:19:49.044: INFO: 90 %ile: 751.12557ms
  Jul 15 12:19:49.044: INFO: 99 %ile: 769.636748ms
  Jul 15 12:19:49.044: INFO: Total sample count: 200
  Jul 15 12:19:49.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-7493" for this suite. @ 07/15/23 12:19:49.05
• [10.753 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 07/15/23 12:19:49.055
  Jul 15 12:19:49.055: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename statefulset @ 07/15/23 12:19:49.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:19:49.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:19:49.072
  STEP: Creating service test in namespace statefulset-65 @ 07/15/23 12:19:49.075
  STEP: Creating statefulset ss in namespace statefulset-65 @ 07/15/23 12:19:49.079
  Jul 15 12:19:49.088: INFO: Found 0 stateful pods, waiting for 1
  Jul 15 12:19:59.093: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 07/15/23 12:19:59.098
  STEP: updating a scale subresource @ 07/15/23 12:19:59.102
  STEP: verifying the statefulset Spec.Replicas was modified @ 07/15/23 12:19:59.107
  STEP: Patch a scale subresource @ 07/15/23 12:19:59.11
  STEP: verifying the statefulset Spec.Replicas was modified @ 07/15/23 12:19:59.117
  Jul 15 12:19:59.119: INFO: Deleting all statefulset in ns statefulset-65
  Jul 15 12:19:59.124: INFO: Scaling statefulset ss to 0
  Jul 15 12:20:09.142: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 15 12:20:09.145: INFO: Deleting statefulset ss
  Jul 15 12:20:09.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-65" for this suite. @ 07/15/23 12:20:09.161
• [20.112 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 07/15/23 12:20:09.168
  Jul 15 12:20:09.168: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-probe @ 07/15/23 12:20:09.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:20:09.181
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:20:09.184
  STEP: Creating pod test-grpc-b8fd1559-5c03-4be8-b561-38a37addf1cd in namespace container-probe-4740 @ 07/15/23 12:20:09.187
  Jul 15 12:20:11.205: INFO: Started pod test-grpc-b8fd1559-5c03-4be8-b561-38a37addf1cd in namespace container-probe-4740
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/15/23 12:20:11.205
  Jul 15 12:20:11.208: INFO: Initial restart count of pod test-grpc-b8fd1559-5c03-4be8-b561-38a37addf1cd is 0
  Jul 15 12:24:11.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/15/23 12:24:11.828
  STEP: Destroying namespace "container-probe-4740" for this suite. @ 07/15/23 12:24:11.838
• [242.678 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 07/15/23 12:24:11.846
  Jul 15 12:24:11.846: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename namespaces @ 07/15/23 12:24:11.847
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:24:11.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:24:11.862
  STEP: Creating a test namespace @ 07/15/23 12:24:11.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:24:11.876
  STEP: Creating a service in the namespace @ 07/15/23 12:24:11.878
  STEP: Deleting the namespace @ 07/15/23 12:24:11.887
  STEP: Waiting for the namespace to be removed. @ 07/15/23 12:24:11.893
  STEP: Recreating the namespace @ 07/15/23 12:24:17.899
  STEP: Verifying there is no service in the namespace @ 07/15/23 12:24:17.911
  Jul 15 12:24:17.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1904" for this suite. @ 07/15/23 12:24:17.921
  STEP: Destroying namespace "nsdeletetest-8726" for this suite. @ 07/15/23 12:24:17.927
  Jul 15 12:24:17.929: INFO: Namespace nsdeletetest-8726 was already deleted
  STEP: Destroying namespace "nsdeletetest-5937" for this suite. @ 07/15/23 12:24:17.929
• [6.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 07/15/23 12:24:17.936
  Jul 15 12:24:17.936: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename daemonsets @ 07/15/23 12:24:17.937
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:24:17.947
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:24:17.951
  STEP: Creating simple DaemonSet "daemon-set" @ 07/15/23 12:24:17.971
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/15/23 12:24:17.977
  Jul 15 12:24:17.980: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:17.980: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:17.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:24:17.982: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  Jul 15 12:24:18.987: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:18.987: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:18.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 15 12:24:18.989: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  Jul 15 12:24:19.989: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:19.989: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:19.992: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 15 12:24:19.992: INFO: Node ip-172-31-84-236 is running 0 daemon pod, expected 1
  Jul 15 12:24:20.987: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:20.987: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:20.990: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 15 12:24:20.990: INFO: Node ip-172-31-84-236 is running 0 daemon pod, expected 1
  Jul 15 12:24:21.987: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:21.987: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:21.990: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 15 12:24:21.990: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 07/15/23 12:24:21.994
  Jul 15 12:24:22.009: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:22.009: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:22.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 15 12:24:22.011: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  Jul 15 12:24:23.016: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:23.016: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:23.019: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 15 12:24:23.019: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  Jul 15 12:24:24.017: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:24.017: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:24.020: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 15 12:24:24.020: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  Jul 15 12:24:25.016: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:25.016: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:24:25.019: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 15 12:24:25.019: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/15/23 12:24:25.022
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-147, will wait for the garbage collector to delete the pods @ 07/15/23 12:24:25.022
  Jul 15 12:24:25.083: INFO: Deleting DaemonSet.extensions daemon-set took: 7.337432ms
  Jul 15 12:24:25.183: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.205347ms
  Jul 15 12:24:26.487: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:24:26.487: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 15 12:24:26.489: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10885"},"items":null}

  Jul 15 12:24:26.493: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10885"},"items":null}

  Jul 15 12:24:26.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-147" for this suite. @ 07/15/23 12:24:26.509
• [8.580 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 07/15/23 12:24:26.517
  Jul 15 12:24:26.517: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename downward-api @ 07/15/23 12:24:26.517
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:24:26.535
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:24:26.538
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 12:24:26.54
  STEP: Saw pod success @ 07/15/23 12:24:28.568
  Jul 15 12:24:28.571: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-7367ebbe-2ce8-474e-af1f-726e03f93ca4 container client-container: <nil>
  STEP: delete the pod @ 07/15/23 12:24:28.586
  Jul 15 12:24:28.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4813" for this suite. @ 07/15/23 12:24:28.605
• [2.095 seconds]
------------------------------
S
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 07/15/23 12:24:28.612
  Jul 15 12:24:28.612: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename proxy @ 07/15/23 12:24:28.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:24:28.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:24:28.625
  Jul 15 12:24:28.627: INFO: Creating pod...
  Jul 15 12:24:30.644: INFO: Creating service...
  Jul 15 12:24:30.655: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2184/pods/agnhost/proxy?method=DELETE
  Jul 15 12:24:30.661: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 15 12:24:30.661: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2184/pods/agnhost/proxy?method=OPTIONS
  Jul 15 12:24:30.664: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 15 12:24:30.664: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2184/pods/agnhost/proxy?method=PATCH
  Jul 15 12:24:30.668: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 15 12:24:30.668: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2184/pods/agnhost/proxy?method=POST
  Jul 15 12:24:30.671: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 15 12:24:30.671: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2184/pods/agnhost/proxy?method=PUT
  Jul 15 12:24:30.674: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 15 12:24:30.674: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2184/services/e2e-proxy-test-service/proxy?method=DELETE
  Jul 15 12:24:30.681: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 15 12:24:30.681: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2184/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Jul 15 12:24:30.686: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 15 12:24:30.686: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2184/services/e2e-proxy-test-service/proxy?method=PATCH
  Jul 15 12:24:30.690: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 15 12:24:30.690: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2184/services/e2e-proxy-test-service/proxy?method=POST
  Jul 15 12:24:30.697: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 15 12:24:30.697: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2184/services/e2e-proxy-test-service/proxy?method=PUT
  Jul 15 12:24:30.702: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 15 12:24:30.702: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2184/pods/agnhost/proxy?method=GET
  Jul 15 12:24:30.704: INFO: http.Client request:GET StatusCode:301
  Jul 15 12:24:30.704: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2184/services/e2e-proxy-test-service/proxy?method=GET
  Jul 15 12:24:30.709: INFO: http.Client request:GET StatusCode:301
  Jul 15 12:24:30.709: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2184/pods/agnhost/proxy?method=HEAD
  Jul 15 12:24:30.711: INFO: http.Client request:HEAD StatusCode:301
  Jul 15 12:24:30.711: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2184/services/e2e-proxy-test-service/proxy?method=HEAD
  Jul 15 12:24:30.716: INFO: http.Client request:HEAD StatusCode:301
  Jul 15 12:24:30.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-2184" for this suite. @ 07/15/23 12:24:30.721
• [2.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 07/15/23 12:24:30.728
  Jul 15 12:24:30.728: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/15/23 12:24:30.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:24:30.742
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:24:30.745
  STEP: fetching the /apis discovery document @ 07/15/23 12:24:30.747
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 07/15/23 12:24:30.748
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 07/15/23 12:24:30.748
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 07/15/23 12:24:30.748
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 07/15/23 12:24:30.75
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 07/15/23 12:24:30.75
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 07/15/23 12:24:30.751
  Jul 15 12:24:30.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4744" for this suite. @ 07/15/23 12:24:30.754
• [0.032 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 07/15/23 12:24:30.76
  Jul 15 12:24:30.760: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl @ 07/15/23 12:24:30.761
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:24:30.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:24:30.774
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/15/23 12:24:30.777
  Jul 15 12:24:30.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-838 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jul 15 12:24:30.826: INFO: stderr: ""
  Jul 15 12:24:30.826: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 07/15/23 12:24:30.826
  Jul 15 12:24:30.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-838 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Jul 15 12:24:30.875: INFO: stderr: ""
  Jul 15 12:24:30.875: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/15/23 12:24:30.875
  Jul 15 12:24:30.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-838 delete pods e2e-test-httpd-pod'
  Jul 15 12:24:33.268: INFO: stderr: ""
  Jul 15 12:24:33.268: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul 15 12:24:33.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-838" for this suite. @ 07/15/23 12:24:33.271
• [2.517 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 07/15/23 12:24:33.278
  Jul 15 12:24:33.278: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 12:24:33.278
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:24:33.289
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:24:33.292
  STEP: Setting up server cert @ 07/15/23 12:24:33.308
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 12:24:33.621
  STEP: Deploying the webhook pod @ 07/15/23 12:24:33.628
  STEP: Wait for the deployment to be ready @ 07/15/23 12:24:33.638
  Jul 15 12:24:33.654: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/15/23 12:24:35.666
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 12:24:35.677
  Jul 15 12:24:36.678: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 15 12:24:36.681: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2818-crds.webhook.example.com via the AdmissionRegistration API @ 07/15/23 12:24:37.191
  STEP: Creating a custom resource while v1 is storage version @ 07/15/23 12:24:37.205
  STEP: Patching Custom Resource Definition to set v2 as storage @ 07/15/23 12:24:39.233
  STEP: Patching the custom resource while v2 is storage version @ 07/15/23 12:24:39.245
  Jul 15 12:24:39.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9485" for this suite. @ 07/15/23 12:24:39.832
  STEP: Destroying namespace "webhook-markers-4297" for this suite. @ 07/15/23 12:24:39.841
• [6.569 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 07/15/23 12:24:39.849
  Jul 15 12:24:39.849: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/15/23 12:24:39.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:24:39.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:24:39.861
  Jul 15 12:24:39.864: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 12:24:46.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-7154" for this suite. @ 07/15/23 12:24:46.054
• [6.213 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 07/15/23 12:24:46.063
  Jul 15 12:24:46.063: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename resourcequota @ 07/15/23 12:24:46.063
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:24:46.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:24:46.076
  STEP: Counting existing ResourceQuota @ 07/15/23 12:25:03.084
  STEP: Creating a ResourceQuota @ 07/15/23 12:25:08.09
  STEP: Ensuring resource quota status is calculated @ 07/15/23 12:25:08.095
  STEP: Creating a ConfigMap @ 07/15/23 12:25:10.099
  STEP: Ensuring resource quota status captures configMap creation @ 07/15/23 12:25:10.109
  STEP: Deleting a ConfigMap @ 07/15/23 12:25:12.115
  STEP: Ensuring resource quota status released usage @ 07/15/23 12:25:12.121
  Jul 15 12:25:14.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2032" for this suite. @ 07/15/23 12:25:14.13
• [28.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 07/15/23 12:25:14.137
  Jul 15 12:25:14.137: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename downward-api @ 07/15/23 12:25:14.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:25:14.15
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:25:14.152
  STEP: Creating a pod to test downward api env vars @ 07/15/23 12:25:14.155
  STEP: Saw pod success @ 07/15/23 12:25:18.192
  Jul 15 12:25:18.195: INFO: Trying to get logs from node ip-172-31-16-190 pod downward-api-4577e207-55e0-41a0-9613-2dc7d4eb71e8 container dapi-container: <nil>
  STEP: delete the pod @ 07/15/23 12:25:18.204
  Jul 15 12:25:18.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3137" for this suite. @ 07/15/23 12:25:18.221
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 07/15/23 12:25:18.23
  Jul 15 12:25:18.230: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename services @ 07/15/23 12:25:18.231
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:25:18.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:25:18.245
  STEP: creating service endpoint-test2 in namespace services-7290 @ 07/15/23 12:25:18.248
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7290 to expose endpoints map[] @ 07/15/23 12:25:18.255
  Jul 15 12:25:18.261: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  Jul 15 12:25:19.270: INFO: successfully validated that service endpoint-test2 in namespace services-7290 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-7290 @ 07/15/23 12:25:19.27
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7290 to expose endpoints map[pod1:[80]] @ 07/15/23 12:25:21.291
  Jul 15 12:25:21.302: INFO: successfully validated that service endpoint-test2 in namespace services-7290 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 07/15/23 12:25:21.302
  Jul 15 12:25:21.302: INFO: Creating new exec pod
  Jul 15 12:25:24.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-7290 exec execpodzz968 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul 15 12:25:24.414: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul 15 12:25:24.415: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:25:24.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-7290 exec execpodzz968 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.170 80'
  Jul 15 12:25:24.517: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.170 80\nConnection to 10.152.183.170 80 port [tcp/http] succeeded!\n"
  Jul 15 12:25:24.517: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-7290 @ 07/15/23 12:25:24.517
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7290 to expose endpoints map[pod1:[80] pod2:[80]] @ 07/15/23 12:25:26.535
  Jul 15 12:25:26.548: INFO: successfully validated that service endpoint-test2 in namespace services-7290 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 07/15/23 12:25:26.548
  Jul 15 12:25:27.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-7290 exec execpodzz968 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul 15 12:25:27.646: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul 15 12:25:27.646: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:25:27.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-7290 exec execpodzz968 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.170 80'
  Jul 15 12:25:27.745: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.170 80\nConnection to 10.152.183.170 80 port [tcp/http] succeeded!\n"
  Jul 15 12:25:27.745: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-7290 @ 07/15/23 12:25:27.745
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7290 to expose endpoints map[pod2:[80]] @ 07/15/23 12:25:27.757
  Jul 15 12:25:28.781: INFO: successfully validated that service endpoint-test2 in namespace services-7290 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 07/15/23 12:25:28.781
  Jul 15 12:25:29.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-7290 exec execpodzz968 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul 15 12:25:29.884: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul 15 12:25:29.884: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:25:29.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-7290 exec execpodzz968 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.170 80'
  Jul 15 12:25:29.980: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.170 80\nConnection to 10.152.183.170 80 port [tcp/http] succeeded!\n"
  Jul 15 12:25:29.980: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-7290 @ 07/15/23 12:25:29.98
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7290 to expose endpoints map[] @ 07/15/23 12:25:29.99
  Jul 15 12:25:29.998: INFO: successfully validated that service endpoint-test2 in namespace services-7290 exposes endpoints map[]
  Jul 15 12:25:29.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7290" for this suite. @ 07/15/23 12:25:30.016
• [11.792 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 07/15/23 12:25:30.022
  Jul 15 12:25:30.023: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename proxy @ 07/15/23 12:25:30.023
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:25:30.033
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:25:30.036
  Jul 15 12:25:30.038: INFO: Creating pod...
  Jul 15 12:25:32.056: INFO: Creating service...
  Jul 15 12:25:32.065: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-4755/pods/agnhost/proxy/some/path/with/DELETE
  Jul 15 12:25:32.071: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 15 12:25:32.071: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-4755/pods/agnhost/proxy/some/path/with/GET
  Jul 15 12:25:32.077: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jul 15 12:25:32.077: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-4755/pods/agnhost/proxy/some/path/with/HEAD
  Jul 15 12:25:32.081: INFO: http.Client request:HEAD | StatusCode:200
  Jul 15 12:25:32.081: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-4755/pods/agnhost/proxy/some/path/with/OPTIONS
  Jul 15 12:25:32.084: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 15 12:25:32.084: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-4755/pods/agnhost/proxy/some/path/with/PATCH
  Jul 15 12:25:32.087: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 15 12:25:32.087: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-4755/pods/agnhost/proxy/some/path/with/POST
  Jul 15 12:25:32.092: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 15 12:25:32.092: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-4755/pods/agnhost/proxy/some/path/with/PUT
  Jul 15 12:25:32.096: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 15 12:25:32.096: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-4755/services/test-service/proxy/some/path/with/DELETE
  Jul 15 12:25:32.101: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 15 12:25:32.101: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-4755/services/test-service/proxy/some/path/with/GET
  Jul 15 12:25:32.108: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jul 15 12:25:32.108: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-4755/services/test-service/proxy/some/path/with/HEAD
  Jul 15 12:25:32.112: INFO: http.Client request:HEAD | StatusCode:200
  Jul 15 12:25:32.112: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-4755/services/test-service/proxy/some/path/with/OPTIONS
  Jul 15 12:25:32.117: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 15 12:25:32.117: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-4755/services/test-service/proxy/some/path/with/PATCH
  Jul 15 12:25:32.123: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 15 12:25:32.123: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-4755/services/test-service/proxy/some/path/with/POST
  Jul 15 12:25:32.127: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 15 12:25:32.127: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-4755/services/test-service/proxy/some/path/with/PUT
  Jul 15 12:25:32.133: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 15 12:25:32.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-4755" for this suite. @ 07/15/23 12:25:32.136
• [2.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 07/15/23 12:25:32.144
  Jul 15 12:25:32.144: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename replication-controller @ 07/15/23 12:25:32.144
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:25:32.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:25:32.161
  STEP: creating a ReplicationController @ 07/15/23 12:25:32.166
  STEP: waiting for RC to be added @ 07/15/23 12:25:32.17
  STEP: waiting for available Replicas @ 07/15/23 12:25:32.171
  STEP: patching ReplicationController @ 07/15/23 12:25:33.065
  STEP: waiting for RC to be modified @ 07/15/23 12:25:33.072
  STEP: patching ReplicationController status @ 07/15/23 12:25:33.072
  STEP: waiting for RC to be modified @ 07/15/23 12:25:33.076
  STEP: waiting for available Replicas @ 07/15/23 12:25:33.076
  STEP: fetching ReplicationController status @ 07/15/23 12:25:33.079
  STEP: patching ReplicationController scale @ 07/15/23 12:25:33.084
  STEP: waiting for RC to be modified @ 07/15/23 12:25:33.089
  STEP: waiting for ReplicationController's scale to be the max amount @ 07/15/23 12:25:33.089
  STEP: fetching ReplicationController; ensuring that it's patched @ 07/15/23 12:25:34.358
  STEP: updating ReplicationController status @ 07/15/23 12:25:34.361
  STEP: waiting for RC to be modified @ 07/15/23 12:25:34.367
  STEP: listing all ReplicationControllers @ 07/15/23 12:25:34.367
  STEP: checking that ReplicationController has expected values @ 07/15/23 12:25:34.37
  STEP: deleting ReplicationControllers by collection @ 07/15/23 12:25:34.37
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 07/15/23 12:25:34.378
  Jul 15 12:25:34.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0715 12:25:34.416851      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-6002" for this suite. @ 07/15/23 12:25:34.42
• [2.283 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 07/15/23 12:25:34.428
  Jul 15 12:25:34.428: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename dns @ 07/15/23 12:25:34.429
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:25:34.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:25:34.441
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 07/15/23 12:25:34.444
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 07/15/23 12:25:34.444
  STEP: creating a pod to probe DNS @ 07/15/23 12:25:34.444
  STEP: submitting the pod to kubernetes @ 07/15/23 12:25:34.444
  E0715 12:25:35.417474      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:36.417839      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:37.418654      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:38.418853      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:39.418938      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:40.419219      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/15/23 12:25:40.47
  STEP: looking for the results for each expected name from probers @ 07/15/23 12:25:40.474
  Jul 15 12:25:40.491: INFO: DNS probes using dns-9036/dns-test-4cb8cddf-6bd1-4952-9bea-04f29ebd6a8c succeeded

  Jul 15 12:25:40.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/15/23 12:25:40.494
  STEP: Destroying namespace "dns-9036" for this suite. @ 07/15/23 12:25:40.504
• [6.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 07/15/23 12:25:40.512
  Jul 15 12:25:40.512: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename namespaces @ 07/15/23 12:25:40.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:25:40.523
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:25:40.526
  STEP: Creating a test namespace @ 07/15/23 12:25:40.528
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:25:40.537
  STEP: Creating a pod in the namespace @ 07/15/23 12:25:40.539
  STEP: Waiting for the pod to have running status @ 07/15/23 12:25:40.546
  E0715 12:25:41.420043      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:42.420760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 07/15/23 12:25:42.553
  STEP: Waiting for the namespace to be removed. @ 07/15/23 12:25:42.56
  E0715 12:25:43.421075      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:44.422140      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:45.422715      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:46.423342      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:47.423699      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:48.424749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:49.425087      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:50.426150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:51.426255      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:52.426329      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:53.426744      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 07/15/23 12:25:53.565
  STEP: Verifying there are no pods in the namespace @ 07/15/23 12:25:53.578
  Jul 15 12:25:53.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7629" for this suite. @ 07/15/23 12:25:53.589
  STEP: Destroying namespace "nsdeletetest-5715" for this suite. @ 07/15/23 12:25:53.595
  Jul 15 12:25:53.598: INFO: Namespace nsdeletetest-5715 was already deleted
  STEP: Destroying namespace "nsdeletetest-4541" for this suite. @ 07/15/23 12:25:53.598
• [13.091 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 07/15/23 12:25:53.604
  Jul 15 12:25:53.604: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename downward-api @ 07/15/23 12:25:53.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:25:53.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:25:53.617
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 12:25:53.622
  E0715 12:25:54.427111      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:55.427758      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:56.428440      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:57.428545      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:25:57.644
  Jul 15 12:25:57.648: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-784b4ec3-023f-4058-a2ac-c28fe86b8a61 container client-container: <nil>
  STEP: delete the pod @ 07/15/23 12:25:57.653
  Jul 15 12:25:57.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3495" for this suite. @ 07/15/23 12:25:57.674
• [4.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 07/15/23 12:25:57.682
  Jul 15 12:25:57.682: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename pods @ 07/15/23 12:25:57.682
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:25:57.694
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:25:57.697
  E0715 12:25:58.428696      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:25:59.428887      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:00.429755      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:01.429948      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:26:01.741
  Jul 15 12:26:01.744: INFO: Trying to get logs from node ip-172-31-16-190 pod client-envvars-dbffcbaa-7b5f-4644-9239-2a61b1e9697b container env3cont: <nil>
  STEP: delete the pod @ 07/15/23 12:26:01.752
  Jul 15 12:26:01.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-106" for this suite. @ 07/15/23 12:26:01.77
• [4.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 07/15/23 12:26:01.779
  Jul 15 12:26:01.779: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 12:26:01.78
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:26:01.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:26:01.794
  STEP: Setting up server cert @ 07/15/23 12:26:01.812
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 12:26:01.95
  STEP: Deploying the webhook pod @ 07/15/23 12:26:01.957
  STEP: Wait for the deployment to be ready @ 07/15/23 12:26:01.968
  Jul 15 12:26:01.977: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0715 12:26:02.431006      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:03.431215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/15/23 12:26:03.988
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 12:26:03.998
  E0715 12:26:04.431743      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:26:04.998: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 07/15/23 12:26:05.004
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/15/23 12:26:05.004
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 07/15/23 12:26:05.019
  E0715 12:26:05.432479      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 07/15/23 12:26:06.03
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/15/23 12:26:06.03
  E0715 12:26:06.433423      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 07/15/23 12:26:07.054
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/15/23 12:26:07.054
  E0715 12:26:07.433937      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:08.434122      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:09.434216      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:10.435171      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:11.435343      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 07/15/23 12:26:12.084
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/15/23 12:26:12.084
  E0715 12:26:12.435452      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:13.435644      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:14.436084      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:15.436169      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:16.436352      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:26:17.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7888" for this suite. @ 07/15/23 12:26:17.165
  STEP: Destroying namespace "webhook-markers-7142" for this suite. @ 07/15/23 12:26:17.17
• [15.399 seconds]
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 07/15/23 12:26:17.178
  Jul 15 12:26:17.178: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename configmap @ 07/15/23 12:26:17.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:26:17.19
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:26:17.192
  STEP: Creating configMap with name cm-test-opt-del-4c873745-34dd-4bc7-8985-d2eca748dd34 @ 07/15/23 12:26:17.199
  STEP: Creating configMap with name cm-test-opt-upd-e5939c55-f2a7-438c-88f5-655050457a03 @ 07/15/23 12:26:17.203
  STEP: Creating the pod @ 07/15/23 12:26:17.206
  E0715 12:26:17.436484      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:18.436673      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-4c873745-34dd-4bc7-8985-d2eca748dd34 @ 07/15/23 12:26:19.241
  STEP: Updating configmap cm-test-opt-upd-e5939c55-f2a7-438c-88f5-655050457a03 @ 07/15/23 12:26:19.248
  STEP: Creating configMap with name cm-test-opt-create-100a5154-a4ae-49ea-b2ee-db9e2f454813 @ 07/15/23 12:26:19.251
  STEP: waiting to observe update in volume @ 07/15/23 12:26:19.256
  E0715 12:26:19.436793      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:20.437037      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:21.437024      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:22.437172      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:23.438113      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:24.438300      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:25.439282      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:26.439476      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:27.440112      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:28.440319      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:29.440632      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:30.440959      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:31.441098      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:32.442149      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:33.442911      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:34.443081      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:35.444116      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:36.444327      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:37.445383      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:38.446157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:39.446677      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:40.446797      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:41.447527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:42.447625      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:43.447857      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:44.448752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:45.449619      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:46.450261      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:47.450995      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:48.451466      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:49.451963      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:50.452252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:51.452419      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:52.452665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:53.453086      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:54.453269      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:55.453611      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:56.454269      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:57.454654      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:58.454796      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:26:59.455498      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:00.455787      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:01.456369      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:02.456462      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:03.456928      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:04.457162      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:05.457220      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:06.457375      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:07.457785      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:08.457969      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:09.458290      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:10.459077      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:11.459349      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:12.459519      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:13.460003      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:14.460423      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:15.461011      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:16.461079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:17.461266      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:18.462194      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:19.462459      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:20.462674      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:21.462963      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:22.463801      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:23.464070      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:24.465178      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:25.465610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:26.466186      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:27.466437      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:28.467463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:29.467814      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:30.467895      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:31.468180      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:32.468649      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:33.468791      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:34.468879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:35.469418      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:36.469856      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:37.469891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:27:37.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8168" for this suite. @ 07/15/23 12:27:37.606
• [80.435 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 07/15/23 12:27:37.614
  Jul 15 12:27:37.614: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/15/23 12:27:37.614
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:27:37.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:27:37.629
  STEP: create the container to handle the HTTPGet hook request. @ 07/15/23 12:27:37.636
  E0715 12:27:38.469841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:39.470082      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 07/15/23 12:27:39.657
  E0715 12:27:40.470255      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:41.470552      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 07/15/23 12:27:41.675
  STEP: delete the pod with lifecycle hook @ 07/15/23 12:27:41.692
  E0715 12:27:42.470766      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:43.471410      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:27:43.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-7379" for this suite. @ 07/15/23 12:27:43.711
• [6.105 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 07/15/23 12:27:43.719
  Jul 15 12:27:43.719: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename replicaset @ 07/15/23 12:27:43.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:27:43.729
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:27:43.731
  Jul 15 12:27:43.734: INFO: Creating ReplicaSet my-hostname-basic-64a156da-53d6-4ae6-964a-58a505279958
  Jul 15 12:27:43.740: INFO: Pod name my-hostname-basic-64a156da-53d6-4ae6-964a-58a505279958: Found 0 pods out of 1
  E0715 12:27:44.471727      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:45.472296      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:46.472597      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:47.472772      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:48.473071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:27:48.747: INFO: Pod name my-hostname-basic-64a156da-53d6-4ae6-964a-58a505279958: Found 1 pods out of 1
  Jul 15 12:27:48.747: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-64a156da-53d6-4ae6-964a-58a505279958" is running
  Jul 15 12:27:48.750: INFO: Pod "my-hostname-basic-64a156da-53d6-4ae6-964a-58a505279958-sw7f5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-15 12:27:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-15 12:27:44 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-15 12:27:44 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-15 12:27:43 +0000 UTC Reason: Message:}])
  Jul 15 12:27:48.750: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 07/15/23 12:27:48.75
  Jul 15 12:27:48.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7087" for this suite. @ 07/15/23 12:27:48.766
• [5.054 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 07/15/23 12:27:48.773
  Jul 15 12:27:48.773: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir @ 07/15/23 12:27:48.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:27:48.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:27:48.786
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 07/15/23 12:27:48.789
  E0715 12:27:49.473153      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:50.473798      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:27:50.802
  Jul 15 12:27:50.806: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-021fb097-abca-44f8-977a-0c1ced3da3b6 container test-container: <nil>
  STEP: delete the pod @ 07/15/23 12:27:50.813
  Jul 15 12:27:50.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8662" for this suite. @ 07/15/23 12:27:50.828
• [2.061 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 07/15/23 12:27:50.834
  Jul 15 12:27:50.834: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/15/23 12:27:50.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:27:50.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:27:50.848
  STEP: set up a multi version CRD @ 07/15/23 12:27:50.85
  Jul 15 12:27:50.851: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  E0715 12:27:51.474232      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:52.475123      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:53.475799      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 07/15/23 12:27:54.073
  STEP: check the new version name is served @ 07/15/23 12:27:54.085
  E0715 12:27:54.476282      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 07/15/23 12:27:55.28
  E0715 12:27:55.476299      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 07/15/23 12:27:55.935
  E0715 12:27:56.476302      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:57.476910      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:27:58.476987      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:27:58.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3352" for this suite. @ 07/15/23 12:27:58.489
• [7.662 seconds]
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 07/15/23 12:27:58.496
  Jul 15 12:27:58.496: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename field-validation @ 07/15/23 12:27:58.497
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:27:58.509
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:27:58.512
  Jul 15 12:27:58.515: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  W0715 12:27:58.515865      23 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc00133d240 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0715 12:27:59.477146      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:00.477653      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0715 12:28:01.064839      23 warnings.go:70] unknown field "alpha"
  W0715 12:28:01.064875      23 warnings.go:70] unknown field "beta"
  W0715 12:28:01.064881      23 warnings.go:70] unknown field "delta"
  W0715 12:28:01.064887      23 warnings.go:70] unknown field "epsilon"
  W0715 12:28:01.064892      23 warnings.go:70] unknown field "gamma"
  Jul 15 12:28:01.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4003" for this suite. @ 07/15/23 12:28:01.093
• [2.604 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 07/15/23 12:28:01.101
  Jul 15 12:28:01.101: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename resourcequota @ 07/15/23 12:28:01.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:28:01.114
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:28:01.116
  STEP: Creating a ResourceQuota @ 07/15/23 12:28:01.119
  STEP: Getting a ResourceQuota @ 07/15/23 12:28:01.123
  STEP: Updating a ResourceQuota @ 07/15/23 12:28:01.127
  STEP: Verifying a ResourceQuota was modified @ 07/15/23 12:28:01.13
  STEP: Deleting a ResourceQuota @ 07/15/23 12:28:01.134
  STEP: Verifying the deleted ResourceQuota @ 07/15/23 12:28:01.14
  Jul 15 12:28:01.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2837" for this suite. @ 07/15/23 12:28:01.146
• [0.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 07/15/23 12:28:01.156
  Jul 15 12:28:01.156: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubelet-test @ 07/15/23 12:28:01.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:28:01.165
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:28:01.167
  STEP: Waiting for pod completion @ 07/15/23 12:28:01.179
  E0715 12:28:01.478682      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:02.479555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:03.480046      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:04.480217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:05.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-5244" for this suite. @ 07/15/23 12:28:05.206
• [4.054 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 07/15/23 12:28:05.211
  Jul 15 12:28:05.211: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename var-expansion @ 07/15/23 12:28:05.212
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:28:05.222
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:28:05.225
  STEP: Creating a pod to test substitution in container's command @ 07/15/23 12:28:05.227
  E0715 12:28:05.480794      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:06.481838      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:07.482636      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:08.482760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:28:09.25
  Jul 15 12:28:09.253: INFO: Trying to get logs from node ip-172-31-16-190 pod var-expansion-e65be822-feac-400a-9001-86f943c8e33a container dapi-container: <nil>
  STEP: delete the pod @ 07/15/23 12:28:09.259
  Jul 15 12:28:09.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7623" for this suite. @ 07/15/23 12:28:09.28
• [4.075 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 07/15/23 12:28:09.286
  Jul 15 12:28:09.286: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:28:09.287
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:28:09.296
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:28:09.298
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 12:28:09.302
  E0715 12:28:09.483754      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:10.484128      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:11.484652      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:12.484787      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:28:13.318
  Jul 15 12:28:13.321: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-f46c2e67-4dd7-4dfe-b871-76163e8fd90e container client-container: <nil>
  STEP: delete the pod @ 07/15/23 12:28:13.328
  Jul 15 12:28:13.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6761" for this suite. @ 07/15/23 12:28:13.345
• [4.066 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 07/15/23 12:28:13.353
  Jul 15 12:28:13.353: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename replication-controller @ 07/15/23 12:28:13.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:28:13.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:28:13.365
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 07/15/23 12:28:13.368
  E0715 12:28:13.485047      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:14.485164      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replication controller with a matching selector is created @ 07/15/23 12:28:15.386
  STEP: Then the orphan pod is adopted @ 07/15/23 12:28:15.391
  E0715 12:28:15.485538      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:16.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8945" for this suite. @ 07/15/23 12:28:16.403
• [3.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 07/15/23 12:28:16.41
  Jul 15 12:28:16.410: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename services @ 07/15/23 12:28:16.41
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:28:16.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:28:16.425
  STEP: creating service in namespace services-5455 @ 07/15/23 12:28:16.427
  STEP: creating service affinity-clusterip in namespace services-5455 @ 07/15/23 12:28:16.427
  STEP: creating replication controller affinity-clusterip in namespace services-5455 @ 07/15/23 12:28:16.435
  I0715 12:28:16.441839      23 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-5455, replica count: 3
  E0715 12:28:16.486076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:17.486511      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:18.487165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:19.487439      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0715 12:28:19.492762      23 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 15 12:28:19.500: INFO: Creating new exec pod
  E0715 12:28:20.487492      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:21.487839      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:22.487892      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:22.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-5455 exec execpod-affinitys6l62 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Jul 15 12:28:22.617: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Jul 15 12:28:22.617: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:28:22.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-5455 exec execpod-affinitys6l62 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.204 80'
  Jul 15 12:28:22.695: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.204 80\nConnection to 10.152.183.204 80 port [tcp/http] succeeded!\n"
  Jul 15 12:28:22.695: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:28:22.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-5455 exec execpod-affinitys6l62 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.204:80/ ; done'
  Jul 15 12:28:22.854: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.204:80/\n"
  Jul 15 12:28:22.854: INFO: stdout: "\naffinity-clusterip-5j9x6\naffinity-clusterip-5j9x6\naffinity-clusterip-5j9x6\naffinity-clusterip-5j9x6\naffinity-clusterip-5j9x6\naffinity-clusterip-5j9x6\naffinity-clusterip-5j9x6\naffinity-clusterip-5j9x6\naffinity-clusterip-5j9x6\naffinity-clusterip-5j9x6\naffinity-clusterip-5j9x6\naffinity-clusterip-5j9x6\naffinity-clusterip-5j9x6\naffinity-clusterip-5j9x6\naffinity-clusterip-5j9x6\naffinity-clusterip-5j9x6"
  Jul 15 12:28:22.854: INFO: Received response from host: affinity-clusterip-5j9x6
  Jul 15 12:28:22.854: INFO: Received response from host: affinity-clusterip-5j9x6
  Jul 15 12:28:22.854: INFO: Received response from host: affinity-clusterip-5j9x6
  Jul 15 12:28:22.854: INFO: Received response from host: affinity-clusterip-5j9x6
  Jul 15 12:28:22.854: INFO: Received response from host: affinity-clusterip-5j9x6
  Jul 15 12:28:22.854: INFO: Received response from host: affinity-clusterip-5j9x6
  Jul 15 12:28:22.854: INFO: Received response from host: affinity-clusterip-5j9x6
  Jul 15 12:28:22.854: INFO: Received response from host: affinity-clusterip-5j9x6
  Jul 15 12:28:22.854: INFO: Received response from host: affinity-clusterip-5j9x6
  Jul 15 12:28:22.854: INFO: Received response from host: affinity-clusterip-5j9x6
  Jul 15 12:28:22.854: INFO: Received response from host: affinity-clusterip-5j9x6
  Jul 15 12:28:22.854: INFO: Received response from host: affinity-clusterip-5j9x6
  Jul 15 12:28:22.854: INFO: Received response from host: affinity-clusterip-5j9x6
  Jul 15 12:28:22.854: INFO: Received response from host: affinity-clusterip-5j9x6
  Jul 15 12:28:22.854: INFO: Received response from host: affinity-clusterip-5j9x6
  Jul 15 12:28:22.854: INFO: Received response from host: affinity-clusterip-5j9x6
  Jul 15 12:28:22.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 15 12:28:22.858: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-5455, will wait for the garbage collector to delete the pods @ 07/15/23 12:28:22.875
  Jul 15 12:28:22.936: INFO: Deleting ReplicationController affinity-clusterip took: 7.141091ms
  Jul 15 12:28:23.037: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.962181ms
  E0715 12:28:23.488690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:24.489322      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-5455" for this suite. @ 07/15/23 12:28:24.956
• [8.552 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 07/15/23 12:28:24.962
  Jul 15 12:28:24.962: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename services @ 07/15/23 12:28:24.963
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:28:24.977
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:28:24.98
  STEP: creating service in namespace services-2342 @ 07/15/23 12:28:24.982
  STEP: creating service affinity-nodeport in namespace services-2342 @ 07/15/23 12:28:24.982
  STEP: creating replication controller affinity-nodeport in namespace services-2342 @ 07/15/23 12:28:24.994
  I0715 12:28:24.999739      23 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-2342, replica count: 3
  E0715 12:28:25.490235      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:26.490843      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:27.491119      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0715 12:28:28.050990      23 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 15 12:28:28.063: INFO: Creating new exec pod
  E0715 12:28:28.491892      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:29.492175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:30.492801      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:31.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-2342 exec execpod-affinityskxm2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Jul 15 12:28:31.187: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Jul 15 12:28:31.187: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:28:31.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-2342 exec execpod-affinityskxm2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.63 80'
  Jul 15 12:28:31.282: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.63 80\nConnection to 10.152.183.63 80 port [tcp/http] succeeded!\n"
  Jul 15 12:28:31.282: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:28:31.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-2342 exec execpod-affinityskxm2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.190 30835'
  Jul 15 12:28:31.394: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.16.190 30835\nConnection to 172.31.16.190 30835 port [tcp/*] succeeded!\n"
  Jul 15 12:28:31.394: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:28:31.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-2342 exec execpod-affinityskxm2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.42.138 30835'
  E0715 12:28:31.493334      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:31.497: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.42.138 30835\nConnection to 172.31.42.138 30835 port [tcp/*] succeeded!\n"
  Jul 15 12:28:31.497: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:28:31.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-2342 exec execpod-affinityskxm2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.16.190:30835/ ; done'
  Jul 15 12:28:31.637: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:30835/\n"
  Jul 15 12:28:31.637: INFO: stdout: "\naffinity-nodeport-wnvf8\naffinity-nodeport-wnvf8\naffinity-nodeport-wnvf8\naffinity-nodeport-wnvf8\naffinity-nodeport-wnvf8\naffinity-nodeport-wnvf8\naffinity-nodeport-wnvf8\naffinity-nodeport-wnvf8\naffinity-nodeport-wnvf8\naffinity-nodeport-wnvf8\naffinity-nodeport-wnvf8\naffinity-nodeport-wnvf8\naffinity-nodeport-wnvf8\naffinity-nodeport-wnvf8\naffinity-nodeport-wnvf8\naffinity-nodeport-wnvf8"
  Jul 15 12:28:31.637: INFO: Received response from host: affinity-nodeport-wnvf8
  Jul 15 12:28:31.637: INFO: Received response from host: affinity-nodeport-wnvf8
  Jul 15 12:28:31.637: INFO: Received response from host: affinity-nodeport-wnvf8
  Jul 15 12:28:31.637: INFO: Received response from host: affinity-nodeport-wnvf8
  Jul 15 12:28:31.637: INFO: Received response from host: affinity-nodeport-wnvf8
  Jul 15 12:28:31.637: INFO: Received response from host: affinity-nodeport-wnvf8
  Jul 15 12:28:31.637: INFO: Received response from host: affinity-nodeport-wnvf8
  Jul 15 12:28:31.637: INFO: Received response from host: affinity-nodeport-wnvf8
  Jul 15 12:28:31.637: INFO: Received response from host: affinity-nodeport-wnvf8
  Jul 15 12:28:31.637: INFO: Received response from host: affinity-nodeport-wnvf8
  Jul 15 12:28:31.637: INFO: Received response from host: affinity-nodeport-wnvf8
  Jul 15 12:28:31.637: INFO: Received response from host: affinity-nodeport-wnvf8
  Jul 15 12:28:31.637: INFO: Received response from host: affinity-nodeport-wnvf8
  Jul 15 12:28:31.637: INFO: Received response from host: affinity-nodeport-wnvf8
  Jul 15 12:28:31.637: INFO: Received response from host: affinity-nodeport-wnvf8
  Jul 15 12:28:31.637: INFO: Received response from host: affinity-nodeport-wnvf8
  Jul 15 12:28:31.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 15 12:28:31.642: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-2342, will wait for the garbage collector to delete the pods @ 07/15/23 12:28:31.654
  Jul 15 12:28:31.715: INFO: Deleting ReplicationController affinity-nodeport took: 6.955391ms
  Jul 15 12:28:31.816: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.151042ms
  E0715 12:28:32.494280      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:33.495266      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-2342" for this suite. @ 07/15/23 12:28:33.945
• [8.987 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 07/15/23 12:28:33.95
  Jul 15 12:28:33.950: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl @ 07/15/23 12:28:33.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:28:33.96
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:28:33.962
  STEP: creating a replication controller @ 07/15/23 12:28:33.965
  Jul 15 12:28:33.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-2643 create -f -'
  Jul 15 12:28:34.398: INFO: stderr: ""
  Jul 15 12:28:34.398: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/15/23 12:28:34.398
  Jul 15 12:28:34.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-2643 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 15 12:28:34.442: INFO: stderr: ""
  Jul 15 12:28:34.443: INFO: stdout: "update-demo-nautilus-nx46f update-demo-nautilus-zjxsw "
  Jul 15 12:28:34.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-2643 get pods update-demo-nautilus-nx46f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 15 12:28:34.483: INFO: stderr: ""
  Jul 15 12:28:34.483: INFO: stdout: ""
  Jul 15 12:28:34.483: INFO: update-demo-nautilus-nx46f is created but not running
  E0715 12:28:34.496112      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:35.496565      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:36.496716      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:37.496921      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:38.497541      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:39.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-2643 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0715 12:28:39.498025      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:39.527: INFO: stderr: ""
  Jul 15 12:28:39.527: INFO: stdout: "update-demo-nautilus-nx46f update-demo-nautilus-zjxsw "
  Jul 15 12:28:39.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-2643 get pods update-demo-nautilus-nx46f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 15 12:28:39.569: INFO: stderr: ""
  Jul 15 12:28:39.569: INFO: stdout: "true"
  Jul 15 12:28:39.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-2643 get pods update-demo-nautilus-nx46f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 15 12:28:39.611: INFO: stderr: ""
  Jul 15 12:28:39.611: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 15 12:28:39.611: INFO: validating pod update-demo-nautilus-nx46f
  Jul 15 12:28:39.616: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 15 12:28:39.616: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 15 12:28:39.616: INFO: update-demo-nautilus-nx46f is verified up and running
  Jul 15 12:28:39.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-2643 get pods update-demo-nautilus-zjxsw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 15 12:28:39.657: INFO: stderr: ""
  Jul 15 12:28:39.657: INFO: stdout: "true"
  Jul 15 12:28:39.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-2643 get pods update-demo-nautilus-zjxsw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 15 12:28:39.697: INFO: stderr: ""
  Jul 15 12:28:39.697: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 15 12:28:39.697: INFO: validating pod update-demo-nautilus-zjxsw
  Jul 15 12:28:39.702: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 15 12:28:39.702: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 15 12:28:39.702: INFO: update-demo-nautilus-zjxsw is verified up and running
  STEP: using delete to clean up resources @ 07/15/23 12:28:39.702
  Jul 15 12:28:39.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-2643 delete --grace-period=0 --force -f -'
  Jul 15 12:28:39.748: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 15 12:28:39.748: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jul 15 12:28:39.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-2643 get rc,svc -l name=update-demo --no-headers'
  Jul 15 12:28:39.802: INFO: stderr: "No resources found in kubectl-2643 namespace.\n"
  Jul 15 12:28:39.802: INFO: stdout: ""
  Jul 15 12:28:39.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-2643 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul 15 12:28:39.851: INFO: stderr: ""
  Jul 15 12:28:39.851: INFO: stdout: ""
  Jul 15 12:28:39.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2643" for this suite. @ 07/15/23 12:28:39.855
• [5.913 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 07/15/23 12:28:39.863
  Jul 15 12:28:39.863: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl @ 07/15/23 12:28:39.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:28:39.873
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:28:39.875
  STEP: starting the proxy server @ 07/15/23 12:28:39.878
  Jul 15 12:28:39.878: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-6974 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 07/15/23 12:28:39.908
  Jul 15 12:28:39.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6974" for this suite. @ 07/15/23 12:28:39.92
• [0.063 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 07/15/23 12:28:39.927
  Jul 15 12:28:39.927: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename secrets @ 07/15/23 12:28:39.927
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:28:39.94
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:28:39.943
  STEP: Creating secret with name secret-test-6390928d-03c7-4d53-9b12-b75d997e4f38 @ 07/15/23 12:28:39.945
  STEP: Creating a pod to test consume secrets @ 07/15/23 12:28:39.949
  E0715 12:28:40.498202      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:41.498388      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:28:41.965
  Jul 15 12:28:41.970: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-secrets-dc43e6bb-27ee-4d88-9828-717911f2a5ee container secret-volume-test: <nil>
  STEP: delete the pod @ 07/15/23 12:28:41.977
  Jul 15 12:28:41.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9059" for this suite. @ 07/15/23 12:28:41.992
• [2.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 07/15/23 12:28:42.001
  Jul 15 12:28:42.001: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename deployment @ 07/15/23 12:28:42.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:28:42.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:28:42.016
  Jul 15 12:28:42.019: INFO: Creating simple deployment test-new-deployment
  Jul 15 12:28:42.031: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0715 12:28:42.499280      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:43.499540      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 07/15/23 12:28:44.046
  STEP: updating a scale subresource @ 07/15/23 12:28:44.048
  STEP: verifying the deployment Spec.Replicas was modified @ 07/15/23 12:28:44.055
  STEP: Patch a scale subresource @ 07/15/23 12:28:44.06
  Jul 15 12:28:44.097: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-2836  707033c4-851f-4f3c-b1db-dede7c578400 13063 3 2023-07-15 12:28:42 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-07-15 12:28:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-15 12:28:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047e9048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:2,UpdatedReplicas:2,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-07-15 12:28:42 +0000 UTC,LastTransitionTime:2023-07-15 12:28:42 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-15 12:28:44 +0000 UTC,LastTransitionTime:2023-07-15 12:28:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 15 12:28:44.102: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-2836  8c8f2298-b3f4-4d00-9fc4-5fcf2581565e 13060 3 2023-07-15 12:28:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 707033c4-851f-4f3c-b1db-dede7c578400 0xc0047e9487 0xc0047e9488}] [] [{kube-controller-manager Update apps/v1 2023-07-15 12:28:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"707033c4-851f-4f3c-b1db-dede7c578400\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-15 12:28:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047e9518 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 15 12:28:44.109: INFO: Pod "test-new-deployment-67bd4bf6dc-7qsst" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-7qsst test-new-deployment-67bd4bf6dc- deployment-2836  1382d94d-4560-4b3a-bc0a-b6b440cddc69 13059 0 2023-07-15 12:28:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 8c8f2298-b3f4-4d00-9fc4-5fcf2581565e 0xc004cb2227 0xc004cb2228}] [] [{kube-controller-manager Update v1 2023-07-15 12:28:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c8f2298-b3f4-4d00-9fc4-5fcf2581565e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:28:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rj2fl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rj2fl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-138,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:28:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:28:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:28:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:28:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.138,PodIP:,StartTime:2023-07-15 12:28:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:28:44.109: INFO: Pod "test-new-deployment-67bd4bf6dc-d7ws7" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-d7ws7 test-new-deployment-67bd4bf6dc- deployment-2836  e4b07f3a-e7a3-471d-91df-1fcd8c219e20 13068 0 2023-07-15 12:28:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 8c8f2298-b3f4-4d00-9fc4-5fcf2581565e 0xc004cb2407 0xc004cb2408}] [] [{kube-controller-manager Update v1 2023-07-15 12:28:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c8f2298-b3f4-4d00-9fc4-5fcf2581565e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nfrkh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nfrkh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:28:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:28:44.109: INFO: Pod "test-new-deployment-67bd4bf6dc-s2kgx" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-s2kgx test-new-deployment-67bd4bf6dc- deployment-2836  b1f71e5b-17a9-44da-b016-756bce655bea 13066 0 2023-07-15 12:28:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 8c8f2298-b3f4-4d00-9fc4-5fcf2581565e 0xc004cb2570 0xc004cb2571}] [] [{kube-controller-manager Update v1 2023-07-15 12:28:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c8f2298-b3f4-4d00-9fc4-5fcf2581565e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8sk6v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8sk6v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-236,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:28:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:28:44.110: INFO: Pod "test-new-deployment-67bd4bf6dc-xj4vs" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-xj4vs test-new-deployment-67bd4bf6dc- deployment-2836  b6dee763-8c57-439a-83f2-c7ecd377ddb9 13042 0 2023-07-15 12:28:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 8c8f2298-b3f4-4d00-9fc4-5fcf2581565e 0xc004cb26d0 0xc004cb26d1}] [] [{kube-controller-manager Update v1 2023-07-15 12:28:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c8f2298-b3f4-4d00-9fc4-5fcf2581565e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:28:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.33.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dwc8l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dwc8l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:28:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:28:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:28:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:28:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.190,PodIP:192.168.33.92,StartTime:2023-07-15 12:28:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-15 12:28:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a8a4b6eb0d8e2299b1214ba340572ced117f7cfafacf00f2eb7d68e3fcad8be0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.33.92,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:28:44.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2836" for this suite. @ 07/15/23 12:28:44.114
• [2.122 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 07/15/23 12:28:44.123
  Jul 15 12:28:44.123: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename controllerrevisions @ 07/15/23 12:28:44.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:28:44.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:28:44.136
  STEP: Creating DaemonSet "e2e-lk5xw-daemon-set" @ 07/15/23 12:28:44.159
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/15/23 12:28:44.164
  Jul 15 12:28:44.167: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:44.168: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:44.171: INFO: Number of nodes with available pods controlled by daemonset e2e-lk5xw-daemon-set: 0
  Jul 15 12:28:44.171: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  E0715 12:28:44.500509      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:45.176: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:45.176: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:45.179: INFO: Number of nodes with available pods controlled by daemonset e2e-lk5xw-daemon-set: 1
  Jul 15 12:28:45.179: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  E0715 12:28:45.500841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:46.177: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:46.177: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:46.180: INFO: Number of nodes with available pods controlled by daemonset e2e-lk5xw-daemon-set: 3
  Jul 15 12:28:46.180: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-lk5xw-daemon-set
  STEP: Confirm DaemonSet "e2e-lk5xw-daemon-set" successfully created with "daemonset-name=e2e-lk5xw-daemon-set" label @ 07/15/23 12:28:46.183
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-lk5xw-daemon-set" @ 07/15/23 12:28:46.19
  Jul 15 12:28:46.193: INFO: Located ControllerRevision: "e2e-lk5xw-daemon-set-5f98b5d9dd"
  STEP: Patching ControllerRevision "e2e-lk5xw-daemon-set-5f98b5d9dd" @ 07/15/23 12:28:46.197
  Jul 15 12:28:46.203: INFO: e2e-lk5xw-daemon-set-5f98b5d9dd has been patched
  STEP: Create a new ControllerRevision @ 07/15/23 12:28:46.203
  Jul 15 12:28:46.207: INFO: Created ControllerRevision: e2e-lk5xw-daemon-set-645f9c9fb6
  STEP: Confirm that there are two ControllerRevisions @ 07/15/23 12:28:46.207
  Jul 15 12:28:46.207: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 15 12:28:46.211: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-lk5xw-daemon-set-5f98b5d9dd" @ 07/15/23 12:28:46.211
  STEP: Confirm that there is only one ControllerRevision @ 07/15/23 12:28:46.216
  Jul 15 12:28:46.216: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 15 12:28:46.220: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-lk5xw-daemon-set-645f9c9fb6" @ 07/15/23 12:28:46.223
  Jul 15 12:28:46.233: INFO: e2e-lk5xw-daemon-set-645f9c9fb6 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 07/15/23 12:28:46.233
  W0715 12:28:46.239669      23 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 07/15/23 12:28:46.239
  Jul 15 12:28:46.239: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0715 12:28:46.501320      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:47.244: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 15 12:28:47.248: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-lk5xw-daemon-set-645f9c9fb6=updated" @ 07/15/23 12:28:47.248
  STEP: Confirm that there is only one ControllerRevision @ 07/15/23 12:28:47.256
  Jul 15 12:28:47.256: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 15 12:28:47.260: INFO: Found 1 ControllerRevisions
  Jul 15 12:28:47.262: INFO: ControllerRevision "e2e-lk5xw-daemon-set-6dbb8b967" has revision 3
  STEP: Deleting DaemonSet "e2e-lk5xw-daemon-set" @ 07/15/23 12:28:47.265
  STEP: deleting DaemonSet.extensions e2e-lk5xw-daemon-set in namespace controllerrevisions-428, will wait for the garbage collector to delete the pods @ 07/15/23 12:28:47.265
  Jul 15 12:28:47.326: INFO: Deleting DaemonSet.extensions e2e-lk5xw-daemon-set took: 5.332418ms
  Jul 15 12:28:47.426: INFO: Terminating DaemonSet.extensions e2e-lk5xw-daemon-set pods took: 100.684135ms
  E0715 12:28:47.502098      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:48.430: INFO: Number of nodes with available pods controlled by daemonset e2e-lk5xw-daemon-set: 0
  Jul 15 12:28:48.430: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-lk5xw-daemon-set
  Jul 15 12:28:48.434: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13209"},"items":null}

  Jul 15 12:28:48.438: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13209"},"items":null}

  Jul 15 12:28:48.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-428" for this suite. @ 07/15/23 12:28:48.453
• [4.335 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 07/15/23 12:28:48.458
  Jul 15 12:28:48.458: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir @ 07/15/23 12:28:48.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:28:48.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:28:48.473
  STEP: Creating Pod @ 07/15/23 12:28:48.475
  E0715 12:28:48.502719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:49.503620      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 07/15/23 12:28:50.495
  Jul 15 12:28:50.495: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-290 PodName:pod-sharedvolume-0c1f9d0e-0299-491a-bb8c-fa8970a430b4 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 12:28:50.495: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 12:28:50.496: INFO: ExecWithOptions: Clientset creation
  Jul 15 12:28:50.496: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-290/pods/pod-sharedvolume-0c1f9d0e-0299-491a-bb8c-fa8970a430b4/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  E0715 12:28:50.504390      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:50.552: INFO: Exec stderr: ""
  Jul 15 12:28:50.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-290" for this suite. @ 07/15/23 12:28:50.555
• [2.104 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 07/15/23 12:28:50.562
  Jul 15 12:28:50.562: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:28:50.563
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:28:50.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:28:50.578
  STEP: Creating projection with secret that has name projected-secret-test-map-d1c7ea90-323a-4947-b6e2-08201192c485 @ 07/15/23 12:28:50.58
  STEP: Creating a pod to test consume secrets @ 07/15/23 12:28:50.584
  E0715 12:28:51.504926      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:52.505093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:53.505246      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:28:54.506147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:28:54.606
  Jul 15 12:28:54.609: INFO: Trying to get logs from node ip-172-31-42-138 pod pod-projected-secrets-a2a83d06-69ad-4ba5-9105-a3ff7c2fad6a container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/15/23 12:28:54.616
  Jul 15 12:28:54.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3604" for this suite. @ 07/15/23 12:28:54.637
• [4.079 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 07/15/23 12:28:54.642
  Jul 15 12:28:54.642: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename daemonsets @ 07/15/23 12:28:54.643
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:28:54.656
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:28:54.659
  STEP: Creating simple DaemonSet "daemon-set" @ 07/15/23 12:28:54.68
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/15/23 12:28:54.685
  Jul 15 12:28:54.689: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:54.689: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:54.692: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:28:54.692: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  E0715 12:28:55.506454      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:55.697: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:55.697: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:55.700: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 15 12:28:55.700: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  E0715 12:28:56.507273      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:56.697: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:56.697: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:56.700: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 15 12:28:56.700: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 07/15/23 12:28:56.704
  STEP: DeleteCollection of the DaemonSets @ 07/15/23 12:28:56.707
  STEP: Verify that ReplicaSets have been deleted @ 07/15/23 12:28:56.715
  Jul 15 12:28:56.729: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13405"},"items":null}

  Jul 15 12:28:56.733: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13407"},"items":[{"metadata":{"name":"daemon-set-2t7bh","generateName":"daemon-set-","namespace":"daemonsets-3945","uid":"2492d512-a449-4e25-a06f-517bdbc46d7f","resourceVersion":"13404","creationTimestamp":"2023-07-15T12:28:54Z","deletionTimestamp":"2023-07-15T12:29:26Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"980f16c5-3878-4408-9032-ec602329639b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-15T12:28:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"980f16c5-3878-4408-9032-ec602329639b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-15T12:28:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.191.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vxwg9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vxwg9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-42-138","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-42-138"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-15T12:28:54Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-15T12:28:55Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-15T12:28:55Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-15T12:28:54Z"}],"hostIP":"172.31.42.138","podIP":"192.168.191.223","podIPs":[{"ip":"192.168.191.223"}],"startTime":"2023-07-15T12:28:54Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-15T12:28:55Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://32a703eccf5b3e3435adf6b739a89c48f34e7fd78dbd6f2053eb1f253270fecb","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-49nhn","generateName":"daemon-set-","namespace":"daemonsets-3945","uid":"45b0e8db-0c11-4823-a236-c86afbd7925a","resourceVersion":"13403","creationTimestamp":"2023-07-15T12:28:54Z","deletionTimestamp":"2023-07-15T12:29:26Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"980f16c5-3878-4408-9032-ec602329639b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-15T12:28:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"980f16c5-3878-4408-9032-ec602329639b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-15T12:28:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.4.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ddwch","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ddwch","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-84-236","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-84-236"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-15T12:28:54Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-15T12:28:55Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-15T12:28:55Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-15T12:28:54Z"}],"hostIP":"172.31.84.236","podIP":"192.168.4.13","podIPs":[{"ip":"192.168.4.13"}],"startTime":"2023-07-15T12:28:54Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-15T12:28:55Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://68a4b8e1edcf871aacc03e78281f1fcab0f96493a079053fd9bdd2c2a5c845dd","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-k4kxl","generateName":"daemon-set-","namespace":"daemonsets-3945","uid":"d3dd0fce-3e5e-43e6-be12-137db492a155","resourceVersion":"13402","creationTimestamp":"2023-07-15T12:28:54Z","deletionTimestamp":"2023-07-15T12:29:26Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"980f16c5-3878-4408-9032-ec602329639b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-15T12:28:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"980f16c5-3878-4408-9032-ec602329639b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-15T12:28:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.33.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4nkqx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4nkqx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-16-190","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-16-190"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-15T12:28:54Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-15T12:28:55Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-15T12:28:55Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-15T12:28:54Z"}],"hostIP":"172.31.16.190","podIP":"192.168.33.94","podIPs":[{"ip":"192.168.33.94"}],"startTime":"2023-07-15T12:28:54Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-15T12:28:55Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4ef25e9ff3e0af0cfb7c0534b8f8923102b9f5f25bec62f75c466f0707c4a5fb","started":true}],"qosClass":"BestEffort"}}]}

  Jul 15 12:28:56.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3945" for this suite. @ 07/15/23 12:28:56.751
• [2.114 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 07/15/23 12:28:56.757
  Jul 15 12:28:56.757: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename daemonsets @ 07/15/23 12:28:56.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:28:56.77
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:28:56.772
  STEP: Creating a simple DaemonSet "daemon-set" @ 07/15/23 12:28:56.793
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/15/23 12:28:56.797
  Jul 15 12:28:56.800: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:56.800: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:56.803: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:28:56.803: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  E0715 12:28:57.508041      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:57.808: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:57.808: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:57.812: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 15 12:28:57.812: INFO: Node ip-172-31-84-236 is running 0 daemon pod, expected 1
  E0715 12:28:58.508655      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:58.808: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:58.808: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:58.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 15 12:28:58.813: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 07/15/23 12:28:58.815
  Jul 15 12:28:58.827: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:58.827: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:58.831: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 15 12:28:58.831: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  E0715 12:28:59.508763      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:28:59.837: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:59.837: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:28:59.840: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 15 12:28:59.840: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 07/15/23 12:28:59.84
  STEP: Deleting DaemonSet "daemon-set" @ 07/15/23 12:28:59.847
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3897, will wait for the garbage collector to delete the pods @ 07/15/23 12:28:59.847
  Jul 15 12:28:59.907: INFO: Deleting DaemonSet.extensions daemon-set took: 5.222044ms
  Jul 15 12:29:00.007: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.798053ms
  E0715 12:29:00.509742      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:01.509988      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:29:01.810: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:29:01.810: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 15 12:29:01.815: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13559"},"items":null}

  Jul 15 12:29:01.819: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13559"},"items":null}

  Jul 15 12:29:01.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3897" for this suite. @ 07/15/23 12:29:01.835
• [5.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 07/15/23 12:29:01.842
  Jul 15 12:29:01.842: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename downward-api @ 07/15/23 12:29:01.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:29:01.856
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:29:01.859
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 12:29:01.861
  E0715 12:29:02.510203      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:03.510402      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:04.511189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:05.511784      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:29:05.885
  Jul 15 12:29:05.888: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-962bbc37-220b-4194-80d5-66f131abae16 container client-container: <nil>
  STEP: delete the pod @ 07/15/23 12:29:05.896
  Jul 15 12:29:05.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1158" for this suite. @ 07/15/23 12:29:05.915
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 07/15/23 12:29:05.923
  Jul 15 12:29:05.923: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename services @ 07/15/23 12:29:05.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:29:05.934
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:29:05.937
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2399 @ 07/15/23 12:29:05.94
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 07/15/23 12:29:05.95
  STEP: creating service externalsvc in namespace services-2399 @ 07/15/23 12:29:05.95
  STEP: creating replication controller externalsvc in namespace services-2399 @ 07/15/23 12:29:05.96
  I0715 12:29:05.964692      23 runners.go:194] Created replication controller with name: externalsvc, namespace: services-2399, replica count: 2
  E0715 12:29:06.512326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:07.513253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:08.513408      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0715 12:29:09.015115      23 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 07/15/23 12:29:09.018
  Jul 15 12:29:09.030: INFO: Creating new exec pod
  E0715 12:29:09.514043      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:10.514384      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:29:11.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-2399 exec execpodk78rf -- /bin/sh -x -c nslookup clusterip-service.services-2399.svc.cluster.local'
  Jul 15 12:29:11.161: INFO: stderr: "+ nslookup clusterip-service.services-2399.svc.cluster.local\n"
  Jul 15 12:29:11.161: INFO: stdout: "Server:\t\t10.152.183.169\nAddress:\t10.152.183.169#53\n\nclusterip-service.services-2399.svc.cluster.local\tcanonical name = externalsvc.services-2399.svc.cluster.local.\nName:\texternalsvc.services-2399.svc.cluster.local\nAddress: 10.152.183.231\n\n"
  Jul 15 12:29:11.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-2399, will wait for the garbage collector to delete the pods @ 07/15/23 12:29:11.165
  Jul 15 12:29:11.226: INFO: Deleting ReplicationController externalsvc took: 7.008133ms
  Jul 15 12:29:11.326: INFO: Terminating ReplicationController externalsvc pods took: 100.068735ms
  E0715 12:29:11.514938      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:12.515252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:13.516189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:29:13.546: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-2399" for this suite. @ 07/15/23 12:29:13.558
• [7.642 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 07/15/23 12:29:13.566
  Jul 15 12:29:13.566: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename resourcequota @ 07/15/23 12:29:13.566
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:29:13.581
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:29:13.583
  STEP: Counting existing ResourceQuota @ 07/15/23 12:29:13.586
  E0715 12:29:14.516850      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:15.517710      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:16.518122      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:17.519193      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:18.519338      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/15/23 12:29:18.589
  STEP: Ensuring resource quota status is calculated @ 07/15/23 12:29:18.594
  E0715 12:29:19.519851      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:20.520327      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 07/15/23 12:29:20.599
  STEP: Ensuring resource quota status captures replication controller creation @ 07/15/23 12:29:20.61
  E0715 12:29:21.520973      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:22.521252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 07/15/23 12:29:22.616
  STEP: Ensuring resource quota status released usage @ 07/15/23 12:29:22.623
  E0715 12:29:23.522252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:24.522276      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:29:24.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6308" for this suite. @ 07/15/23 12:29:24.631
• [11.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 07/15/23 12:29:24.639
  Jul 15 12:29:24.639: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename replicaset @ 07/15/23 12:29:24.64
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:29:24.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:29:24.654
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 07/15/23 12:29:24.656
  Jul 15 12:29:24.663: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0715 12:29:25.523087      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:26.523185      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:27.523626      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:28.523738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:29.524153      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:29:29.669: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/15/23 12:29:29.669
  STEP: getting scale subresource @ 07/15/23 12:29:29.669
  STEP: updating a scale subresource @ 07/15/23 12:29:29.672
  STEP: verifying the replicaset Spec.Replicas was modified @ 07/15/23 12:29:29.679
  STEP: Patch a scale subresource @ 07/15/23 12:29:29.683
  Jul 15 12:29:29.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1072" for this suite. @ 07/15/23 12:29:29.706
• [5.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 07/15/23 12:29:29.713
  Jul 15 12:29:29.713: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename downward-api @ 07/15/23 12:29:29.714
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:29:29.727
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:29:29.729
  STEP: Creating a pod to test downward api env vars @ 07/15/23 12:29:29.732
  E0715 12:29:30.524264      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:31.524586      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:32.524688      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:33.525136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:29:33.753
  Jul 15 12:29:33.758: INFO: Trying to get logs from node ip-172-31-16-190 pod downward-api-fbf9877f-ff03-445c-842c-149b049ee390 container dapi-container: <nil>
  STEP: delete the pod @ 07/15/23 12:29:33.768
  Jul 15 12:29:33.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7337" for this suite. @ 07/15/23 12:29:33.786
• [4.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 07/15/23 12:29:33.795
  Jul 15 12:29:33.795: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename downward-api @ 07/15/23 12:29:33.795
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:29:33.808
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:29:33.81
  STEP: Creating a pod to test downward api env vars @ 07/15/23 12:29:33.812
  E0715 12:29:34.526114      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:35.526754      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:36.527797      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:37.528027      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:29:37.838
  Jul 15 12:29:37.841: INFO: Trying to get logs from node ip-172-31-16-190 pod downward-api-171f9b89-6e56-4101-aefd-65159dee10c9 container dapi-container: <nil>
  STEP: delete the pod @ 07/15/23 12:29:37.849
  Jul 15 12:29:37.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9209" for this suite. @ 07/15/23 12:29:37.867
• [4.079 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 07/15/23 12:29:37.874
  Jul 15 12:29:37.875: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename configmap @ 07/15/23 12:29:37.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:29:37.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:29:37.89
  STEP: Creating configMap configmap-814/configmap-test-54d0dddb-441f-4eb3-b9dc-b8fb05b6288e @ 07/15/23 12:29:37.892
  STEP: Creating a pod to test consume configMaps @ 07/15/23 12:29:37.897
  E0715 12:29:38.528518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:39.528628      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:40.529094      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:41.529148      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:29:41.92
  Jul 15 12:29:41.923: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-configmaps-96fcbd62-f360-44b4-aafe-c6a3d2b6e708 container env-test: <nil>
  STEP: delete the pod @ 07/15/23 12:29:41.93
  Jul 15 12:29:41.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-814" for this suite. @ 07/15/23 12:29:41.949
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 07/15/23 12:29:41.956
  Jul 15 12:29:41.956: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-runtime @ 07/15/23 12:29:41.956
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:29:41.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:29:41.971
  STEP: create the container @ 07/15/23 12:29:41.974
  W0715 12:29:41.982319      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 07/15/23 12:29:41.982
  E0715 12:29:42.529857      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:43.530238      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:44.531187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/15/23 12:29:44.998
  STEP: the container should be terminated @ 07/15/23 12:29:45.002
  STEP: the termination message should be set @ 07/15/23 12:29:45.002
  Jul 15 12:29:45.002: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 07/15/23 12:29:45.002
  Jul 15 12:29:45.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8033" for this suite. @ 07/15/23 12:29:45.018
• [3.067 seconds]
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 07/15/23 12:29:45.023
  Jul 15 12:29:45.023: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename downward-api @ 07/15/23 12:29:45.023
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:29:45.036
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:29:45.039
  STEP: Creating a pod to test downward api env vars @ 07/15/23 12:29:45.041
  E0715 12:29:45.531252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:46.531398      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:47.532174      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:48.532378      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:29:49.064
  Jul 15 12:29:49.067: INFO: Trying to get logs from node ip-172-31-16-190 pod downward-api-d6932d34-12b3-4939-8f5b-886ed70c3080 container dapi-container: <nil>
  STEP: delete the pod @ 07/15/23 12:29:49.074
  Jul 15 12:29:49.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-354" for this suite. @ 07/15/23 12:29:49.091
• [4.075 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 07/15/23 12:29:49.098
  Jul 15 12:29:49.098: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename job @ 07/15/23 12:29:49.099
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:29:49.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:29:49.115
  STEP: Creating a job @ 07/15/23 12:29:49.117
  STEP: Ensuring active pods == parallelism @ 07/15/23 12:29:49.122
  E0715 12:29:49.532419      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:50.532970      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 07/15/23 12:29:51.128
  STEP: deleting Job.batch foo in namespace job-3561, will wait for the garbage collector to delete the pods @ 07/15/23 12:29:51.128
  Jul 15 12:29:51.190: INFO: Deleting Job.batch foo took: 7.105039ms
  Jul 15 12:29:51.291: INFO: Terminating Job.batch foo pods took: 101.149958ms
  E0715 12:29:51.533783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:52.534713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:53.535554      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:54.536103      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:55.536983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:56.537262      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:57.537757      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:58.538242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:29:59.538745      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:00.539454      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:01.539981      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:02.540179      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:03.540419      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:04.540977      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:05.541597      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:06.541890      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:07.542518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:08.543252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:09.543767      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:10.544243      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:11.544929      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:12.545290      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:13.545911      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:14.546532      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:15.547475      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:16.548212      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:17.548983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:18.549704      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:19.550096      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:20.550712      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:21.551423      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 07/15/23 12:30:21.992
  Jul 15 12:30:21.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3561" for this suite. @ 07/15/23 12:30:22.001
• [32.908 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 07/15/23 12:30:22.007
  Jul 15 12:30:22.007: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename statefulset @ 07/15/23 12:30:22.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:30:22.017
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:30:22.02
  STEP: Creating service test in namespace statefulset-8043 @ 07/15/23 12:30:22.022
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 07/15/23 12:30:22.026
  STEP: Creating stateful set ss in namespace statefulset-8043 @ 07/15/23 12:30:22.03
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8043 @ 07/15/23 12:30:22.035
  Jul 15 12:30:22.039: INFO: Found 0 stateful pods, waiting for 1
  E0715 12:30:22.552224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:23.553066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:24.553704      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:25.554193      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:26.554446      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:27.554676      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:28.554912      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:29.555174      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:30.555343      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:31.555643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:30:32.045: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 07/15/23 12:30:32.045
  Jul 15 12:30:32.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-8043 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 15 12:30:32.149: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 15 12:30:32.149: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 15 12:30:32.149: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 15 12:30:32.152: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0715 12:30:32.556497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:33.556801      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:34.556978      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:35.557500      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:36.557580      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:37.558240      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:38.558341      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:39.558547      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:40.559556      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:41.559824      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:30:42.157: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 15 12:30:42.157: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 15 12:30:42.173: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999975s
  E0715 12:30:42.560101      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:30:43.178: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997153566s
  E0715 12:30:43.560884      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:30:44.182: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992256266s
  E0715 12:30:44.561925      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:30:45.187: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987824273s
  E0715 12:30:45.562223      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:30:46.192: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982850239s
  E0715 12:30:46.562698      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:30:47.195: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.978412556s
  E0715 12:30:47.562732      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:30:48.201: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.974313911s
  E0715 12:30:48.563739      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:30:49.205: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.969371139s
  E0715 12:30:49.563851      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:30:50.209: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.965091284s
  E0715 12:30:50.564328      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:30:51.215: INFO: Verifying statefulset ss doesn't scale past 1 for another 960.677622ms
  E0715 12:30:51.564441      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8043 @ 07/15/23 12:30:52.216
  Jul 15 12:30:52.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-8043 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 15 12:30:52.325: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 15 12:30:52.325: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 15 12:30:52.325: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 15 12:30:52.329: INFO: Found 1 stateful pods, waiting for 3
  E0715 12:30:52.565033      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:53.565181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:54.565262      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:55.565867      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:56.566111      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:57.566309      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:58.566464      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:30:59.566611      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:00.567145      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:01.567504      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:31:02.333: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 15 12:31:02.333: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 15 12:31:02.333: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 07/15/23 12:31:02.333
  STEP: Scale down will halt with unhealthy stateful pod @ 07/15/23 12:31:02.333
  Jul 15 12:31:02.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-8043 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 15 12:31:02.445: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 15 12:31:02.445: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 15 12:31:02.445: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 15 12:31:02.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-8043 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 15 12:31:02.558: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 15 12:31:02.558: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 15 12:31:02.558: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 15 12:31:02.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-8043 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0715 12:31:02.567692      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:31:02.662: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 15 12:31:02.662: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 15 12:31:02.662: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 15 12:31:02.662: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 15 12:31:02.667: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0715 12:31:03.567754      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:04.567984      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:05.568559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:06.568869      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:07.569147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:08.569186      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:09.570204      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:10.570525      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:11.570727      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:12.570962      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:31:12.676: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 15 12:31:12.676: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jul 15 12:31:12.676: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jul 15 12:31:12.692: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999988s
  E0715 12:31:13.571001      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:31:13.697: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996310552s
  E0715 12:31:14.571312      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:31:14.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992040235s
  E0715 12:31:15.572170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:31:15.707: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986070541s
  E0715 12:31:16.572685      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:31:16.712: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981660456s
  E0715 12:31:17.573643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:31:17.717: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976613815s
  E0715 12:31:18.574065      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:31:18.721: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972249402s
  E0715 12:31:19.574145      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:31:19.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.967071173s
  E0715 12:31:20.575053      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:31:20.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.961232607s
  E0715 12:31:21.575285      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:31:21.736: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.052955ms
  E0715 12:31:22.575970      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8043 @ 07/15/23 12:31:22.737
  Jul 15 12:31:22.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-8043 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 15 12:31:22.845: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 15 12:31:22.845: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 15 12:31:22.845: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 15 12:31:22.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-8043 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 15 12:31:22.948: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 15 12:31:22.948: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 15 12:31:22.948: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 15 12:31:22.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-8043 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 15 12:31:23.040: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 15 12:31:23.040: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 15 12:31:23.040: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 15 12:31:23.040: INFO: Scaling statefulset ss to 0
  E0715 12:31:23.576549      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:24.576695      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:25.576722      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:26.577116      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:27.577726      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:28.578242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:29.578475      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:30.578881      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:31.579168      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:32.579457      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 07/15/23 12:31:33.056
  Jul 15 12:31:33.056: INFO: Deleting all statefulset in ns statefulset-8043
  Jul 15 12:31:33.060: INFO: Scaling statefulset ss to 0
  Jul 15 12:31:33.073: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 15 12:31:33.076: INFO: Deleting statefulset ss
  Jul 15 12:31:33.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8043" for this suite. @ 07/15/23 12:31:33.094
• [71.094 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 07/15/23 12:31:33.102
  Jul 15 12:31:33.102: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename pods @ 07/15/23 12:31:33.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:31:33.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:31:33.115
  STEP: creating the pod @ 07/15/23 12:31:33.118
  STEP: submitting the pod to kubernetes @ 07/15/23 12:31:33.118
  E0715 12:31:33.579491      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:34.579763      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 07/15/23 12:31:35.136
  STEP: updating the pod @ 07/15/23 12:31:35.14
  E0715 12:31:35.580449      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:31:35.651: INFO: Successfully updated pod "pod-update-dd4ca92f-b130-48b6-a836-688648ceacb1"
  STEP: verifying the updated pod is in kubernetes @ 07/15/23 12:31:35.655
  Jul 15 12:31:35.660: INFO: Pod update OK
  Jul 15 12:31:35.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6252" for this suite. @ 07/15/23 12:31:35.665
• [2.569 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 07/15/23 12:31:35.672
  Jul 15 12:31:35.672: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:31:35.672
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:31:35.683
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:31:35.685
  STEP: Creating projection with secret that has name projected-secret-test-map-284e92c5-087d-4929-aa66-237bfae9fd28 @ 07/15/23 12:31:35.688
  STEP: Creating a pod to test consume secrets @ 07/15/23 12:31:35.692
  E0715 12:31:36.581476      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:37.581746      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:38.582309      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:39.582500      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:31:39.714
  Jul 15 12:31:39.717: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-projected-secrets-9a9c9cbc-f111-4494-8607-f61b29c63eab container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/15/23 12:31:39.73
  Jul 15 12:31:39.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3908" for this suite. @ 07/15/23 12:31:39.75
• [4.086 seconds]
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 07/15/23 12:31:39.758
  Jul 15 12:31:39.758: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-runtime @ 07/15/23 12:31:39.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:31:39.768
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:31:39.77
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 07/15/23 12:31:39.778
  E0715 12:31:40.583463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:41.584509      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:42.584882      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:43.584976      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:44.585076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:45.585152      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:46.586139      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:47.587207      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:48.587292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:49.587709      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:50.588130      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:51.588400      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:52.588736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:53.589686      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:54.590293      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:55.590502      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:56.590694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:57.590736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 07/15/23 12:31:57.866
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 07/15/23 12:31:57.869
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 07/15/23 12:31:57.876
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 07/15/23 12:31:57.876
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 07/15/23 12:31:57.898
  E0715 12:31:58.590769      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:31:59.591585      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:00.592109      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 07/15/23 12:32:00.917
  E0715 12:32:01.592154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 07/15/23 12:32:01.926
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 07/15/23 12:32:01.931
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 07/15/23 12:32:01.931
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 07/15/23 12:32:01.952
  E0715 12:32:02.592365      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 07/15/23 12:32:02.962
  E0715 12:32:03.593112      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:04.593086      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 07/15/23 12:32:04.975
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 07/15/23 12:32:04.983
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 07/15/23 12:32:04.983
  Jul 15 12:32:04.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6511" for this suite. @ 07/15/23 12:32:05.012
• [25.263 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 07/15/23 12:32:05.021
  Jul 15 12:32:05.021: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/15/23 12:32:05.021
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:32:05.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:32:05.034
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 07/15/23 12:32:05.038
  Jul 15 12:32:05.038: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  E0715 12:32:05.593605      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:32:06.281: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  E0715 12:32:06.594226      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:07.594474      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:08.595083      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:09.595621      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:10.595577      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:32:11.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1385" for this suite. @ 07/15/23 12:32:11.344
• [6.330 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 07/15/23 12:32:11.352
  Jul 15 12:32:11.352: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename crd-webhook @ 07/15/23 12:32:11.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:32:11.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:32:11.366
  STEP: Setting up server cert @ 07/15/23 12:32:11.369
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 07/15/23 12:32:11.566
  STEP: Deploying the custom resource conversion webhook pod @ 07/15/23 12:32:11.573
  STEP: Wait for the deployment to be ready @ 07/15/23 12:32:11.584
  Jul 15 12:32:11.595: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0715 12:32:11.596214      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:12.596691      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:13.596647      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/15/23 12:32:13.606
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 12:32:13.615
  E0715 12:32:14.597133      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:32:14.616: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jul 15 12:32:14.619: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  E0715 12:32:15.597852      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:16.598251      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 07/15/23 12:32:17.176
  STEP: Create a v2 custom resource @ 07/15/23 12:32:17.19
  STEP: List CRs in v1 @ 07/15/23 12:32:17.214
  STEP: List CRs in v2 @ 07/15/23 12:32:17.218
  Jul 15 12:32:17.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0715 12:32:17.598580      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-6967" for this suite. @ 07/15/23 12:32:17.773
• [6.428 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 07/15/23 12:32:17.781
  Jul 15 12:32:17.781: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename job @ 07/15/23 12:32:17.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:32:17.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:32:17.793
  STEP: Creating a suspended job @ 07/15/23 12:32:17.799
  STEP: Patching the Job @ 07/15/23 12:32:17.802
  STEP: Watching for Job to be patched @ 07/15/23 12:32:17.816
  Jul 15 12:32:17.817: INFO: Event ADDED observed for Job e2e-rbp2l in namespace job-9945 with labels: map[e2e-job-label:e2e-rbp2l] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jul 15 12:32:17.817: INFO: Event MODIFIED observed for Job e2e-rbp2l in namespace job-9945 with labels: map[e2e-job-label:e2e-rbp2l] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jul 15 12:32:17.817: INFO: Event MODIFIED found for Job e2e-rbp2l in namespace job-9945 with labels: map[e2e-job-label:e2e-rbp2l e2e-rbp2l:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 07/15/23 12:32:17.817
  STEP: Watching for Job to be updated @ 07/15/23 12:32:17.826
  Jul 15 12:32:17.827: INFO: Event MODIFIED found for Job e2e-rbp2l in namespace job-9945 with labels: map[e2e-job-label:e2e-rbp2l e2e-rbp2l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 15 12:32:17.827: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 07/15/23 12:32:17.827
  Jul 15 12:32:17.830: INFO: Job: e2e-rbp2l as labels: map[e2e-job-label:e2e-rbp2l e2e-rbp2l:patched]
  STEP: Waiting for job to complete @ 07/15/23 12:32:17.83
  E0715 12:32:18.599313      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:19.599974      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:20.600292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:21.600650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:22.601356      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:23.602170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:24.602417      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:25.602944      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 07/15/23 12:32:25.835
  STEP: Watching for Job to be deleted @ 07/15/23 12:32:25.843
  Jul 15 12:32:25.845: INFO: Event MODIFIED observed for Job e2e-rbp2l in namespace job-9945 with labels: map[e2e-job-label:e2e-rbp2l e2e-rbp2l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 15 12:32:25.845: INFO: Event MODIFIED observed for Job e2e-rbp2l in namespace job-9945 with labels: map[e2e-job-label:e2e-rbp2l e2e-rbp2l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 15 12:32:25.845: INFO: Event MODIFIED observed for Job e2e-rbp2l in namespace job-9945 with labels: map[e2e-job-label:e2e-rbp2l e2e-rbp2l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 15 12:32:25.845: INFO: Event MODIFIED observed for Job e2e-rbp2l in namespace job-9945 with labels: map[e2e-job-label:e2e-rbp2l e2e-rbp2l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 15 12:32:25.845: INFO: Event MODIFIED observed for Job e2e-rbp2l in namespace job-9945 with labels: map[e2e-job-label:e2e-rbp2l e2e-rbp2l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 15 12:32:25.845: INFO: Event DELETED found for Job e2e-rbp2l in namespace job-9945 with labels: map[e2e-job-label:e2e-rbp2l e2e-rbp2l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 07/15/23 12:32:25.845
  Jul 15 12:32:25.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9945" for this suite. @ 07/15/23 12:32:25.853
• [8.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 07/15/23 12:32:25.864
  Jul 15 12:32:25.864: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:32:25.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:32:25.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:32:25.878
  STEP: Creating configMap with name projected-configmap-test-volume-70abe26b-475a-450e-a64c-821c07ebeba5 @ 07/15/23 12:32:25.881
  STEP: Creating a pod to test consume configMaps @ 07/15/23 12:32:25.885
  E0715 12:32:26.603382      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:27.603473      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:28.604369      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:29.604401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:32:29.905
  Jul 15 12:32:29.909: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-projected-configmaps-18559120-e6db-418a-a783-0301c9903190 container agnhost-container: <nil>
  STEP: delete the pod @ 07/15/23 12:32:29.915
  Jul 15 12:32:29.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7509" for this suite. @ 07/15/23 12:32:29.934
• [4.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 07/15/23 12:32:29.942
  Jul 15 12:32:29.942: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename resourcequota @ 07/15/23 12:32:29.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:32:29.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:32:29.957
  STEP: Discovering how many secrets are in namespace by default @ 07/15/23 12:32:29.96
  E0715 12:32:30.605478      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:31.605623      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:32.606065      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:33.606358      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:34.606463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 07/15/23 12:32:34.964
  E0715 12:32:35.607350      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:36.607748      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:37.608796      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:38.609582      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:39.610167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/15/23 12:32:39.969
  STEP: Ensuring resource quota status is calculated @ 07/15/23 12:32:39.975
  E0715 12:32:40.610295      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:41.610488      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 07/15/23 12:32:41.98
  STEP: Ensuring resource quota status captures secret creation @ 07/15/23 12:32:41.99
  E0715 12:32:42.610764      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:43.610957      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 07/15/23 12:32:43.994
  STEP: Ensuring resource quota status released usage @ 07/15/23 12:32:44.002
  E0715 12:32:44.611324      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:45.611474      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:32:46.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6388" for this suite. @ 07/15/23 12:32:46.011
• [16.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 07/15/23 12:32:46.017
  Jul 15 12:32:46.017: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename hostport @ 07/15/23 12:32:46.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:32:46.029
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:32:46.032
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 07/15/23 12:32:46.038
  E0715 12:32:46.611672      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:47.612423      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.42.138 on the node which pod1 resides and expect scheduled @ 07/15/23 12:32:48.055
  E0715 12:32:48.612932      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:49.613633      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:50.613864      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:51.614258      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:52.614954      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:53.615108      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:54.616090      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:55.616645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:56.617511      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:57.618222      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:58.619015      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:32:59.619191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.42.138 but use UDP protocol on the node which pod2 resides @ 07/15/23 12:33:00.092
  E0715 12:33:00.619247      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:01.619426      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:02.620363      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:03.620618      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 07/15/23 12:33:04.124
  Jul 15 12:33:04.124: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.42.138 http://127.0.0.1:54323/hostname] Namespace:hostport-3530 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 12:33:04.124: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 12:33:04.124: INFO: ExecWithOptions: Clientset creation
  Jul 15 12:33:04.124: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-3530/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.42.138+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.42.138, port: 54323 @ 07/15/23 12:33:04.192
  Jul 15 12:33:04.193: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.42.138:54323/hostname] Namespace:hostport-3530 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 12:33:04.193: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 12:33:04.193: INFO: ExecWithOptions: Clientset creation
  Jul 15 12:33:04.193: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-3530/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.42.138%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.42.138, port: 54323 UDP @ 07/15/23 12:33:04.248
  Jul 15 12:33:04.248: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.42.138 54323] Namespace:hostport-3530 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 12:33:04.248: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 12:33:04.249: INFO: ExecWithOptions: Clientset creation
  Jul 15 12:33:04.249: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-3530/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.42.138+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0715 12:33:04.621528      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:05.622041      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:06.622285      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:07.622554      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:08.622760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:33:09.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-3530" for this suite. @ 07/15/23 12:33:09.291
• [23.281 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 07/15/23 12:33:09.299
  Jul 15 12:33:09.299: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 12:33:09.299
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:33:09.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:33:09.313
  STEP: Setting up server cert @ 07/15/23 12:33:09.425
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 12:33:09.621
  E0715 12:33:09.623378      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook pod @ 07/15/23 12:33:09.629
  STEP: Wait for the deployment to be ready @ 07/15/23 12:33:09.639
  Jul 15 12:33:09.649: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0715 12:33:10.624242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:11.624534      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/15/23 12:33:11.662
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 12:33:11.675
  E0715 12:33:12.625088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:33:12.676: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 07/15/23 12:33:12.679
  STEP: create a configmap that should be updated by the webhook @ 07/15/23 12:33:12.694
  Jul 15 12:33:12.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8768" for this suite. @ 07/15/23 12:33:12.742
  STEP: Destroying namespace "webhook-markers-1522" for this suite. @ 07/15/23 12:33:12.749
• [3.457 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 07/15/23 12:33:12.756
  Jul 15 12:33:12.756: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename job @ 07/15/23 12:33:12.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:33:12.767
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:33:12.769
  STEP: Creating a job @ 07/15/23 12:33:12.772
  STEP: Ensure pods equal to parallelism count is attached to the job @ 07/15/23 12:33:12.778
  E0715 12:33:13.625141      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:14.626157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 07/15/23 12:33:14.783
  STEP: updating /status @ 07/15/23 12:33:14.79
  STEP: get /status @ 07/15/23 12:33:14.799
  Jul 15 12:33:14.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2275" for this suite. @ 07/15/23 12:33:14.806
• [2.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 07/15/23 12:33:14.813
  Jul 15 12:33:14.813: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename resourcequota @ 07/15/23 12:33:14.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:33:14.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:33:14.826
  STEP: Counting existing ResourceQuota @ 07/15/23 12:33:14.829
  E0715 12:33:15.626565      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:16.627036      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:17.627457      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:18.628079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:19.628392      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/15/23 12:33:19.834
  STEP: Ensuring resource quota status is calculated @ 07/15/23 12:33:19.84
  E0715 12:33:20.629146      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:21.630143      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:33:21.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9606" for this suite. @ 07/15/23 12:33:21.848
• [7.042 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 07/15/23 12:33:21.855
  Jul 15 12:33:21.855: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename var-expansion @ 07/15/23 12:33:21.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:33:21.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:33:21.869
  STEP: Creating a pod to test substitution in container's args @ 07/15/23 12:33:21.872
  E0715 12:33:22.630237      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:23.630476      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:24.631454      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:25.631530      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:33:25.892
  Jul 15 12:33:25.895: INFO: Trying to get logs from node ip-172-31-16-190 pod var-expansion-a143550a-9de8-4954-bc47-74826dfea616 container dapi-container: <nil>
  STEP: delete the pod @ 07/15/23 12:33:25.903
  Jul 15 12:33:25.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1433" for this suite. @ 07/15/23 12:33:25.922
• [4.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 07/15/23 12:33:25.929
  Jul 15 12:33:25.929: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:33:25.929
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:33:25.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:33:25.942
  STEP: Creating configMap with name cm-test-opt-del-17c7d018-9bce-4178-b9c3-21e9e4e8284a @ 07/15/23 12:33:25.947
  STEP: Creating configMap with name cm-test-opt-upd-1d7efdee-b1ea-41d5-960f-0453a177f7bb @ 07/15/23 12:33:25.95
  STEP: Creating the pod @ 07/15/23 12:33:25.956
  E0715 12:33:26.632412      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:27.632597      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-17c7d018-9bce-4178-b9c3-21e9e4e8284a @ 07/15/23 12:33:28.008
  STEP: Updating configmap cm-test-opt-upd-1d7efdee-b1ea-41d5-960f-0453a177f7bb @ 07/15/23 12:33:28.015
  STEP: Creating configMap with name cm-test-opt-create-24a552a6-ea63-49cd-a49d-463c3964d9f7 @ 07/15/23 12:33:28.019
  STEP: waiting to observe update in volume @ 07/15/23 12:33:28.023
  E0715 12:33:28.632668      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:29.632905      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:30.633072      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:31.633264      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:32.633590      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:33.634164      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:34.634829      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:35.635448      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:36.635440      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:37.635730      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:38.636187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:39.636442      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:40.636676      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:41.636861      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:42.637165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:43.637275      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:44.637998      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:45.638595      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:46.638926      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:47.639191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:48.639661      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:49.639940      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:50.640180      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:51.640427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:52.640979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:53.641106      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:54.641588      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:55.641795      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:56.642182      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:57.643224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:58.644157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:33:59.644336      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:00.644467      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:01.644605      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:02.645561      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:03.645766      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:04.646076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:05.646677      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:06.647539      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:07.647665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:08.648223      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:09.648524      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:10.649268      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:11.650067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:12.650206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:13.650534      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:14.651210      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:15.651328      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:16.651931      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:17.652156      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:18.652245      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:19.652384      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:20.653214      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:21.654170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:22.654359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:23.654541      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:24.654678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:25.655144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:26.655873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:27.656084      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:28.657136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:29.658207      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:30.658931      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:31.659193      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:32.659711      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:33.659960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:34.660235      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:35.660720      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:36.661231      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:37.661346      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:38.662130      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:39.662773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:40.663531      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:41.663798      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:42.664479      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:43.664694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:44.665422      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:45.666022      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:46.666946      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:47.667062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:48.667853      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:49.668074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:50.668169      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:51.668419      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:34:52.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6013" for this suite. @ 07/15/23 12:34:52.426
• [86.502 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 07/15/23 12:34:52.432
  Jul 15 12:34:52.432: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:34:52.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:34:52.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:34:52.448
  STEP: Creating the pod @ 07/15/23 12:34:52.45
  E0715 12:34:52.669233      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:53.669505      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:54.670336      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:34:54.990: INFO: Successfully updated pod "annotationupdatefe33ff87-783a-495b-bd2e-7857a851a483"
  E0715 12:34:55.670889      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:56.671028      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:57.671545      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:34:58.671723      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:34:59.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4591" for this suite. @ 07/15/23 12:34:59.016
• [6.591 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 07/15/23 12:34:59.024
  Jul 15 12:34:59.024: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 12:34:59.024
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:34:59.036
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:34:59.039
  STEP: Setting up server cert @ 07/15/23 12:34:59.07
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 12:34:59.427
  STEP: Deploying the webhook pod @ 07/15/23 12:34:59.435
  STEP: Wait for the deployment to be ready @ 07/15/23 12:34:59.447
  Jul 15 12:34:59.459: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0715 12:34:59.672370      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:00.672548      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/15/23 12:35:01.471
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 12:35:01.48
  E0715 12:35:01.672699      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:35:02.480: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 07/15/23 12:35:02.486
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 07/15/23 12:35:02.487
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 07/15/23 12:35:02.487
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 07/15/23 12:35:02.487
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 07/15/23 12:35:02.488
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 07/15/23 12:35:02.488
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 07/15/23 12:35:02.489
  Jul 15 12:35:02.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9759" for this suite. @ 07/15/23 12:35:02.527
  STEP: Destroying namespace "webhook-markers-6512" for this suite. @ 07/15/23 12:35:02.535
• [3.516 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 07/15/23 12:35:02.54
  Jul 15 12:35:02.540: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename containers @ 07/15/23 12:35:02.541
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:35:02.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:35:02.555
  STEP: Creating a pod to test override command @ 07/15/23 12:35:02.557
  E0715 12:35:02.673329      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:03.674360      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:04.675167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:05.675709      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:35:06.579
  Jul 15 12:35:06.583: INFO: Trying to get logs from node ip-172-31-16-190 pod client-containers-6daa21a1-f5e8-4fdf-a069-c1be93ac2e40 container agnhost-container: <nil>
  STEP: delete the pod @ 07/15/23 12:35:06.589
  Jul 15 12:35:06.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-978" for this suite. @ 07/15/23 12:35:06.606
• [4.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 07/15/23 12:35:06.613
  Jul 15 12:35:06.613: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename subpath @ 07/15/23 12:35:06.614
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:35:06.629
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:35:06.632
  STEP: Setting up data @ 07/15/23 12:35:06.635
  STEP: Creating pod pod-subpath-test-configmap-tw2d @ 07/15/23 12:35:06.643
  STEP: Creating a pod to test atomic-volume-subpath @ 07/15/23 12:35:06.643
  E0715 12:35:06.676173      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:07.676431      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:08.676820      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:09.677026      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:10.677183      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:11.678167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:12.679158      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:13.679372      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:14.680105      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:15.680645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:16.681360      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:17.681530      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:18.681988      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:19.682490      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:20.682819      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:21.683037      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:22.683172      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:23.683579      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:24.683903      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:25.684701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:26.685035      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:27.685083      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:28.685184      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:35:28.704
  Jul 15 12:35:28.708: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-subpath-test-configmap-tw2d container test-container-subpath-configmap-tw2d: <nil>
  STEP: delete the pod @ 07/15/23 12:35:28.714
  STEP: Deleting pod pod-subpath-test-configmap-tw2d @ 07/15/23 12:35:28.726
  Jul 15 12:35:28.726: INFO: Deleting pod "pod-subpath-test-configmap-tw2d" in namespace "subpath-5571"
  Jul 15 12:35:28.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5571" for this suite. @ 07/15/23 12:35:28.733
• [22.126 seconds]
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 07/15/23 12:35:28.74
  Jul 15 12:35:28.740: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename subpath @ 07/15/23 12:35:28.74
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:35:28.758
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:35:28.761
  STEP: Setting up data @ 07/15/23 12:35:28.763
  STEP: Creating pod pod-subpath-test-secret-rqv9 @ 07/15/23 12:35:28.771
  STEP: Creating a pod to test atomic-volume-subpath @ 07/15/23 12:35:28.771
  E0715 12:35:29.685434      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:30.686029      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:31.686224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:32.686496      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:33.686595      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:34.686716      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:35.687675      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:36.687929      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:37.688130      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:38.688419      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:39.688469      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:40.688547      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:41.688624      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:42.688786      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:43.689434      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:44.689713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:45.690312      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:46.690410      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:47.690592      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:48.690932      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:49.691345      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:50.691592      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:35:50.835
  Jul 15 12:35:50.838: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-subpath-test-secret-rqv9 container test-container-subpath-secret-rqv9: <nil>
  STEP: delete the pod @ 07/15/23 12:35:50.846
  STEP: Deleting pod pod-subpath-test-secret-rqv9 @ 07/15/23 12:35:50.86
  Jul 15 12:35:50.860: INFO: Deleting pod "pod-subpath-test-secret-rqv9" in namespace "subpath-1241"
  Jul 15 12:35:50.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1241" for this suite. @ 07/15/23 12:35:50.867
• [22.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 07/15/23 12:35:50.874
  Jul 15 12:35:50.874: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename namespaces @ 07/15/23 12:35:50.874
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:35:50.888
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:35:50.891
  STEP: Updating Namespace "namespaces-3027" @ 07/15/23 12:35:50.894
  Jul 15 12:35:50.903: INFO: Namespace "namespaces-3027" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"18cdf2d3-4826-4237-a5b2-e9decef30f4d", "kubernetes.io/metadata.name":"namespaces-3027", "namespaces-3027":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Jul 15 12:35:50.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3027" for this suite. @ 07/15/23 12:35:50.908
• [0.041 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 07/15/23 12:35:50.914
  Jul 15 12:35:50.915: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename var-expansion @ 07/15/23 12:35:50.915
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:35:50.925
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:35:50.928
  STEP: Creating a pod to test substitution in volume subpath @ 07/15/23 12:35:50.93
  E0715 12:35:51.692139      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:52.692425      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:53.692884      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:54.693073      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:35:54.948
  Jul 15 12:35:54.952: INFO: Trying to get logs from node ip-172-31-16-190 pod var-expansion-3269e7e1-9f52-4cc8-8934-cb9f9af9f0e9 container dapi-container: <nil>
  STEP: delete the pod @ 07/15/23 12:35:54.959
  Jul 15 12:35:54.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9161" for this suite. @ 07/15/23 12:35:54.974
• [4.066 seconds]
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 07/15/23 12:35:54.981
  Jul 15 12:35:54.981: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename security-context-test @ 07/15/23 12:35:54.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:35:54.993
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:35:54.995
  E0715 12:35:55.693971      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:56.694175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:57.695220      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:35:58.695401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:35:59.027: INFO: Got logs for pod "busybox-privileged-false-1355f0f8-6e76-4ef2-933f-2e80071101b7": "ip: RTNETLINK answers: Operation not permitted\n"
  Jul 15 12:35:59.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-844" for this suite. @ 07/15/23 12:35:59.031
• [4.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 07/15/23 12:35:59.039
  Jul 15 12:35:59.039: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename tables @ 07/15/23 12:35:59.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:35:59.051
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:35:59.054
  Jul 15 12:35:59.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-1956" for this suite. @ 07/15/23 12:35:59.062
• [0.029 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 07/15/23 12:35:59.071
  Jul 15 12:35:59.071: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename pod-network-test @ 07/15/23 12:35:59.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:35:59.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:35:59.085
  STEP: Performing setup for networking test in namespace pod-network-test-9056 @ 07/15/23 12:35:59.087
  STEP: creating a selector @ 07/15/23 12:35:59.087
  STEP: Creating the service pods in kubernetes @ 07/15/23 12:35:59.087
  Jul 15 12:35:59.087: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0715 12:35:59.695705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:00.696060      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:01.696200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:02.696405      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:03.697223      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:04.697330      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:05.697452      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:06.697530      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:07.698148      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:08.698321      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:09.698371      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:10.698749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 07/15/23 12:36:11.161
  E0715 12:36:11.699434      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:12.699605      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:36:13.188: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 15 12:36:13.188: INFO: Going to poll 192.168.33.68 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul 15 12:36:13.191: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.33.68 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9056 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 12:36:13.191: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 12:36:13.191: INFO: ExecWithOptions: Clientset creation
  Jul 15 12:36:13.191: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9056/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.33.68+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0715 12:36:13.700587      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:36:14.261: INFO: Found all 1 expected endpoints: [netserver-0]
  Jul 15 12:36:14.261: INFO: Going to poll 192.168.191.231 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul 15 12:36:14.266: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.191.231 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9056 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 12:36:14.266: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 12:36:14.267: INFO: ExecWithOptions: Clientset creation
  Jul 15 12:36:14.267: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9056/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.191.231+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0715 12:36:14.700823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:36:15.319: INFO: Found all 1 expected endpoints: [netserver-1]
  Jul 15 12:36:15.319: INFO: Going to poll 192.168.4.16 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul 15 12:36:15.322: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.4.16 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9056 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 12:36:15.322: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 12:36:15.323: INFO: ExecWithOptions: Clientset creation
  Jul 15 12:36:15.323: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9056/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.4.16+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0715 12:36:15.701373      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:36:16.378: INFO: Found all 1 expected endpoints: [netserver-2]
  Jul 15 12:36:16.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-9056" for this suite. @ 07/15/23 12:36:16.383
• [17.320 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 07/15/23 12:36:16.391
  Jul 15 12:36:16.391: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename configmap @ 07/15/23 12:36:16.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:36:16.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:36:16.404
  Jul 15 12:36:16.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6133" for this suite. @ 07/15/23 12:36:16.446
• [0.062 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 07/15/23 12:36:16.453
  Jul 15 12:36:16.453: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename svcaccounts @ 07/15/23 12:36:16.454
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:36:16.463
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:36:16.465
  STEP: Creating a pod to test service account token:  @ 07/15/23 12:36:16.468
  E0715 12:36:16.702152      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:17.702872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:18.703531      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:19.703741      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:36:20.489
  Jul 15 12:36:20.493: INFO: Trying to get logs from node ip-172-31-16-190 pod test-pod-784418a4-4083-467d-8b73-c393906d4963 container agnhost-container: <nil>
  STEP: delete the pod @ 07/15/23 12:36:20.499
  Jul 15 12:36:20.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3025" for this suite. @ 07/15/23 12:36:20.518
• [4.071 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 07/15/23 12:36:20.525
  Jul 15 12:36:20.525: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename limitrange @ 07/15/23 12:36:20.526
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:36:20.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:36:20.54
  STEP: Creating LimitRange "e2e-limitrange-77qbc" in namespace "limitrange-6314" @ 07/15/23 12:36:20.542
  STEP: Creating another limitRange in another namespace @ 07/15/23 12:36:20.546
  Jul 15 12:36:20.555: INFO: Namespace "e2e-limitrange-77qbc-6648" created
  Jul 15 12:36:20.555: INFO: Creating LimitRange "e2e-limitrange-77qbc" in namespace "e2e-limitrange-77qbc-6648"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-77qbc" @ 07/15/23 12:36:20.559
  Jul 15 12:36:20.563: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-77qbc" in "limitrange-6314" namespace @ 07/15/23 12:36:20.563
  Jul 15 12:36:20.569: INFO: LimitRange "e2e-limitrange-77qbc" has been patched
  STEP: Delete LimitRange "e2e-limitrange-77qbc" by Collection with labelSelector: "e2e-limitrange-77qbc=patched" @ 07/15/23 12:36:20.569
  STEP: Confirm that the limitRange "e2e-limitrange-77qbc" has been deleted @ 07/15/23 12:36:20.576
  Jul 15 12:36:20.576: INFO: Requesting list of LimitRange to confirm quantity
  Jul 15 12:36:20.579: INFO: Found 0 LimitRange with label "e2e-limitrange-77qbc=patched"
  Jul 15 12:36:20.579: INFO: LimitRange "e2e-limitrange-77qbc" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-77qbc" @ 07/15/23 12:36:20.579
  Jul 15 12:36:20.583: INFO: Found 1 limitRange
  Jul 15 12:36:20.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-6314" for this suite. @ 07/15/23 12:36:20.588
  STEP: Destroying namespace "e2e-limitrange-77qbc-6648" for this suite. @ 07/15/23 12:36:20.595
• [0.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 07/15/23 12:36:20.612
  Jul 15 12:36:20.612: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename daemonsets @ 07/15/23 12:36:20.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:36:20.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:36:20.627
  STEP: Creating simple DaemonSet "daemon-set" @ 07/15/23 12:36:20.648
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/15/23 12:36:20.654
  Jul 15 12:36:20.657: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:36:20.657: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:36:20.660: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:36:20.660: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  E0715 12:36:20.704339      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:36:21.664: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:36:21.664: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:36:21.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 15 12:36:21.669: INFO: Node ip-172-31-42-138 is running 0 daemon pod, expected 1
  E0715 12:36:21.705298      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:36:22.664: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:36:22.664: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:36:22.668: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 15 12:36:22.668: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 07/15/23 12:36:22.671
  Jul 15 12:36:22.675: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 07/15/23 12:36:22.675
  Jul 15 12:36:22.684: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 07/15/23 12:36:22.684
  Jul 15 12:36:22.685: INFO: Observed &DaemonSet event: ADDED
  Jul 15 12:36:22.686: INFO: Observed &DaemonSet event: MODIFIED
  Jul 15 12:36:22.686: INFO: Observed &DaemonSet event: MODIFIED
  Jul 15 12:36:22.686: INFO: Observed &DaemonSet event: MODIFIED
  Jul 15 12:36:22.686: INFO: Observed &DaemonSet event: MODIFIED
  Jul 15 12:36:22.686: INFO: Found daemon set daemon-set in namespace daemonsets-371 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 15 12:36:22.686: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 07/15/23 12:36:22.686
  STEP: watching for the daemon set status to be patched @ 07/15/23 12:36:22.693
  Jul 15 12:36:22.694: INFO: Observed &DaemonSet event: ADDED
  Jul 15 12:36:22.694: INFO: Observed &DaemonSet event: MODIFIED
  Jul 15 12:36:22.694: INFO: Observed &DaemonSet event: MODIFIED
  Jul 15 12:36:22.694: INFO: Observed &DaemonSet event: MODIFIED
  Jul 15 12:36:22.694: INFO: Observed &DaemonSet event: MODIFIED
  Jul 15 12:36:22.694: INFO: Observed daemon set daemon-set in namespace daemonsets-371 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 15 12:36:22.694: INFO: Observed &DaemonSet event: MODIFIED
  Jul 15 12:36:22.695: INFO: Found daemon set daemon-set in namespace daemonsets-371 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Jul 15 12:36:22.695: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 07/15/23 12:36:22.698
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-371, will wait for the garbage collector to delete the pods @ 07/15/23 12:36:22.698
  E0715 12:36:22.705501      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:36:22.758: INFO: Deleting DaemonSet.extensions daemon-set took: 5.668258ms
  Jul 15 12:36:22.859: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.74044ms
  E0715 12:36:23.706463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:36:24.263: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:36:24.263: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 15 12:36:24.267: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16556"},"items":null}

  Jul 15 12:36:24.270: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16556"},"items":null}

  Jul 15 12:36:24.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-371" for this suite. @ 07/15/23 12:36:24.289
• [3.685 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 07/15/23 12:36:24.297
  Jul 15 12:36:24.297: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:36:24.297
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:36:24.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:36:24.309
  STEP: Creating configMap with name projected-configmap-test-volume-6da43299-3941-42cc-ba73-31668152a233 @ 07/15/23 12:36:24.311
  STEP: Creating a pod to test consume configMaps @ 07/15/23 12:36:24.316
  E0715 12:36:24.707111      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:25.707750      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:26.708504      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:27.708723      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:36:28.339
  Jul 15 12:36:28.342: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-projected-configmaps-3e36b315-6e95-4de8-83c5-3c83253d95fe container agnhost-container: <nil>
  STEP: delete the pod @ 07/15/23 12:36:28.348
  Jul 15 12:36:28.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-716" for this suite. @ 07/15/23 12:36:28.372
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 07/15/23 12:36:28.378
  Jul 15 12:36:28.378: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/15/23 12:36:28.379
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:36:28.388
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:36:28.391
  Jul 15 12:36:28.393: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  E0715 12:36:28.709494      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:36:28.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5982" for this suite. @ 07/15/23 12:36:28.933
• [0.561 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 07/15/23 12:36:28.941
  Jul 15 12:36:28.941: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-probe @ 07/15/23 12:36:28.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:36:28.96
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:36:28.963
  E0715 12:36:29.710173      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:30.710559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:31.711187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:32.711372      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:33.712001      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:34.712318      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:35.712532      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:36.713321      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:37.714163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:38.714253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:39.714732      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:40.715125      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:41.715305      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:42.715678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:43.715873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:44.716260      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:45.716991      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:46.717186      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:47.717758      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:48.717835      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:49.718496      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:50.718991      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:36:51.037: INFO: Container started at 2023-07-15 12:36:29 +0000 UTC, pod became ready at 2023-07-15 12:36:49 +0000 UTC
  Jul 15 12:36:51.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-6757" for this suite. @ 07/15/23 12:36:51.04
• [22.106 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 07/15/23 12:36:51.048
  Jul 15 12:36:51.048: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename pod-network-test @ 07/15/23 12:36:51.049
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:36:51.06
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:36:51.063
  STEP: Performing setup for networking test in namespace pod-network-test-6451 @ 07/15/23 12:36:51.065
  STEP: creating a selector @ 07/15/23 12:36:51.065
  STEP: Creating the service pods in kubernetes @ 07/15/23 12:36:51.065
  Jul 15 12:36:51.065: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0715 12:36:51.719128      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:52.719471      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:53.719698      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:54.719937      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:55.720373      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:56.720592      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:57.721487      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:58.722259      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:36:59.723047      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:00.723381      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:01.723494      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:02.723982      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 07/15/23 12:37:03.149
  E0715 12:37:03.724552      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:04.724809      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:37:05.164: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 15 12:37:05.164: INFO: Breadth first check of 192.168.33.75 on host 172.31.16.190...
  Jul 15 12:37:05.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.33.73:9080/dial?request=hostname&protocol=udp&host=192.168.33.75&port=8081&tries=1'] Namespace:pod-network-test-6451 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 12:37:05.168: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 12:37:05.169: INFO: ExecWithOptions: Clientset creation
  Jul 15 12:37:05.169: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6451/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.33.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.33.75%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 15 12:37:05.226: INFO: Waiting for responses: map[]
  Jul 15 12:37:05.226: INFO: reached 192.168.33.75 after 0/1 tries
  Jul 15 12:37:05.226: INFO: Breadth first check of 192.168.191.233 on host 172.31.42.138...
  Jul 15 12:37:05.229: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.33.73:9080/dial?request=hostname&protocol=udp&host=192.168.191.233&port=8081&tries=1'] Namespace:pod-network-test-6451 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 12:37:05.229: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 12:37:05.229: INFO: ExecWithOptions: Clientset creation
  Jul 15 12:37:05.229: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6451/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.33.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.191.233%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 15 12:37:05.287: INFO: Waiting for responses: map[]
  Jul 15 12:37:05.287: INFO: reached 192.168.191.233 after 0/1 tries
  Jul 15 12:37:05.287: INFO: Breadth first check of 192.168.4.18 on host 172.31.84.236...
  Jul 15 12:37:05.291: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.33.73:9080/dial?request=hostname&protocol=udp&host=192.168.4.18&port=8081&tries=1'] Namespace:pod-network-test-6451 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 12:37:05.291: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 12:37:05.291: INFO: ExecWithOptions: Clientset creation
  Jul 15 12:37:05.291: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6451/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.33.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.4.18%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 15 12:37:05.328: INFO: Waiting for responses: map[]
  Jul 15 12:37:05.328: INFO: reached 192.168.4.18 after 0/1 tries
  Jul 15 12:37:05.328: INFO: Going to retry 0 out of 3 pods....
  Jul 15 12:37:05.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6451" for this suite. @ 07/15/23 12:37:05.332
• [14.291 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 07/15/23 12:37:05.339
  Jul 15 12:37:05.339: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename configmap @ 07/15/23 12:37:05.34
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:37:05.352
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:37:05.354
  STEP: Creating configMap with name configmap-test-volume-map-4fa5f13c-5180-4881-ad5f-154e77bce36c @ 07/15/23 12:37:05.357
  STEP: Creating a pod to test consume configMaps @ 07/15/23 12:37:05.36
  E0715 12:37:05.724858      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:06.725154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:07.725129      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:08.726189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:37:09.383
  Jul 15 12:37:09.386: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-configmaps-1df9bbb1-cdd4-4d15-88b9-2db43d9fe31e container agnhost-container: <nil>
  STEP: delete the pod @ 07/15/23 12:37:09.393
  Jul 15 12:37:09.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5576" for this suite. @ 07/15/23 12:37:09.413
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 07/15/23 12:37:09.421
  Jul 15 12:37:09.421: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir @ 07/15/23 12:37:09.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:37:09.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:37:09.435
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 07/15/23 12:37:09.438
  E0715 12:37:09.727191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:10.728058      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:11.728879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:12.729158      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:37:13.46
  Jul 15 12:37:13.463: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-12701826-fd2b-4af9-a15a-c62fb6ffc4cc container test-container: <nil>
  STEP: delete the pod @ 07/15/23 12:37:13.471
  Jul 15 12:37:13.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6557" for this suite. @ 07/15/23 12:37:13.489
• [4.075 seconds]
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 07/15/23 12:37:13.496
  Jul 15 12:37:13.496: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename replication-controller @ 07/15/23 12:37:13.497
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:37:13.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:37:13.511
  STEP: Creating replication controller my-hostname-basic-cf274063-89fa-43a1-b40e-96b45ee14279 @ 07/15/23 12:37:13.513
  Jul 15 12:37:13.524: INFO: Pod name my-hostname-basic-cf274063-89fa-43a1-b40e-96b45ee14279: Found 0 pods out of 1
  E0715 12:37:13.729939      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:14.730072      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:15.730521      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:16.730988      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:17.731175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:37:18.531: INFO: Pod name my-hostname-basic-cf274063-89fa-43a1-b40e-96b45ee14279: Found 1 pods out of 1
  Jul 15 12:37:18.531: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-cf274063-89fa-43a1-b40e-96b45ee14279" are running
  Jul 15 12:37:18.534: INFO: Pod "my-hostname-basic-cf274063-89fa-43a1-b40e-96b45ee14279-gfdfp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-15 12:37:13 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-15 12:37:14 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-15 12:37:14 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-15 12:37:13 +0000 UTC Reason: Message:}])
  Jul 15 12:37:18.534: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 07/15/23 12:37:18.534
  Jul 15 12:37:18.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2165" for this suite. @ 07/15/23 12:37:18.549
• [5.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 07/15/23 12:37:18.557
  Jul 15 12:37:18.557: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename pods @ 07/15/23 12:37:18.558
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:37:18.569
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:37:18.572
  STEP: creating a Pod with a static label @ 07/15/23 12:37:18.581
  STEP: watching for Pod to be ready @ 07/15/23 12:37:18.59
  Jul 15 12:37:18.592: INFO: observed Pod pod-test in namespace pods-6658 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Jul 15 12:37:18.594: INFO: observed Pod pod-test in namespace pods-6658 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 12:37:18 +0000 UTC  }]
  Jul 15 12:37:18.607: INFO: observed Pod pod-test in namespace pods-6658 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 12:37:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-15 12:37:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-15 12:37:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 12:37:18 +0000 UTC  }]
  E0715 12:37:18.731931      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:37:19.657: INFO: Found Pod pod-test in namespace pods-6658 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 12:37:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 12:37:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 12:37:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 12:37:18 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 07/15/23 12:37:19.66
  STEP: getting the Pod and ensuring that it's patched @ 07/15/23 12:37:19.666
  STEP: replacing the Pod's status Ready condition to False @ 07/15/23 12:37:19.672
  STEP: check the Pod again to ensure its Ready conditions are False @ 07/15/23 12:37:19.68
  STEP: deleting the Pod via a Collection with a LabelSelector @ 07/15/23 12:37:19.68
  STEP: watching for the Pod to be deleted @ 07/15/23 12:37:19.688
  Jul 15 12:37:19.689: INFO: observed event type MODIFIED
  E0715 12:37:19.732095      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:20.732616      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:37:21.663: INFO: observed event type MODIFIED
  E0715 12:37:21.733199      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:37:21.967: INFO: observed event type MODIFIED
  Jul 15 12:37:22.663: INFO: observed event type MODIFIED
  Jul 15 12:37:22.674: INFO: observed event type MODIFIED
  Jul 15 12:37:22.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6658" for this suite. @ 07/15/23 12:37:22.684
• [4.134 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 07/15/23 12:37:22.692
  Jul 15 12:37:22.692: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename crd-watch @ 07/15/23 12:37:22.692
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:37:22.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:37:22.705
  Jul 15 12:37:22.708: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  E0715 12:37:22.733745      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:23.734044      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:24.734292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 07/15/23 12:37:25.246
  Jul 15 12:37:25.251: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-15T12:37:25Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-15T12:37:25Z]] name:name1 resourceVersion:17076 uid:49b76b78-87be-4d34-bf23-e34493b84281] num:map[num1:9223372036854775807 num2:1000000]]}
  E0715 12:37:25.734360      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:26.734715      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:27.734903      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:28.735495      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:29.735665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:30.735792      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:31.735979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:32.736228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:33.736497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:34.737073      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 07/15/23 12:37:35.252
  Jul 15 12:37:35.259: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-15T12:37:35Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-15T12:37:35Z]] name:name2 resourceVersion:17108 uid:55e940e3-b625-4442-9e90-ff2da4b89c9a] num:map[num1:9223372036854775807 num2:1000000]]}
  E0715 12:37:35.737170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:36.738244      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:37.738854      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:38.739144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:39.739424      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:40.739624      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:41.739807      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:42.740066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:43.740260      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:44.740438      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 07/15/23 12:37:45.26
  Jul 15 12:37:45.267: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-15T12:37:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-15T12:37:45Z]] name:name1 resourceVersion:17128 uid:49b76b78-87be-4d34-bf23-e34493b84281] num:map[num1:9223372036854775807 num2:1000000]]}
  E0715 12:37:45.740930      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:46.741052      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:47.741235      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:48.741514      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:49.742136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:50.742234      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:51.742462      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:52.742720      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:53.742860      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:54.743059      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 07/15/23 12:37:55.268
  Jul 15 12:37:55.276: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-15T12:37:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-15T12:37:55Z]] name:name2 resourceVersion:17148 uid:55e940e3-b625-4442-9e90-ff2da4b89c9a] num:map[num1:9223372036854775807 num2:1000000]]}
  E0715 12:37:55.743981      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:56.744668      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:57.744918      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:58.745120      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:37:59.746215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:00.746586      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:01.746847      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:02.747176      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:03.747417      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:04.747669      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 07/15/23 12:38:05.277
  Jul 15 12:38:05.285: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-15T12:37:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-15T12:37:45Z]] name:name1 resourceVersion:17169 uid:49b76b78-87be-4d34-bf23-e34493b84281] num:map[num1:9223372036854775807 num2:1000000]]}
  E0715 12:38:05.748478      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:06.748876      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:07.749170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:08.750137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:09.750303      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:10.750437      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:11.751271      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:12.751408      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:13.751700      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:14.751914      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 07/15/23 12:38:15.286
  Jul 15 12:38:15.296: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-15T12:37:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-15T12:37:55Z]] name:name2 resourceVersion:17188 uid:55e940e3-b625-4442-9e90-ff2da4b89c9a] num:map[num1:9223372036854775807 num2:1000000]]}
  E0715 12:38:15.752498      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:16.752734      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:17.753073      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:18.753150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:19.754221      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:20.754668      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:21.754703      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:22.754660      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:23.754770      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:24.755088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:25.755150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:38:25.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-3101" for this suite. @ 07/15/23 12:38:25.814
• [63.130 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 07/15/23 12:38:25.822
  Jul 15 12:38:25.822: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename var-expansion @ 07/15/23 12:38:25.822
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:38:25.834
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:38:25.836
  E0715 12:38:26.756066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:27.756246      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:38:27.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 15 12:38:27.861: INFO: Deleting pod "var-expansion-7a90041e-68ad-435d-a3f4-64723e43cd5f" in namespace "var-expansion-3542"
  Jul 15 12:38:27.868: INFO: Wait up to 5m0s for pod "var-expansion-7a90041e-68ad-435d-a3f4-64723e43cd5f" to be fully deleted
  E0715 12:38:28.756420      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:29.756710      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-3542" for this suite. @ 07/15/23 12:38:29.876
• [4.060 seconds]
------------------------------
SS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 07/15/23 12:38:29.882
  Jul 15 12:38:29.882: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename cronjob @ 07/15/23 12:38:29.882
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:38:29.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:38:29.896
  STEP: Creating a ReplaceConcurrent cronjob @ 07/15/23 12:38:29.898
  STEP: Ensuring a job is scheduled @ 07/15/23 12:38:29.903
  E0715 12:38:30.757087      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:31.757461      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:32.757621      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:33.758257      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:34.759051      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:35.759343      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:36.759698      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:37.759973      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:38.760131      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:39.760350      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:40.760753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:41.761408      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:42.762162      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:43.762379      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:44.763200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:45.763901      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:46.764691      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:47.765009      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:48.765176      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:49.766170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:50.766777      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:51.767027      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:52.767960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:53.768173      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:54.768873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:55.769592      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:56.769750      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:57.770067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:58.770171      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:38:59.770464      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:00.770827      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:01.771104      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 07/15/23 12:39:01.912
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 07/15/23 12:39:01.916
  STEP: Ensuring the job is replaced with a new one @ 07/15/23 12:39:01.919
  E0715 12:39:02.771084      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:03.771369      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:04.772307      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:05.772853      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:06.773165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:07.774178      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:08.775214      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:09.775486      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:10.775722      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:11.776017      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:12.776925      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:13.777164      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:14.777326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:15.778161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:16.778680      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:17.778790      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:18.778976      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:19.779256      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:20.780088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:21.780370      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:22.781240      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:23.782185      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:24.782272      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:25.783095      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:26.783391      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:27.783597      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:28.783826      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:29.784116      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:30.784759      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:31.785009      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:32.785821      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:33.786213      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:34.787224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:35.787721      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:36.788157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:37.788437      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:38.788736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:39.789034      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:40.789389      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:41.790143      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:42.790200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:43.790401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:44.791144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:45.791399      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:46.792332      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:47.792533      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:48.793071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:49.793077      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:50.794155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:51.794333      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:52.794549      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:53.794633      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:54.795258      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:55.795890      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:56.796004      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:57.796290      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:58.796372      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:39:59.796545      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:00.796966      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:01.797210      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 07/15/23 12:40:01.923
  Jul 15 12:40:01.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5020" for this suite. @ 07/15/23 12:40:01.934
• [92.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 07/15/23 12:40:01.94
  Jul 15 12:40:01.940: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename namespaces @ 07/15/23 12:40:01.941
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:40:01.954
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:40:01.957
  STEP: Creating namespace "e2e-ns-88mjm" @ 07/15/23 12:40:01.959
  Jul 15 12:40:01.970: INFO: Namespace "e2e-ns-88mjm-9814" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-88mjm-9814" @ 07/15/23 12:40:01.97
  Jul 15 12:40:01.979: INFO: Namespace "e2e-ns-88mjm-9814" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-88mjm-9814" @ 07/15/23 12:40:01.979
  Jul 15 12:40:01.986: INFO: Namespace "e2e-ns-88mjm-9814" has []v1.FinalizerName{"kubernetes"}
  Jul 15 12:40:01.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5644" for this suite. @ 07/15/23 12:40:01.989
  STEP: Destroying namespace "e2e-ns-88mjm-9814" for this suite. @ 07/15/23 12:40:01.996
• [0.062 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 07/15/23 12:40:02.003
  Jul 15 12:40:02.003: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename gc @ 07/15/23 12:40:02.003
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:40:02.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:40:02.016
  STEP: create the rc @ 07/15/23 12:40:02.023
  W0715 12:40:02.029308      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0715 12:40:02.797801      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:03.798170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:04.798662      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:05.799072      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:06.799427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:07.799717      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 07/15/23 12:40:08.034
  STEP: wait for the rc to be deleted @ 07/15/23 12:40:08.041
  E0715 12:40:08.800474      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:09.800771      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:10.801103      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:11.801290      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:12.802161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 07/15/23 12:40:13.046
  E0715 12:40:13.802363      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:14.802545      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:15.803058      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:16.803214      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:17.803338      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:18.803497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:19.803742      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:20.804200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:21.804637      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:22.804911      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:23.805062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:24.805181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:25.805210      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:26.805421      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:27.806202      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:28.807223      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:29.807403      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:30.807603      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:31.807878      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:32.807998      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:33.808258      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:34.808357      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:35.808920      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:36.809215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:37.809466      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:38.810147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:39.810266      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:40.810512      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:41.810785      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:42.811030      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/15/23 12:40:43.058
  W0715 12:40:43.061672      23 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 15 12:40:43.061: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 15 12:40:43.061: INFO: Deleting pod "simpletest.rc-2cwqq" in namespace "gc-3145"
  Jul 15 12:40:43.072: INFO: Deleting pod "simpletest.rc-2ffbq" in namespace "gc-3145"
  Jul 15 12:40:43.082: INFO: Deleting pod "simpletest.rc-2vz6w" in namespace "gc-3145"
  Jul 15 12:40:43.094: INFO: Deleting pod "simpletest.rc-45bw9" in namespace "gc-3145"
  Jul 15 12:40:43.103: INFO: Deleting pod "simpletest.rc-4gfj5" in namespace "gc-3145"
  Jul 15 12:40:43.116: INFO: Deleting pod "simpletest.rc-542ks" in namespace "gc-3145"
  Jul 15 12:40:43.128: INFO: Deleting pod "simpletest.rc-596hz" in namespace "gc-3145"
  Jul 15 12:40:43.139: INFO: Deleting pod "simpletest.rc-5npmg" in namespace "gc-3145"
  Jul 15 12:40:43.154: INFO: Deleting pod "simpletest.rc-68tcn" in namespace "gc-3145"
  Jul 15 12:40:43.167: INFO: Deleting pod "simpletest.rc-6x982" in namespace "gc-3145"
  Jul 15 12:40:43.181: INFO: Deleting pod "simpletest.rc-7bkzj" in namespace "gc-3145"
  Jul 15 12:40:43.191: INFO: Deleting pod "simpletest.rc-7ghfl" in namespace "gc-3145"
  Jul 15 12:40:43.203: INFO: Deleting pod "simpletest.rc-7k2cg" in namespace "gc-3145"
  Jul 15 12:40:43.216: INFO: Deleting pod "simpletest.rc-7rfcd" in namespace "gc-3145"
  Jul 15 12:40:43.232: INFO: Deleting pod "simpletest.rc-7s9hg" in namespace "gc-3145"
  Jul 15 12:40:43.244: INFO: Deleting pod "simpletest.rc-7t9rw" in namespace "gc-3145"
  Jul 15 12:40:43.258: INFO: Deleting pod "simpletest.rc-7txcl" in namespace "gc-3145"
  Jul 15 12:40:43.266: INFO: Deleting pod "simpletest.rc-84xgc" in namespace "gc-3145"
  Jul 15 12:40:43.281: INFO: Deleting pod "simpletest.rc-8fx7s" in namespace "gc-3145"
  Jul 15 12:40:43.294: INFO: Deleting pod "simpletest.rc-8hdsz" in namespace "gc-3145"
  Jul 15 12:40:43.302: INFO: Deleting pod "simpletest.rc-8jdxp" in namespace "gc-3145"
  Jul 15 12:40:43.314: INFO: Deleting pod "simpletest.rc-8kjsd" in namespace "gc-3145"
  Jul 15 12:40:43.324: INFO: Deleting pod "simpletest.rc-8lfb5" in namespace "gc-3145"
  Jul 15 12:40:43.336: INFO: Deleting pod "simpletest.rc-9flxv" in namespace "gc-3145"
  Jul 15 12:40:43.348: INFO: Deleting pod "simpletest.rc-9mxz7" in namespace "gc-3145"
  Jul 15 12:40:43.357: INFO: Deleting pod "simpletest.rc-9nkfl" in namespace "gc-3145"
  Jul 15 12:40:43.369: INFO: Deleting pod "simpletest.rc-9wk69" in namespace "gc-3145"
  Jul 15 12:40:43.383: INFO: Deleting pod "simpletest.rc-bb8d8" in namespace "gc-3145"
  Jul 15 12:40:43.394: INFO: Deleting pod "simpletest.rc-bdqzt" in namespace "gc-3145"
  Jul 15 12:40:43.408: INFO: Deleting pod "simpletest.rc-btv9g" in namespace "gc-3145"
  Jul 15 12:40:43.420: INFO: Deleting pod "simpletest.rc-c69dc" in namespace "gc-3145"
  Jul 15 12:40:43.436: INFO: Deleting pod "simpletest.rc-cnjbp" in namespace "gc-3145"
  Jul 15 12:40:43.444: INFO: Deleting pod "simpletest.rc-d4snn" in namespace "gc-3145"
  Jul 15 12:40:43.457: INFO: Deleting pod "simpletest.rc-dsr4n" in namespace "gc-3145"
  Jul 15 12:40:43.466: INFO: Deleting pod "simpletest.rc-f8g8v" in namespace "gc-3145"
  Jul 15 12:40:43.478: INFO: Deleting pod "simpletest.rc-f9jjs" in namespace "gc-3145"
  Jul 15 12:40:43.488: INFO: Deleting pod "simpletest.rc-fjvm5" in namespace "gc-3145"
  Jul 15 12:40:43.498: INFO: Deleting pod "simpletest.rc-fxxtq" in namespace "gc-3145"
  Jul 15 12:40:43.510: INFO: Deleting pod "simpletest.rc-ggfzw" in namespace "gc-3145"
  Jul 15 12:40:43.523: INFO: Deleting pod "simpletest.rc-gjjjt" in namespace "gc-3145"
  Jul 15 12:40:43.533: INFO: Deleting pod "simpletest.rc-gt57d" in namespace "gc-3145"
  Jul 15 12:40:43.546: INFO: Deleting pod "simpletest.rc-h2hqr" in namespace "gc-3145"
  Jul 15 12:40:43.558: INFO: Deleting pod "simpletest.rc-h98cf" in namespace "gc-3145"
  Jul 15 12:40:43.645: INFO: Deleting pod "simpletest.rc-hlddq" in namespace "gc-3145"
  Jul 15 12:40:43.655: INFO: Deleting pod "simpletest.rc-hp8tt" in namespace "gc-3145"
  Jul 15 12:40:43.665: INFO: Deleting pod "simpletest.rc-hrmqw" in namespace "gc-3145"
  Jul 15 12:40:43.678: INFO: Deleting pod "simpletest.rc-j5mhs" in namespace "gc-3145"
  Jul 15 12:40:43.688: INFO: Deleting pod "simpletest.rc-jbggk" in namespace "gc-3145"
  Jul 15 12:40:43.697: INFO: Deleting pod "simpletest.rc-jqvf9" in namespace "gc-3145"
  Jul 15 12:40:43.709: INFO: Deleting pod "simpletest.rc-jwcrv" in namespace "gc-3145"
  Jul 15 12:40:43.721: INFO: Deleting pod "simpletest.rc-k2tmd" in namespace "gc-3145"
  Jul 15 12:40:43.734: INFO: Deleting pod "simpletest.rc-kd5dh" in namespace "gc-3145"
  Jul 15 12:40:43.749: INFO: Deleting pod "simpletest.rc-kfd6r" in namespace "gc-3145"
  Jul 15 12:40:43.758: INFO: Deleting pod "simpletest.rc-krq7r" in namespace "gc-3145"
  Jul 15 12:40:43.769: INFO: Deleting pod "simpletest.rc-l4mph" in namespace "gc-3145"
  Jul 15 12:40:43.780: INFO: Deleting pod "simpletest.rc-l88sg" in namespace "gc-3145"
  Jul 15 12:40:43.789: INFO: Deleting pod "simpletest.rc-lk4bs" in namespace "gc-3145"
  Jul 15 12:40:43.801: INFO: Deleting pod "simpletest.rc-llvwj" in namespace "gc-3145"
  E0715 12:40:43.812149      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:40:43.812: INFO: Deleting pod "simpletest.rc-lwwj5" in namespace "gc-3145"
  Jul 15 12:40:43.822: INFO: Deleting pod "simpletest.rc-mjtmt" in namespace "gc-3145"
  Jul 15 12:40:43.836: INFO: Deleting pod "simpletest.rc-mxb8n" in namespace "gc-3145"
  Jul 15 12:40:43.854: INFO: Deleting pod "simpletest.rc-mxclr" in namespace "gc-3145"
  Jul 15 12:40:43.867: INFO: Deleting pod "simpletest.rc-mz9cg" in namespace "gc-3145"
  Jul 15 12:40:43.877: INFO: Deleting pod "simpletest.rc-ncf47" in namespace "gc-3145"
  Jul 15 12:40:43.887: INFO: Deleting pod "simpletest.rc-nwgww" in namespace "gc-3145"
  Jul 15 12:40:43.911: INFO: Deleting pod "simpletest.rc-nxztw" in namespace "gc-3145"
  Jul 15 12:40:43.961: INFO: Deleting pod "simpletest.rc-pbddz" in namespace "gc-3145"
  Jul 15 12:40:44.006: INFO: Deleting pod "simpletest.rc-pjkfd" in namespace "gc-3145"
  Jul 15 12:40:44.059: INFO: Deleting pod "simpletest.rc-pw56d" in namespace "gc-3145"
  Jul 15 12:40:44.115: INFO: Deleting pod "simpletest.rc-qpkqg" in namespace "gc-3145"
  Jul 15 12:40:44.159: INFO: Deleting pod "simpletest.rc-qs2p6" in namespace "gc-3145"
  Jul 15 12:40:44.211: INFO: Deleting pod "simpletest.rc-r9klh" in namespace "gc-3145"
  Jul 15 12:40:44.260: INFO: Deleting pod "simpletest.rc-rg85p" in namespace "gc-3145"
  Jul 15 12:40:44.313: INFO: Deleting pod "simpletest.rc-rhj8d" in namespace "gc-3145"
  Jul 15 12:40:44.357: INFO: Deleting pod "simpletest.rc-rk8mh" in namespace "gc-3145"
  Jul 15 12:40:44.411: INFO: Deleting pod "simpletest.rc-rl96f" in namespace "gc-3145"
  Jul 15 12:40:44.459: INFO: Deleting pod "simpletest.rc-s9kvx" in namespace "gc-3145"
  Jul 15 12:40:44.511: INFO: Deleting pod "simpletest.rc-sd2c4" in namespace "gc-3145"
  Jul 15 12:40:44.558: INFO: Deleting pod "simpletest.rc-slfcj" in namespace "gc-3145"
  Jul 15 12:40:44.607: INFO: Deleting pod "simpletest.rc-t2cln" in namespace "gc-3145"
  Jul 15 12:40:44.662: INFO: Deleting pod "simpletest.rc-tj99v" in namespace "gc-3145"
  Jul 15 12:40:44.708: INFO: Deleting pod "simpletest.rc-v566c" in namespace "gc-3145"
  Jul 15 12:40:44.756: INFO: Deleting pod "simpletest.rc-v6wq2" in namespace "gc-3145"
  E0715 12:40:44.812323      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:40:44.812: INFO: Deleting pod "simpletest.rc-vtjv8" in namespace "gc-3145"
  Jul 15 12:40:44.858: INFO: Deleting pod "simpletest.rc-wdpck" in namespace "gc-3145"
  Jul 15 12:40:44.909: INFO: Deleting pod "simpletest.rc-wkdzf" in namespace "gc-3145"
  Jul 15 12:40:44.957: INFO: Deleting pod "simpletest.rc-wkkcx" in namespace "gc-3145"
  Jul 15 12:40:45.011: INFO: Deleting pod "simpletest.rc-wpcvk" in namespace "gc-3145"
  Jul 15 12:40:45.060: INFO: Deleting pod "simpletest.rc-x2m6f" in namespace "gc-3145"
  Jul 15 12:40:45.112: INFO: Deleting pod "simpletest.rc-xd2dz" in namespace "gc-3145"
  Jul 15 12:40:45.158: INFO: Deleting pod "simpletest.rc-xkwcm" in namespace "gc-3145"
  Jul 15 12:40:45.207: INFO: Deleting pod "simpletest.rc-xnr8s" in namespace "gc-3145"
  Jul 15 12:40:45.259: INFO: Deleting pod "simpletest.rc-xx9sh" in namespace "gc-3145"
  Jul 15 12:40:45.307: INFO: Deleting pod "simpletest.rc-z42h7" in namespace "gc-3145"
  Jul 15 12:40:45.362: INFO: Deleting pod "simpletest.rc-zk4lf" in namespace "gc-3145"
  Jul 15 12:40:45.411: INFO: Deleting pod "simpletest.rc-zkhnf" in namespace "gc-3145"
  Jul 15 12:40:45.458: INFO: Deleting pod "simpletest.rc-zmtz8" in namespace "gc-3145"
  Jul 15 12:40:45.509: INFO: Deleting pod "simpletest.rc-zpjch" in namespace "gc-3145"
  Jul 15 12:40:45.561: INFO: Deleting pod "simpletest.rc-zpsd6" in namespace "gc-3145"
  Jul 15 12:40:45.610: INFO: Deleting pod "simpletest.rc-zsppl" in namespace "gc-3145"
  Jul 15 12:40:45.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3145" for this suite. @ 07/15/23 12:40:45.702
• [43.749 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 07/15/23 12:40:45.753
  Jul 15 12:40:45.753: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename deployment @ 07/15/23 12:40:45.754
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:40:45.77
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:40:45.773
  Jul 15 12:40:45.776: INFO: Creating deployment "webserver-deployment"
  Jul 15 12:40:45.781: INFO: Waiting for observed generation 1
  E0715 12:40:45.813093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:46.813228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:40:47.787: INFO: Waiting for all required pods to come up
  Jul 15 12:40:47.790: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 07/15/23 12:40:47.79
  E0715 12:40:47.813776      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:48.813952      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:40:49.798: INFO: Waiting for deployment "webserver-deployment" to complete
  Jul 15 12:40:49.805: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Jul 15 12:40:49.813: INFO: Updating deployment webserver-deployment
  Jul 15 12:40:49.813: INFO: Waiting for observed generation 2
  E0715 12:40:49.814154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:50.815145      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:51.815339      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:40:51.824: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Jul 15 12:40:51.826: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Jul 15 12:40:51.830: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jul 15 12:40:51.840: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Jul 15 12:40:51.840: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Jul 15 12:40:51.843: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jul 15 12:40:51.849: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Jul 15 12:40:51.849: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Jul 15 12:40:51.859: INFO: Updating deployment webserver-deployment
  Jul 15 12:40:51.859: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Jul 15 12:40:51.872: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Jul 15 12:40:51.881: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Jul 15 12:40:51.911: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-2826  1229025b-65a6-4abf-9202-dae6216e48f8 20011 3 2023-07-15 12:40:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a33c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-07-15 12:40:49 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-15 12:40:51 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Jul 15 12:40:51.925: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-2826  0e3134b6-a68c-42c3-a838-dce45ed2d02e 19983 3 2023-07-15 12:40:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1229025b-65a6-4abf-9202-dae6216e48f8 0xc0051a38e7 0xc0051a38e8}] [] [{kube-controller-manager Update apps/v1 2023-07-15 12:40:49 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1229025b-65a6-4abf-9202-dae6216e48f8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a3988 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 15 12:40:51.925: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Jul 15 12:40:51.925: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-2826  178e7144-6582-45d4-95c7-c7961c800497 19980 3 2023-07-15 12:40:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1229025b-65a6-4abf-9202-dae6216e48f8 0xc0051a37f7 0xc0051a37f8}] [] [{kube-controller-manager Update apps/v1 2023-07-15 12:40:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1229025b-65a6-4abf-9202-dae6216e48f8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a3888 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Jul 15 12:40:51.932: INFO: Pod "webserver-deployment-67bd4bf6dc-4bm4w" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4bm4w webserver-deployment-67bd4bf6dc- deployment-2826  c49354a9-2992-4536-9fd2-6d1a8af3c5b4 19631 0 2023-07-15 12:40:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc0051a3e87 0xc0051a3e88}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.33.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c6rfl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c6rfl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.190,PodIP:192.168.33.119,StartTime:2023-07-15 12:40:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-15 12:40:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://95df36f5eaa0b00d27580a2d8180f7ea3625aa77631dd6aeea67732398dc5b42,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.33.119,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.932: INFO: Pod "webserver-deployment-67bd4bf6dc-5jbm2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5jbm2 webserver-deployment-67bd4bf6dc- deployment-2826  096bb47d-d0ba-4721-b1e2-6719a884a48a 20016 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc00301a127 0xc00301a128}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x9tkz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x9tkz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.933: INFO: Pod "webserver-deployment-67bd4bf6dc-6tnns" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6tnns webserver-deployment-67bd4bf6dc- deployment-2826  3e8cabf1-b468-439e-8fcb-e48a653d3983 20032 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc00301a450 0xc00301a451}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x2w79,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x2w79,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-138,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.933: INFO: Pod "webserver-deployment-67bd4bf6dc-7ltl9" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7ltl9 webserver-deployment-67bd4bf6dc- deployment-2826  758ab308-1af2-4b9d-9453-eaeb3922d507 19692 0 2023-07-15 12:40:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc00301a5b0 0xc00301a5b1}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.191.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nvb54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nvb54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-138,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.138,PodIP:192.168.191.207,StartTime:2023-07-15 12:40:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-15 12:40:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://553cdcd642327105eb2689989bb7ddd8c5efbc9bce9f290b4e51c4680ea0433a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.191.207,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.933: INFO: Pod "webserver-deployment-67bd4bf6dc-9wsb7" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9wsb7 webserver-deployment-67bd4bf6dc- deployment-2826  9aa918b3-babf-4033-968d-6ffba55d5552 19639 0 2023-07-15 12:40:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc00301a7b7 0xc00301a7b8}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.33.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hktt5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hktt5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.190,PodIP:192.168.33.117,StartTime:2023-07-15 12:40:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-15 12:40:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a9f00a64800995fcdd16fe79aaf0a2569978ff8a7d8e49c3aa424304ec4e7963,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.33.117,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.933: INFO: Pod "webserver-deployment-67bd4bf6dc-dcv4v" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dcv4v webserver-deployment-67bd4bf6dc- deployment-2826  5be6152c-d76c-4fbd-baa8-bfcc71d6854a 20029 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc00301a9a7 0xc00301a9a8}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rx5rz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rx5rz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-138,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.138,PodIP:,StartTime:2023-07-15 12:40:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.933: INFO: Pod "webserver-deployment-67bd4bf6dc-dm299" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dm299 webserver-deployment-67bd4bf6dc- deployment-2826  4ab30f18-523a-4311-993f-c49dd02bcea3 19542 0 2023-07-15 12:40:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc00301ab77 0xc00301ab78}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.191.208\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbj5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbj5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-138,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.138,PodIP:192.168.191.208,StartTime:2023-07-15 12:40:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-15 12:40:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4f426abe0659c3587385908ef49031f5b23bf16d87d401042227d79f2bfc254c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.191.208,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.933: INFO: Pod "webserver-deployment-67bd4bf6dc-ffjqh" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-ffjqh webserver-deployment-67bd4bf6dc- deployment-2826  71f7ca4b-42bf-4dd6-a838-95a1eb69d45f 20033 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc00301ad67 0xc00301ad68}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dbbqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dbbqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-236,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.933: INFO: Pod "webserver-deployment-67bd4bf6dc-h4lf9" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-h4lf9 webserver-deployment-67bd4bf6dc- deployment-2826  7b21f8c2-0faa-441e-a757-cb72d04b65a0 20038 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc00301aed0 0xc00301aed1}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-82pqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-82pqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.934: INFO: Pod "webserver-deployment-67bd4bf6dc-hkdfh" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hkdfh webserver-deployment-67bd4bf6dc- deployment-2826  f5be5405-626c-4e4d-ac5c-ed4b3f806360 19608 0 2023-07-15 12:40:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc00301b030 0xc00301b031}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.4.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ldkk9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ldkk9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-236,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.236,PodIP:192.168.4.52,StartTime:2023-07-15 12:40:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-15 12:40:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ddbed2d63cd749feac940cca7c13f9ba4d259489f4336d2ca6251fcd6ea3ed35,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.4.52,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.934: INFO: Pod "webserver-deployment-67bd4bf6dc-jm5g7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jm5g7 webserver-deployment-67bd4bf6dc- deployment-2826  6b7e1d3a-239f-4352-8283-dae690456b2b 20034 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc00301b227 0xc00301b228}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bf7cd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bf7cd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.190,PodIP:,StartTime:2023-07-15 12:40:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.934: INFO: Pod "webserver-deployment-67bd4bf6dc-jxx4b" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jxx4b webserver-deployment-67bd4bf6dc- deployment-2826  eb0e3389-4d62-444c-815e-7d0d557cc171 20026 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc00301b5e7 0xc00301b5e8}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mkssp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mkssp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.934: INFO: Pod "webserver-deployment-67bd4bf6dc-mqktx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-mqktx webserver-deployment-67bd4bf6dc- deployment-2826  e2d32de2-ba1b-4c5a-9b66-40bf51c44567 20014 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc00301b737 0xc00301b738}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4fb8l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4fb8l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-138,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.934: INFO: Pod "webserver-deployment-67bd4bf6dc-mrtg2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-mrtg2 webserver-deployment-67bd4bf6dc- deployment-2826  30a574d7-471b-4527-a782-c655fde020b7 20031 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc00301ba50 0xc00301ba51}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cgjft,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cgjft,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.934: INFO: Pod "webserver-deployment-67bd4bf6dc-nxsg7" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nxsg7 webserver-deployment-67bd4bf6dc- deployment-2826  1980a5c0-31fa-4efe-b384-5923fe81713f 19696 0 2023-07-15 12:40:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc00301bbe0 0xc00301bbe1}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.4.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qdzjt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qdzjt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-236,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.236,PodIP:192.168.4.54,StartTime:2023-07-15 12:40:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-15 12:40:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cd1c8ea3d6de6ac8d555019c50fbd20636a9034e220e2260cb8416dce9870e19,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.4.54,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.934: INFO: Pod "webserver-deployment-67bd4bf6dc-pkk6p" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-pkk6p webserver-deployment-67bd4bf6dc- deployment-2826  fd25f262-84e8-4422-a3eb-5150dd3f65cc 19603 0 2023-07-15 12:40:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc00301bdd7 0xc00301bdd8}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.4.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m45mv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m45mv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-236,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.236,PodIP:192.168.4.53,StartTime:2023-07-15 12:40:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-15 12:40:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5e73e40659e60e61bb9bd2632cba476c38dcc8c4199eed1815dc92dfa22fb188,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.4.53,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.935: INFO: Pod "webserver-deployment-67bd4bf6dc-qvhns" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-qvhns webserver-deployment-67bd4bf6dc- deployment-2826  9c140b46-301f-42c4-92d2-23e2ce10814f 20019 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc00301bfc7 0xc00301bfc8}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fjlfc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fjlfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-236,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.935: INFO: Pod "webserver-deployment-67bd4bf6dc-tz466" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tz466 webserver-deployment-67bd4bf6dc- deployment-2826  6981aa0e-dee5-4264-948c-c2763a241059 20015 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc000b26230 0xc000b26231}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7kq8t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7kq8t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.190,PodIP:,StartTime:2023-07-15 12:40:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.935: INFO: Pod "webserver-deployment-67bd4bf6dc-wrr7l" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-wrr7l webserver-deployment-67bd4bf6dc- deployment-2826  ad67b411-f1a7-468e-9413-5e2268c66f67 19689 0 2023-07-15 12:40:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc000b265e7 0xc000b265e8}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.191.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6ml4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6ml4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-138,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.138,PodIP:192.168.191.209,StartTime:2023-07-15 12:40:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-15 12:40:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://de4dc544b3ef7c07ef0120f9f28f2bb7b1c6d0bf6bac9ddd9d634606e8761e7d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.191.209,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.935: INFO: Pod "webserver-deployment-67bd4bf6dc-zcrf9" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-zcrf9 webserver-deployment-67bd4bf6dc- deployment-2826  08b46fa0-8778-4556-b0e5-bb485d56fa8c 20037 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 178e7144-6582-45d4-95c7-c7961c800497 0xc000b267d7 0xc000b267d8}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178e7144-6582-45d4-95c7-c7961c800497\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wpzwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wpzwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-236,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.236,PodIP:,StartTime:2023-07-15 12:40:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.935: INFO: Pod "webserver-deployment-7b75d79cf5-4c44n" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-4c44n webserver-deployment-7b75d79cf5- deployment-2826  8351a5f8-1204-4395-b62f-11ec841586e6 20020 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0e3134b6-a68c-42c3-a838-dce45ed2d02e 0xc000b269a7 0xc000b269a8}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e3134b6-a68c-42c3-a838-dce45ed2d02e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-swc5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-swc5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-236,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.935: INFO: Pod "webserver-deployment-7b75d79cf5-5x7zr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-5x7zr webserver-deployment-7b75d79cf5- deployment-2826  91f8ccad-28aa-4e67-92c4-29ee28c69315 20039 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0e3134b6-a68c-42c3-a838-dce45ed2d02e 0xc000b26b20 0xc000b26b21}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e3134b6-a68c-42c3-a838-dce45ed2d02e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-djhqt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-djhqt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.935: INFO: Pod "webserver-deployment-7b75d79cf5-cd88m" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-cd88m webserver-deployment-7b75d79cf5- deployment-2826  7d5b863a-8948-44a2-9efe-56c973d42563 19864 0 2023-07-15 12:40:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0e3134b6-a68c-42c3-a838-dce45ed2d02e 0xc000b26dd0 0xc000b26dd1}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e3134b6-a68c-42c3-a838-dce45ed2d02e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.33.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6k79s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6k79s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.190,PodIP:192.168.33.123,StartTime:2023-07-15 12:40:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.33.123,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.936: INFO: Pod "webserver-deployment-7b75d79cf5-crxx7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-crxx7 webserver-deployment-7b75d79cf5- deployment-2826  eb201ece-7b52-4252-9ebe-959867d27cdc 20030 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0e3134b6-a68c-42c3-a838-dce45ed2d02e 0xc000b27317 0xc000b27318}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e3134b6-a68c-42c3-a838-dce45ed2d02e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fkn9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkn9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.936: INFO: Pod "webserver-deployment-7b75d79cf5-dsrfh" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-dsrfh webserver-deployment-7b75d79cf5- deployment-2826  f2828668-9662-4e86-a5ab-d4a50503ee40 19902 0 2023-07-15 12:40:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0e3134b6-a68c-42c3-a838-dce45ed2d02e 0xc000b27700 0xc000b27701}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e3134b6-a68c-42c3-a838-dce45ed2d02e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.4.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g9wnt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g9wnt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-236,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.236,PodIP:192.168.4.55,StartTime:2023-07-15 12:40:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.4.55,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.936: INFO: Pod "webserver-deployment-7b75d79cf5-jv5k8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-jv5k8 webserver-deployment-7b75d79cf5- deployment-2826  9d0b14cd-4b3a-40a2-8938-433ee5d6c389 20017 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0e3134b6-a68c-42c3-a838-dce45ed2d02e 0xc000b27ac7 0xc000b27ac8}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e3134b6-a68c-42c3-a838-dce45ed2d02e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c9mzc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c9mzc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-236,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.936: INFO: Pod "webserver-deployment-7b75d79cf5-mfpbk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-mfpbk webserver-deployment-7b75d79cf5- deployment-2826  0caf7f23-d363-4eeb-b369-89ebe3d6ca86 19859 0 2023-07-15 12:40:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0e3134b6-a68c-42c3-a838-dce45ed2d02e 0xc000b27e60 0xc000b27e61}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e3134b6-a68c-42c3-a838-dce45ed2d02e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.33.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-whcg6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-whcg6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.190,PodIP:192.168.33.120,StartTime:2023-07-15 12:40:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.33.120,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.936: INFO: Pod "webserver-deployment-7b75d79cf5-msr5h" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-msr5h webserver-deployment-7b75d79cf5- deployment-2826  d333ca61-f337-4fbd-b05e-a736ea1afdce 20024 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0e3134b6-a68c-42c3-a838-dce45ed2d02e 0xc0019b03d7 0xc0019b03d8}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e3134b6-a68c-42c3-a838-dce45ed2d02e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x69zd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x69zd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-138,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.936: INFO: Pod "webserver-deployment-7b75d79cf5-rrmnc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-rrmnc webserver-deployment-7b75d79cf5- deployment-2826  109ebc5b-2956-4980-9cbd-12aa67b790d1 19810 0 2023-07-15 12:40:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0e3134b6-a68c-42c3-a838-dce45ed2d02e 0xc0019b0930 0xc0019b0931}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e3134b6-a68c-42c3-a838-dce45ed2d02e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.191.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9rfq7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9rfq7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-138,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.138,PodIP:192.168.191.211,StartTime:2023-07-15 12:40:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.191.211,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.936: INFO: Pod "webserver-deployment-7b75d79cf5-srmc9" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-srmc9 webserver-deployment-7b75d79cf5- deployment-2826  cdeb2c90-c6a6-4f76-96c4-1193c6cf708f 20000 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0e3134b6-a68c-42c3-a838-dce45ed2d02e 0xc0019b0c27 0xc0019b0c28}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e3134b6-a68c-42c3-a838-dce45ed2d02e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c72n7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c72n7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-138,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.937: INFO: Pod "webserver-deployment-7b75d79cf5-wc6wc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-wc6wc webserver-deployment-7b75d79cf5- deployment-2826  42eed244-eb2e-4dc8-aa5e-23f343ccee1b 20013 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0e3134b6-a68c-42c3-a838-dce45ed2d02e 0xc0019b0da0 0xc0019b0da1}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e3134b6-a68c-42c3-a838-dce45ed2d02e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87lqd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87lqd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-236,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.236,PodIP:,StartTime:2023-07-15 12:40:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.937: INFO: Pod "webserver-deployment-7b75d79cf5-wcqds" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-wcqds webserver-deployment-7b75d79cf5- deployment-2826  e441e02e-925b-46df-9288-cf4c701ea903 20006 0 2023-07-15 12:40:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0e3134b6-a68c-42c3-a838-dce45ed2d02e 0xc0019b0f97 0xc0019b0f98}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e3134b6-a68c-42c3-a838-dce45ed2d02e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wb7f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wb7f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.937: INFO: Pod "webserver-deployment-7b75d79cf5-xmp96" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-xmp96 webserver-deployment-7b75d79cf5- deployment-2826  5ac04820-1784-48c6-96a2-28abf2c7ae27 19806 0 2023-07-15 12:40:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0e3134b6-a68c-42c3-a838-dce45ed2d02e 0xc0019b1110 0xc0019b1111}] [] [{kube-controller-manager Update v1 2023-07-15 12:40:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e3134b6-a68c-42c3-a838-dce45ed2d02e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:40:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.191.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t9znx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t9znx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-138,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:40:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.138,PodIP:192.168.191.210,StartTime:2023-07-15 12:40:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.191.210,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:40:51.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2826" for this suite. @ 07/15/23 12:40:51.944
• [6.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 07/15/23 12:40:51.956
  Jul 15 12:40:51.956: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename runtimeclass @ 07/15/23 12:40:51.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:40:51.973
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:40:51.976
  Jul 15 12:40:52.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3904" for this suite. @ 07/15/23 12:40:52.013
• [0.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 07/15/23 12:40:52.021
  Jul 15 12:40:52.021: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename services @ 07/15/23 12:40:52.022
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:40:52.033
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:40:52.036
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-9212 @ 07/15/23 12:40:52.038
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 07/15/23 12:40:52.052
  STEP: creating service externalsvc in namespace services-9212 @ 07/15/23 12:40:52.053
  STEP: creating replication controller externalsvc in namespace services-9212 @ 07/15/23 12:40:52.069
  I0715 12:40:52.076361      23 runners.go:194] Created replication controller with name: externalsvc, namespace: services-9212, replica count: 2
  E0715 12:40:52.816040      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:53.816970      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:54.817238      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0715 12:40:55.127140      23 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 07/15/23 12:40:55.131
  Jul 15 12:40:55.149: INFO: Creating new exec pod
  E0715 12:40:55.817837      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:56.818230      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:40:57.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-9212 exec execpod6s4bl -- /bin/sh -x -c nslookup nodeport-service.services-9212.svc.cluster.local'
  Jul 15 12:40:57.286: INFO: stderr: "+ nslookup nodeport-service.services-9212.svc.cluster.local\n"
  Jul 15 12:40:57.286: INFO: stdout: "Server:\t\t10.152.183.169\nAddress:\t10.152.183.169#53\n\nnodeport-service.services-9212.svc.cluster.local\tcanonical name = externalsvc.services-9212.svc.cluster.local.\nName:\texternalsvc.services-9212.svc.cluster.local\nAddress: 10.152.183.154\n\n"
  Jul 15 12:40:57.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-9212, will wait for the garbage collector to delete the pods @ 07/15/23 12:40:57.291
  Jul 15 12:40:57.352: INFO: Deleting ReplicationController externalsvc took: 6.944789ms
  E0715 12:40:57.818521      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:40:58.152: INFO: Terminating ReplicationController externalsvc pods took: 800.270306ms
  E0715 12:40:58.819208      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:40:59.819928      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:41:00.273: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-9212" for this suite. @ 07/15/23 12:41:00.287
• [8.271 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 07/15/23 12:41:00.294
  Jul 15 12:41:00.294: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename replicaset @ 07/15/23 12:41:00.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:41:00.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:41:00.31
  STEP: Create a ReplicaSet @ 07/15/23 12:41:00.312
  STEP: Verify that the required pods have come up @ 07/15/23 12:41:00.318
  Jul 15 12:41:00.321: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0715 12:41:00.820771      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:01.821086      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:02.821175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:03.821270      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:04.821387      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:41:05.326: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 07/15/23 12:41:05.326
  Jul 15 12:41:05.328: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 07/15/23 12:41:05.328
  STEP: DeleteCollection of the ReplicaSets @ 07/15/23 12:41:05.333
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 07/15/23 12:41:05.341
  Jul 15 12:41:05.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6854" for this suite. @ 07/15/23 12:41:05.352
• [5.067 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 07/15/23 12:41:05.361
  Jul 15 12:41:05.361: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename secrets @ 07/15/23 12:41:05.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:41:05.372
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:41:05.375
  STEP: Creating secret with name secret-test-map-1196988c-1fd3-4054-8b6e-7f21c711867b @ 07/15/23 12:41:05.378
  STEP: Creating a pod to test consume secrets @ 07/15/23 12:41:05.382
  E0715 12:41:05.821696      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:06.822027      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:07.822158      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:08.822241      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:41:09.405
  Jul 15 12:41:09.408: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-secrets-07c0586f-e816-4a0b-ba48-f86d5a5caeaa container secret-volume-test: <nil>
  STEP: delete the pod @ 07/15/23 12:41:09.428
  Jul 15 12:41:09.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-186" for this suite. @ 07/15/23 12:41:09.447
• [4.092 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 07/15/23 12:41:09.454
  Jul 15 12:41:09.454: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl @ 07/15/23 12:41:09.455
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:41:09.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:41:09.47
  STEP: creating Agnhost RC @ 07/15/23 12:41:09.472
  Jul 15 12:41:09.472: INFO: namespace kubectl-9329
  Jul 15 12:41:09.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-9329 create -f -'
  Jul 15 12:41:09.678: INFO: stderr: ""
  Jul 15 12:41:09.678: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/15/23 12:41:09.678
  E0715 12:41:09.822690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:41:10.682: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 15 12:41:10.682: INFO: Found 0 / 1
  E0715 12:41:10.823735      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:41:11.685: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 15 12:41:11.685: INFO: Found 1 / 1
  Jul 15 12:41:11.685: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jul 15 12:41:11.688: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 15 12:41:11.688: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 15 12:41:11.688: INFO: wait on agnhost-primary startup in kubectl-9329 
  Jul 15 12:41:11.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-9329 logs agnhost-primary-qbh5x agnhost-primary'
  Jul 15 12:41:11.746: INFO: stderr: ""
  Jul 15 12:41:11.746: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 07/15/23 12:41:11.746
  Jul 15 12:41:11.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-9329 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Jul 15 12:41:11.814: INFO: stderr: ""
  Jul 15 12:41:11.814: INFO: stdout: "service/rm2 exposed\n"
  Jul 15 12:41:11.817: INFO: Service rm2 in namespace kubectl-9329 found.
  E0715 12:41:11.824021      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:12.824334      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:13.824724      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 07/15/23 12:41:13.825
  Jul 15 12:41:13.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-9329 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Jul 15 12:41:13.878: INFO: stderr: ""
  Jul 15 12:41:13.878: INFO: stdout: "service/rm3 exposed\n"
  Jul 15 12:41:13.882: INFO: Service rm3 in namespace kubectl-9329 found.
  E0715 12:41:14.824876      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:15.825163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:41:15.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9329" for this suite. @ 07/15/23 12:41:15.895
• [6.446 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 07/15/23 12:41:15.901
  Jul 15 12:41:15.901: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:41:15.901
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:41:15.914
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:41:15.916
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 12:41:15.918
  E0715 12:41:16.825881      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:17.826118      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:18.826209      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:19.826430      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:41:19.939
  Jul 15 12:41:19.942: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-8fc3c9da-ec96-48bc-93cc-55727011ebfe container client-container: <nil>
  STEP: delete the pod @ 07/15/23 12:41:19.95
  Jul 15 12:41:19.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2457" for this suite. @ 07/15/23 12:41:19.968
• [4.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 07/15/23 12:41:19.977
  Jul 15 12:41:19.977: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:41:19.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:41:19.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:41:19.993
  STEP: Creating projection with secret that has name projected-secret-test-0c45eb0a-7c5e-4e54-af67-05f92cfa0f55 @ 07/15/23 12:41:19.997
  STEP: Creating a pod to test consume secrets @ 07/15/23 12:41:20
  E0715 12:41:20.827077      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:21.827555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:22.828070      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:23.828254      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:41:24.026
  Jul 15 12:41:24.029: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-projected-secrets-b24a2754-ac96-4efb-9f9f-9181782f1066 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/15/23 12:41:24.034
  Jul 15 12:41:24.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9249" for this suite. @ 07/15/23 12:41:24.056
• [4.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 07/15/23 12:41:24.064
  Jul 15 12:41:24.064: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename resourcequota @ 07/15/23 12:41:24.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:41:24.078
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:41:24.08
  STEP: Counting existing ResourceQuota @ 07/15/23 12:41:24.083
  E0715 12:41:24.829061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:25.829186      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:26.830184      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:27.830419      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:28.830495      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/15/23 12:41:29.088
  STEP: Ensuring resource quota status is calculated @ 07/15/23 12:41:29.091
  E0715 12:41:29.830783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:30.831236      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 07/15/23 12:41:31.096
  STEP: Ensuring resource quota status captures replicaset creation @ 07/15/23 12:41:31.108
  E0715 12:41:31.832231      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:32.832889      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 07/15/23 12:41:33.113
  STEP: Ensuring resource quota status released usage @ 07/15/23 12:41:33.12
  E0715 12:41:33.833051      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:34.833175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:41:35.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3900" for this suite. @ 07/15/23 12:41:35.13
• [11.072 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 07/15/23 12:41:35.137
  Jul 15 12:41:35.137: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename field-validation @ 07/15/23 12:41:35.137
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:41:35.149
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:41:35.151
  STEP: apply creating a deployment @ 07/15/23 12:41:35.154
  Jul 15 12:41:35.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1944" for this suite. @ 07/15/23 12:41:35.17
• [0.040 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 07/15/23 12:41:35.177
  Jul 15 12:41:35.177: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename secrets @ 07/15/23 12:41:35.177
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:41:35.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:41:35.189
  STEP: Creating secret with name secret-test-8cc45561-834c-4bc6-a8a3-09018bd5d69e @ 07/15/23 12:41:35.192
  STEP: Creating a pod to test consume secrets @ 07/15/23 12:41:35.195
  E0715 12:41:35.834148      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:36.834386      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:37.834716      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:38.834961      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:41:39.219
  Jul 15 12:41:39.221: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-secrets-cadaa698-210f-45c4-b201-9c23e64d4df5 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/15/23 12:41:39.23
  Jul 15 12:41:39.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3100" for this suite. @ 07/15/23 12:41:39.247
• [4.076 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 07/15/23 12:41:39.253
  Jul 15 12:41:39.253: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/15/23 12:41:39.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:41:39.266
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:41:39.269
  Jul 15 12:41:39.272: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  E0715 12:41:39.835483      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/15/23 12:41:40.521
  Jul 15 12:41:40.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-4772 --namespace=crd-publish-openapi-4772 create -f -'
  E0715 12:41:40.836361      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:41.836616      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:42.836694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:41:42.906: INFO: stderr: ""
  Jul 15 12:41:42.906: INFO: stdout: "e2e-test-crd-publish-openapi-8641-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jul 15 12:41:42.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-4772 --namespace=crd-publish-openapi-4772 delete e2e-test-crd-publish-openapi-8641-crds test-cr'
  Jul 15 12:41:42.955: INFO: stderr: ""
  Jul 15 12:41:42.955: INFO: stdout: "e2e-test-crd-publish-openapi-8641-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Jul 15 12:41:42.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-4772 --namespace=crd-publish-openapi-4772 apply -f -'
  Jul 15 12:41:43.317: INFO: stderr: ""
  Jul 15 12:41:43.317: INFO: stdout: "e2e-test-crd-publish-openapi-8641-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jul 15 12:41:43.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-4772 --namespace=crd-publish-openapi-4772 delete e2e-test-crd-publish-openapi-8641-crds test-cr'
  Jul 15 12:41:43.368: INFO: stderr: ""
  Jul 15 12:41:43.368: INFO: stdout: "e2e-test-crd-publish-openapi-8641-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 07/15/23 12:41:43.368
  Jul 15 12:41:43.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-4772 explain e2e-test-crd-publish-openapi-8641-crds'
  Jul 15 12:41:43.490: INFO: stderr: ""
  Jul 15 12:41:43.490: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-8641-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0715 12:41:43.836931      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:41:44.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4772" for this suite. @ 07/15/23 12:41:44.743
• [5.496 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 07/15/23 12:41:44.75
  Jul 15 12:41:44.750: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename subpath @ 07/15/23 12:41:44.751
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:41:44.769
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:41:44.772
  STEP: Setting up data @ 07/15/23 12:41:44.775
  STEP: Creating pod pod-subpath-test-downwardapi-bdnk @ 07/15/23 12:41:44.785
  STEP: Creating a pod to test atomic-volume-subpath @ 07/15/23 12:41:44.785
  E0715 12:41:44.837231      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:45.837781      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:46.838665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:47.838941      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:48.839182      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:49.839345      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:50.840177      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:51.840478      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:52.840663      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:53.840918      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:54.840917      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:55.841487      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:56.841478      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:57.842153      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:58.842218      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:41:59.842870      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:00.843678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:01.843924      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:02.844142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:03.844657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:04.845455      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:05.846229      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:06.846512      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:07.847356      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:08.847648      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:42:08.858
  Jul 15 12:42:08.862: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-subpath-test-downwardapi-bdnk container test-container-subpath-downwardapi-bdnk: <nil>
  STEP: delete the pod @ 07/15/23 12:42:08.87
  STEP: Deleting pod pod-subpath-test-downwardapi-bdnk @ 07/15/23 12:42:08.883
  Jul 15 12:42:08.883: INFO: Deleting pod "pod-subpath-test-downwardapi-bdnk" in namespace "subpath-4533"
  Jul 15 12:42:08.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4533" for this suite. @ 07/15/23 12:42:08.89
• [24.146 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 07/15/23 12:42:08.897
  Jul 15 12:42:08.897: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-runtime @ 07/15/23 12:42:08.897
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:42:08.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:42:08.917
  STEP: create the container @ 07/15/23 12:42:08.92
  W0715 12:42:08.928053      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/15/23 12:42:08.928
  E0715 12:42:09.847729      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:10.847795      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:11.848682      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/15/23 12:42:11.946
  STEP: the container should be terminated @ 07/15/23 12:42:11.949
  STEP: the termination message should be set @ 07/15/23 12:42:11.949
  Jul 15 12:42:11.949: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 07/15/23 12:42:11.949
  Jul 15 12:42:11.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2700" for this suite. @ 07/15/23 12:42:11.971
• [3.079 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 07/15/23 12:42:11.976
  Jul 15 12:42:11.977: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename job @ 07/15/23 12:42:11.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:42:11.993
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:42:11.996
  STEP: Creating a job @ 07/15/23 12:42:11.999
  STEP: Ensuring active pods == parallelism @ 07/15/23 12:42:12.005
  E0715 12:42:12.849075      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:13.849203      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 07/15/23 12:42:14.01
  Jul 15 12:42:14.526: INFO: Successfully updated pod "adopt-release-r7khr"
  STEP: Checking that the Job readopts the Pod @ 07/15/23 12:42:14.526
  E0715 12:42:14.849790      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:15.850807      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 07/15/23 12:42:16.535
  E0715 12:42:16.851103      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:42:17.047: INFO: Successfully updated pod "adopt-release-r7khr"
  STEP: Checking that the Job releases the Pod @ 07/15/23 12:42:17.047
  E0715 12:42:17.851455      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:18.851760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:42:19.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2365" for this suite. @ 07/15/23 12:42:19.06
• [7.090 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 07/15/23 12:42:19.067
  Jul 15 12:42:19.067: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename disruption @ 07/15/23 12:42:19.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:42:19.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:42:19.087
  STEP: Waiting for the pdb to be processed @ 07/15/23 12:42:19.095
  E0715 12:42:19.852688      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:20.852990      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating PodDisruptionBudget status @ 07/15/23 12:42:21.104
  STEP: Waiting for all pods to be running @ 07/15/23 12:42:21.113
  Jul 15 12:42:21.118: INFO: running pods: 0 < 1
  E0715 12:42:21.853878      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:22.854257      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 07/15/23 12:42:23.124
  STEP: Waiting for the pdb to be processed @ 07/15/23 12:42:23.138
  STEP: Patching PodDisruptionBudget status @ 07/15/23 12:42:23.146
  STEP: Waiting for the pdb to be processed @ 07/15/23 12:42:23.155
  Jul 15 12:42:23.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9010" for this suite. @ 07/15/23 12:42:23.163
• [4.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 07/15/23 12:42:23.172
  Jul 15 12:42:23.172: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename init-container @ 07/15/23 12:42:23.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:42:23.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:42:23.191
  STEP: creating the pod @ 07/15/23 12:42:23.194
  Jul 15 12:42:23.194: INFO: PodSpec: initContainers in spec.initContainers
  E0715 12:42:23.854555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:24.854610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:25.854703      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:42:26.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-4637" for this suite. @ 07/15/23 12:42:26.68
• [3.515 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 07/15/23 12:42:26.689
  Jul 15 12:42:26.689: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename gc @ 07/15/23 12:42:26.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:42:26.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:42:26.706
  STEP: create the rc1 @ 07/15/23 12:42:26.712
  STEP: create the rc2 @ 07/15/23 12:42:26.716
  E0715 12:42:26.855005      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:27.857098      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:28.858008      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:29.858625      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:30.859451      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:31.859587      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 07/15/23 12:42:32.728
  E0715 12:42:32.860200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc simpletest-rc-to-be-deleted @ 07/15/23 12:42:33.073
  STEP: wait for the rc to be deleted @ 07/15/23 12:42:33.079
  E0715 12:42:33.861024      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:34.861172      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:35.861223      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:36.861346      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:37.861428      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:42:38.092: INFO: 72 pods remaining
  Jul 15 12:42:38.092: INFO: 72 pods has nil DeletionTimestamp
  Jul 15 12:42:38.092: INFO: 
  E0715 12:42:38.862264      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:39.862565      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:40.862877      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:41.863064      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:42.863352      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/15/23 12:42:43.09
  W0715 12:42:43.093804      23 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 15 12:42:43.093: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 15 12:42:43.093: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lq6l" in namespace "gc-6036"
  Jul 15 12:42:43.103: INFO: Deleting pod "simpletest-rc-to-be-deleted-2ptrz" in namespace "gc-6036"
  Jul 15 12:42:43.116: INFO: Deleting pod "simpletest-rc-to-be-deleted-45psr" in namespace "gc-6036"
  Jul 15 12:42:43.129: INFO: Deleting pod "simpletest-rc-to-be-deleted-482ms" in namespace "gc-6036"
  Jul 15 12:42:43.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dl2k" in namespace "gc-6036"
  Jul 15 12:42:43.153: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rf4j" in namespace "gc-6036"
  Jul 15 12:42:43.166: INFO: Deleting pod "simpletest-rc-to-be-deleted-4tt6s" in namespace "gc-6036"
  Jul 15 12:42:43.178: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v9k9" in namespace "gc-6036"
  Jul 15 12:42:43.189: INFO: Deleting pod "simpletest-rc-to-be-deleted-52bw6" in namespace "gc-6036"
  Jul 15 12:42:43.201: INFO: Deleting pod "simpletest-rc-to-be-deleted-52qhx" in namespace "gc-6036"
  Jul 15 12:42:43.212: INFO: Deleting pod "simpletest-rc-to-be-deleted-5drfr" in namespace "gc-6036"
  Jul 15 12:42:43.223: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hzfs" in namespace "gc-6036"
  Jul 15 12:42:43.232: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m8kf" in namespace "gc-6036"
  Jul 15 12:42:43.245: INFO: Deleting pod "simpletest-rc-to-be-deleted-5tbmz" in namespace "gc-6036"
  Jul 15 12:42:43.258: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dlpm" in namespace "gc-6036"
  Jul 15 12:42:43.268: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gxnc" in namespace "gc-6036"
  Jul 15 12:42:43.279: INFO: Deleting pod "simpletest-rc-to-be-deleted-6sc44" in namespace "gc-6036"
  Jul 15 12:42:43.292: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wr6q" in namespace "gc-6036"
  Jul 15 12:42:43.303: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zh9s" in namespace "gc-6036"
  Jul 15 12:42:43.314: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wfbj" in namespace "gc-6036"
  Jul 15 12:42:43.325: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wph8" in namespace "gc-6036"
  Jul 15 12:42:43.338: INFO: Deleting pod "simpletest-rc-to-be-deleted-8g649" in namespace "gc-6036"
  Jul 15 12:42:43.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-987t7" in namespace "gc-6036"
  Jul 15 12:42:43.359: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kn8t" in namespace "gc-6036"
  Jul 15 12:42:43.372: INFO: Deleting pod "simpletest-rc-to-be-deleted-9p6p5" in namespace "gc-6036"
  Jul 15 12:42:43.383: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdmm4" in namespace "gc-6036"
  Jul 15 12:42:43.395: INFO: Deleting pod "simpletest-rc-to-be-deleted-bff7x" in namespace "gc-6036"
  Jul 15 12:42:43.405: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqf8v" in namespace "gc-6036"
  Jul 15 12:42:43.418: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqxgb" in namespace "gc-6036"
  Jul 15 12:42:43.433: INFO: Deleting pod "simpletest-rc-to-be-deleted-br5qg" in namespace "gc-6036"
  Jul 15 12:42:43.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-br6gt" in namespace "gc-6036"
  Jul 15 12:42:43.454: INFO: Deleting pod "simpletest-rc-to-be-deleted-bs2wf" in namespace "gc-6036"
  Jul 15 12:42:43.464: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6l5p" in namespace "gc-6036"
  Jul 15 12:42:43.476: INFO: Deleting pod "simpletest-rc-to-be-deleted-d68rm" in namespace "gc-6036"
  Jul 15 12:42:43.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-dbzlt" in namespace "gc-6036"
  Jul 15 12:42:43.495: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnwdl" in namespace "gc-6036"
  Jul 15 12:42:43.505: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvqwv" in namespace "gc-6036"
  Jul 15 12:42:43.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-dw4bz" in namespace "gc-6036"
  Jul 15 12:42:43.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-dw68n" in namespace "gc-6036"
  Jul 15 12:42:43.538: INFO: Deleting pod "simpletest-rc-to-be-deleted-f96w8" in namespace "gc-6036"
  Jul 15 12:42:43.547: INFO: Deleting pod "simpletest-rc-to-be-deleted-fd2zj" in namespace "gc-6036"
  Jul 15 12:42:43.562: INFO: Deleting pod "simpletest-rc-to-be-deleted-fft8r" in namespace "gc-6036"
  Jul 15 12:42:43.573: INFO: Deleting pod "simpletest-rc-to-be-deleted-g4zgx" in namespace "gc-6036"
  Jul 15 12:42:43.585: INFO: Deleting pod "simpletest-rc-to-be-deleted-gg6cz" in namespace "gc-6036"
  Jul 15 12:42:43.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-hch58" in namespace "gc-6036"
  Jul 15 12:42:43.613: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdnnq" in namespace "gc-6036"
  Jul 15 12:42:43.625: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgv79" in namespace "gc-6036"
  Jul 15 12:42:43.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-hk5wf" in namespace "gc-6036"
  Jul 15 12:42:43.650: INFO: Deleting pod "simpletest-rc-to-be-deleted-hkfl4" in namespace "gc-6036"
  Jul 15 12:42:43.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqv2w" in namespace "gc-6036"
  Jul 15 12:42:43.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6036" for this suite. @ 07/15/23 12:42:43.678
• [16.995 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 07/15/23 12:42:43.685
  Jul 15 12:42:43.685: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir @ 07/15/23 12:42:43.685
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:42:43.702
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:42:43.705
  STEP: Creating a pod to test emptydir volume type on node default medium @ 07/15/23 12:42:43.708
  E0715 12:42:43.863716      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:44.863934      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:45.864424      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:46.864683      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:42:47.731
  Jul 15 12:42:47.735: INFO: Trying to get logs from node ip-172-31-42-138 pod pod-3d573cd3-f398-4da7-a576-6263bd57f6ed container test-container: <nil>
  STEP: delete the pod @ 07/15/23 12:42:47.754
  Jul 15 12:42:47.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8106" for this suite. @ 07/15/23 12:42:47.774
• [4.095 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 07/15/23 12:42:47.78
  Jul 15 12:42:47.780: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename pods @ 07/15/23 12:42:47.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:42:47.795
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:42:47.799
  STEP: creating the pod @ 07/15/23 12:42:47.802
  STEP: submitting the pod to kubernetes @ 07/15/23 12:42:47.802
  W0715 12:42:47.810215      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0715 12:42:47.865501      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:48.865772      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 07/15/23 12:42:49.821
  STEP: updating the pod @ 07/15/23 12:42:49.826
  E0715 12:42:49.866067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:42:50.338: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1b6d201b-35ad-4c4e-b3de-5a13e57d3846"
  E0715 12:42:50.866895      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:51.867076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:52.868090      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:53.868348      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:42:54.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3537" for this suite. @ 07/15/23 12:42:54.358
• [6.586 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 07/15/23 12:42:54.366
  Jul 15 12:42:54.366: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 12:42:54.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:42:54.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:42:54.387
  STEP: Setting up server cert @ 07/15/23 12:42:54.408
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 12:42:54.682
  STEP: Deploying the webhook pod @ 07/15/23 12:42:54.689
  STEP: Wait for the deployment to be ready @ 07/15/23 12:42:54.701
  Jul 15 12:42:54.710: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0715 12:42:54.869092      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:55.869464      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/15/23 12:42:56.721
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 12:42:56.732
  E0715 12:42:56.869838      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:42:57.732: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 07/15/23 12:42:57.736
  STEP: create a pod @ 07/15/23 12:42:57.751
  E0715 12:42:57.870005      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:42:58.870312      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 07/15/23 12:42:59.77
  Jul 15 12:42:59.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=webhook-633 attach --namespace=webhook-633 to-be-attached-pod -i -c=container1'
  Jul 15 12:42:59.827: INFO: rc: 1
  Jul 15 12:42:59.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0715 12:42:59.871249      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-633" for this suite. @ 07/15/23 12:42:59.876
  STEP: Destroying namespace "webhook-markers-2345" for this suite. @ 07/15/23 12:42:59.882
• [5.522 seconds]
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 07/15/23 12:42:59.888
  Jul 15 12:42:59.889: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubelet-test @ 07/15/23 12:42:59.889
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:42:59.906
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:42:59.908
  E0715 12:43:00.871543      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:01.872403      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:43:01.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2292" for this suite. @ 07/15/23 12:43:01.943
• [2.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 07/15/23 12:43:01.951
  Jul 15 12:43:01.951: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename deployment @ 07/15/23 12:43:01.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:43:01.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:43:01.969
  Jul 15 12:43:01.980: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0715 12:43:02.872882      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:03.873187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:04.873259      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:05.874254      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:06.874571      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:43:06.985: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/15/23 12:43:06.985
  Jul 15 12:43:06.985: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0715 12:43:07.875495      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:08.875775      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:43:08.990: INFO: Creating deployment "test-rollover-deployment"
  Jul 15 12:43:09.001: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0715 12:43:09.876414      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:10.876819      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:43:11.010: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Jul 15 12:43:11.016: INFO: Ensure that both replica sets have 1 created replica
  Jul 15 12:43:11.022: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Jul 15 12:43:11.033: INFO: Updating deployment test-rollover-deployment
  Jul 15 12:43:11.033: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0715 12:43:11.877080      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:12.877189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:43:13.042: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Jul 15 12:43:13.049: INFO: Make sure deployment "test-rollover-deployment" is complete
  Jul 15 12:43:13.055: INFO: all replica sets need to contain the pod-template-hash label
  Jul 15 12:43:13.055: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 12, 43, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 12, 43, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 12, 43, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 12, 43, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 12:43:13.878163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:14.878356      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:43:15.064: INFO: all replica sets need to contain the pod-template-hash label
  Jul 15 12:43:15.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 12, 43, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 12, 43, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 12, 43, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 12, 43, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 12:43:15.879376      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:16.879602      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:43:17.063: INFO: all replica sets need to contain the pod-template-hash label
  Jul 15 12:43:17.063: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 12, 43, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 12, 43, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 12, 43, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 12, 43, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 12:43:17.880432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:18.880708      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:43:19.063: INFO: all replica sets need to contain the pod-template-hash label
  Jul 15 12:43:19.063: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 12, 43, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 12, 43, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 12, 43, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 12, 43, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 12:43:19.881553      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:20.882372      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:43:21.064: INFO: all replica sets need to contain the pod-template-hash label
  Jul 15 12:43:21.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 12, 43, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 12, 43, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 12, 43, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 12, 43, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 12:43:21.882413      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:22.882476      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:43:23.063: INFO: 
  Jul 15 12:43:23.063: INFO: Ensure that both old replica sets have no replicas
  Jul 15 12:43:23.073: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-7959  b29516f1-f2d8-42f2-ab79-31af88fff15f 24467 2 2023-07-15 12:43:08 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-15 12:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-15 12:43:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0066aa8a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-15 12:43:09 +0000 UTC,LastTransitionTime:2023-07-15 12:43:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-07-15 12:43:22 +0000 UTC,LastTransitionTime:2023-07-15 12:43:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 15 12:43:23.077: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-7959  bd9ad089-c13f-4326-aea7-8cb3a0752176 24457 2 2023-07-15 12:43:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b29516f1-f2d8-42f2-ab79-31af88fff15f 0xc0066aad57 0xc0066aad58}] [] [{kube-controller-manager Update apps/v1 2023-07-15 12:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b29516f1-f2d8-42f2-ab79-31af88fff15f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-15 12:43:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0066aae08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 15 12:43:23.077: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Jul 15 12:43:23.077: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7959  be4e26ea-8f03-4afd-9f95-21a09d1e5010 24466 2 2023-07-15 12:43:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b29516f1-f2d8-42f2-ab79-31af88fff15f 0xc0066aac27 0xc0066aac28}] [] [{e2e.test Update apps/v1 2023-07-15 12:43:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-15 12:43:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b29516f1-f2d8-42f2-ab79-31af88fff15f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-15 12:43:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0066aace8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 15 12:43:23.077: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-7959  97c9462e-1120-46fc-9d2f-378fa0e1d3bb 24419 2 2023-07-15 12:43:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b29516f1-f2d8-42f2-ab79-31af88fff15f 0xc0066aae77 0xc0066aae78}] [] [{kube-controller-manager Update apps/v1 2023-07-15 12:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b29516f1-f2d8-42f2-ab79-31af88fff15f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-15 12:43:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0066aaf28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 15 12:43:23.080: INFO: Pod "test-rollover-deployment-57777854c9-vfjwn" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-vfjwn test-rollover-deployment-57777854c9- deployment-7959  a280cadc-9054-4d9e-b97d-a8afe6c5eec9 24435 0 2023-07-15 12:43:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 bd9ad089-c13f-4326-aea7-8cb3a0752176 0xc006595a37 0xc006595a38}] [] [{kube-controller-manager Update v1 2023-07-15 12:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd9ad089-c13f-4326-aea7-8cb3a0752176\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:43:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.191.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqn8m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqn8m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-138,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:43:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:43:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:43:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:43:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.138,PodIP:192.168.191.203,StartTime:2023-07-15 12:43:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-15 12:43:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://2456132d2ccb5d7e95a0dc5b24143b33c94a907b3d37d30db12334bf1def93cc,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.191.203,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:43:23.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7959" for this suite. @ 07/15/23 12:43:23.084
• [21.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 07/15/23 12:43:23.091
  Jul 15 12:43:23.091: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:43:23.092
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:43:23.109
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:43:23.112
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 12:43:23.115
  E0715 12:43:23.883213      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:24.883399      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:25.883501      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:26.883641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:43:27.138
  Jul 15 12:43:27.142: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-a1f46702-72d3-4636-a0e1-d1767d9ccb03 container client-container: <nil>
  STEP: delete the pod @ 07/15/23 12:43:27.149
  Jul 15 12:43:27.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4559" for this suite. @ 07/15/23 12:43:27.166
• [4.081 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 07/15/23 12:43:27.172
  Jul 15 12:43:27.172: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir @ 07/15/23 12:43:27.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:43:27.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:43:27.19
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 07/15/23 12:43:27.193
  E0715 12:43:27.884604      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:28.884923      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:29.885534      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:30.886111      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:43:31.215
  Jul 15 12:43:31.218: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-7df8bbcc-ba04-43c0-9cc4-4c1181c4470b container test-container: <nil>
  STEP: delete the pod @ 07/15/23 12:43:31.226
  Jul 15 12:43:31.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7425" for this suite. @ 07/15/23 12:43:31.244
• [4.080 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 07/15/23 12:43:31.253
  Jul 15 12:43:31.253: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename configmap @ 07/15/23 12:43:31.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:43:31.268
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:43:31.271
  STEP: Creating configMap with name configmap-test-upd-01ba1b2b-0b2b-4bf3-acc3-e50ce59b6bf1 @ 07/15/23 12:43:31.279
  STEP: Creating the pod @ 07/15/23 12:43:31.282
  E0715 12:43:31.886642      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:32.887292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap configmap-test-upd-01ba1b2b-0b2b-4bf3-acc3-e50ce59b6bf1 @ 07/15/23 12:43:33.313
  STEP: waiting to observe update in volume @ 07/15/23 12:43:33.318
  E0715 12:43:33.887398      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:34.887691      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:35.887709      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:36.887907      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:37.888986      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:38.889246      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:39.889283      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:40.890150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:41.891020      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:42.891215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:43.891604      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:44.891907      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:45.892690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:46.893486      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:47.894042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:48.894240      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:49.894528      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:50.894891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:51.895872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:52.896170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:53.897071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:54.897171      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:55.897825      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:56.898066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:57.898181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:58.898605      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:43:59.898962      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:00.899374      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:01.899880      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:02.900412      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:03.901131      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:04.901317      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:05.902207      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:06.902829      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:07.903293      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:08.903586      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:09.904458      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:10.904834      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:11.905499      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:12.906146      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:13.907050      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:14.907337      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:15.907966      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:16.908214      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:17.908588      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:18.908819      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:19.909676      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:20.910207      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:21.910724      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:22.911009      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:23.911688      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:24.912723      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:25.913809      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:26.914206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:27.914744      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:28.915018      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:29.915867      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:30.916202      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:31.916736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:32.916913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:33.917779      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:34.918237      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:35.918537      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:36.918758      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:37.919217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:38.919453      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:39.919497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:40.919870      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:41.920902      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:42.921119      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:43.922084      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:44.922283      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:45.923102      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:46.923312      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:47.924258      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:48.924450      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:49.925281      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:50.925299      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:51.926136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:52.926361      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:44:53.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8560" for this suite. @ 07/15/23 12:44:53.678
• [82.431 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 07/15/23 12:44:53.684
  Jul 15 12:44:53.684: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename endpointslicemirroring @ 07/15/23 12:44:53.685
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:44:53.7
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:44:53.703
  STEP: mirroring a new custom Endpoint @ 07/15/23 12:44:53.716
  Jul 15 12:44:53.724: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0715 12:44:53.927289      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:54.927469      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 07/15/23 12:44:55.73
  STEP: mirroring deletion of a custom Endpoint @ 07/15/23 12:44:55.739
  Jul 15 12:44:55.750: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0715 12:44:55.928469      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:56.928599      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:44:57.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-2701" for this suite. @ 07/15/23 12:44:57.757
• [4.080 seconds]
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 07/15/23 12:44:57.764
  Jul 15 12:44:57.764: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-runtime @ 07/15/23 12:44:57.765
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:44:57.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:44:57.784
  STEP: create the container @ 07/15/23 12:44:57.789
  W0715 12:44:57.797261      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/15/23 12:44:57.797
  E0715 12:44:57.929460      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:58.929973      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:44:59.930723      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/15/23 12:45:00.817
  STEP: the container should be terminated @ 07/15/23 12:45:00.82
  STEP: the termination message should be set @ 07/15/23 12:45:00.82
  Jul 15 12:45:00.820: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 07/15/23 12:45:00.82
  Jul 15 12:45:00.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7317" for this suite. @ 07/15/23 12:45:00.841
• [3.082 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 07/15/23 12:45:00.847
  Jul 15 12:45:00.847: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename deployment @ 07/15/23 12:45:00.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:45:00.864
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:45:00.866
  STEP: creating a Deployment @ 07/15/23 12:45:00.874
  Jul 15 12:45:00.874: INFO: Creating simple deployment test-deployment-vznzs
  Jul 15 12:45:00.891: INFO: deployment "test-deployment-vznzs" doesn't have the required revision set
  E0715 12:45:00.931568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:01.931763      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Getting /status @ 07/15/23 12:45:02.904
  Jul 15 12:45:02.908: INFO: Deployment test-deployment-vznzs has Conditions: [{Available True 2023-07-15 12:45:01 +0000 UTC 2023-07-15 12:45:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-07-15 12:45:01 +0000 UTC 2023-07-15 12:45:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vznzs-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 07/15/23 12:45:02.908
  Jul 15 12:45:02.918: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 12, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 12, 45, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 12, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 12, 45, 0, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-vznzs-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 07/15/23 12:45:02.918
  Jul 15 12:45:02.920: INFO: Observed &Deployment event: ADDED
  Jul 15 12:45:02.920: INFO: Observed Deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-15 12:45:00 +0000 UTC 2023-07-15 12:45:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vznzs-5994cf9475"}
  Jul 15 12:45:02.920: INFO: Observed &Deployment event: MODIFIED
  Jul 15 12:45:02.920: INFO: Observed Deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-15 12:45:00 +0000 UTC 2023-07-15 12:45:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vznzs-5994cf9475"}
  Jul 15 12:45:02.920: INFO: Observed Deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-15 12:45:00 +0000 UTC 2023-07-15 12:45:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 15 12:45:02.920: INFO: Observed &Deployment event: MODIFIED
  Jul 15 12:45:02.920: INFO: Observed Deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-15 12:45:00 +0000 UTC 2023-07-15 12:45:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 15 12:45:02.920: INFO: Observed Deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-15 12:45:00 +0000 UTC 2023-07-15 12:45:00 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-vznzs-5994cf9475" is progressing.}
  Jul 15 12:45:02.920: INFO: Observed &Deployment event: MODIFIED
  Jul 15 12:45:02.920: INFO: Observed Deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-15 12:45:01 +0000 UTC 2023-07-15 12:45:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 15 12:45:02.920: INFO: Observed Deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-15 12:45:01 +0000 UTC 2023-07-15 12:45:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vznzs-5994cf9475" has successfully progressed.}
  Jul 15 12:45:02.920: INFO: Observed &Deployment event: MODIFIED
  Jul 15 12:45:02.920: INFO: Observed Deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-15 12:45:01 +0000 UTC 2023-07-15 12:45:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 15 12:45:02.920: INFO: Observed Deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-15 12:45:01 +0000 UTC 2023-07-15 12:45:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vznzs-5994cf9475" has successfully progressed.}
  Jul 15 12:45:02.920: INFO: Found Deployment test-deployment-vznzs in namespace deployment-4942 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 15 12:45:02.920: INFO: Deployment test-deployment-vznzs has an updated status
  STEP: patching the Statefulset Status @ 07/15/23 12:45:02.92
  Jul 15 12:45:02.920: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul 15 12:45:02.926: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 07/15/23 12:45:02.926
  Jul 15 12:45:02.927: INFO: Observed &Deployment event: ADDED
  Jul 15 12:45:02.927: INFO: Observed deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-15 12:45:00 +0000 UTC 2023-07-15 12:45:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vznzs-5994cf9475"}
  Jul 15 12:45:02.927: INFO: Observed &Deployment event: MODIFIED
  Jul 15 12:45:02.927: INFO: Observed deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-15 12:45:00 +0000 UTC 2023-07-15 12:45:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vznzs-5994cf9475"}
  Jul 15 12:45:02.927: INFO: Observed deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-15 12:45:00 +0000 UTC 2023-07-15 12:45:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 15 12:45:02.927: INFO: Observed &Deployment event: MODIFIED
  Jul 15 12:45:02.927: INFO: Observed deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-15 12:45:00 +0000 UTC 2023-07-15 12:45:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 15 12:45:02.927: INFO: Observed deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-15 12:45:00 +0000 UTC 2023-07-15 12:45:00 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-vznzs-5994cf9475" is progressing.}
  Jul 15 12:45:02.927: INFO: Observed &Deployment event: MODIFIED
  Jul 15 12:45:02.928: INFO: Observed deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-15 12:45:01 +0000 UTC 2023-07-15 12:45:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 15 12:45:02.928: INFO: Observed deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-15 12:45:01 +0000 UTC 2023-07-15 12:45:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vznzs-5994cf9475" has successfully progressed.}
  Jul 15 12:45:02.928: INFO: Observed &Deployment event: MODIFIED
  Jul 15 12:45:02.928: INFO: Observed deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-15 12:45:01 +0000 UTC 2023-07-15 12:45:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 15 12:45:02.928: INFO: Observed deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-15 12:45:01 +0000 UTC 2023-07-15 12:45:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vznzs-5994cf9475" has successfully progressed.}
  Jul 15 12:45:02.928: INFO: Observed deployment test-deployment-vznzs in namespace deployment-4942 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 15 12:45:02.928: INFO: Observed &Deployment event: MODIFIED
  Jul 15 12:45:02.928: INFO: Found deployment test-deployment-vznzs in namespace deployment-4942 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Jul 15 12:45:02.928: INFO: Deployment test-deployment-vznzs has a patched status
  E0715 12:45:02.931759      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:45:02.934: INFO: Deployment "test-deployment-vznzs":
  &Deployment{ObjectMeta:{test-deployment-vznzs  deployment-4942  b4170337-81fc-4e66-90f1-f30d89ce7799 24886 1 2023-07-15 12:45:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-07-15 12:45:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-07-15 12:45:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-07-15 12:45:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046f9dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-vznzs-5994cf9475",LastUpdateTime:2023-07-15 12:45:02 +0000 UTC,LastTransitionTime:2023-07-15 12:45:02 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 15 12:45:02.938: INFO: New ReplicaSet "test-deployment-vznzs-5994cf9475" of Deployment "test-deployment-vznzs":
  &ReplicaSet{ObjectMeta:{test-deployment-vznzs-5994cf9475  deployment-4942  79ea7960-7b58-45ae-b986-ada9debfcac4 24874 1 2023-07-15 12:45:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-vznzs b4170337-81fc-4e66-90f1-f30d89ce7799 0xc004900ae0 0xc004900ae1}] [] [{kube-controller-manager Update apps/v1 2023-07-15 12:45:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b4170337-81fc-4e66-90f1-f30d89ce7799\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-15 12:45:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004900d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 15 12:45:02.942: INFO: Pod "test-deployment-vznzs-5994cf9475-hclfw" is available:
  &Pod{ObjectMeta:{test-deployment-vznzs-5994cf9475-hclfw test-deployment-vznzs-5994cf9475- deployment-4942  56aa554b-d432-44ab-8d8b-f52dee96104a 24873 0 2023-07-15 12:45:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-vznzs-5994cf9475 79ea7960-7b58-45ae-b986-ada9debfcac4 0xc004bd6d47 0xc004bd6d48}] [] [{kube-controller-manager Update v1 2023-07-15 12:45:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79ea7960-7b58-45ae-b986-ada9debfcac4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 12:45:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.33.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-st4cs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-st4cs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:45:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:45:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 12:45:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.190,PodIP:192.168.33.116,StartTime:2023-07-15 12:45:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-15 12:45:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aba8a7933f70590035f5129d6ec5d7314e88434aba936393a18f91ff0dfe4ca4,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.33.116,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 12:45:02.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4942" for this suite. @ 07/15/23 12:45:02.947
• [2.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 07/15/23 12:45:02.953
  Jul 15 12:45:02.953: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename subpath @ 07/15/23 12:45:02.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:45:02.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:45:02.973
  STEP: Setting up data @ 07/15/23 12:45:02.976
  STEP: Creating pod pod-subpath-test-configmap-q4bl @ 07/15/23 12:45:02.984
  STEP: Creating a pod to test atomic-volume-subpath @ 07/15/23 12:45:02.984
  E0715 12:45:03.932320      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:04.932623      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:05.933167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:06.934177      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:07.934823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:08.934921      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:09.935810      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:10.936003      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:11.936229      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:12.936524      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:13.936694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:14.936840      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:15.937107      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:16.938142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:17.939124      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:18.939376      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:19.939576      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:20.940420      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:21.941062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:22.941187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:23.941335      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:24.942180      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:25.942445      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:26.943100      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:45:27.058
  Jul 15 12:45:27.062: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-subpath-test-configmap-q4bl container test-container-subpath-configmap-q4bl: <nil>
  STEP: delete the pod @ 07/15/23 12:45:27.069
  STEP: Deleting pod pod-subpath-test-configmap-q4bl @ 07/15/23 12:45:27.085
  Jul 15 12:45:27.085: INFO: Deleting pod "pod-subpath-test-configmap-q4bl" in namespace "subpath-604"
  Jul 15 12:45:27.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-604" for this suite. @ 07/15/23 12:45:27.091
• [24.145 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 07/15/23 12:45:27.098
  Jul 15 12:45:27.098: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/15/23 12:45:27.099
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:45:27.112
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:45:27.115
  STEP: set up a multi version CRD @ 07/15/23 12:45:27.118
  Jul 15 12:45:27.118: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  E0715 12:45:27.943682      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:28.944331      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:29.945197      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 07/15/23 12:45:30.347
  STEP: check the unserved version gets removed @ 07/15/23 12:45:30.366
  E0715 12:45:30.945677      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 07/15/23 12:45:31.627
  E0715 12:45:31.946316      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:32.947317      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:33.947440      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:45:34.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9464" for this suite. @ 07/15/23 12:45:34.221
• [7.130 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 07/15/23 12:45:34.229
  Jul 15 12:45:34.229: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename containers @ 07/15/23 12:45:34.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:45:34.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:45:34.25
  E0715 12:45:34.948127      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:35.948307      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:45:36.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2897" for this suite. @ 07/15/23 12:45:36.28
• [2.058 seconds]
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 07/15/23 12:45:36.288
  Jul 15 12:45:36.288: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubelet-test @ 07/15/23 12:45:36.288
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:45:36.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:45:36.309
  E0715 12:45:36.948474      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:37.948827      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:45:38.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1010" for this suite. @ 07/15/23 12:45:38.343
• [2.061 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 07/15/23 12:45:38.349
  Jul 15 12:45:38.349: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename secrets @ 07/15/23 12:45:38.35
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:45:38.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:45:38.366
  STEP: creating secret secrets-9012/secret-test-c2984e71-612a-452b-871a-c372d8318027 @ 07/15/23 12:45:38.369
  STEP: Creating a pod to test consume secrets @ 07/15/23 12:45:38.374
  E0715 12:45:38.949812      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:39.950393      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:40.951160      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:41.951277      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:45:42.395
  Jul 15 12:45:42.400: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-configmaps-a549bfd6-b058-409b-9f81-db9beaad141a container env-test: <nil>
  STEP: delete the pod @ 07/15/23 12:45:42.407
  Jul 15 12:45:42.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9012" for this suite. @ 07/15/23 12:45:42.425
• [4.082 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 07/15/23 12:45:42.431
  Jul 15 12:45:42.431: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:45:42.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:45:42.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:45:42.453
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 12:45:42.456
  E0715 12:45:42.952020      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:43.953014      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:44.953396      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:45.954135      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:45:46.477
  Jul 15 12:45:46.481: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-fbbcd518-f422-4afb-9806-4f4bb21a4fbd container client-container: <nil>
  STEP: delete the pod @ 07/15/23 12:45:46.489
  Jul 15 12:45:46.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4317" for this suite. @ 07/15/23 12:45:46.506
• [4.082 seconds]
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 07/15/23 12:45:46.513
  Jul 15 12:45:46.513: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename init-container @ 07/15/23 12:45:46.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:45:46.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:45:46.532
  STEP: creating the pod @ 07/15/23 12:45:46.535
  Jul 15 12:45:46.535: INFO: PodSpec: initContainers in spec.initContainers
  E0715 12:45:46.954330      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:47.954700      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:48.955280      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:45:49.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3638" for this suite. @ 07/15/23 12:45:49.948
• [3.441 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 07/15/23 12:45:49.954
  Jul 15 12:45:49.954: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-probe @ 07/15/23 12:45:49.955
  E0715 12:45:49.955421      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:45:49.969
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:45:49.972
  STEP: Creating pod busybox-2be2f0f7-6a7e-421b-b716-87344c9f0fe5 in namespace container-probe-6750 @ 07/15/23 12:45:49.975
  E0715 12:45:50.955629      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:51.955981      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:45:51.993: INFO: Started pod busybox-2be2f0f7-6a7e-421b-b716-87344c9f0fe5 in namespace container-probe-6750
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/15/23 12:45:51.993
  Jul 15 12:45:51.997: INFO: Initial restart count of pod busybox-2be2f0f7-6a7e-421b-b716-87344c9f0fe5 is 0
  E0715 12:45:52.956243      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:53.956486      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:54.957427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:55.958156      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:56.958837      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:57.959020      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:58.959191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:45:59.959174      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:00.960133      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:01.960394      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:02.960542      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:03.961436      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:04.962479      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:05.962687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:06.963642      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:07.963915      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:08.964510      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:09.964776      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:10.965083      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:11.966161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:12.967210      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:13.967305      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:14.967454      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:15.967894      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:16.968805      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:17.969011      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:18.969163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:19.969244      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:20.970141      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:21.970394      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:22.970938      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:23.970960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:24.971463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:25.971712      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:26.972188      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:27.973041      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:28.973137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:29.973378      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:30.974063      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:31.974340      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:32.974857      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:33.975139      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:34.975372      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:35.975738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:36.975879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:37.975947      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:38.976231      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:39.976742      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:40.977074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:41.977184      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:46:42.129: INFO: Restart count of pod container-probe-6750/busybox-2be2f0f7-6a7e-421b-b716-87344c9f0fe5 is now 1 (50.131970966s elapsed)
  Jul 15 12:46:42.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/15/23 12:46:42.133
  STEP: Destroying namespace "container-probe-6750" for this suite. @ 07/15/23 12:46:42.147
• [52.198 seconds]
------------------------------
S
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 07/15/23 12:46:42.152
  Jul 15 12:46:42.152: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename endpointslice @ 07/15/23 12:46:42.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:46:42.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:46:42.17
  STEP: getting /apis @ 07/15/23 12:46:42.173
  STEP: getting /apis/discovery.k8s.io @ 07/15/23 12:46:42.178
  STEP: getting /apis/discovery.k8s.iov1 @ 07/15/23 12:46:42.179
  STEP: creating @ 07/15/23 12:46:42.18
  STEP: getting @ 07/15/23 12:46:42.194
  STEP: listing @ 07/15/23 12:46:42.198
  STEP: watching @ 07/15/23 12:46:42.201
  Jul 15 12:46:42.201: INFO: starting watch
  STEP: cluster-wide listing @ 07/15/23 12:46:42.203
  STEP: cluster-wide watching @ 07/15/23 12:46:42.206
  Jul 15 12:46:42.206: INFO: starting watch
  STEP: patching @ 07/15/23 12:46:42.207
  STEP: updating @ 07/15/23 12:46:42.212
  Jul 15 12:46:42.219: INFO: waiting for watch events with expected annotations
  Jul 15 12:46:42.219: INFO: saw patched and updated annotations
  STEP: deleting @ 07/15/23 12:46:42.219
  STEP: deleting a collection @ 07/15/23 12:46:42.235
  Jul 15 12:46:42.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7752" for this suite. @ 07/15/23 12:46:42.254
• [0.108 seconds]
------------------------------
S
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 07/15/23 12:46:42.26
  Jul 15 12:46:42.260: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename podtemplate @ 07/15/23 12:46:42.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:46:42.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:46:42.279
  STEP: Create set of pod templates @ 07/15/23 12:46:42.282
  Jul 15 12:46:42.287: INFO: created test-podtemplate-1
  Jul 15 12:46:42.291: INFO: created test-podtemplate-2
  Jul 15 12:46:42.295: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 07/15/23 12:46:42.295
  STEP: delete collection of pod templates @ 07/15/23 12:46:42.299
  Jul 15 12:46:42.299: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 07/15/23 12:46:42.316
  Jul 15 12:46:42.316: INFO: requesting list of pod templates to confirm quantity
  Jul 15 12:46:42.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1226" for this suite. @ 07/15/23 12:46:42.322
• [0.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 07/15/23 12:46:42.329
  Jul 15 12:46:42.329: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename events @ 07/15/23 12:46:42.33
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:46:42.347
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:46:42.35
  STEP: Create set of events @ 07/15/23 12:46:42.353
  Jul 15 12:46:42.356: INFO: created test-event-1
  Jul 15 12:46:42.361: INFO: created test-event-2
  Jul 15 12:46:42.367: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 07/15/23 12:46:42.367
  STEP: delete collection of events @ 07/15/23 12:46:42.37
  Jul 15 12:46:42.370: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 07/15/23 12:46:42.393
  Jul 15 12:46:42.393: INFO: requesting list of events to confirm quantity
  Jul 15 12:46:42.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7445" for this suite. @ 07/15/23 12:46:42.401
• [0.077 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 07/15/23 12:46:42.406
  Jul 15 12:46:42.406: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir @ 07/15/23 12:46:42.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:46:42.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:46:42.422
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 07/15/23 12:46:42.425
  E0715 12:46:42.977651      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:43.977738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:44.978207      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:45.978401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:46:46.448
  Jul 15 12:46:46.451: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-8776dc01-22aa-44e4-bf32-3f2f71dadcce container test-container: <nil>
  STEP: delete the pod @ 07/15/23 12:46:46.461
  Jul 15 12:46:46.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3115" for this suite. @ 07/15/23 12:46:46.482
• [4.083 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 07/15/23 12:46:46.49
  Jul 15 12:46:46.490: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename services @ 07/15/23 12:46:46.49
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:46:46.505
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:46:46.509
  STEP: creating service in namespace services-5135 @ 07/15/23 12:46:46.512
  STEP: creating service affinity-nodeport-transition in namespace services-5135 @ 07/15/23 12:46:46.512
  STEP: creating replication controller affinity-nodeport-transition in namespace services-5135 @ 07/15/23 12:46:46.527
  I0715 12:46:46.534098      23 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-5135, replica count: 3
  E0715 12:46:46.979456      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:47.980512      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:48.981085      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0715 12:46:49.584842      23 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 15 12:46:49.596: INFO: Creating new exec pod
  E0715 12:46:49.982068      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:50.982436      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:51.982599      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:46:52.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-5135 exec execpod-affinityhdzts -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Jul 15 12:46:52.723: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Jul 15 12:46:52.723: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:46:52.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-5135 exec execpod-affinityhdzts -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.50 80'
  Jul 15 12:46:52.825: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.50 80\nConnection to 10.152.183.50 80 port [tcp/http] succeeded!\n"
  Jul 15 12:46:52.825: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:46:52.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-5135 exec execpod-affinityhdzts -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.190 32305'
  Jul 15 12:46:52.924: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.16.190 32305\nConnection to 172.31.16.190 32305 port [tcp/*] succeeded!\n"
  Jul 15 12:46:52.924: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:46:52.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-5135 exec execpod-affinityhdzts -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.84.236 32305'
  E0715 12:46:52.983637      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:46:53.023: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.84.236 32305\nConnection to 172.31.84.236 32305 port [tcp/*] succeeded!\n"
  Jul 15 12:46:53.023: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:46:53.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-5135 exec execpod-affinityhdzts -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.16.190:32305/ ; done'
  Jul 15 12:46:53.177: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n"
  Jul 15 12:46:53.177: INFO: stdout: "\naffinity-nodeport-transition-w8fp2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-w8fp2\naffinity-nodeport-transition-w8fp2\naffinity-nodeport-transition-w8fp2\naffinity-nodeport-transition-w8fp2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-bvfld\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-w8fp2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-bvfld\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-w8fp2"
  Jul 15 12:46:53.177: INFO: Received response from host: affinity-nodeport-transition-w8fp2
  Jul 15 12:46:53.177: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.177: INFO: Received response from host: affinity-nodeport-transition-w8fp2
  Jul 15 12:46:53.177: INFO: Received response from host: affinity-nodeport-transition-w8fp2
  Jul 15 12:46:53.177: INFO: Received response from host: affinity-nodeport-transition-w8fp2
  Jul 15 12:46:53.177: INFO: Received response from host: affinity-nodeport-transition-w8fp2
  Jul 15 12:46:53.177: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.177: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.177: INFO: Received response from host: affinity-nodeport-transition-bvfld
  Jul 15 12:46:53.177: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.177: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.177: INFO: Received response from host: affinity-nodeport-transition-w8fp2
  Jul 15 12:46:53.177: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.177: INFO: Received response from host: affinity-nodeport-transition-bvfld
  Jul 15 12:46:53.177: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.177: INFO: Received response from host: affinity-nodeport-transition-w8fp2
  Jul 15 12:46:53.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-5135 exec execpod-affinityhdzts -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.16.190:32305/ ; done'
  Jul 15 12:46:53.336: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.16.190:32305/\n"
  Jul 15 12:46:53.336: INFO: stdout: "\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2\naffinity-nodeport-transition-tfzs2"
  Jul 15 12:46:53.336: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.336: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.336: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.336: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.336: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.336: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.336: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.336: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.336: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.336: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.336: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.336: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.336: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.336: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.336: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.336: INFO: Received response from host: affinity-nodeport-transition-tfzs2
  Jul 15 12:46:53.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 15 12:46:53.341: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5135, will wait for the garbage collector to delete the pods @ 07/15/23 12:46:53.353
  Jul 15 12:46:53.414: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.36972ms
  Jul 15 12:46:53.515: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.08285ms
  E0715 12:46:53.984065      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:54.984843      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-5135" for this suite. @ 07/15/23 12:46:55.838
• [9.353 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 07/15/23 12:46:55.844
  Jul 15 12:46:55.844: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename endpointslice @ 07/15/23 12:46:55.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:46:55.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:46:55.862
  Jul 15 12:46:55.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-3719" for this suite. @ 07/15/23 12:46:55.908
• [0.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 07/15/23 12:46:55.915
  Jul 15 12:46:55.915: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl @ 07/15/23 12:46:55.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:46:55.93
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:46:55.933
  STEP: validating cluster-info @ 07/15/23 12:46:55.936
  Jul 15 12:46:55.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-6486 cluster-info'
  Jul 15 12:46:55.978: INFO: stderr: ""
  Jul 15 12:46:55.978: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Jul 15 12:46:55.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6486" for this suite. @ 07/15/23 12:46:55.983
  E0715 12:46:55.985483      23 retrywatcher.go:130] "Watch failed" err="context canceled"
• [0.077 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 07/15/23 12:46:55.992
  Jul 15 12:46:55.992: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename containers @ 07/15/23 12:46:55.993
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:46:56.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:46:56.015
  STEP: Creating a pod to test override arguments @ 07/15/23 12:46:56.018
  E0715 12:46:56.986142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:57.986355      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:58.987199      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:46:59.988254      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:47:00.039
  Jul 15 12:47:00.043: INFO: Trying to get logs from node ip-172-31-16-190 pod client-containers-bfed6136-2ff2-488f-900d-f4ac3035c84b container agnhost-container: <nil>
  STEP: delete the pod @ 07/15/23 12:47:00.05
  Jul 15 12:47:00.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-1380" for this suite. @ 07/15/23 12:47:00.071
• [4.086 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 07/15/23 12:47:00.079
  Jul 15 12:47:00.079: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl @ 07/15/23 12:47:00.079
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:47:00.099
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:47:00.102
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/15/23 12:47:00.105
  Jul 15 12:47:00.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-3926 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Jul 15 12:47:00.154: INFO: stderr: ""
  Jul 15 12:47:00.154: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 07/15/23 12:47:00.154
  Jul 15 12:47:00.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-3926 delete pods e2e-test-httpd-pod'
  E0715 12:47:00.989113      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:01.990139      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:47:02.091: INFO: stderr: ""
  Jul 15 12:47:02.091: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul 15 12:47:02.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3926" for this suite. @ 07/15/23 12:47:02.094
• [2.022 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 07/15/23 12:47:02.101
  Jul 15 12:47:02.101: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename secrets @ 07/15/23 12:47:02.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:47:02.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:47:02.123
  STEP: Creating secret with name s-test-opt-del-7c31fa09-1488-4ced-9418-c718387b2bf8 @ 07/15/23 12:47:02.131
  STEP: Creating secret with name s-test-opt-upd-cafee658-b67b-44b0-b35c-b2ca3e8f448b @ 07/15/23 12:47:02.136
  STEP: Creating the pod @ 07/15/23 12:47:02.14
  E0715 12:47:02.990260      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:03.990581      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-7c31fa09-1488-4ced-9418-c718387b2bf8 @ 07/15/23 12:47:04.182
  STEP: Updating secret s-test-opt-upd-cafee658-b67b-44b0-b35c-b2ca3e8f448b @ 07/15/23 12:47:04.19
  STEP: Creating secret with name s-test-opt-create-43d97fa7-1dd6-4f5b-acb5-15f5e56633a9 @ 07/15/23 12:47:04.194
  STEP: waiting to observe update in volume @ 07/15/23 12:47:04.204
  E0715 12:47:04.991381      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:05.991622      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:06.991850      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:07.992121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:08.992767      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:09.993754      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:10.993878      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:11.994051      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:12.994454      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:13.994718      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:14.994831      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:15.995077      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:16.995204      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:17.995425      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:18.995944      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:19.996283      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:20.997233      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:21.997346      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:22.997862      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:23.998176      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:24.998211      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:25.998597      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:26.998829      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:27.998916      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:28.998974      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:29.999363      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:30.999936      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:32.000119      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:33.001127      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:34.001349      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:35.001648      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:36.002370      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:37.002585      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:38.002602      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:39.003326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:40.004033      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:41.004255      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:42.004384      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:43.004776      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:44.005570      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:45.006482      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:46.006683      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:47.007118      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:48.007375      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:49.007942      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:50.008286      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:51.008433      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:52.008682      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:53.009063      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:54.009241      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:55.010265      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:56.010375      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:57.010748      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:58.011123      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:47:59.011421      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:00.012373      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:01.012657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:02.012886      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:03.013539      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:04.014235      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:05.015049      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:06.015229      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:07.015332      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:08.015543      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:09.016544      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:10.016756      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:11.016989      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:12.017073      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:13.017980      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:14.018226      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:15.018724      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:16.019014      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:17.019278      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:18.019564      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:19.019833      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:20.020306      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:21.020644      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:22.020913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:23.021146      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:24.022220      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:25.022907      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:26.023449      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:27.023716      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:28.024050      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:48:28.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2040" for this suite. @ 07/15/23 12:48:28.6
• [86.507 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 07/15/23 12:48:28.609
  Jul 15 12:48:28.609: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename pod-network-test @ 07/15/23 12:48:28.609
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:48:28.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:48:28.628
  STEP: Performing setup for networking test in namespace pod-network-test-8861 @ 07/15/23 12:48:28.631
  STEP: creating a selector @ 07/15/23 12:48:28.631
  STEP: Creating the service pods in kubernetes @ 07/15/23 12:48:28.631
  Jul 15 12:48:28.631: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0715 12:48:29.024374      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:30.024864      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:31.025812      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:32.026682      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:33.027527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:34.028495      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:35.029407      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:36.030253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:37.030899      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:38.031023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:39.031532      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:40.031880      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:41.032557      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:42.032841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:43.033621      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:44.033698      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:45.034386      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:46.034636      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:47.035567      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:48.036011      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:49.036147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:50.036293      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 07/15/23 12:48:50.737
  E0715 12:48:51.037226      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:52.037427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:48:52.768: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 15 12:48:52.768: INFO: Going to poll 192.168.33.66 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul 15 12:48:52.771: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.33.66:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8861 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 12:48:52.771: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 12:48:52.771: INFO: ExecWithOptions: Clientset creation
  Jul 15 12:48:52.771: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8861/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.33.66%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 15 12:48:52.834: INFO: Found all 1 expected endpoints: [netserver-0]
  Jul 15 12:48:52.834: INFO: Going to poll 192.168.191.248 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul 15 12:48:52.839: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.191.248:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8861 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 12:48:52.839: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 12:48:52.840: INFO: ExecWithOptions: Clientset creation
  Jul 15 12:48:52.840: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8861/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.191.248%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 15 12:48:52.900: INFO: Found all 1 expected endpoints: [netserver-1]
  Jul 15 12:48:52.900: INFO: Going to poll 192.168.4.21 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul 15 12:48:52.903: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.4.21:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8861 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 12:48:52.903: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 12:48:52.904: INFO: ExecWithOptions: Clientset creation
  Jul 15 12:48:52.904: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8861/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.4.21%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 15 12:48:52.967: INFO: Found all 1 expected endpoints: [netserver-2]
  Jul 15 12:48:52.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-8861" for this suite. @ 07/15/23 12:48:52.972
• [24.370 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 07/15/23 12:48:52.981
  Jul 15 12:48:52.981: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename dns @ 07/15/23 12:48:52.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:48:53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:48:53.003
  STEP: Creating a test headless service @ 07/15/23 12:48:53.006
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8560.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8560.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 07/15/23 12:48:53.012
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8560.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8560.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 07/15/23 12:48:53.012
  STEP: creating a pod to probe DNS @ 07/15/23 12:48:53.012
  STEP: submitting the pod to kubernetes @ 07/15/23 12:48:53.013
  E0715 12:48:53.038030      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:54.038186      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/15/23 12:48:55.029
  STEP: looking for the results for each expected name from probers @ 07/15/23 12:48:55.032
  E0715 12:48:55.038504      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:48:55.050: INFO: DNS probes using dns-8560/dns-test-39daf4f8-2406-4ef0-908b-9a27521458ba succeeded

  Jul 15 12:48:55.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/15/23 12:48:55.054
  STEP: deleting the test headless service @ 07/15/23 12:48:55.071
  STEP: Destroying namespace "dns-8560" for this suite. @ 07/15/23 12:48:55.083
• [2.110 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 07/15/23 12:48:55.091
  Jul 15 12:48:55.091: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename services @ 07/15/23 12:48:55.092
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:48:55.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:48:55.113
  STEP: creating service multi-endpoint-test in namespace services-4805 @ 07/15/23 12:48:55.116
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4805 to expose endpoints map[] @ 07/15/23 12:48:55.126
  Jul 15 12:48:55.129: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  E0715 12:48:56.038674      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:48:56.139: INFO: successfully validated that service multi-endpoint-test in namespace services-4805 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-4805 @ 07/15/23 12:48:56.139
  E0715 12:48:57.039149      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:48:58.039543      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4805 to expose endpoints map[pod1:[100]] @ 07/15/23 12:48:58.159
  Jul 15 12:48:58.170: INFO: successfully validated that service multi-endpoint-test in namespace services-4805 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-4805 @ 07/15/23 12:48:58.17
  E0715 12:48:59.040070      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:00.040276      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4805 to expose endpoints map[pod1:[100] pod2:[101]] @ 07/15/23 12:49:00.188
  Jul 15 12:49:00.202: INFO: successfully validated that service multi-endpoint-test in namespace services-4805 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 07/15/23 12:49:00.202
  Jul 15 12:49:00.202: INFO: Creating new exec pod
  E0715 12:49:01.040711      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:02.041071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:03.041131      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:49:03.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-4805 exec execpod8hnzm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Jul 15 12:49:03.322: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Jul 15 12:49:03.322: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:49:03.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-4805 exec execpod8hnzm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.126 80'
  Jul 15 12:49:03.416: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.126 80\nConnection to 10.152.183.126 80 port [tcp/http] succeeded!\n"
  Jul 15 12:49:03.416: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:49:03.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-4805 exec execpod8hnzm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Jul 15 12:49:03.514: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Jul 15 12:49:03.514: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:49:03.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-4805 exec execpod8hnzm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.126 81'
  Jul 15 12:49:03.610: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.126 81\nConnection to 10.152.183.126 81 port [tcp/*] succeeded!\n"
  Jul 15 12:49:03.610: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-4805 @ 07/15/23 12:49:03.61
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4805 to expose endpoints map[pod2:[101]] @ 07/15/23 12:49:03.622
  Jul 15 12:49:03.635: INFO: successfully validated that service multi-endpoint-test in namespace services-4805 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-4805 @ 07/15/23 12:49:03.635
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4805 to expose endpoints map[] @ 07/15/23 12:49:03.649
  Jul 15 12:49:03.658: INFO: successfully validated that service multi-endpoint-test in namespace services-4805 exposes endpoints map[]
  Jul 15 12:49:03.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4805" for this suite. @ 07/15/23 12:49:03.679
• [8.593 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 07/15/23 12:49:03.685
  Jul 15 12:49:03.685: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:49:03.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:49:03.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:49:03.708
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 12:49:03.711
  E0715 12:49:04.041752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:05.042006      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:06.042156      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:07.043061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:49:07.737
  Jul 15 12:49:07.741: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-7bf1b31c-74fd-47f5-8452-68c04b19c431 container client-container: <nil>
  STEP: delete the pod @ 07/15/23 12:49:07.747
  Jul 15 12:49:07.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5632" for this suite. @ 07/15/23 12:49:07.767
• [4.090 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 07/15/23 12:49:07.775
  Jul 15 12:49:07.775: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename gc @ 07/15/23 12:49:07.775
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:49:07.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:49:07.794
  Jul 15 12:49:07.826: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"bfc5f935-d17c-49ed-aa20-422a9d6d1dd7", Controller:(*bool)(0xc00564bd4e), BlockOwnerDeletion:(*bool)(0xc00564bd4f)}}
  Jul 15 12:49:07.833: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1b9e6170-bb0c-4537-bde9-e1a0c98e5427", Controller:(*bool)(0xc0039a1b6e), BlockOwnerDeletion:(*bool)(0xc0039a1b6f)}}
  Jul 15 12:49:07.840: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"28c7b47e-4b7d-4fe9-ab34-a16a6535c1fc", Controller:(*bool)(0xc004b69956), BlockOwnerDeletion:(*bool)(0xc004b69957)}}
  E0715 12:49:08.043192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:09.043455      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:10.043979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:11.044101      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:12.044324      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:49:12.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6477" for this suite. @ 07/15/23 12:49:12.856
• [5.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 07/15/23 12:49:12.865
  Jul 15 12:49:12.865: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir @ 07/15/23 12:49:12.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:49:12.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:49:12.881
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 07/15/23 12:49:12.884
  E0715 12:49:13.044582      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:14.044799      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:15.045052      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:16.045172      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:49:16.906
  Jul 15 12:49:16.910: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-3e2d15a0-996d-4b47-a948-e87bbe9b2271 container test-container: <nil>
  STEP: delete the pod @ 07/15/23 12:49:16.917
  Jul 15 12:49:16.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1441" for this suite. @ 07/15/23 12:49:16.934
• [4.076 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 07/15/23 12:49:16.941
  Jul 15 12:49:16.941: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename services @ 07/15/23 12:49:16.941
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:49:16.956
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:49:16.959
  STEP: creating a Service @ 07/15/23 12:49:16.966
  STEP: watching for the Service to be added @ 07/15/23 12:49:16.977
  Jul 15 12:49:16.978: INFO: Found Service test-service-dfnzr in namespace services-9345 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Jul 15 12:49:16.978: INFO: Service test-service-dfnzr created
  STEP: Getting /status @ 07/15/23 12:49:16.978
  Jul 15 12:49:16.982: INFO: Service test-service-dfnzr has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 07/15/23 12:49:16.982
  STEP: watching for the Service to be patched @ 07/15/23 12:49:16.986
  Jul 15 12:49:16.988: INFO: observed Service test-service-dfnzr in namespace services-9345 with annotations: map[] & LoadBalancer: {[]}
  Jul 15 12:49:16.988: INFO: Found Service test-service-dfnzr in namespace services-9345 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Jul 15 12:49:16.988: INFO: Service test-service-dfnzr has service status patched
  STEP: updating the ServiceStatus @ 07/15/23 12:49:16.988
  Jul 15 12:49:16.997: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 07/15/23 12:49:16.997
  Jul 15 12:49:16.999: INFO: Observed Service test-service-dfnzr in namespace services-9345 with annotations: map[] & Conditions: {[]}
  Jul 15 12:49:16.999: INFO: Observed event: &Service{ObjectMeta:{test-service-dfnzr  services-9345  af0d2890-9ac9-4c02-94c0-e5393330539b 26464 0 2023-07-15 12:49:16 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-07-15 12:49:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-07-15 12:49:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.176,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.176],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Jul 15 12:49:16.999: INFO: Found Service test-service-dfnzr in namespace services-9345 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 15 12:49:16.999: INFO: Service test-service-dfnzr has service status updated
  STEP: patching the service @ 07/15/23 12:49:16.999
  STEP: watching for the Service to be patched @ 07/15/23 12:49:17.007
  Jul 15 12:49:17.009: INFO: observed Service test-service-dfnzr in namespace services-9345 with labels: map[test-service-static:true]
  Jul 15 12:49:17.010: INFO: observed Service test-service-dfnzr in namespace services-9345 with labels: map[test-service-static:true]
  Jul 15 12:49:17.010: INFO: observed Service test-service-dfnzr in namespace services-9345 with labels: map[test-service-static:true]
  Jul 15 12:49:17.010: INFO: Found Service test-service-dfnzr in namespace services-9345 with labels: map[test-service:patched test-service-static:true]
  Jul 15 12:49:17.010: INFO: Service test-service-dfnzr patched
  STEP: deleting the service @ 07/15/23 12:49:17.01
  STEP: watching for the Service to be deleted @ 07/15/23 12:49:17.022
  Jul 15 12:49:17.024: INFO: Observed event: ADDED
  Jul 15 12:49:17.024: INFO: Observed event: MODIFIED
  Jul 15 12:49:17.024: INFO: Observed event: MODIFIED
  Jul 15 12:49:17.024: INFO: Observed event: MODIFIED
  Jul 15 12:49:17.024: INFO: Found Service test-service-dfnzr in namespace services-9345 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Jul 15 12:49:17.024: INFO: Service test-service-dfnzr deleted
  Jul 15 12:49:17.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9345" for this suite. @ 07/15/23 12:49:17.028
• [0.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 07/15/23 12:49:17.035
  Jul 15 12:49:17.035: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename runtimeclass @ 07/15/23 12:49:17.036
  E0715 12:49:17.045107      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:49:17.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:49:17.056
  STEP: Deleting RuntimeClass runtimeclass-7653-delete-me @ 07/15/23 12:49:17.063
  STEP: Waiting for the RuntimeClass to disappear @ 07/15/23 12:49:17.069
  Jul 15 12:49:17.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7653" for this suite. @ 07/15/23 12:49:17.083
• [0.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 07/15/23 12:49:17.09
  Jul 15 12:49:17.090: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename daemonsets @ 07/15/23 12:49:17.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:49:17.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:49:17.106
  Jul 15 12:49:17.129: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/15/23 12:49:17.134
  Jul 15 12:49:17.138: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:17.138: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:17.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:49:17.141: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  E0715 12:49:18.045606      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:49:18.148: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:18.148: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:18.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 15 12:49:18.152: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  E0715 12:49:19.046149      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:49:19.147: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:19.147: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:19.150: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 15 12:49:19.150: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 07/15/23 12:49:19.163
  STEP: Check that daemon pods images are updated. @ 07/15/23 12:49:19.174
  Jul 15 12:49:19.177: INFO: Wrong image for pod: daemon-set-j9lbr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 15 12:49:19.177: INFO: Wrong image for pod: daemon-set-msz9c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 15 12:49:19.177: INFO: Wrong image for pod: daemon-set-vvhzr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 15 12:49:19.184: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:19.184: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0715 12:49:20.047175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:49:20.188: INFO: Wrong image for pod: daemon-set-j9lbr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 15 12:49:20.188: INFO: Wrong image for pod: daemon-set-msz9c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 15 12:49:20.193: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:20.193: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0715 12:49:21.047346      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:49:21.189: INFO: Wrong image for pod: daemon-set-j9lbr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 15 12:49:21.189: INFO: Wrong image for pod: daemon-set-msz9c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 15 12:49:21.189: INFO: Pod daemon-set-pjm6m is not available
  Jul 15 12:49:21.194: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:21.194: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0715 12:49:22.048233      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:49:22.189: INFO: Wrong image for pod: daemon-set-msz9c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 15 12:49:22.193: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:22.193: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0715 12:49:23.048597      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:49:23.189: INFO: Wrong image for pod: daemon-set-msz9c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 15 12:49:23.189: INFO: Pod daemon-set-qq2jk is not available
  Jul 15 12:49:23.193: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:23.193: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0715 12:49:24.049179      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:49:24.192: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:24.192: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0715 12:49:25.049659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:49:25.193: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:25.193: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0715 12:49:26.050616      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:49:26.189: INFO: Pod daemon-set-4s88b is not available
  Jul 15 12:49:26.193: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:26.193: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 07/15/23 12:49:26.193
  Jul 15 12:49:26.197: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:26.197: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:26.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 15 12:49:26.200: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  E0715 12:49:27.051436      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:49:27.206: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:27.206: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:49:27.211: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 15 12:49:27.211: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/15/23 12:49:27.23
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8633, will wait for the garbage collector to delete the pods @ 07/15/23 12:49:27.23
  Jul 15 12:49:27.290: INFO: Deleting DaemonSet.extensions daemon-set took: 6.492644ms
  Jul 15 12:49:27.391: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.755737ms
  E0715 12:49:28.052167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:49:28.695: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:49:28.695: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 15 12:49:28.699: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26690"},"items":null}

  Jul 15 12:49:28.703: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26690"},"items":null}

  Jul 15 12:49:28.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8633" for this suite. @ 07/15/23 12:49:28.723
• [11.639 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 07/15/23 12:49:28.73
  Jul 15 12:49:28.730: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename pods @ 07/15/23 12:49:28.73
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:49:28.747
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:49:28.75
  STEP: creating the pod @ 07/15/23 12:49:28.752
  STEP: setting up watch @ 07/15/23 12:49:28.752
  STEP: submitting the pod to kubernetes @ 07/15/23 12:49:28.856
  STEP: verifying the pod is in kubernetes @ 07/15/23 12:49:28.866
  STEP: verifying pod creation was observed @ 07/15/23 12:49:28.872
  E0715 12:49:29.053089      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:30.053583      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 07/15/23 12:49:30.885
  STEP: verifying pod deletion was observed @ 07/15/23 12:49:30.892
  E0715 12:49:31.054139      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:32.054324      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:49:32.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5078" for this suite. @ 07/15/23 12:49:32.347
• [3.624 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 07/15/23 12:49:32.355
  Jul 15 12:49:32.355: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename configmap @ 07/15/23 12:49:32.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:49:32.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:49:32.372
  STEP: Creating configMap with name configmap-test-volume-6c6bbffa-fdc9-43b0-9fe1-c30408ab2e89 @ 07/15/23 12:49:32.375
  STEP: Creating a pod to test consume configMaps @ 07/15/23 12:49:32.379
  E0715 12:49:33.054386      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:34.054678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:35.055125      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:36.055314      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:49:36.403
  Jul 15 12:49:36.407: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-configmaps-1d3fc7e3-297c-46ba-ba79-985bad9c9989 container agnhost-container: <nil>
  STEP: delete the pod @ 07/15/23 12:49:36.413
  Jul 15 12:49:36.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2693" for this suite. @ 07/15/23 12:49:36.431
• [4.082 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 07/15/23 12:49:36.437
  Jul 15 12:49:36.437: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir @ 07/15/23 12:49:36.438
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:49:36.453
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:49:36.456
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 07/15/23 12:49:36.458
  E0715 12:49:37.056270      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:38.056355      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:39.057269      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:40.057292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:49:40.48
  Jul 15 12:49:40.484: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-8537d921-e8f9-4d50-aa30-202b247bfd95 container test-container: <nil>
  STEP: delete the pod @ 07/15/23 12:49:40.491
  Jul 15 12:49:40.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6480" for this suite. @ 07/15/23 12:49:40.509
• [4.078 seconds]
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 07/15/23 12:49:40.515
  Jul 15 12:49:40.515: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename configmap @ 07/15/23 12:49:40.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:49:40.531
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:49:40.534
  STEP: Creating configMap that has name configmap-test-emptyKey-d9f18356-f400-4cbf-bf76-1d306fa3f97c @ 07/15/23 12:49:40.537
  Jul 15 12:49:40.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4946" for this suite. @ 07/15/23 12:49:40.542
• [0.034 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 07/15/23 12:49:40.55
  Jul 15 12:49:40.550: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename endpointslice @ 07/15/23 12:49:40.55
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:49:40.566
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:49:40.569
  E0715 12:49:41.058167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:42.058307      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:43.058925      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:44.059348      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:45.060363      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 07/15/23 12:49:45.627
  E0715 12:49:46.061375      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:47.062123      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:48.063174      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:49.063401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:50.063819      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 07/15/23 12:49:50.636
  E0715 12:49:51.063824      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:52.064793      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:53.065091      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:54.065142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:55.065512      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 07/15/23 12:49:55.645
  E0715 12:49:56.065656      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:57.066223      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:58.066665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:49:59.067049      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:00.067426      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 07/15/23 12:50:00.655
  Jul 15 12:50:00.675: INFO: EndpointSlice for Service endpointslice-5061/example-named-port not found
  E0715 12:50:01.068237      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:02.068441      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:03.068721      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:04.069021      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:05.069443      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:06.070217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:07.070513      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:08.070643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:09.070772      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:10.071278      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:50:10.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5061" for this suite. @ 07/15/23 12:50:10.69
• [30.146 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 07/15/23 12:50:10.696
  Jul 15 12:50:10.696: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename security-context-test @ 07/15/23 12:50:10.697
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:50:10.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:50:10.716
  E0715 12:50:11.071315      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:12.071473      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:13.072097      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:14.072334      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:50:14.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8789" for this suite. @ 07/15/23 12:50:14.751
• [4.063 seconds]
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 07/15/23 12:50:14.759
  Jul 15 12:50:14.759: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename disruption @ 07/15/23 12:50:14.76
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:50:14.773
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:50:14.776
  STEP: creating the pdb @ 07/15/23 12:50:14.779
  STEP: Waiting for the pdb to be processed @ 07/15/23 12:50:14.783
  E0715 12:50:15.072993      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:16.073854      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 07/15/23 12:50:16.794
  STEP: Waiting for the pdb to be processed @ 07/15/23 12:50:16.802
  E0715 12:50:17.074616      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:18.074935      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 07/15/23 12:50:18.81
  STEP: Waiting for the pdb to be processed @ 07/15/23 12:50:18.821
  STEP: Waiting for the pdb to be deleted @ 07/15/23 12:50:18.831
  Jul 15 12:50:18.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-535" for this suite. @ 07/15/23 12:50:18.837
• [4.084 seconds]
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 07/15/23 12:50:18.843
  Jul 15 12:50:18.843: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename sched-preemption @ 07/15/23 12:50:18.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:50:18.858
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:50:18.861
  Jul 15 12:50:18.876: INFO: Waiting up to 1m0s for all nodes to be ready
  E0715 12:50:19.075917      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:20.076265      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:21.077033      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:22.077093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:23.077126      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:24.077288      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:25.077567      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:26.077615      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:27.077790      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:28.078006      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:29.078228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:30.078348      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:31.078719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:32.078994      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:33.079480      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:34.079783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:35.079950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:36.080100      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:37.080129      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:38.080662      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:39.080835      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:40.081395      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:41.081948      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:42.082404      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:43.083149      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:44.083427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:45.084482      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:46.084615      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:47.085095      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:48.085221      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:49.086198      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:50.086553      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:51.087114      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:52.087416      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:53.088248      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:54.089067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:55.089729      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:56.089854      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:57.090225      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:58.090499      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:50:59.090727      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:00.091153      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:01.091783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:02.092049      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:03.092763      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:04.093527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:05.094210      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:06.094471      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:07.094905      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:08.095137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:09.095464      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:10.095715      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:11.095915      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:12.096239      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:13.097241      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:14.097503      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:15.097781      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:16.097984      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:17.098031      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:18.098294      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:51:18.894: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 07/15/23 12:51:18.898
  Jul 15 12:51:18.899: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename sched-preemption-path @ 07/15/23 12:51:18.899
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:51:18.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:51:18.918
  Jul 15 12:51:18.934: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Jul 15 12:51:18.938: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Jul 15 12:51:18.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 15 12:51:18.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-2667" for this suite. @ 07/15/23 12:51:19.014
  STEP: Destroying namespace "sched-preemption-6792" for this suite. @ 07/15/23 12:51:19.026
• [60.189 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 07/15/23 12:51:19.032
  Jul 15 12:51:19.032: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename downward-api @ 07/15/23 12:51:19.033
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:51:19.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:51:19.051
  STEP: Creating the pod @ 07/15/23 12:51:19.054
  E0715 12:51:19.098309      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:20.098945      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:21.099757      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:51:21.600: INFO: Successfully updated pod "annotationupdate4ecdb349-f3e9-4dac-a2c9-aeb64fc9e177"
  E0715 12:51:22.099969      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:23.100220      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:51:23.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3840" for this suite. @ 07/15/23 12:51:23.62
• [4.594 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 07/15/23 12:51:23.627
  Jul 15 12:51:23.627: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:51:23.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:51:23.646
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:51:23.649
  STEP: Creating configMap with name projected-configmap-test-volume-5b9c27bb-1bc8-400d-b41e-18b0632ba527 @ 07/15/23 12:51:23.652
  STEP: Creating a pod to test consume configMaps @ 07/15/23 12:51:23.658
  E0715 12:51:24.101042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:25.101353      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:26.102121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:27.103039      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:51:27.683
  Jul 15 12:51:27.687: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-projected-configmaps-89bfc60e-8c8b-477a-ac31-f1ef4b56dffc container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 07/15/23 12:51:27.694
  Jul 15 12:51:27.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1614" for this suite. @ 07/15/23 12:51:27.711
• [4.091 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 07/15/23 12:51:27.718
  Jul 15 12:51:27.718: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename sysctl @ 07/15/23 12:51:27.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:51:27.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:51:27.736
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 07/15/23 12:51:27.739
  STEP: Watching for error events or started pod @ 07/15/23 12:51:27.748
  E0715 12:51:28.103469      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:29.103777      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 07/15/23 12:51:29.753
  E0715 12:51:30.104414      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:31.104743      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 07/15/23 12:51:31.765
  STEP: Getting logs from the pod @ 07/15/23 12:51:31.766
  STEP: Checking that the sysctl is actually updated @ 07/15/23 12:51:31.772
  Jul 15 12:51:31.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-7525" for this suite. @ 07/15/23 12:51:31.777
• [4.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 07/15/23 12:51:31.783
  Jul 15 12:51:31.783: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 12:51:31.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:51:31.797
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:51:31.802
  STEP: Setting up server cert @ 07/15/23 12:51:31.824
  E0715 12:51:32.105535      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 12:51:32.169
  STEP: Deploying the webhook pod @ 07/15/23 12:51:32.179
  STEP: Wait for the deployment to be ready @ 07/15/23 12:51:32.191
  Jul 15 12:51:32.204: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0715 12:51:33.106075      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:34.106356      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/15/23 12:51:34.216
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 12:51:34.226
  E0715 12:51:35.106468      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:51:35.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 07/15/23 12:51:35.231
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 07/15/23 12:51:35.246
  STEP: Creating a dummy validating-webhook-configuration object @ 07/15/23 12:51:35.258
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 07/15/23 12:51:35.266
  STEP: Creating a dummy mutating-webhook-configuration object @ 07/15/23 12:51:35.272
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 07/15/23 12:51:35.28
  Jul 15 12:51:35.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1236" for this suite. @ 07/15/23 12:51:35.339
  STEP: Destroying namespace "webhook-markers-7107" for this suite. @ 07/15/23 12:51:35.345
• [3.568 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 07/15/23 12:51:35.352
  Jul 15 12:51:35.352: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename var-expansion @ 07/15/23 12:51:35.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:51:35.367
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:51:35.37
  STEP: creating the pod with failed condition @ 07/15/23 12:51:35.373
  E0715 12:51:36.106605      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:37.106841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:38.106900      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:39.107172      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:40.107199      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:41.107511      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:42.108509      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:43.109381      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:44.110182      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:45.110584      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:46.111100      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:47.111252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:48.112138      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:49.112413      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:50.112554      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:51.112831      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:52.113087      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:53.113175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:54.113811      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:55.114316      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:56.115374      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:57.115657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:58.116299      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:51:59.116598      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:00.117131      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:01.117393      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:02.118278      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:03.118593      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:04.118857      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:05.119337      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:06.119705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:07.120007      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:08.120962      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:09.121156      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:10.121666      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:11.122167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:12.122898      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:13.123173      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:14.123626      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:15.124170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:16.124477      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:17.124883      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:18.125152      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:19.126162      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:20.126752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:21.126815      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:22.127891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:23.127978      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:24.128272      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:25.128709      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:26.128823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:27.129042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:28.129366      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:29.129841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:30.130216      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:31.130407      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:32.130668      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:33.130851      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:34.131704      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:35.132483      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:36.132587      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:37.132906      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:38.133160      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:39.134173      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:40.134868      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:41.135203      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:42.135857      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:43.135984      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:44.137044      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:45.137714      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:46.137970      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:47.138165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:48.139119      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:49.139403      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:50.139999      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:51.140269      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:52.140322      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:53.140461      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:54.141000      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:55.141480      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:56.142237      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:57.142964      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:58.143067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:52:59.143394      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:00.144001      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:01.144429      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:02.145237      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:03.146157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:04.146527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:05.147004      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:06.147474      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:07.147754      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:08.148272      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:09.148424      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:10.148561      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:11.148770      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:12.149508      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:13.149631      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:14.150566      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:15.151615      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:16.152394      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:17.152488      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:18.152664      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:19.153061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:20.153139      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:21.153323      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:22.154275      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:23.154553      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:24.154741      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:25.155273      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:26.155389      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:27.155523      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:28.156329      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:29.156731      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:30.157222      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:31.157365      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:32.158158      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:33.158382      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:34.158527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:35.159085      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 07/15/23 12:53:35.382
  Jul 15 12:53:35.895: INFO: Successfully updated pod "var-expansion-a22b6f3b-81aa-4a69-adda-bb03248329c4"
  STEP: waiting for pod running @ 07/15/23 12:53:35.895
  E0715 12:53:36.159683      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:37.160168      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 07/15/23 12:53:37.905
  Jul 15 12:53:37.905: INFO: Deleting pod "var-expansion-a22b6f3b-81aa-4a69-adda-bb03248329c4" in namespace "var-expansion-5177"
  Jul 15 12:53:37.912: INFO: Wait up to 5m0s for pod "var-expansion-a22b6f3b-81aa-4a69-adda-bb03248329c4" to be fully deleted
  E0715 12:53:38.160851      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:39.161269      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:40.161897      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:41.162183      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:42.162965      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:43.163215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:44.163913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:45.164563      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:46.164764      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:47.164990      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:48.165131      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:49.166181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:50.166600      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:51.166932      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:52.167825      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:53.168708      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:54.169656      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:55.170174      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:56.170841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:57.171066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:58.171874      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:53:59.172619      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:00.173494      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:01.173807      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:02.174127      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:03.174323      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:04.175321      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:05.175628      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:06.176521      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:07.176713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:08.177334      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:09.177512      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:09.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5177" for this suite. @ 07/15/23 12:54:10.002
• [154.658 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 07/15/23 12:54:10.01
  Jul 15 12:54:10.010: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename sysctl @ 07/15/23 12:54:10.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:54:10.029
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:54:10.032
  STEP: Creating a pod with one valid and two invalid sysctls @ 07/15/23 12:54:10.034
  Jul 15 12:54:10.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-9401" for this suite. @ 07/15/23 12:54:10.043
• [0.039 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 07/15/23 12:54:10.049
  Jul 15 12:54:10.049: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename configmap @ 07/15/23 12:54:10.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:54:10.063
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:54:10.066
  STEP: Creating configMap configmap-7245/configmap-test-7e78b1a0-91cc-4f84-ac24-9fa5720f6462 @ 07/15/23 12:54:10.069
  STEP: Creating a pod to test consume configMaps @ 07/15/23 12:54:10.074
  E0715 12:54:10.178425      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:11.178674      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:12.179372      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:13.180177      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:54:14.099
  Jul 15 12:54:14.102: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-configmaps-95b96b71-c33f-43cf-8e07-0f1ebbb2aee5 container env-test: <nil>
  STEP: delete the pod @ 07/15/23 12:54:14.119
  Jul 15 12:54:14.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7245" for this suite. @ 07/15/23 12:54:14.137
• [4.093 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 07/15/23 12:54:14.143
  Jul 15 12:54:14.143: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename services @ 07/15/23 12:54:14.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:54:14.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:54:14.162
  STEP: creating a collection of services @ 07/15/23 12:54:14.165
  Jul 15 12:54:14.165: INFO: Creating e2e-svc-a-5srcd
  Jul 15 12:54:14.175: INFO: Creating e2e-svc-b-zfrfv
  E0715 12:54:14.180377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:14.185: INFO: Creating e2e-svc-c-wmmsk
  STEP: deleting service collection @ 07/15/23 12:54:14.197
  Jul 15 12:54:14.227: INFO: Collection of services has been deleted
  Jul 15 12:54:14.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-219" for this suite. @ 07/15/23 12:54:14.232
• [0.096 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 07/15/23 12:54:14.239
  Jul 15 12:54:14.239: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename proxy @ 07/15/23 12:54:14.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:54:14.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:54:14.259
  STEP: starting an echo server on multiple ports @ 07/15/23 12:54:14.272
  STEP: creating replication controller proxy-service-2dcgd in namespace proxy-3968 @ 07/15/23 12:54:14.272
  I0715 12:54:14.279745      23 runners.go:194] Created replication controller with name: proxy-service-2dcgd, namespace: proxy-3968, replica count: 1
  E0715 12:54:15.180858      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0715 12:54:15.331267      23 runners.go:194] proxy-service-2dcgd Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0715 12:54:16.181748      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0715 12:54:16.332112      23 runners.go:194] proxy-service-2dcgd Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 15 12:54:16.337: INFO: setup took 2.075134126s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 07/15/23 12:54:16.337
  Jul 15 12:54:16.342: INFO: (0) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.763977ms)
  Jul 15 12:54:16.342: INFO: (0) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 5.52386ms)
  Jul 15 12:54:16.343: INFO: (0) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 5.817776ms)
  Jul 15 12:54:16.343: INFO: (0) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 5.895581ms)
  Jul 15 12:54:16.343: INFO: (0) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 5.88278ms)
  Jul 15 12:54:16.343: INFO: (0) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 6.346006ms)
  Jul 15 12:54:16.344: INFO: (0) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 6.75772ms)
  Jul 15 12:54:16.344: INFO: (0) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 6.907678ms)
  Jul 15 12:54:16.344: INFO: (0) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 6.787111ms)
  Jul 15 12:54:16.344: INFO: (0) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 7.344023ms)
  Jul 15 12:54:16.345: INFO: (0) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 7.540754ms)
  Jul 15 12:54:16.346: INFO: (0) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 9.152644ms)
  Jul 15 12:54:16.347: INFO: (0) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 9.682373ms)
  Jul 15 12:54:16.347: INFO: (0) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 9.551727ms)
  Jul 15 12:54:16.348: INFO: (0) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 11.405701ms)
  Jul 15 12:54:16.349: INFO: (0) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 11.485145ms)
  Jul 15 12:54:16.352: INFO: (1) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 3.515448ms)
  Jul 15 12:54:16.353: INFO: (1) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 4.381266ms)
  Jul 15 12:54:16.353: INFO: (1) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 4.385386ms)
  Jul 15 12:54:16.353: INFO: (1) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 4.481941ms)
  Jul 15 12:54:16.353: INFO: (1) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.628361ms)
  Jul 15 12:54:16.354: INFO: (1) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 4.80786ms)
  Jul 15 12:54:16.354: INFO: (1) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.933747ms)
  Jul 15 12:54:16.354: INFO: (1) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 5.178921ms)
  Jul 15 12:54:16.354: INFO: (1) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 5.843099ms)
  Jul 15 12:54:16.355: INFO: (1) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 5.993957ms)
  Jul 15 12:54:16.355: INFO: (1) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 6.014908ms)
  Jul 15 12:54:16.355: INFO: (1) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 6.006398ms)
  Jul 15 12:54:16.356: INFO: (1) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 7.351163ms)
  Jul 15 12:54:16.356: INFO: (1) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 7.426617ms)
  Jul 15 12:54:16.356: INFO: (1) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 7.481671ms)
  Jul 15 12:54:16.356: INFO: (1) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 7.627289ms)
  Jul 15 12:54:16.360: INFO: (2) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 3.246763ms)
  Jul 15 12:54:16.360: INFO: (2) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 3.744841ms)
  Jul 15 12:54:16.361: INFO: (2) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 4.28061ms)
  Jul 15 12:54:16.361: INFO: (2) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 4.204406ms)
  Jul 15 12:54:16.361: INFO: (2) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 4.760897ms)
  Jul 15 12:54:16.361: INFO: (2) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 4.943018ms)
  Jul 15 12:54:16.361: INFO: (2) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 4.899805ms)
  Jul 15 12:54:16.362: INFO: (2) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 5.309459ms)
  Jul 15 12:54:16.362: INFO: (2) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 5.991227ms)
  Jul 15 12:54:16.362: INFO: (2) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 5.911782ms)
  Jul 15 12:54:16.362: INFO: (2) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 5.859159ms)
  Jul 15 12:54:16.362: INFO: (2) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 5.801136ms)
  Jul 15 12:54:16.363: INFO: (2) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 6.463293ms)
  Jul 15 12:54:16.363: INFO: (2) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 6.77247ms)
  Jul 15 12:54:16.364: INFO: (2) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 7.13462ms)
  Jul 15 12:54:16.365: INFO: (2) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 8.807545ms)
  Jul 15 12:54:16.369: INFO: (3) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 3.436883ms)
  Jul 15 12:54:16.369: INFO: (3) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 4.147103ms)
  Jul 15 12:54:16.369: INFO: (3) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.0801ms)
  Jul 15 12:54:16.370: INFO: (3) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 4.61929ms)
  Jul 15 12:54:16.370: INFO: (3) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 4.522174ms)
  Jul 15 12:54:16.370: INFO: (3) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.814361ms)
  Jul 15 12:54:16.370: INFO: (3) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 4.919237ms)
  Jul 15 12:54:16.370: INFO: (3) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 5.14987ms)
  Jul 15 12:54:16.370: INFO: (3) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 5.285508ms)
  Jul 15 12:54:16.371: INFO: (3) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 5.590895ms)
  Jul 15 12:54:16.371: INFO: (3) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 5.955775ms)
  Jul 15 12:54:16.371: INFO: (3) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 5.920263ms)
  Jul 15 12:54:16.371: INFO: (3) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 6.210489ms)
  Jul 15 12:54:16.373: INFO: (3) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 7.360644ms)
  Jul 15 12:54:16.373: INFO: (3) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 7.550304ms)
  Jul 15 12:54:16.374: INFO: (3) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 8.977795ms)
  Jul 15 12:54:16.378: INFO: (4) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 3.323816ms)
  Jul 15 12:54:16.379: INFO: (4) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 4.408138ms)
  Jul 15 12:54:16.379: INFO: (4) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 4.328093ms)
  Jul 15 12:54:16.379: INFO: (4) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 4.394797ms)
  Jul 15 12:54:16.379: INFO: (4) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 4.775628ms)
  Jul 15 12:54:16.379: INFO: (4) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.955398ms)
  Jul 15 12:54:16.379: INFO: (4) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 4.938867ms)
  Jul 15 12:54:16.379: INFO: (4) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 5.079336ms)
  Jul 15 12:54:16.380: INFO: (4) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 5.453787ms)
  Jul 15 12:54:16.380: INFO: (4) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 5.69759ms)
  Jul 15 12:54:16.380: INFO: (4) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 5.808786ms)
  Jul 15 12:54:16.381: INFO: (4) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 6.161896ms)
  Jul 15 12:54:16.381: INFO: (4) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 7.063097ms)
  Jul 15 12:54:16.382: INFO: (4) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 7.403306ms)
  Jul 15 12:54:16.382: INFO: (4) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 7.387755ms)
  Jul 15 12:54:16.382: INFO: (4) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 7.6528ms)
  Jul 15 12:54:16.387: INFO: (5) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 4.609688ms)
  Jul 15 12:54:16.387: INFO: (5) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.490332ms)
  Jul 15 12:54:16.387: INFO: (5) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.526274ms)
  Jul 15 12:54:16.387: INFO: (5) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 4.740687ms)
  Jul 15 12:54:16.387: INFO: (5) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 4.952028ms)
  Jul 15 12:54:16.387: INFO: (5) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 5.313549ms)
  Jul 15 12:54:16.388: INFO: (5) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 5.600845ms)
  Jul 15 12:54:16.388: INFO: (5) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 5.617186ms)
  Jul 15 12:54:16.388: INFO: (5) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 5.967995ms)
  Jul 15 12:54:16.388: INFO: (5) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 5.904992ms)
  Jul 15 12:54:16.388: INFO: (5) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 5.970276ms)
  Jul 15 12:54:16.388: INFO: (5) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 6.309755ms)
  Jul 15 12:54:16.389: INFO: (5) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 6.677895ms)
  Jul 15 12:54:16.389: INFO: (5) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 6.95764ms)
  Jul 15 12:54:16.389: INFO: (5) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 7.210035ms)
  Jul 15 12:54:16.391: INFO: (5) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 8.734191ms)
  Jul 15 12:54:16.394: INFO: (6) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 3.482246ms)
  Jul 15 12:54:16.395: INFO: (6) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 3.847076ms)
  Jul 15 12:54:16.395: INFO: (6) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.017536ms)
  Jul 15 12:54:16.395: INFO: (6) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 4.058318ms)
  Jul 15 12:54:16.395: INFO: (6) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 4.509514ms)
  Jul 15 12:54:16.396: INFO: (6) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 5.137299ms)
  Jul 15 12:54:16.396: INFO: (6) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 5.172171ms)
  Jul 15 12:54:16.396: INFO: (6) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 5.483159ms)
  Jul 15 12:54:16.396: INFO: (6) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 5.549882ms)
  Jul 15 12:54:16.397: INFO: (6) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 6.437912ms)
  Jul 15 12:54:16.397: INFO: (6) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 6.303905ms)
  Jul 15 12:54:16.397: INFO: (6) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 6.336147ms)
  Jul 15 12:54:16.397: INFO: (6) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 6.529197ms)
  Jul 15 12:54:16.398: INFO: (6) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 7.423028ms)
  Jul 15 12:54:16.399: INFO: (6) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 7.793599ms)
  Jul 15 12:54:16.400: INFO: (6) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 8.976534ms)
  Jul 15 12:54:16.404: INFO: (7) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 3.530538ms)
  Jul 15 12:54:16.404: INFO: (7) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 3.966042ms)
  Jul 15 12:54:16.404: INFO: (7) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 4.002525ms)
  Jul 15 12:54:16.404: INFO: (7) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.321752ms)
  Jul 15 12:54:16.405: INFO: (7) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 4.316502ms)
  Jul 15 12:54:16.405: INFO: (7) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 4.582997ms)
  Jul 15 12:54:16.405: INFO: (7) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 4.896325ms)
  Jul 15 12:54:16.405: INFO: (7) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 4.99729ms)
  Jul 15 12:54:16.406: INFO: (7) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 5.514279ms)
  Jul 15 12:54:16.406: INFO: (7) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 5.841608ms)
  Jul 15 12:54:16.406: INFO: (7) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 5.790085ms)
  Jul 15 12:54:16.406: INFO: (7) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 5.921473ms)
  Jul 15 12:54:16.407: INFO: (7) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 7.045656ms)
  Jul 15 12:54:16.407: INFO: (7) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 7.251817ms)
  Jul 15 12:54:16.407: INFO: (7) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 7.292389ms)
  Jul 15 12:54:16.407: INFO: (7) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 7.266158ms)
  Jul 15 12:54:16.411: INFO: (8) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 3.433243ms)
  Jul 15 12:54:16.411: INFO: (8) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 3.780833ms)
  Jul 15 12:54:16.411: INFO: (8) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 3.90831ms)
  Jul 15 12:54:16.412: INFO: (8) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 4.300182ms)
  Jul 15 12:54:16.412: INFO: (8) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 4.44432ms)
  Jul 15 12:54:16.412: INFO: (8) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 4.960928ms)
  Jul 15 12:54:16.413: INFO: (8) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 5.006841ms)
  Jul 15 12:54:16.413: INFO: (8) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 5.603015ms)
  Jul 15 12:54:16.414: INFO: (8) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 5.943154ms)
  Jul 15 12:54:16.414: INFO: (8) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 5.929894ms)
  Jul 15 12:54:16.414: INFO: (8) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 6.427741ms)
  Jul 15 12:54:16.414: INFO: (8) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 6.380249ms)
  Jul 15 12:54:16.414: INFO: (8) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 6.839405ms)
  Jul 15 12:54:16.415: INFO: (8) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 7.080378ms)
  Jul 15 12:54:16.415: INFO: (8) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 7.322342ms)
  Jul 15 12:54:16.416: INFO: (8) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 8.676379ms)
  Jul 15 12:54:16.420: INFO: (9) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 3.514568ms)
  Jul 15 12:54:16.420: INFO: (9) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 3.673977ms)
  Jul 15 12:54:16.421: INFO: (9) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 4.267329ms)
  Jul 15 12:54:16.421: INFO: (9) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 4.43885ms)
  Jul 15 12:54:16.421: INFO: (9) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.886795ms)
  Jul 15 12:54:16.421: INFO: (9) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 4.936557ms)
  Jul 15 12:54:16.421: INFO: (9) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 4.919497ms)
  Jul 15 12:54:16.421: INFO: (9) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 5.098476ms)
  Jul 15 12:54:16.422: INFO: (9) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 5.134489ms)
  Jul 15 12:54:16.422: INFO: (9) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 5.754043ms)
  Jul 15 12:54:16.422: INFO: (9) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 5.899792ms)
  Jul 15 12:54:16.423: INFO: (9) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 6.078811ms)
  Jul 15 12:54:16.423: INFO: (9) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 6.311865ms)
  Jul 15 12:54:16.424: INFO: (9) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 7.256748ms)
  Jul 15 12:54:16.424: INFO: (9) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 7.891594ms)
  Jul 15 12:54:16.425: INFO: (9) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 8.804625ms)
  Jul 15 12:54:16.429: INFO: (10) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 3.55948ms)
  Jul 15 12:54:16.429: INFO: (10) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 3.902599ms)
  Jul 15 12:54:16.429: INFO: (10) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 3.994345ms)
  Jul 15 12:54:16.430: INFO: (10) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 4.153924ms)
  Jul 15 12:54:16.430: INFO: (10) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.470231ms)
  Jul 15 12:54:16.430: INFO: (10) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 4.610519ms)
  Jul 15 12:54:16.430: INFO: (10) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.949368ms)
  Jul 15 12:54:16.431: INFO: (10) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 5.173561ms)
  Jul 15 12:54:16.431: INFO: (10) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 5.825158ms)
  Jul 15 12:54:16.432: INFO: (10) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 6.099943ms)
  Jul 15 12:54:16.432: INFO: (10) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 6.20797ms)
  Jul 15 12:54:16.432: INFO: (10) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 6.238611ms)
  Jul 15 12:54:16.433: INFO: (10) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 7.165593ms)
  Jul 15 12:54:16.433: INFO: (10) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 7.100809ms)
  Jul 15 12:54:16.433: INFO: (10) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 7.164323ms)
  Jul 15 12:54:16.433: INFO: (10) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 7.46994ms)
  Jul 15 12:54:16.436: INFO: (11) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 3.284004ms)
  Jul 15 12:54:16.437: INFO: (11) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 3.793073ms)
  Jul 15 12:54:16.437: INFO: (11) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 3.840926ms)
  Jul 15 12:54:16.437: INFO: (11) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 4.186165ms)
  Jul 15 12:54:16.437: INFO: (11) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.318043ms)
  Jul 15 12:54:16.438: INFO: (11) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.565427ms)
  Jul 15 12:54:16.438: INFO: (11) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 4.784318ms)
  Jul 15 12:54:16.439: INFO: (11) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 5.529541ms)
  Jul 15 12:54:16.439: INFO: (11) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 5.921063ms)
  Jul 15 12:54:16.439: INFO: (11) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 5.998167ms)
  Jul 15 12:54:16.439: INFO: (11) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 6.005908ms)
  Jul 15 12:54:16.439: INFO: (11) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 6.23711ms)
  Jul 15 12:54:16.439: INFO: (11) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 6.563089ms)
  Jul 15 12:54:16.440: INFO: (11) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 6.978042ms)
  Jul 15 12:54:16.440: INFO: (11) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 7.156783ms)
  Jul 15 12:54:16.442: INFO: (11) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 8.649316ms)
  Jul 15 12:54:16.445: INFO: (12) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 3.220811ms)
  Jul 15 12:54:16.445: INFO: (12) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 3.811204ms)
  Jul 15 12:54:16.446: INFO: (12) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 4.300121ms)
  Jul 15 12:54:16.446: INFO: (12) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 4.383966ms)
  Jul 15 12:54:16.447: INFO: (12) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.768558ms)
  Jul 15 12:54:16.447: INFO: (12) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 5.027612ms)
  Jul 15 12:54:16.447: INFO: (12) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 5.124427ms)
  Jul 15 12:54:16.447: INFO: (12) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 5.14867ms)
  Jul 15 12:54:16.447: INFO: (12) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 5.497979ms)
  Jul 15 12:54:16.448: INFO: (12) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 5.895311ms)
  Jul 15 12:54:16.448: INFO: (12) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 6.015488ms)
  Jul 15 12:54:16.448: INFO: (12) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 6.084472ms)
  Jul 15 12:54:16.448: INFO: (12) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 6.602851ms)
  Jul 15 12:54:16.449: INFO: (12) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 7.157273ms)
  Jul 15 12:54:16.449: INFO: (12) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 7.424757ms)
  Jul 15 12:54:16.450: INFO: (12) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 8.575892ms)
  Jul 15 12:54:16.454: INFO: (13) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 3.314616ms)
  Jul 15 12:54:16.454: INFO: (13) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 3.879978ms)
  Jul 15 12:54:16.455: INFO: (13) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 4.234488ms)
  Jul 15 12:54:16.455: INFO: (13) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 4.203707ms)
  Jul 15 12:54:16.456: INFO: (13) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 4.918546ms)
  Jul 15 12:54:16.456: INFO: (13) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.995091ms)
  Jul 15 12:54:16.456: INFO: (13) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 5.171061ms)
  Jul 15 12:54:16.456: INFO: (13) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 5.141129ms)
  Jul 15 12:54:16.456: INFO: (13) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 5.398884ms)
  Jul 15 12:54:16.456: INFO: (13) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 5.587314ms)
  Jul 15 12:54:16.456: INFO: (13) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 5.778585ms)
  Jul 15 12:54:16.457: INFO: (13) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 6.090002ms)
  Jul 15 12:54:16.458: INFO: (13) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 7.032166ms)
  Jul 15 12:54:16.458: INFO: (13) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 7.042646ms)
  Jul 15 12:54:16.458: INFO: (13) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 7.273489ms)
  Jul 15 12:54:16.458: INFO: (13) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 7.371284ms)
  Jul 15 12:54:16.461: INFO: (14) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 3.39367ms)
  Jul 15 12:54:16.462: INFO: (14) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 3.815154ms)
  Jul 15 12:54:16.462: INFO: (14) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 4.007155ms)
  Jul 15 12:54:16.462: INFO: (14) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 4.075099ms)
  Jul 15 12:54:16.462: INFO: (14) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 4.232008ms)
  Jul 15 12:54:16.463: INFO: (14) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 4.876034ms)
  Jul 15 12:54:16.463: INFO: (14) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 5.107727ms)
  Jul 15 12:54:16.464: INFO: (14) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 5.634956ms)
  Jul 15 12:54:16.464: INFO: (14) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 5.764364ms)
  Jul 15 12:54:16.464: INFO: (14) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 5.801676ms)
  Jul 15 12:54:16.464: INFO: (14) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 6.005197ms)
  Jul 15 12:54:16.464: INFO: (14) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 6.008907ms)
  Jul 15 12:54:16.465: INFO: (14) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 6.539277ms)
  Jul 15 12:54:16.465: INFO: (14) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 6.95084ms)
  Jul 15 12:54:16.465: INFO: (14) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 7.180523ms)
  Jul 15 12:54:16.467: INFO: (14) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 8.56106ms)
  Jul 15 12:54:16.470: INFO: (15) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 3.394211ms)
  Jul 15 12:54:16.471: INFO: (15) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 3.878078ms)
  Jul 15 12:54:16.471: INFO: (15) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 4.454231ms)
  Jul 15 12:54:16.471: INFO: (15) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 4.535835ms)
  Jul 15 12:54:16.472: INFO: (15) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 4.733837ms)
  Jul 15 12:54:16.472: INFO: (15) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.80238ms)
  Jul 15 12:54:16.472: INFO: (15) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 4.757117ms)
  Jul 15 12:54:16.472: INFO: (15) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 4.931487ms)
  Jul 15 12:54:16.472: INFO: (15) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 5.215713ms)
  Jul 15 12:54:16.473: INFO: (15) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 5.841639ms)
  Jul 15 12:54:16.473: INFO: (15) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 6.034599ms)
  Jul 15 12:54:16.473: INFO: (15) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 6.195898ms)
  Jul 15 12:54:16.473: INFO: (15) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 6.528367ms)
  Jul 15 12:54:16.474: INFO: (15) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 7.11117ms)
  Jul 15 12:54:16.474: INFO: (15) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 7.518953ms)
  Jul 15 12:54:16.476: INFO: (15) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 8.812536ms)
  Jul 15 12:54:16.479: INFO: (16) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 3.36564ms)
  Jul 15 12:54:16.479: INFO: (16) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 3.577271ms)
  Jul 15 12:54:16.480: INFO: (16) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 4.08244ms)
  Jul 15 12:54:16.480: INFO: (16) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.244808ms)
  Jul 15 12:54:16.480: INFO: (16) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.714945ms)
  Jul 15 12:54:16.480: INFO: (16) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 4.659882ms)
  Jul 15 12:54:16.480: INFO: (16) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 4.581307ms)
  Jul 15 12:54:16.481: INFO: (16) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 4.918576ms)
  Jul 15 12:54:16.481: INFO: (16) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 5.389014ms)
  Jul 15 12:54:16.481: INFO: (16) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 5.656588ms)
  Jul 15 12:54:16.481: INFO: (16) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 5.834488ms)
  Jul 15 12:54:16.482: INFO: (16) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 5.890242ms)
  Jul 15 12:54:16.482: INFO: (16) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 6.081192ms)
  Jul 15 12:54:16.483: INFO: (16) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 7.11998ms)
  Jul 15 12:54:16.483: INFO: (16) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 7.220286ms)
  Jul 15 12:54:16.483: INFO: (16) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 7.519073ms)
  Jul 15 12:54:16.488: INFO: (17) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 4.422168ms)
  Jul 15 12:54:16.488: INFO: (17) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 4.381106ms)
  Jul 15 12:54:16.488: INFO: (17) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 4.470551ms)
  Jul 15 12:54:16.488: INFO: (17) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 4.772778ms)
  Jul 15 12:54:16.488: INFO: (17) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 4.812731ms)
  Jul 15 12:54:16.488: INFO: (17) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 4.961749ms)
  Jul 15 12:54:16.488: INFO: (17) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 5.042744ms)
  Jul 15 12:54:16.489: INFO: (17) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 5.260826ms)
  Jul 15 12:54:16.489: INFO: (17) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 6.05244ms)
  Jul 15 12:54:16.489: INFO: (17) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 6.005177ms)
  Jul 15 12:54:16.490: INFO: (17) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 6.202569ms)
  Jul 15 12:54:16.490: INFO: (17) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 6.257852ms)
  Jul 15 12:54:16.490: INFO: (17) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 6.913638ms)
  Jul 15 12:54:16.491: INFO: (17) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 7.11371ms)
  Jul 15 12:54:16.491: INFO: (17) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 7.263939ms)
  Jul 15 12:54:16.492: INFO: (17) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 8.738561ms)
  Jul 15 12:54:16.496: INFO: (18) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 3.36736ms)
  Jul 15 12:54:16.496: INFO: (18) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 4.060629ms)
  Jul 15 12:54:16.497: INFO: (18) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 4.45645ms)
  Jul 15 12:54:16.497: INFO: (18) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 4.610779ms)
  Jul 15 12:54:16.497: INFO: (18) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 4.523075ms)
  Jul 15 12:54:16.497: INFO: (18) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 5.214123ms)
  Jul 15 12:54:16.497: INFO: (18) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 5.179432ms)
  Jul 15 12:54:16.498: INFO: (18) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 5.228934ms)
  Jul 15 12:54:16.498: INFO: (18) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 5.367012ms)
  Jul 15 12:54:16.498: INFO: (18) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 5.997288ms)
  Jul 15 12:54:16.499: INFO: (18) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 6.204249ms)
  Jul 15 12:54:16.499: INFO: (18) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 6.315046ms)
  Jul 15 12:54:16.499: INFO: (18) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 6.498855ms)
  Jul 15 12:54:16.500: INFO: (18) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 7.28968ms)
  Jul 15 12:54:16.500: INFO: (18) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 7.252148ms)
  Jul 15 12:54:16.501: INFO: (18) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 8.937002ms)
  Jul 15 12:54:16.505: INFO: (19) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 3.467375ms)
  Jul 15 12:54:16.505: INFO: (19) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:443/proxy/tlsrewritem... (200; 3.898479ms)
  Jul 15 12:54:16.506: INFO: (19) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">test<... (200; 4.27679ms)
  Jul 15 12:54:16.506: INFO: (19) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl/proxy/rewriteme">test</a> (200; 4.43912ms)
  Jul 15 12:54:16.506: INFO: (19) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 4.899665ms)
  Jul 15 12:54:16.506: INFO: (19) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:160/proxy/: foo (200; 4.838931ms)
  Jul 15 12:54:16.506: INFO: (19) /api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3968/pods/http:proxy-service-2dcgd-nfjbl:1080/proxy/rewriteme">... (200; 5.059134ms)
  Jul 15 12:54:16.506: INFO: (19) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:462/proxy/: tls qux (200; 5.150479ms)
  Jul 15 12:54:16.507: INFO: (19) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname1/proxy/: foo (200; 5.603145ms)
  Jul 15 12:54:16.507: INFO: (19) /api/v1/namespaces/proxy-3968/pods/proxy-service-2dcgd-nfjbl:162/proxy/: bar (200; 6.000407ms)
  Jul 15 12:54:16.507: INFO: (19) /api/v1/namespaces/proxy-3968/pods/https:proxy-service-2dcgd-nfjbl:460/proxy/: tls baz (200; 6.166996ms)
  Jul 15 12:54:16.507: INFO: (19) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname2/proxy/: bar (200; 6.271122ms)
  Jul 15 12:54:16.508: INFO: (19) /api/v1/namespaces/proxy-3968/services/proxy-service-2dcgd:portname2/proxy/: bar (200; 6.632032ms)
  Jul 15 12:54:16.509: INFO: (19) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname2/proxy/: tls qux (200; 7.160022ms)
  Jul 15 12:54:16.509: INFO: (19) /api/v1/namespaces/proxy-3968/services/http:proxy-service-2dcgd:portname1/proxy/: foo (200; 7.279169ms)
  Jul 15 12:54:16.509: INFO: (19) /api/v1/namespaces/proxy-3968/services/https:proxy-service-2dcgd:tlsportname1/proxy/: tls baz (200; 7.454109ms)
  Jul 15 12:54:16.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-2dcgd in namespace proxy-3968, will wait for the garbage collector to delete the pods @ 07/15/23 12:54:16.513
  Jul 15 12:54:16.575: INFO: Deleting ReplicationController proxy-service-2dcgd took: 8.070024ms
  Jul 15 12:54:16.675: INFO: Terminating ReplicationController proxy-service-2dcgd pods took: 100.618536ms
  E0715 12:54:17.182163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:18.183053      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-3968" for this suite. @ 07/15/23 12:54:18.876
• [4.645 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 07/15/23 12:54:18.885
  Jul 15 12:54:18.885: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/15/23 12:54:18.886
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:54:18.9
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:54:18.903
  Jul 15 12:54:18.906: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  E0715 12:54:19.183858      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 07/15/23 12:54:20.153
  Jul 15 12:54:20.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-2510 --namespace=crd-publish-openapi-2510 create -f -'
  E0715 12:54:20.184103      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:20.536: INFO: stderr: ""
  Jul 15 12:54:20.536: INFO: stdout: "e2e-test-crd-publish-openapi-6202-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jul 15 12:54:20.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-2510 --namespace=crd-publish-openapi-2510 delete e2e-test-crd-publish-openapi-6202-crds test-foo'
  Jul 15 12:54:20.586: INFO: stderr: ""
  Jul 15 12:54:20.586: INFO: stdout: "e2e-test-crd-publish-openapi-6202-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Jul 15 12:54:20.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-2510 --namespace=crd-publish-openapi-2510 apply -f -'
  Jul 15 12:54:20.709: INFO: stderr: ""
  Jul 15 12:54:20.709: INFO: stdout: "e2e-test-crd-publish-openapi-6202-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jul 15 12:54:20.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-2510 --namespace=crd-publish-openapi-2510 delete e2e-test-crd-publish-openapi-6202-crds test-foo'
  Jul 15 12:54:20.773: INFO: stderr: ""
  Jul 15 12:54:20.773: INFO: stdout: "e2e-test-crd-publish-openapi-6202-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 07/15/23 12:54:20.773
  Jul 15 12:54:20.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-2510 --namespace=crd-publish-openapi-2510 create -f -'
  Jul 15 12:54:20.889: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 07/15/23 12:54:20.889
  Jul 15 12:54:20.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-2510 --namespace=crd-publish-openapi-2510 create -f -'
  E0715 12:54:21.184903      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:21.228: INFO: rc: 1
  Jul 15 12:54:21.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-2510 --namespace=crd-publish-openapi-2510 apply -f -'
  Jul 15 12:54:21.343: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 07/15/23 12:54:21.343
  Jul 15 12:54:21.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-2510 --namespace=crd-publish-openapi-2510 create -f -'
  Jul 15 12:54:21.455: INFO: rc: 1
  Jul 15 12:54:21.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-2510 --namespace=crd-publish-openapi-2510 apply -f -'
  Jul 15 12:54:21.573: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 07/15/23 12:54:21.573
  Jul 15 12:54:21.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-2510 explain e2e-test-crd-publish-openapi-6202-crds'
  Jul 15 12:54:21.683: INFO: stderr: ""
  Jul 15 12:54:21.683: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-6202-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 07/15/23 12:54:21.683
  Jul 15 12:54:21.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-2510 explain e2e-test-crd-publish-openapi-6202-crds.metadata'
  Jul 15 12:54:21.793: INFO: stderr: ""
  Jul 15 12:54:21.793: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-6202-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Jul 15 12:54:21.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-2510 explain e2e-test-crd-publish-openapi-6202-crds.spec'
  Jul 15 12:54:21.901: INFO: stderr: ""
  Jul 15 12:54:21.901: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-6202-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Jul 15 12:54:21.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-2510 explain e2e-test-crd-publish-openapi-6202-crds.spec.bars'
  Jul 15 12:54:22.015: INFO: stderr: ""
  Jul 15 12:54:22.015: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-6202-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 07/15/23 12:54:22.015
  Jul 15 12:54:22.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=crd-publish-openapi-2510 explain e2e-test-crd-publish-openapi-6202-crds.spec.bars2'
  Jul 15 12:54:22.133: INFO: rc: 1
  E0715 12:54:22.185301      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:23.185766      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:23.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2510" for this suite. @ 07/15/23 12:54:23.383
• [4.505 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 07/15/23 12:54:23.391
  Jul 15 12:54:23.391: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename field-validation @ 07/15/23 12:54:23.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:54:23.406
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:54:23.408
  Jul 15 12:54:23.411: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  E0715 12:54:24.186200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:25.186777      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:25.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4551" for this suite. @ 07/15/23 12:54:25.974
• [2.591 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 07/15/23 12:54:25.982
  Jul 15 12:54:25.982: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 12:54:25.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:54:26.037
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:54:26.039
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 12:54:26.042
  E0715 12:54:26.187580      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:27.187759      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:28.188208      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:29.189164      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:54:30.062
  Jul 15 12:54:30.066: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-ea13b580-8837-4d4e-bf07-439257a294ff container client-container: <nil>
  STEP: delete the pod @ 07/15/23 12:54:30.08
  Jul 15 12:54:30.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2729" for this suite. @ 07/15/23 12:54:30.096
• [4.119 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 07/15/23 12:54:30.102
  Jul 15 12:54:30.102: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename daemonsets @ 07/15/23 12:54:30.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:54:30.115
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:54:30.118
  Jul 15 12:54:30.141: INFO: Create a RollingUpdate DaemonSet
  Jul 15 12:54:30.148: INFO: Check that daemon pods launch on every node of the cluster
  Jul 15 12:54:30.152: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:54:30.152: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:54:30.155: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:54:30.155: INFO: Node ip-172-31-16-190 is running 0 daemon pod, expected 1
  E0715 12:54:30.189992      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:31.159: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:54:31.159: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:54:31.163: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 15 12:54:31.163: INFO: Node ip-172-31-42-138 is running 0 daemon pod, expected 1
  E0715 12:54:31.190690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:32.161: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:54:32.161: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:54:32.164: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 15 12:54:32.164: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Jul 15 12:54:32.164: INFO: Update the DaemonSet to trigger a rollout
  Jul 15 12:54:32.172: INFO: Updating DaemonSet daemon-set
  E0715 12:54:32.190871      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:33.186: INFO: Roll back the DaemonSet before rollout is complete
  E0715 12:54:33.191460      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:33.196: INFO: Updating DaemonSet daemon-set
  Jul 15 12:54:33.196: INFO: Make sure DaemonSet rollback is complete
  Jul 15 12:54:33.199: INFO: Wrong image for pod: daemon-set-qt7wb. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Jul 15 12:54:33.199: INFO: Pod daemon-set-qt7wb is not available
  Jul 15 12:54:33.203: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:54:33.203: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0715 12:54:34.192082      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:34.213: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:54:34.213: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0715 12:54:35.192653      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:35.208: INFO: Pod daemon-set-55src is not available
  Jul 15 12:54:35.211: INFO: DaemonSet pods can't tolerate node ip-172-31-2-164 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 15 12:54:35.211: INFO: DaemonSet pods can't tolerate node ip-172-31-95-215 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 07/15/23 12:54:35.218
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6897, will wait for the garbage collector to delete the pods @ 07/15/23 12:54:35.218
  Jul 15 12:54:35.278: INFO: Deleting DaemonSet.extensions daemon-set took: 6.073962ms
  Jul 15 12:54:35.379: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.139025ms
  E0715 12:54:36.193026      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:37.183: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 15 12:54:37.183: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 15 12:54:37.186: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28237"},"items":null}

  Jul 15 12:54:37.190: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28237"},"items":null}

  E0715 12:54:37.193691      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:37.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6897" for this suite. @ 07/15/23 12:54:37.207
• [7.111 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 07/15/23 12:54:37.213
  Jul 15 12:54:37.213: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename pods @ 07/15/23 12:54:37.214
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:54:37.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:54:37.228
  STEP: Create set of pods @ 07/15/23 12:54:37.23
  Jul 15 12:54:37.239: INFO: created test-pod-1
  Jul 15 12:54:37.245: INFO: created test-pod-2
  Jul 15 12:54:37.251: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 07/15/23 12:54:37.251
  E0715 12:54:38.194052      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:39.194435      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 07/15/23 12:54:39.293
  Jul 15 12:54:39.296: INFO: Pod quantity 3 is different from expected quantity 0
  E0715 12:54:40.195102      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:40.299: INFO: Pod quantity 3 is different from expected quantity 0
  E0715 12:54:41.195863      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:41.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4032" for this suite. @ 07/15/23 12:54:41.304
• [4.096 seconds]
------------------------------
S
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 07/15/23 12:54:41.309
  Jul 15 12:54:41.309: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename services @ 07/15/23 12:54:41.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:54:41.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:54:41.325
  STEP: creating service in namespace services-9257 @ 07/15/23 12:54:41.328
  STEP: creating service affinity-clusterip-transition in namespace services-9257 @ 07/15/23 12:54:41.329
  STEP: creating replication controller affinity-clusterip-transition in namespace services-9257 @ 07/15/23 12:54:41.338
  I0715 12:54:41.345090      23 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-9257, replica count: 3
  E0715 12:54:42.196533      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:43.196805      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:44.197113      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0715 12:54:44.395478      23 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 15 12:54:44.401: INFO: Creating new exec pod
  E0715 12:54:45.198060      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:46.198528      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:47.199566      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:54:47.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-9257 exec execpod-affinityk2q9h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Jul 15 12:54:47.525: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Jul 15 12:54:47.525: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:54:47.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-9257 exec execpod-affinityk2q9h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.82 80'
  Jul 15 12:54:47.624: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.82 80\nConnection to 10.152.183.82 80 port [tcp/http] succeeded!\n"
  Jul 15 12:54:47.624: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 15 12:54:47.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-9257 exec execpod-affinityk2q9h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.82:80/ ; done'
  Jul 15 12:54:47.799: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n"
  Jul 15 12:54:47.799: INFO: stdout: "\naffinity-clusterip-transition-sbm8v\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-vb59h\naffinity-clusterip-transition-sbm8v\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-vb59h\naffinity-clusterip-transition-vb59h\naffinity-clusterip-transition-vb59h\naffinity-clusterip-transition-sbm8v\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-sbm8v\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-sbm8v"
  Jul 15 12:54:47.799: INFO: Received response from host: affinity-clusterip-transition-sbm8v
  Jul 15 12:54:47.799: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.799: INFO: Received response from host: affinity-clusterip-transition-vb59h
  Jul 15 12:54:47.799: INFO: Received response from host: affinity-clusterip-transition-sbm8v
  Jul 15 12:54:47.799: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.799: INFO: Received response from host: affinity-clusterip-transition-vb59h
  Jul 15 12:54:47.799: INFO: Received response from host: affinity-clusterip-transition-vb59h
  Jul 15 12:54:47.799: INFO: Received response from host: affinity-clusterip-transition-vb59h
  Jul 15 12:54:47.799: INFO: Received response from host: affinity-clusterip-transition-sbm8v
  Jul 15 12:54:47.799: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.799: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.799: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.799: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.799: INFO: Received response from host: affinity-clusterip-transition-sbm8v
  Jul 15 12:54:47.799: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.799: INFO: Received response from host: affinity-clusterip-transition-sbm8v
  Jul 15 12:54:47.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-9257 exec execpod-affinityk2q9h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.82:80/ ; done'
  Jul 15 12:54:47.952: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.82:80/\n"
  Jul 15 12:54:47.952: INFO: stdout: "\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj\naffinity-clusterip-transition-l28fj"
  Jul 15 12:54:47.952: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.952: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.952: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.952: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.952: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.952: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.952: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.952: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.952: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.952: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.952: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.952: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.952: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.952: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.952: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.952: INFO: Received response from host: affinity-clusterip-transition-l28fj
  Jul 15 12:54:47.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 15 12:54:47.957: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9257, will wait for the garbage collector to delete the pods @ 07/15/23 12:54:47.967
  Jul 15 12:54:48.027: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.986982ms
  Jul 15 12:54:48.127: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.11274ms
  E0715 12:54:48.200182      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:49.200618      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:50.201042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-9257" for this suite. @ 07/15/23 12:54:50.242
• [8.941 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 07/15/23 12:54:50.251
  Jul 15 12:54:50.251: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename configmap @ 07/15/23 12:54:50.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:54:50.262
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:54:50.266
  STEP: Creating configMap with name configmap-test-volume-e4a51bae-1d4b-4bc6-a841-b102e2cd7e63 @ 07/15/23 12:54:50.269
  STEP: Creating a pod to test consume configMaps @ 07/15/23 12:54:50.273
  E0715 12:54:51.201117      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:52.202152      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:53.202251      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:54.202322      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:54:54.296
  Jul 15 12:54:54.301: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-configmaps-f3f7fd84-d6ac-43b7-a574-ed63d8a98e3c container agnhost-container: <nil>
  STEP: delete the pod @ 07/15/23 12:54:54.308
  Jul 15 12:54:54.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1227" for this suite. @ 07/15/23 12:54:54.323
• [4.078 seconds]
------------------------------
S
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 07/15/23 12:54:54.33
  Jul 15 12:54:54.330: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename containers @ 07/15/23 12:54:54.33
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:54:54.342
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:54:54.344
  STEP: Creating a pod to test override all @ 07/15/23 12:54:54.346
  E0715 12:54:55.202573      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:56.202888      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:57.203471      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:54:58.203821      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:54:58.369
  Jul 15 12:54:58.372: INFO: Trying to get logs from node ip-172-31-16-190 pod client-containers-d7faa95a-aec4-4399-90dd-4f9b25beeda6 container agnhost-container: <nil>
  STEP: delete the pod @ 07/15/23 12:54:58.379
  Jul 15 12:54:58.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5294" for this suite. @ 07/15/23 12:54:58.403
• [4.078 seconds]
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 07/15/23 12:54:58.408
  Jul 15 12:54:58.408: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename subpath @ 07/15/23 12:54:58.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:54:58.424
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:54:58.426
  STEP: Setting up data @ 07/15/23 12:54:58.429
  STEP: Creating pod pod-subpath-test-projected-9krf @ 07/15/23 12:54:58.437
  STEP: Creating a pod to test atomic-volume-subpath @ 07/15/23 12:54:58.437
  E0715 12:54:59.204785      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:00.204903      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:01.205017      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:02.205191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:03.206151      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:04.206420      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:05.206614      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:06.206908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:07.207862      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:08.208137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:09.208780      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:10.209439      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:11.210282      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:12.210580      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:13.211576      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:14.212373      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:15.212595      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:16.212854      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:17.213077      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:18.213254      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:19.213566      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:20.214505      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:21.215207      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:22.215492      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:55:22.511
  Jul 15 12:55:22.514: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-subpath-test-projected-9krf container test-container-subpath-projected-9krf: <nil>
  STEP: delete the pod @ 07/15/23 12:55:22.521
  STEP: Deleting pod pod-subpath-test-projected-9krf @ 07/15/23 12:55:22.54
  Jul 15 12:55:22.541: INFO: Deleting pod "pod-subpath-test-projected-9krf" in namespace "subpath-7747"
  Jul 15 12:55:22.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7747" for this suite. @ 07/15/23 12:55:22.547
• [24.144 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 07/15/23 12:55:22.552
  Jul 15 12:55:22.552: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename secrets @ 07/15/23 12:55:22.553
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:55:22.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:55:22.567
  STEP: creating a secret @ 07/15/23 12:55:22.57
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 07/15/23 12:55:22.573
  STEP: patching the secret @ 07/15/23 12:55:22.577
  STEP: deleting the secret using a LabelSelector @ 07/15/23 12:55:22.586
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 07/15/23 12:55:22.593
  Jul 15 12:55:22.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4593" for this suite. @ 07/15/23 12:55:22.602
• [0.054 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 07/15/23 12:55:22.607
  Jul 15 12:55:22.607: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 12:55:22.607
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:55:22.619
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:55:22.622
  STEP: Setting up server cert @ 07/15/23 12:55:22.639
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 12:55:23.165
  STEP: Deploying the webhook pod @ 07/15/23 12:55:23.173
  STEP: Wait for the deployment to be ready @ 07/15/23 12:55:23.184
  Jul 15 12:55:23.195: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0715 12:55:23.216345      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:24.216482      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/15/23 12:55:25.21
  E0715 12:55:25.217095      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 12:55:25.221
  E0715 12:55:26.218151      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:55:26.221: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 07/15/23 12:55:26.285
  STEP: Creating a configMap that should be mutated @ 07/15/23 12:55:26.295
  STEP: Deleting the collection of validation webhooks @ 07/15/23 12:55:26.319
  STEP: Creating a configMap that should not be mutated @ 07/15/23 12:55:26.366
  Jul 15 12:55:26.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6634" for this suite. @ 07/15/23 12:55:26.418
  STEP: Destroying namespace "webhook-markers-7259" for this suite. @ 07/15/23 12:55:26.426
• [3.826 seconds]
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 07/15/23 12:55:26.433
  Jul 15 12:55:26.433: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename sched-preemption @ 07/15/23 12:55:26.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:55:26.448
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:55:26.45
  Jul 15 12:55:26.467: INFO: Waiting up to 1m0s for all nodes to be ready
  E0715 12:55:27.218621      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:28.218695      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:29.219183      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:30.219455      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:31.219847      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:32.219994      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:33.220472      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:34.220598      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:35.221038      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:36.221168      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:37.222178      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:38.222896      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:39.223681      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:40.223922      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:41.224029      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:42.224260      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:43.224536      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:44.225233      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:45.225363      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:46.226159      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:47.226707      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:48.226871      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:49.227319      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:50.228157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:51.228470      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:52.228810      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:53.228888      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:54.229111      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:55.229593      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:56.229766      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:57.229829      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:58.230175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:55:59.231042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:00.231116      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:01.231631      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:02.231722      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:03.232356      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:04.232579      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:05.233303      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:06.234144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:07.235169      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:08.235458      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:09.236043      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:10.236503      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:11.236669      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:12.237015      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:13.237144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:14.238165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:15.238734      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:16.238927      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:17.239873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:18.240111      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:19.240627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:20.240980      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:21.241943      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:22.242102      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:23.242667      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:24.243037      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:25.243300      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:26.243579      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:56:26.482: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 07/15/23 12:56:26.485
  Jul 15 12:56:26.502: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jul 15 12:56:26.508: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jul 15 12:56:26.523: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jul 15 12:56:26.529: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jul 15 12:56:26.544: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jul 15 12:56:26.553: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 07/15/23 12:56:26.553
  E0715 12:56:27.244218      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:28.244490      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 07/15/23 12:56:28.58
  E0715 12:56:29.244523      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:30.244848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:31.245698      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:32.245993      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:56:32.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-7564" for this suite. @ 07/15/23 12:56:32.68
• [66.254 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 07/15/23 12:56:32.688
  Jul 15 12:56:32.688: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename replication-controller @ 07/15/23 12:56:32.688
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:56:32.702
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:56:32.705
  STEP: Given a ReplicationController is created @ 07/15/23 12:56:32.708
  STEP: When the matched label of one of its pods change @ 07/15/23 12:56:32.713
  Jul 15 12:56:32.717: INFO: Pod name pod-release: Found 0 pods out of 1
  E0715 12:56:33.246085      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:34.246546      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:35.246646      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:36.246680      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:37.246932      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:56:37.722: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 07/15/23 12:56:37.73
  Jul 15 12:56:37.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4954" for this suite. @ 07/15/23 12:56:37.74
• [5.060 seconds]
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 07/15/23 12:56:37.748
  Jul 15 12:56:37.748: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename downward-api @ 07/15/23 12:56:37.748
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:56:37.759
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:56:37.761
  STEP: Creating a pod to test downward api env vars @ 07/15/23 12:56:37.764
  E0715 12:56:38.247228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:39.247486      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:40.248271      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:41.248493      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:56:41.781
  Jul 15 12:56:41.785: INFO: Trying to get logs from node ip-172-31-16-190 pod downward-api-75ca0f8b-c4f3-467b-83f9-1cd4c5979b6e container dapi-container: <nil>
  STEP: delete the pod @ 07/15/23 12:56:41.791
  Jul 15 12:56:41.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-216" for this suite. @ 07/15/23 12:56:41.808
• [4.065 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 07/15/23 12:56:41.814
  Jul 15 12:56:41.814: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename certificates @ 07/15/23 12:56:41.815
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:56:41.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:56:41.829
  STEP: getting /apis @ 07/15/23 12:56:42.111
  STEP: getting /apis/certificates.k8s.io @ 07/15/23 12:56:42.114
  STEP: getting /apis/certificates.k8s.io/v1 @ 07/15/23 12:56:42.115
  STEP: creating @ 07/15/23 12:56:42.116
  STEP: getting @ 07/15/23 12:56:42.132
  STEP: listing @ 07/15/23 12:56:42.136
  STEP: watching @ 07/15/23 12:56:42.139
  Jul 15 12:56:42.139: INFO: starting watch
  STEP: patching @ 07/15/23 12:56:42.14
  STEP: updating @ 07/15/23 12:56:42.144
  Jul 15 12:56:42.151: INFO: waiting for watch events with expected annotations
  Jul 15 12:56:42.151: INFO: saw patched and updated annotations
  STEP: getting /approval @ 07/15/23 12:56:42.151
  STEP: patching /approval @ 07/15/23 12:56:42.154
  STEP: updating /approval @ 07/15/23 12:56:42.159
  STEP: getting /status @ 07/15/23 12:56:42.166
  STEP: patching /status @ 07/15/23 12:56:42.169
  STEP: updating /status @ 07/15/23 12:56:42.175
  STEP: deleting @ 07/15/23 12:56:42.182
  STEP: deleting a collection @ 07/15/23 12:56:42.193
  Jul 15 12:56:42.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-8813" for this suite. @ 07/15/23 12:56:42.212
• [0.402 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 07/15/23 12:56:42.217
  Jul 15 12:56:42.217: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl @ 07/15/23 12:56:42.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:56:42.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:56:42.232
  STEP: validating api versions @ 07/15/23 12:56:42.234
  Jul 15 12:56:42.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-939 api-versions'
  E0715 12:56:42.249168      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:56:42.276: INFO: stderr: ""
  Jul 15 12:56:42.276: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Jul 15 12:56:42.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-939" for this suite. @ 07/15/23 12:56:42.279
• [0.068 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 07/15/23 12:56:42.285
  Jul 15 12:56:42.285: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename deployment @ 07/15/23 12:56:42.286
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:56:42.298
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:56:42.301
  STEP: creating a Deployment @ 07/15/23 12:56:42.306
  STEP: waiting for Deployment to be created @ 07/15/23 12:56:42.312
  STEP: waiting for all Replicas to be Ready @ 07/15/23 12:56:42.313
  Jul 15 12:56:42.314: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 15 12:56:42.314: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 15 12:56:42.320: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 15 12:56:42.320: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 15 12:56:42.332: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 15 12:56:42.332: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 15 12:56:42.351: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 15 12:56:42.351: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 15 12:56:43.056: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jul 15 12:56:43.056: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  E0715 12:56:43.249715      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:56:43.713: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 07/15/23 12:56:43.713
  W0715 12:56:43.721062      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul 15 12:56:43.722: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 07/15/23 12:56:43.722
  Jul 15 12:56:43.723: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 0
  Jul 15 12:56:43.723: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 0
  Jul 15 12:56:43.723: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 0
  Jul 15 12:56:43.723: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 0
  Jul 15 12:56:43.723: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 0
  Jul 15 12:56:43.723: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 0
  Jul 15 12:56:43.723: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 0
  Jul 15 12:56:43.723: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 0
  Jul 15 12:56:43.724: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1
  Jul 15 12:56:43.724: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1
  Jul 15 12:56:43.724: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2
  Jul 15 12:56:43.724: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2
  Jul 15 12:56:43.724: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2
  Jul 15 12:56:43.724: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2
  Jul 15 12:56:43.732: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2
  Jul 15 12:56:43.732: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2
  Jul 15 12:56:43.742: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2
  Jul 15 12:56:43.742: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2
  Jul 15 12:56:43.760: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1
  Jul 15 12:56:43.760: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1
  Jul 15 12:56:43.765: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1
  Jul 15 12:56:43.765: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1
  E0715 12:56:44.249986      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:56:45.067: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2
  Jul 15 12:56:45.067: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2
  Jul 15 12:56:45.088: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1
  STEP: listing Deployments @ 07/15/23 12:56:45.088
  Jul 15 12:56:45.091: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 07/15/23 12:56:45.091
  Jul 15 12:56:45.103: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 07/15/23 12:56:45.103
  Jul 15 12:56:45.108: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 15 12:56:45.113: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 15 12:56:45.139: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 15 12:56:45.149: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0715 12:56:45.250366      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:56:46.067: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 15 12:56:46.077: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 15 12:56:46.084: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 15 12:56:46.092: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 15 12:56:46.099: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0715 12:56:46.250406      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:56:46.732: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 07/15/23 12:56:46.752
  STEP: fetching the DeploymentStatus @ 07/15/23 12:56:46.757
  Jul 15 12:56:46.764: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1
  Jul 15 12:56:46.764: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1
  Jul 15 12:56:46.764: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1
  Jul 15 12:56:46.764: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 1
  Jul 15 12:56:46.764: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2
  Jul 15 12:56:46.764: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2
  Jul 15 12:56:46.764: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2
  Jul 15 12:56:46.764: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2
  Jul 15 12:56:46.765: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 2
  Jul 15 12:56:46.765: INFO: observed Deployment test-deployment in namespace deployment-4787 with ReadyReplicas 3
  STEP: deleting the Deployment @ 07/15/23 12:56:46.765
  Jul 15 12:56:46.775: INFO: observed event type MODIFIED
  Jul 15 12:56:46.775: INFO: observed event type MODIFIED
  Jul 15 12:56:46.775: INFO: observed event type MODIFIED
  Jul 15 12:56:46.775: INFO: observed event type MODIFIED
  Jul 15 12:56:46.775: INFO: observed event type MODIFIED
  Jul 15 12:56:46.775: INFO: observed event type MODIFIED
  Jul 15 12:56:46.775: INFO: observed event type MODIFIED
  Jul 15 12:56:46.775: INFO: observed event type MODIFIED
  Jul 15 12:56:46.775: INFO: observed event type MODIFIED
  Jul 15 12:56:46.775: INFO: observed event type MODIFIED
  Jul 15 12:56:46.775: INFO: observed event type MODIFIED
  Jul 15 12:56:46.779: INFO: Log out all the ReplicaSets if there is no deployment created
  Jul 15 12:56:46.783: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-4787  62bc748a-edc5-48b3-ba6d-552920abf80b 29348 3 2023-07-15 12:56:42 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 61be5f2b-67a0-4b3b-90b9-d8fe798d6182 0xc000b265d7 0xc000b265d8}] [] [{kube-controller-manager Update apps/v1 2023-07-15 12:56:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61be5f2b-67a0-4b3b-90b9-d8fe798d6182\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-15 12:56:45 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000b26660 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jul 15 12:56:46.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4787" for this suite. @ 07/15/23 12:56:46.792
• [4.514 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 07/15/23 12:56:46.799
  Jul 15 12:56:46.800: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename podtemplate @ 07/15/23 12:56:46.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:56:46.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:56:46.814
  Jul 15 12:56:46.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-5110" for this suite. @ 07/15/23 12:56:46.846
• [0.052 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 07/15/23 12:56:46.851
  Jul 15 12:56:46.852: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename downward-api @ 07/15/23 12:56:46.852
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:56:46.862
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:56:46.865
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 12:56:46.867
  E0715 12:56:47.250582      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:48.250918      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:49.252038      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:50.252559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 12:56:50.887
  Jul 15 12:56:50.890: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-1faec1c5-8161-4803-b66c-687a3cc279b2 container client-container: <nil>
  STEP: delete the pod @ 07/15/23 12:56:50.897
  Jul 15 12:56:50.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7623" for this suite. @ 07/15/23 12:56:50.917
• [4.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 07/15/23 12:56:50.922
  Jul 15 12:56:50.922: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl @ 07/15/23 12:56:50.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:56:50.934
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:56:50.936
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/15/23 12:56:50.939
  Jul 15 12:56:50.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-3760 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jul 15 12:56:50.989: INFO: stderr: ""
  Jul 15 12:56:50.989: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 07/15/23 12:56:50.989
  E0715 12:56:51.253553      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:52.254001      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:53.254189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:54.254353      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:55.255014      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 07/15/23 12:56:56.041
  Jul 15 12:56:56.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-3760 get pod e2e-test-httpd-pod -o json'
  Jul 15 12:56:56.081: INFO: stderr: ""
  Jul 15 12:56:56.082: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-07-15T12:56:50Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3760\",\n        \"resourceVersion\": \"29602\",\n        \"uid\": \"88aa843e-59dd-4a1e-a257-c5041803d11d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-n2b5z\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-16-190\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-n2b5z\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-15T12:56:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-15T12:56:52Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-15T12:56:52Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-15T12:56:50Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://6b1c4c4dbfaec3f297c24c6d7a686cd708fd5c7291f81c92d411de1e8b2c9440\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-07-15T12:56:51Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.16.190\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.33.98\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.33.98\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-07-15T12:56:50Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 07/15/23 12:56:56.082
  Jul 15 12:56:56.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-3760 replace -f -'
  E0715 12:56:56.255687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:56:56.265: INFO: stderr: ""
  Jul 15 12:56:56.265: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 07/15/23 12:56:56.266
  Jul 15 12:56:56.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-3760 delete pods e2e-test-httpd-pod'
  E0715 12:56:57.255884      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:56:58.100: INFO: stderr: ""
  Jul 15 12:56:58.100: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul 15 12:56:58.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3760" for this suite. @ 07/15/23 12:56:58.104
• [7.188 seconds]
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 07/15/23 12:56:58.111
  Jul 15 12:56:58.111: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename taint-single-pod @ 07/15/23 12:56:58.111
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:56:58.122
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:56:58.125
  Jul 15 12:56:58.128: INFO: Waiting up to 1m0s for all nodes to be ready
  E0715 12:56:58.256768      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:56:59.257035      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:00.257660      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:01.258175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:02.258575      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:03.258777      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:04.259641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:05.260171      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:06.260603      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:07.260801      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:08.261639      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:09.262464      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:10.262620      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:11.263300      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:12.263619      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:13.263976      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:14.263953      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:15.264713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:16.265542      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:17.265976      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:18.266116      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:19.266406      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:20.266459      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:21.266753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:22.267392      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:23.267608      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:24.268226      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:25.268509      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:26.268845      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:27.269161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:28.270107      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:29.270299      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:30.271330      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:31.271664      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:32.271912      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:33.272134      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:34.272307      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:35.272908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:36.273331      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:37.274168      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:38.274428      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:39.274704      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:40.275340      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:41.275622      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:42.276611      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:43.276854      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:44.277749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:45.278127      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:46.278727      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:47.278916      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:48.279224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:49.280149      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:50.281197      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:51.281888      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:52.282211      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:53.282448      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:54.283059      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:55.283730      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:56.284692      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:57:57.284847      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:57:58.139: INFO: Waiting for terminating namespaces to be deleted...
  Jul 15 12:57:58.144: INFO: Starting informer...
  STEP: Starting pod... @ 07/15/23 12:57:58.144
  E0715 12:57:58.285135      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:57:58.359: INFO: Pod is running on ip-172-31-16-190. Tainting Node
  STEP: Trying to apply a taint on the Node @ 07/15/23 12:57:58.359
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/15/23 12:57:58.369
  STEP: Waiting short time to make sure Pod is queued for deletion @ 07/15/23 12:57:58.372
  Jul 15 12:57:58.372: INFO: Pod wasn't evicted. Proceeding
  Jul 15 12:57:58.372: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/15/23 12:57:58.379
  STEP: Waiting some time to make sure that toleration time passed. @ 07/15/23 12:57:58.387
  E0715 12:57:59.285692      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:00.286019      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:01.286302      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:02.286532      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:03.286700      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:04.286918      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:05.287783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:06.288004      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:07.288267      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:08.288473      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:09.288700      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:10.288884      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:11.289132      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:12.290218      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:13.290362      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:14.290656      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:15.291339      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:16.292345      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:17.292566      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:18.292780      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:19.293045      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:20.294081      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:21.294052      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:22.294165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:23.294436      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:24.294690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:25.295274      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:26.295612      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:27.295940      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:28.296168      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:29.296709      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:30.297067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:31.297136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:32.298156      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:33.298381      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:34.298644      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:35.299308      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:36.299578      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:37.299829      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:38.300278      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:39.300642      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:40.300701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:41.301211      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:42.301489      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:43.302219      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:44.302807      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:45.303454      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:46.303761      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:47.304068      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:48.304352      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:49.304548      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:50.305832      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:51.306208      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:52.306479      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:53.306770      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:54.307050      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:55.307721      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:56.307948      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:57.308636      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:58.308775      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:58:59.309159      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:00.309539      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:01.310163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:02.310275      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:03.310401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:04.310752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:05.311429      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:06.312300      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:07.312359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:08.312587      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:09.312721      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:10.313132      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:11.314173      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:12.314403      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:13.314646      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 12:59:13.388: INFO: Pod wasn't evicted. Test successful
  Jul 15 12:59:13.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-774" for this suite. @ 07/15/23 12:59:13.395
• [135.292 seconds]
------------------------------
S
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 07/15/23 12:59:13.403
  Jul 15 12:59:13.403: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename csistoragecapacity @ 07/15/23 12:59:13.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:59:13.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:59:13.416
  STEP: getting /apis @ 07/15/23 12:59:13.419
  STEP: getting /apis/storage.k8s.io @ 07/15/23 12:59:13.422
  STEP: getting /apis/storage.k8s.io/v1 @ 07/15/23 12:59:13.423
  STEP: creating @ 07/15/23 12:59:13.425
  STEP: watching @ 07/15/23 12:59:13.439
  Jul 15 12:59:13.439: INFO: starting watch
  STEP: getting @ 07/15/23 12:59:13.446
  STEP: listing in namespace @ 07/15/23 12:59:13.45
  STEP: listing across namespaces @ 07/15/23 12:59:13.452
  STEP: patching @ 07/15/23 12:59:13.455
  STEP: updating @ 07/15/23 12:59:13.46
  Jul 15 12:59:13.464: INFO: waiting for watch events with expected annotations in namespace
  Jul 15 12:59:13.464: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 07/15/23 12:59:13.464
  STEP: deleting a collection @ 07/15/23 12:59:13.476
  Jul 15 12:59:13.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-8421" for this suite. @ 07/15/23 12:59:13.495
• [0.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 07/15/23 12:59:13.503
  Jul 15 12:59:13.503: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename taint-multiple-pods @ 07/15/23 12:59:13.503
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 12:59:13.512
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 12:59:13.515
  Jul 15 12:59:13.517: INFO: Waiting up to 1m0s for all nodes to be ready
  E0715 12:59:14.315326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:15.316364      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:16.316811      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:17.317105      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:18.317153      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:19.317439      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:20.318158      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:21.318421      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:22.318734      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:23.318867      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:24.319345      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:25.319898      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:26.320427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:27.320602      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:28.321177      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:29.321470      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:30.322184      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:31.322397      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:32.323162      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:33.323447      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:34.323639      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:35.324256      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:36.324805      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:37.325613      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:38.326311      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:39.326516      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:40.327020      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:41.327130      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:42.327818      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:43.327990      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:44.328599      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:45.329181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:46.330269      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:47.330576      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:48.330702      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:49.330861      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:50.331736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:51.332036      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:52.332985      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:53.333078      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:54.334109      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:55.334774      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:56.335694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:57.335877      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:58.337040      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 12:59:59.337276      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:00.337401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:01.338204      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:02.339115      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:03.340002      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:04.340585      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:05.341036      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:06.341147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:07.342263      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:08.342583      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:09.342634      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:10.342769      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:11.343484      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:12.343650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:13.343904      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:00:13.530: INFO: Waiting for terminating namespaces to be deleted...
  Jul 15 13:00:13.534: INFO: Starting informer...
  STEP: Starting pods... @ 07/15/23 13:00:13.534
  Jul 15 13:00:13.752: INFO: Pod1 is running on ip-172-31-16-190. Tainting Node
  E0715 13:00:14.344427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:15.345002      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:00:15.972: INFO: Pod2 is running on ip-172-31-16-190. Tainting Node
  STEP: Trying to apply a taint on the Node @ 07/15/23 13:00:15.972
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/15/23 13:00:15.98
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 07/15/23 13:00:15.984
  E0715 13:00:16.345100      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:17.346159      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:18.346303      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:19.346587      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:20.346989      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:21.347709      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:00:21.706: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0715 13:00:22.347846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:23.348033      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:24.348247      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:25.348812      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:26.348913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:27.349154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:28.350167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:29.350429      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:30.351399      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:31.351589      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:32.352575      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:33.352823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:34.353047      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:35.353696      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:36.354195      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:37.354388      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:38.354555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:39.354845      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:40.355181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:41.355958      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:00:41.734: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Jul 15 13:00:41.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/15/23 13:00:41.746
  STEP: Destroying namespace "taint-multiple-pods-4365" for this suite. @ 07/15/23 13:00:41.75
• [88.254 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 07/15/23 13:00:41.757
  Jul 15 13:00:41.757: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename statefulset @ 07/15/23 13:00:41.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:00:41.774
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:00:41.776
  STEP: Creating service test in namespace statefulset-6380 @ 07/15/23 13:00:41.779
  Jul 15 13:00:41.794: INFO: Found 0 stateful pods, waiting for 1
  E0715 13:00:42.356068      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:43.356478      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:44.356631      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:45.356770      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:46.357252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:47.358129      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:48.358297      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:49.358569      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:50.359074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:51.359204      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:00:51.800: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 07/15/23 13:00:51.806
  W0715 13:00:51.815146      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul 15 13:00:51.822: INFO: Found 1 stateful pods, waiting for 2
  E0715 13:00:52.360036      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:53.360107      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:54.360285      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:55.360800      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:56.361024      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:57.361078      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:58.361215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:00:59.361315      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:00.361718      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:01.361883      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:01:01.826: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 15 13:01:01.826: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 07/15/23 13:01:01.833
  STEP: Delete all of the StatefulSets @ 07/15/23 13:01:01.837
  STEP: Verify that StatefulSets have been deleted @ 07/15/23 13:01:01.844
  Jul 15 13:01:01.847: INFO: Deleting all statefulset in ns statefulset-6380
  Jul 15 13:01:01.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6380" for this suite. @ 07/15/23 13:01:01.864
• [20.114 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 07/15/23 13:01:01.871
  Jul 15 13:01:01.871: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename configmap @ 07/15/23 13:01:01.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:01:01.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:01:01.887
  STEP: Creating configMap with name configmap-test-volume-11901357-9fc2-42e7-94c2-e58e662b728b @ 07/15/23 13:01:01.89
  STEP: Creating a pod to test consume configMaps @ 07/15/23 13:01:01.893
  E0715 13:01:02.362631      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:03.363659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:04.364126      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:05.364747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:01:05.914
  Jul 15 13:01:05.916: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-configmaps-f47a2f3d-ea2a-42c7-9915-2ccdd37cb1e3 container agnhost-container: <nil>
  STEP: delete the pod @ 07/15/23 13:01:05.933
  Jul 15 13:01:05.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1572" for this suite. @ 07/15/23 13:01:05.953
• [4.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 07/15/23 13:01:05.959
  Jul 15 13:01:05.959: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-probe @ 07/15/23 13:01:05.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:01:05.972
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:01:05.974
  STEP: Creating pod liveness-8cc54574-eff2-4ad0-a8a5-dece0dd47928 in namespace container-probe-7906 @ 07/15/23 13:01:05.977
  E0715 13:01:06.364800      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:07.365066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:01:07.993: INFO: Started pod liveness-8cc54574-eff2-4ad0-a8a5-dece0dd47928 in namespace container-probe-7906
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/15/23 13:01:07.993
  Jul 15 13:01:07.997: INFO: Initial restart count of pod liveness-8cc54574-eff2-4ad0-a8a5-dece0dd47928 is 0
  E0715 13:01:08.366036      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:09.366267      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:10.366929      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:11.367207      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:12.367506      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:13.367828      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:14.368785      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:15.369487      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:16.370171      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:17.370425      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:18.370556      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:19.370785      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:20.371686      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:21.371837      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:22.372533      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:23.373409      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:24.374124      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:25.374779      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:26.375226      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:27.375561      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:01:28.047: INFO: Restart count of pod container-probe-7906/liveness-8cc54574-eff2-4ad0-a8a5-dece0dd47928 is now 1 (20.050822302s elapsed)
  E0715 13:01:28.376255      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:29.377286      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:30.378020      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:31.378173      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:32.378500      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:33.378684      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:34.378952      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:35.379766      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:36.379787      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:37.380049      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:38.380494      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:39.380760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:40.381667      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:41.382207      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:42.383225      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:43.383471      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:44.383589      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:45.384026      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:46.385084      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:47.385370      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:01:48.096: INFO: Restart count of pod container-probe-7906/liveness-8cc54574-eff2-4ad0-a8a5-dece0dd47928 is now 2 (40.099673351s elapsed)
  E0715 13:01:48.386167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:49.386432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:50.387004      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:51.387140      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:52.387282      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:53.387518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:54.387666      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:55.388133      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:56.388198      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:57.388447      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:58.388982      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:01:59.389072      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:00.389831      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:01.390214      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:02.390330      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:03.391099      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:04.391302      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:05.391674      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:06.391867      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:07.392124      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:02:08.143: INFO: Restart count of pod container-probe-7906/liveness-8cc54574-eff2-4ad0-a8a5-dece0dd47928 is now 3 (1m0.146780562s elapsed)
  E0715 13:02:08.392200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:09.392467      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:10.393488      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:11.394204      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:12.394786      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:13.394974      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:14.395035      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:15.395498      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:16.396093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:17.396357      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:18.397389      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:19.397579      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:20.397833      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:21.398195      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:22.399164      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:23.399421      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:24.400368      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:25.401092      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:26.401685      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:27.401849      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:02:28.192: INFO: Restart count of pod container-probe-7906/liveness-8cc54574-eff2-4ad0-a8a5-dece0dd47928 is now 4 (1m20.194978596s elapsed)
  E0715 13:02:28.402173      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:29.402324      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:30.403069      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:31.403339      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:32.404144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:33.404317      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:34.405373      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:35.405969      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:36.405990      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:37.406234      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:38.406685      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:39.406943      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:40.407463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:41.407673      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:42.407749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:43.408045      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:44.408656      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:45.409263      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:46.409963      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:47.410164      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:48.410528      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:49.410796      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:50.411471      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:51.412219      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:52.412821      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:53.413865      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:54.414151      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:55.414313      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:56.414627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:57.414896      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:58.415943      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:02:59.416107      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:00.416195      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:01.416384      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:02.417161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:03.417270      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:04.418273      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:05.418749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:06.418864      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:07.419110      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:08.419129      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:09.419330      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:10.420093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:11.420530      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:12.421075      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:13.421255      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:14.422021      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:15.422513      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:16.423405      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:17.423703      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:18.423924      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:19.424215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:20.425005      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:21.425079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:22.425492      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:23.426154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:24.426187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:25.426761      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:26.427099      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:27.427342      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:28.427437      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:29.427722      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:30.428774      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:31.429127      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:32.429942      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:33.430134      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:34.430374      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:35.430730      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:03:36.356: INFO: Restart count of pod container-probe-7906/liveness-8cc54574-eff2-4ad0-a8a5-dece0dd47928 is now 5 (2m28.358945772s elapsed)
  Jul 15 13:03:36.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/15/23 13:03:36.359
  STEP: Destroying namespace "container-probe-7906" for this suite. @ 07/15/23 13:03:36.37
• [150.418 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 07/15/23 13:03:36.379
  Jul 15 13:03:36.379: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename ingress @ 07/15/23 13:03:36.379
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:03:36.389
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:03:36.391
  STEP: getting /apis @ 07/15/23 13:03:36.394
  STEP: getting /apis/networking.k8s.io @ 07/15/23 13:03:36.398
  STEP: getting /apis/networking.k8s.iov1 @ 07/15/23 13:03:36.399
  STEP: creating @ 07/15/23 13:03:36.4
  STEP: getting @ 07/15/23 13:03:36.415
  STEP: listing @ 07/15/23 13:03:36.419
  STEP: watching @ 07/15/23 13:03:36.422
  Jul 15 13:03:36.422: INFO: starting watch
  STEP: cluster-wide listing @ 07/15/23 13:03:36.423
  E0715 13:03:36.431231      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: cluster-wide watching @ 07/15/23 13:03:36.431
  Jul 15 13:03:36.431: INFO: starting watch
  STEP: patching @ 07/15/23 13:03:36.432
  STEP: updating @ 07/15/23 13:03:36.436
  Jul 15 13:03:36.545: INFO: waiting for watch events with expected annotations
  Jul 15 13:03:36.545: INFO: saw patched and updated annotations
  STEP: patching /status @ 07/15/23 13:03:36.545
  STEP: updating /status @ 07/15/23 13:03:36.551
  STEP: get /status @ 07/15/23 13:03:36.559
  STEP: deleting @ 07/15/23 13:03:36.562
  STEP: deleting a collection @ 07/15/23 13:03:36.579
  Jul 15 13:03:36.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-7720" for this suite. @ 07/15/23 13:03:36.597
• [0.225 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 07/15/23 13:03:36.604
  Jul 15 13:03:36.604: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename services @ 07/15/23 13:03:36.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:03:36.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:03:36.617
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-8476 @ 07/15/23 13:03:36.62
  STEP: changing the ExternalName service to type=NodePort @ 07/15/23 13:03:36.624
  STEP: creating replication controller externalname-service in namespace services-8476 @ 07/15/23 13:03:36.642
  I0715 13:03:36.648048      23 runners.go:194] Created replication controller with name: externalname-service, namespace: services-8476, replica count: 2
  E0715 13:03:37.431702      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:38.432307      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:39.433042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0715 13:03:39.699519      23 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 15 13:03:39.699: INFO: Creating new exec pod
  E0715 13:03:40.433613      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:41.434665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:42.435059      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:03:42.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-8476 exec execpodwsqb7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul 15 13:03:42.824: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul 15 13:03:42.824: INFO: stdout: "externalname-service-7fz79"
  Jul 15 13:03:42.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-8476 exec execpodwsqb7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.118 80'
  Jul 15 13:03:42.920: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.118 80\nConnection to 10.152.183.118 80 port [tcp/http] succeeded!\n"
  Jul 15 13:03:42.920: INFO: stdout: "externalname-service-7fz79"
  Jul 15 13:03:42.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-8476 exec execpodwsqb7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.42.138 32596'
  Jul 15 13:03:43.019: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.42.138 32596\nConnection to 172.31.42.138 32596 port [tcp/*] succeeded!\n"
  Jul 15 13:03:43.019: INFO: stdout: "externalname-service-7fz79"
  Jul 15 13:03:43.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-8476 exec execpodwsqb7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.190 32596'
  Jul 15 13:03:43.117: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.16.190 32596\nConnection to 172.31.16.190 32596 port [tcp/*] succeeded!\n"
  Jul 15 13:03:43.117: INFO: stdout: ""
  E0715 13:03:43.435789      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:03:44.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-8476 exec execpodwsqb7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.16.190 32596'
  Jul 15 13:03:44.226: INFO: stderr: "+ nc -v -t -w 2 172.31.16.190 32596\nConnection to 172.31.16.190 32596 port [tcp/*] succeeded!\n+ echo hostName\n"
  Jul 15 13:03:44.226: INFO: stdout: "externalname-service-sz5vf"
  Jul 15 13:03:44.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 15 13:03:44.230: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-8476" for this suite. @ 07/15/23 13:03:44.25
• [7.653 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 07/15/23 13:03:44.258
  Jul 15 13:03:44.258: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename job @ 07/15/23 13:03:44.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:03:44.269
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:03:44.271
  STEP: Creating a job @ 07/15/23 13:03:44.275
  STEP: Ensuring job reaches completions @ 07/15/23 13:03:44.281
  E0715 13:03:44.436432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:45.437051      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:46.438045      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:47.438285      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:48.438642      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:49.438991      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:50.439838      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:51.439939      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:52.440606      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:53.440656      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:03:54.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2771" for this suite. @ 07/15/23 13:03:54.289
• [10.038 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 07/15/23 13:03:54.296
  Jul 15 13:03:54.296: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename disruption @ 07/15/23 13:03:54.297
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:03:54.308
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:03:54.311
  STEP: Creating a kubernetes client @ 07/15/23 13:03:54.315
  Jul 15 13:03:54.315: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename disruption-2 @ 07/15/23 13:03:54.315
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:03:54.326
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:03:54.329
  STEP: Waiting for the pdb to be processed @ 07/15/23 13:03:54.335
  E0715 13:03:54.441147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:55.441647      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 07/15/23 13:03:56.347
  E0715 13:03:56.442004      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:57.442327      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 07/15/23 13:03:58.361
  STEP: listing a collection of PDBs across all namespaces @ 07/15/23 13:03:58.364
  STEP: listing a collection of PDBs in namespace disruption-9962 @ 07/15/23 13:03:58.368
  STEP: deleting a collection of PDBs @ 07/15/23 13:03:58.371
  STEP: Waiting for the PDB collection to be deleted @ 07/15/23 13:03:58.384
  Jul 15 13:03:58.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 15 13:03:58.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-9356" for this suite. @ 07/15/23 13:03:58.394
  STEP: Destroying namespace "disruption-9962" for this suite. @ 07/15/23 13:03:58.4
• [4.109 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 07/15/23 13:03:58.405
  Jul 15 13:03:58.406: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename resourcequota @ 07/15/23 13:03:58.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:03:58.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:03:58.42
  STEP: Creating a ResourceQuota with best effort scope @ 07/15/23 13:03:58.426
  STEP: Ensuring ResourceQuota status is calculated @ 07/15/23 13:03:58.432
  E0715 13:03:58.442758      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:03:59.443793      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 07/15/23 13:04:00.436
  STEP: Ensuring ResourceQuota status is calculated @ 07/15/23 13:04:00.441
  E0715 13:04:00.443990      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:01.444190      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:02.444482      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 07/15/23 13:04:02.446
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 07/15/23 13:04:02.459
  E0715 13:04:03.445061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:04.445288      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 07/15/23 13:04:04.464
  E0715 13:04:05.445677      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:06.446029      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 07/15/23 13:04:06.468
  STEP: Ensuring resource quota status released the pod usage @ 07/15/23 13:04:06.482
  E0715 13:04:07.446113      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:08.446598      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 07/15/23 13:04:08.486
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 07/15/23 13:04:08.493
  E0715 13:04:09.446850      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:10.447222      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 07/15/23 13:04:10.499
  E0715 13:04:11.447274      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:12.447576      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 07/15/23 13:04:12.503
  STEP: Ensuring resource quota status released the pod usage @ 07/15/23 13:04:12.516
  E0715 13:04:13.448258      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:14.448365      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:04:14.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6342" for this suite. @ 07/15/23 13:04:14.525
• [16.127 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 07/15/23 13:04:14.533
  Jul 15 13:04:14.533: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename replicaset @ 07/15/23 13:04:14.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:04:14.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:04:14.545
  Jul 15 13:04:14.558: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0715 13:04:15.448985      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:16.449171      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:17.449352      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:18.450125      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:19.450313      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:04:19.562: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/15/23 13:04:19.562
  STEP: Scaling up "test-rs" replicaset  @ 07/15/23 13:04:19.562
  Jul 15 13:04:19.572: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 07/15/23 13:04:19.572
  W0715 13:04:19.577629      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul 15 13:04:19.578: INFO: observed ReplicaSet test-rs in namespace replicaset-1584 with ReadyReplicas 1, AvailableReplicas 1
  Jul 15 13:04:19.589: INFO: observed ReplicaSet test-rs in namespace replicaset-1584 with ReadyReplicas 1, AvailableReplicas 1
  Jul 15 13:04:19.601: INFO: observed ReplicaSet test-rs in namespace replicaset-1584 with ReadyReplicas 1, AvailableReplicas 1
  Jul 15 13:04:19.606: INFO: observed ReplicaSet test-rs in namespace replicaset-1584 with ReadyReplicas 1, AvailableReplicas 1
  E0715 13:04:20.451341      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:04:20.492: INFO: observed ReplicaSet test-rs in namespace replicaset-1584 with ReadyReplicas 2, AvailableReplicas 2
  Jul 15 13:04:21.148: INFO: observed Replicaset test-rs in namespace replicaset-1584 with ReadyReplicas 3 found true
  Jul 15 13:04:21.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1584" for this suite. @ 07/15/23 13:04:21.153
• [6.627 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 07/15/23 13:04:21.161
  Jul 15 13:04:21.161: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename secrets @ 07/15/23 13:04:21.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:04:21.17
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:04:21.172
  STEP: Creating secret with name secret-test-map-8dca0da8-837c-4299-808f-f655bcd17acd @ 07/15/23 13:04:21.175
  STEP: Creating a pod to test consume secrets @ 07/15/23 13:04:21.178
  E0715 13:04:21.451497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:22.451857      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:23.452088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:24.452627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:04:25.2
  Jul 15 13:04:25.203: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-secrets-3b93951d-47bf-4124-8708-92ffbfa182ec container secret-volume-test: <nil>
  STEP: delete the pod @ 07/15/23 13:04:25.217
  Jul 15 13:04:25.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3165" for this suite. @ 07/15/23 13:04:25.234
• [4.079 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 07/15/23 13:04:25.24
  Jul 15 13:04:25.240: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename configmap @ 07/15/23 13:04:25.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:04:25.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:04:25.256
  STEP: Creating configMap with name configmap-test-volume-map-884db5ac-11e9-4c69-bbb4-6b7556a4c7bd @ 07/15/23 13:04:25.259
  STEP: Creating a pod to test consume configMaps @ 07/15/23 13:04:25.262
  E0715 13:04:25.453590      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:26.454176      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:27.454320      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:28.454627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:04:29.283
  Jul 15 13:04:29.285: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-configmaps-30a5faca-fbe3-417d-ae07-709cf0a93e93 container agnhost-container: <nil>
  STEP: delete the pod @ 07/15/23 13:04:29.294
  Jul 15 13:04:29.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4683" for this suite. @ 07/15/23 13:04:29.313
• [4.082 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 07/15/23 13:04:29.322
  Jul 15 13:04:29.322: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename sched-pred @ 07/15/23 13:04:29.323
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:04:29.334
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:04:29.336
  Jul 15 13:04:29.339: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 15 13:04:29.346: INFO: Waiting for terminating namespaces to be deleted...
  Jul 15 13:04:29.350: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-16-190 before test
  Jul 15 13:04:29.356: INFO: nginx-ingress-controller-kubernetes-worker-rg5kx from ingress-nginx-kubernetes-worker started at 2023-07-15 13:00:41 +0000 UTC (1 container statuses recorded)
  Jul 15 13:04:29.356: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 15 13:04:29.356: INFO: sonobuoy-systemd-logs-daemon-set-ff6bac3670ee4930-cffz4 from sonobuoy started at 2023-07-15 11:58:26 +0000 UTC (2 container statuses recorded)
  Jul 15 13:04:29.356: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 15 13:04:29.356: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 15 13:04:29.356: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-42-138 before test
  Jul 15 13:04:29.360: INFO: nginx-ingress-controller-kubernetes-worker-7nm9g from ingress-nginx-kubernetes-worker started at 2023-07-15 11:50:47 +0000 UTC (1 container statuses recorded)
  Jul 15 13:04:29.360: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 15 13:04:29.360: INFO: calico-kube-controllers-7d466d5f7-7h6g9 from kube-system started at 2023-07-15 13:00:16 +0000 UTC (1 container statuses recorded)
  Jul 15 13:04:29.360: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul 15 13:04:29.360: INFO: sonobuoy from sonobuoy started at 2023-07-15 11:58:24 +0000 UTC (1 container statuses recorded)
  Jul 15 13:04:29.360: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 15 13:04:29.360: INFO: sonobuoy-e2e-job-7bfe7aae7c0f4f05 from sonobuoy started at 2023-07-15 11:58:25 +0000 UTC (2 container statuses recorded)
  Jul 15 13:04:29.360: INFO: 	Container e2e ready: true, restart count 0
  Jul 15 13:04:29.360: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 15 13:04:29.360: INFO: sonobuoy-systemd-logs-daemon-set-ff6bac3670ee4930-rmnwj from sonobuoy started at 2023-07-15 11:58:26 +0000 UTC (2 container statuses recorded)
  Jul 15 13:04:29.360: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 15 13:04:29.360: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 15 13:04:29.360: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-84-236 before test
  Jul 15 13:04:29.364: INFO: default-http-backend-kubernetes-worker-65fc475d49-z5npc from ingress-nginx-kubernetes-worker started at 2023-07-15 11:48:26 +0000 UTC (1 container statuses recorded)
  Jul 15 13:04:29.364: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul 15 13:04:29.364: INFO: nginx-ingress-controller-kubernetes-worker-r7qgp from ingress-nginx-kubernetes-worker started at 2023-07-15 11:48:26 +0000 UTC (1 container statuses recorded)
  Jul 15 13:04:29.364: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 15 13:04:29.364: INFO: coredns-5c7f76ccb8-cn56l from kube-system started at 2023-07-15 11:48:22 +0000 UTC (1 container statuses recorded)
  Jul 15 13:04:29.364: INFO: 	Container coredns ready: true, restart count 0
  Jul 15 13:04:29.364: INFO: kube-state-metrics-5b95b4459c-dcf9f from kube-system started at 2023-07-15 11:48:22 +0000 UTC (1 container statuses recorded)
  Jul 15 13:04:29.364: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul 15 13:04:29.364: INFO: metrics-server-v0.5.2-6cf8c8b69c-kdwqb from kube-system started at 2023-07-15 11:48:22 +0000 UTC (2 container statuses recorded)
  Jul 15 13:04:29.364: INFO: 	Container metrics-server ready: true, restart count 0
  Jul 15 13:04:29.364: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul 15 13:04:29.364: INFO: dashboard-metrics-scraper-6b8586b5c9-h8plq from kubernetes-dashboard started at 2023-07-15 11:48:22 +0000 UTC (1 container statuses recorded)
  Jul 15 13:04:29.364: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul 15 13:04:29.364: INFO: kubernetes-dashboard-6869f4cd5f-2knpl from kubernetes-dashboard started at 2023-07-15 11:48:22 +0000 UTC (1 container statuses recorded)
  Jul 15 13:04:29.364: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul 15 13:04:29.364: INFO: sonobuoy-systemd-logs-daemon-set-ff6bac3670ee4930-k229l from sonobuoy started at 2023-07-15 11:58:26 +0000 UTC (2 container statuses recorded)
  Jul 15 13:04:29.364: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 15 13:04:29.364: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 07/15/23 13:04:29.364
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.17720c3db4c41450], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] @ 07/15/23 13:04:29.387
  E0715 13:04:29.455013      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:04:30.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-2725" for this suite. @ 07/15/23 13:04:30.391
• [1.075 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 07/15/23 13:04:30.397
  Jul 15 13:04:30.397: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename dns @ 07/15/23 13:04:30.398
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:04:30.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:04:30.41
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 07/15/23 13:04:30.413
  Jul 15 13:04:30.421: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3471  a912a56d-5cc4-436e-8b73-ea9e821945b6 31550 0 2023-07-15 13:04:30 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-07-15 13:04:30 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmlfm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmlfm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0715 13:04:30.455339      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:31.456424      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 07/15/23 13:04:32.43
  Jul 15 13:04:32.430: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3471 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:04:32.430: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:04:32.431: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:04:32.431: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-3471/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0715 13:04:32.456914      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS server is configured on pod... @ 07/15/23 13:04:32.492
  Jul 15 13:04:32.492: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3471 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:04:32.492: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:04:32.493: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:04:32.493: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-3471/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 15 13:04:32.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 15 13:04:32.568: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-3471" for this suite. @ 07/15/23 13:04:32.578
• [2.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 07/15/23 13:04:32.586
  Jul 15 13:04:32.586: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename replication-controller @ 07/15/23 13:04:32.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:04:32.596
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:04:32.599
  STEP: Creating ReplicationController "e2e-rc-8zfpr" @ 07/15/23 13:04:32.602
  Jul 15 13:04:32.606: INFO: Get Replication Controller "e2e-rc-8zfpr" to confirm replicas
  E0715 13:04:33.457988      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:04:33.609: INFO: Get Replication Controller "e2e-rc-8zfpr" to confirm replicas
  Jul 15 13:04:33.612: INFO: Found 1 replicas for "e2e-rc-8zfpr" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-8zfpr" @ 07/15/23 13:04:33.612
  STEP: Updating a scale subresource @ 07/15/23 13:04:33.616
  STEP: Verifying replicas where modified for replication controller "e2e-rc-8zfpr" @ 07/15/23 13:04:33.62
  Jul 15 13:04:33.620: INFO: Get Replication Controller "e2e-rc-8zfpr" to confirm replicas
  E0715 13:04:34.458540      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:04:34.624: INFO: Get Replication Controller "e2e-rc-8zfpr" to confirm replicas
  Jul 15 13:04:34.629: INFO: Found 2 replicas for "e2e-rc-8zfpr" replication controller
  Jul 15 13:04:34.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3745" for this suite. @ 07/15/23 13:04:34.635
• [2.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 07/15/23 13:04:34.643
  Jul 15 13:04:34.643: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 13:04:34.644
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:04:34.653
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:04:34.656
  STEP: Creating configMap with name configmap-projected-all-test-volume-8d5231d7-8e2f-4c91-9336-119bf193f240 @ 07/15/23 13:04:34.658
  STEP: Creating secret with name secret-projected-all-test-volume-711921d5-d4b2-4d5f-a6b3-43e88990122e @ 07/15/23 13:04:34.662
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 07/15/23 13:04:34.665
  E0715 13:04:35.458826      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:36.459078      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:37.460092      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:38.460285      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:04:38.687
  Jul 15 13:04:38.690: INFO: Trying to get logs from node ip-172-31-16-190 pod projected-volume-0ae487b8-442b-4d25-9976-9b1c56fcbcaa container projected-all-volume-test: <nil>
  STEP: delete the pod @ 07/15/23 13:04:38.697
  Jul 15 13:04:38.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6227" for this suite. @ 07/15/23 13:04:38.716
• [4.080 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 07/15/23 13:04:38.724
  Jul 15 13:04:38.724: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename disruption @ 07/15/23 13:04:38.724
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:04:38.736
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:04:38.739
  STEP: Creating a pdb that targets all three pods in a test replica set @ 07/15/23 13:04:38.741
  STEP: Waiting for the pdb to be processed @ 07/15/23 13:04:38.745
  E0715 13:04:39.460603      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:40.461206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 07/15/23 13:04:40.76
  STEP: Waiting for all pods to be running @ 07/15/23 13:04:40.76
  Jul 15 13:04:40.763: INFO: pods: 0 < 3
  E0715 13:04:41.461274      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:42.462278      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 07/15/23 13:04:42.767
  STEP: Updating the pdb to allow a pod to be evicted @ 07/15/23 13:04:42.777
  STEP: Waiting for the pdb to be processed @ 07/15/23 13:04:42.787
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 07/15/23 13:04:42.79
  STEP: Waiting for all pods to be running @ 07/15/23 13:04:42.79
  STEP: Waiting for the pdb to observed all healthy pods @ 07/15/23 13:04:42.793
  STEP: Patching the pdb to disallow a pod to be evicted @ 07/15/23 13:04:42.815
  STEP: Waiting for the pdb to be processed @ 07/15/23 13:04:42.826
  E0715 13:04:43.463031      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:44.463214      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 07/15/23 13:04:44.834
  STEP: locating a running pod @ 07/15/23 13:04:44.837
  STEP: Deleting the pdb to allow a pod to be evicted @ 07/15/23 13:04:44.846
  STEP: Waiting for the pdb to be deleted @ 07/15/23 13:04:44.854
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 07/15/23 13:04:44.856
  STEP: Waiting for all pods to be running @ 07/15/23 13:04:44.856
  Jul 15 13:04:44.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9009" for this suite. @ 07/15/23 13:04:44.878
• [6.163 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 07/15/23 13:04:44.888
  Jul 15 13:04:44.888: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir-wrapper @ 07/15/23 13:04:44.889
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:04:44.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:04:44.904
  E0715 13:04:45.463926      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:46.464206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:04:46.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 07/15/23 13:04:46.941
  STEP: Cleaning up the configmap @ 07/15/23 13:04:46.947
  STEP: Cleaning up the pod @ 07/15/23 13:04:46.954
  STEP: Destroying namespace "emptydir-wrapper-1662" for this suite. @ 07/15/23 13:04:46.966
• [2.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 07/15/23 13:04:46.974
  Jul 15 13:04:46.974: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename csiinlinevolumes @ 07/15/23 13:04:46.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:04:46.986
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:04:46.988
  STEP: creating @ 07/15/23 13:04:46.991
  STEP: getting @ 07/15/23 13:04:47.006
  STEP: listing @ 07/15/23 13:04:47.012
  STEP: deleting @ 07/15/23 13:04:47.015
  Jul 15 13:04:47.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-2865" for this suite. @ 07/15/23 13:04:47.036
• [0.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 07/15/23 13:04:47.043
  Jul 15 13:04:47.043: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl-logs @ 07/15/23 13:04:47.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:04:47.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:04:47.056
  STEP: creating an pod @ 07/15/23 13:04:47.059
  Jul 15 13:04:47.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-logs-2288 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Jul 15 13:04:47.109: INFO: stderr: ""
  Jul 15 13:04:47.109: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 07/15/23 13:04:47.109
  Jul 15 13:04:47.109: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0715 13:04:47.464545      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:48.464854      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:04:49.116: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 07/15/23 13:04:49.116
  Jul 15 13:04:49.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-logs-2288 logs logs-generator logs-generator'
  Jul 15 13:04:49.167: INFO: stderr: ""
  Jul 15 13:04:49.167: INFO: stdout: "I0715 13:04:47.692602       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/qqdx 244\nI0715 13:04:47.892920       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/lr2 554\nI0715 13:04:48.093216       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/vkl 246\nI0715 13:04:48.293522       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/52mq 234\nI0715 13:04:48.492713       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/klnw 518\nI0715 13:04:48.693063       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/4xx 283\nI0715 13:04:48.893355       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/fsx 538\nI0715 13:04:49.093659       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/blb 583\n"
  STEP: limiting log lines @ 07/15/23 13:04:49.167
  Jul 15 13:04:49.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-logs-2288 logs logs-generator logs-generator --tail=1'
  Jul 15 13:04:49.226: INFO: stderr: ""
  Jul 15 13:04:49.226: INFO: stdout: "I0715 13:04:49.093659       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/blb 583\n"
  Jul 15 13:04:49.226: INFO: got output "I0715 13:04:49.093659       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/blb 583\n"
  STEP: limiting log bytes @ 07/15/23 13:04:49.226
  Jul 15 13:04:49.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-logs-2288 logs logs-generator logs-generator --limit-bytes=1'
  Jul 15 13:04:49.277: INFO: stderr: ""
  Jul 15 13:04:49.277: INFO: stdout: "I"
  Jul 15 13:04:49.277: INFO: got output "I"
  STEP: exposing timestamps @ 07/15/23 13:04:49.277
  Jul 15 13:04:49.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-logs-2288 logs logs-generator logs-generator --tail=1 --timestamps'
  Jul 15 13:04:49.324: INFO: stderr: ""
  Jul 15 13:04:49.324: INFO: stdout: "2023-07-15T13:04:49.293040595Z I0715 13:04:49.292958       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/xrs 211\n"
  Jul 15 13:04:49.324: INFO: got output "2023-07-15T13:04:49.293040595Z I0715 13:04:49.292958       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/xrs 211\n"
  STEP: restricting to a time range @ 07/15/23 13:04:49.324
  E0715 13:04:49.465519      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:50.466153      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:51.466442      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:04:51.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-logs-2288 logs logs-generator logs-generator --since=1s'
  Jul 15 13:04:51.875: INFO: stderr: ""
  Jul 15 13:04:51.875: INFO: stdout: "I0715 13:04:50.892946       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/5sz 547\nI0715 13:04:51.093249       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/fqkf 317\nI0715 13:04:51.293558       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/tgw 255\nI0715 13:04:51.492830       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/6z8 582\nI0715 13:04:51.693123       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/m77 506\n"
  Jul 15 13:04:51.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-logs-2288 logs logs-generator logs-generator --since=24h'
  Jul 15 13:04:51.926: INFO: stderr: ""
  Jul 15 13:04:51.926: INFO: stdout: "I0715 13:04:47.692602       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/qqdx 244\nI0715 13:04:47.892920       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/lr2 554\nI0715 13:04:48.093216       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/vkl 246\nI0715 13:04:48.293522       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/52mq 234\nI0715 13:04:48.492713       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/klnw 518\nI0715 13:04:48.693063       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/4xx 283\nI0715 13:04:48.893355       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/fsx 538\nI0715 13:04:49.093659       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/blb 583\nI0715 13:04:49.292958       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/xrs 211\nI0715 13:04:49.493276       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/wg4 373\nI0715 13:04:49.693583       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/cfw 535\nI0715 13:04:49.892824       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/f6rf 364\nI0715 13:04:50.093023       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/jpd 324\nI0715 13:04:50.293140       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/x5c 538\nI0715 13:04:50.493430       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/qfnk 254\nI0715 13:04:50.692688       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/sxk 524\nI0715 13:04:50.892946       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/5sz 547\nI0715 13:04:51.093249       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/fqkf 317\nI0715 13:04:51.293558       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/tgw 255\nI0715 13:04:51.492830       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/6z8 582\nI0715 13:04:51.693123       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/m77 506\nI0715 13:04:51.893424       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/6wh 595\n"
  Jul 15 13:04:51.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-logs-2288 delete pod logs-generator'
  E0715 13:04:52.466434      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:04:52.901: INFO: stderr: ""
  Jul 15 13:04:52.901: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Jul 15 13:04:52.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-2288" for this suite. @ 07/15/23 13:04:52.905
• [5.869 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 07/15/23 13:04:52.913
  Jul 15 13:04:52.913: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename job @ 07/15/23 13:04:52.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:04:52.925
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:04:52.927
  STEP: Creating Indexed job @ 07/15/23 13:04:52.93
  STEP: Ensuring job reaches completions @ 07/15/23 13:04:52.935
  E0715 13:04:53.466753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:54.467211      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:55.467465      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:56.467811      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:57.467893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:58.468217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:04:59.469188      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:00.470158      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 07/15/23 13:05:00.939
  Jul 15 13:05:00.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9073" for this suite. @ 07/15/23 13:05:00.947
• [8.039 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 07/15/23 13:05:00.953
  Jul 15 13:05:00.953: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename replication-controller @ 07/15/23 13:05:00.953
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:05:00.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:05:00.967
  Jul 15 13:05:00.970: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 07/15/23 13:05:00.977
  STEP: Checking rc "condition-test" has the desired failure condition set @ 07/15/23 13:05:00.982
  E0715 13:05:01.471106      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 07/15/23 13:05:01.989
  Jul 15 13:05:01.997: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 07/15/23 13:05:01.997
  Jul 15 13:05:02.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-352" for this suite. @ 07/15/23 13:05:02.006
• [1.060 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 07/15/23 13:05:02.012
  Jul 15 13:05:02.012: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename secrets @ 07/15/23 13:05:02.013
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:05:02.023
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:05:02.026
  STEP: Creating secret with name secret-test-be2166f0-cf53-4f77-b472-fdc01fb18eca @ 07/15/23 13:05:02.029
  STEP: Creating a pod to test consume secrets @ 07/15/23 13:05:02.033
  E0715 13:05:02.471975      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:03.472467      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:04.473416      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:05.473876      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:05:06.054
  Jul 15 13:05:06.057: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-secrets-a96f4e4b-a050-4ba5-9e13-4d6150e55d60 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/15/23 13:05:06.063
  Jul 15 13:05:06.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5005" for this suite. @ 07/15/23 13:05:06.082
• [4.075 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 07/15/23 13:05:06.088
  Jul 15 13:05:06.088: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename security-context-test @ 07/15/23 13:05:06.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:05:06.1
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:05:06.104
  E0715 13:05:06.474543      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:07.474854      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:08.475355      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:09.475576      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:10.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5280" for this suite. @ 07/15/23 13:05:10.134
• [4.051 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 07/15/23 13:05:10.139
  Jul 15 13:05:10.139: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename secrets @ 07/15/23 13:05:10.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:05:10.151
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:05:10.154
  STEP: Creating secret with name secret-test-7874737a-0700-433d-8624-5b109d7c2c86 @ 07/15/23 13:05:10.158
  STEP: Creating a pod to test consume secrets @ 07/15/23 13:05:10.161
  E0715 13:05:10.476175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:11.476390      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:05:12.176
  Jul 15 13:05:12.179: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-secrets-8ec7ceb7-373c-4054-bf5b-38979b7c7c82 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/15/23 13:05:12.186
  Jul 15 13:05:12.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8792" for this suite. @ 07/15/23 13:05:12.204
• [2.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 07/15/23 13:05:12.21
  Jul 15 13:05:12.210: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename deployment @ 07/15/23 13:05:12.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:05:12.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:05:12.223
  Jul 15 13:05:12.226: INFO: Creating deployment "test-recreate-deployment"
  Jul 15 13:05:12.235: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Jul 15 13:05:12.245: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  E0715 13:05:12.476885      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:13.477035      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:14.252: INFO: Waiting deployment "test-recreate-deployment" to complete
  Jul 15 13:05:14.255: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Jul 15 13:05:14.265: INFO: Updating deployment test-recreate-deployment
  Jul 15 13:05:14.265: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Jul 15 13:05:14.325: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9865  648dbc3b-b00c-4871-b25c-4e99b87ecde2 32381 2 2023-07-15 13:05:12 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-15 13:05:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-15 13:05:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ac4358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-15 13:05:14 +0000 UTC,LastTransitionTime:2023-07-15 13:05:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-07-15 13:05:14 +0000 UTC,LastTransitionTime:2023-07-15 13:05:12 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Jul 15 13:05:14.329: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-9865  67bcb7c6-d3fa-49d1-aee6-7cbb84718db5 32378 1 2023-07-15 13:05:14 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 648dbc3b-b00c-4871-b25c-4e99b87ecde2 0xc004ac4717 0xc004ac4718}] [] [{kube-controller-manager Update apps/v1 2023-07-15 13:05:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"648dbc3b-b00c-4871-b25c-4e99b87ecde2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-15 13:05:14 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ac47b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 15 13:05:14.329: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Jul 15 13:05:14.329: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-9865  78732f62-0901-4da8-8265-2addbe028847 32369 2 2023-07-15 13:05:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 648dbc3b-b00c-4871-b25c-4e99b87ecde2 0xc004ac4827 0xc004ac4828}] [] [{kube-controller-manager Update apps/v1 2023-07-15 13:05:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"648dbc3b-b00c-4871-b25c-4e99b87ecde2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-15 13:05:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ac48d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 15 13:05:14.333: INFO: Pod "test-recreate-deployment-54757ffd6c-sbq9p" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-sbq9p test-recreate-deployment-54757ffd6c- deployment-9865  42fb6509-976c-4ef2-8c45-1248136c17ec 32380 0 2023-07-15 13:05:14 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 67bcb7c6-d3fa-49d1-aee6-7cbb84718db5 0xc00479a947 0xc00479a948}] [] [{kube-controller-manager Update v1 2023-07-15 13:05:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67bcb7c6-d3fa-49d1-aee6-7cbb84718db5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 13:05:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bd88s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bd88s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 13:05:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 13:05:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 13:05:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 13:05:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.190,PodIP:,StartTime:2023-07-15 13:05:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 13:05:14.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9865" for this suite. @ 07/15/23 13:05:14.338
• [2.134 seconds]
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 07/15/23 13:05:14.344
  Jul 15 13:05:14.344: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename services @ 07/15/23 13:05:14.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:05:14.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:05:14.358
  Jul 15 13:05:14.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2016" for this suite. @ 07/15/23 13:05:14.366
• [0.029 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 07/15/23 13:05:14.373
  Jul 15 13:05:14.373: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename watch @ 07/15/23 13:05:14.374
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:05:14.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:05:14.386
  STEP: creating a new configmap @ 07/15/23 13:05:14.388
  STEP: modifying the configmap once @ 07/15/23 13:05:14.392
  STEP: modifying the configmap a second time @ 07/15/23 13:05:14.399
  STEP: deleting the configmap @ 07/15/23 13:05:14.405
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 07/15/23 13:05:14.411
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 07/15/23 13:05:14.412
  Jul 15 13:05:14.412: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4440  38b27174-ee95-44b5-ad09-9a08f057d547 32396 0 2023-07-15 13:05:14 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-15 13:05:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 13:05:14.413: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4440  38b27174-ee95-44b5-ad09-9a08f057d547 32397 0 2023-07-15 13:05:14 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-15 13:05:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 13:05:14.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4440" for this suite. @ 07/15/23 13:05:14.416
• [0.048 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 07/15/23 13:05:14.422
  Jul 15 13:05:14.422: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename aggregator @ 07/15/23 13:05:14.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:05:14.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:05:14.435
  Jul 15 13:05:14.439: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Registering the sample API server. @ 07/15/23 13:05:14.439
  E0715 13:05:14.477235      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:14.629: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Jul 15 13:05:14.655: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  E0715 13:05:15.477991      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:16.478324      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:16.690: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 13:05:17.478676      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:18.479052      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:18.695: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 13:05:19.479489      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:20.479830      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:20.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 13:05:21.480056      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:22.480292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:22.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 13:05:23.480841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:24.481135      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:24.695: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 13:05:25.481866      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:26.482052      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:26.695: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 13:05:27.482176      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:28.482468      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:28.695: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 13:05:29.483417      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:30.483810      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:30.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 13:05:31.483899      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:32.484097      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:32.695: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 13:05:33.484550      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:34.485217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:34.694: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 13:05:35.485979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:36.486185      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:36.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 13:05:37.486433      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:38.487515      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:38.695: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 13:05:39.487488      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:40.487851      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:40.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0715 13:05:41.488837      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:42.489059      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:42.815: INFO: Waited 112.567646ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 07/15/23 13:05:42.844
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 07/15/23 13:05:42.848
  STEP: List APIServices @ 07/15/23 13:05:42.854
  Jul 15 13:05:42.858: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 07/15/23 13:05:42.858
  Jul 15 13:05:42.870: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 07/15/23 13:05:42.87
  Jul 15 13:05:42.879: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.July, 15, 13, 5, 42, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 07/15/23 13:05:42.879
  Jul 15 13:05:42.882: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-07-15 13:05:42 +0000 UTC Passed all checks passed}
  Jul 15 13:05:42.882: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 15 13:05:42.882: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 07/15/23 13:05:42.882
  Jul 15 13:05:42.890: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1176607699" @ 07/15/23 13:05:42.89
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 07/15/23 13:05:42.898
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 07/15/23 13:05:42.904
  STEP: Patch APIService Status @ 07/15/23 13:05:42.907
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 07/15/23 13:05:42.913
  Jul 15 13:05:42.917: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-07-15 13:05:42 +0000 UTC Passed all checks passed}
  Jul 15 13:05:42.917: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 15 13:05:42.917: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Jul 15 13:05:42.917: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 07/15/23 13:05:42.917
  STEP: Confirm that the generated APIService has been deleted @ 07/15/23 13:05:42.921
  Jul 15 13:05:42.921: INFO: Requesting list of APIServices to confirm quantity
  Jul 15 13:05:42.925: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Jul 15 13:05:42.925: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Jul 15 13:05:42.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-7508" for this suite. @ 07/15/23 13:05:43.021
• [28.604 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 07/15/23 13:05:43.026
  Jul 15 13:05:43.026: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir @ 07/15/23 13:05:43.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:05:43.04
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:05:43.042
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 07/15/23 13:05:43.045
  E0715 13:05:43.489803      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:44.490943      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:45.491857      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:46.492119      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:05:47.065
  Jul 15 13:05:47.068: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-0261e428-331e-46a8-8401-8e19156c0545 container test-container: <nil>
  STEP: delete the pod @ 07/15/23 13:05:47.075
  Jul 15 13:05:47.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8836" for this suite. @ 07/15/23 13:05:47.095
• [4.077 seconds]
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 07/15/23 13:05:47.103
  Jul 15 13:05:47.103: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename runtimeclass @ 07/15/23 13:05:47.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:05:47.112
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:05:47.115
  STEP: getting /apis @ 07/15/23 13:05:47.117
  STEP: getting /apis/node.k8s.io @ 07/15/23 13:05:47.122
  STEP: getting /apis/node.k8s.io/v1 @ 07/15/23 13:05:47.124
  STEP: creating @ 07/15/23 13:05:47.125
  STEP: watching @ 07/15/23 13:05:47.14
  Jul 15 13:05:47.140: INFO: starting watch
  STEP: getting @ 07/15/23 13:05:47.145
  STEP: listing @ 07/15/23 13:05:47.148
  STEP: patching @ 07/15/23 13:05:47.152
  STEP: updating @ 07/15/23 13:05:47.156
  Jul 15 13:05:47.160: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 07/15/23 13:05:47.16
  STEP: deleting a collection @ 07/15/23 13:05:47.173
  Jul 15 13:05:47.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2544" for this suite. @ 07/15/23 13:05:47.193
• [0.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 07/15/23 13:05:47.208
  Jul 15 13:05:47.208: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 13:05:47.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:05:47.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:05:47.223
  STEP: Setting up server cert @ 07/15/23 13:05:47.24
  E0715 13:05:47.492408      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 13:05:47.505
  STEP: Deploying the webhook pod @ 07/15/23 13:05:47.511
  STEP: Wait for the deployment to be ready @ 07/15/23 13:05:47.52
  Jul 15 13:05:47.529: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0715 13:05:48.492536      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:49.492865      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/15/23 13:05:49.541
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 13:05:49.551
  E0715 13:05:50.493162      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:50.552: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 15 13:05:50.556: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4978-crds.webhook.example.com via the AdmissionRegistration API @ 07/15/23 13:05:51.067
  STEP: Creating a custom resource that should be mutated by the webhook @ 07/15/23 13:05:51.081
  E0715 13:05:51.493248      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:52.493400      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:53.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0715 13:05:53.493964      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-9116" for this suite. @ 07/15/23 13:05:53.664
  STEP: Destroying namespace "webhook-markers-8183" for this suite. @ 07/15/23 13:05:53.669
• [6.468 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 07/15/23 13:05:53.678
  Jul 15 13:05:53.678: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 13:05:53.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:05:53.691
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:05:53.694
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 13:05:53.697
  E0715 13:05:54.494167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:55.494879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:56.495196      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:05:57.495931      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:05:57.72
  Jul 15 13:05:57.723: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-c3569bba-b229-4fa5-838c-62e62ecc5245 container client-container: <nil>
  STEP: delete the pod @ 07/15/23 13:05:57.73
  Jul 15 13:05:57.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1213" for this suite. @ 07/15/23 13:05:57.752
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 07/15/23 13:05:57.76
  Jul 15 13:05:57.760: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl @ 07/15/23 13:05:57.761
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:05:57.771
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:05:57.773
  STEP: creating Agnhost RC @ 07/15/23 13:05:57.776
  Jul 15 13:05:57.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-830 create -f -'
  Jul 15 13:05:58.201: INFO: stderr: ""
  Jul 15 13:05:58.201: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/15/23 13:05:58.201
  E0715 13:05:58.496623      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:05:59.207: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 15 13:05:59.207: INFO: Found 1 / 1
  Jul 15 13:05:59.207: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 07/15/23 13:05:59.207
  Jul 15 13:05:59.210: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 15 13:05:59.210: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 15 13:05:59.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-830 patch pod agnhost-primary-s78tg -p {"metadata":{"annotations":{"x":"y"}}}'
  Jul 15 13:05:59.261: INFO: stderr: ""
  Jul 15 13:05:59.261: INFO: stdout: "pod/agnhost-primary-s78tg patched\n"
  STEP: checking annotations @ 07/15/23 13:05:59.261
  Jul 15 13:05:59.264: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 15 13:05:59.264: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 15 13:05:59.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-830" for this suite. @ 07/15/23 13:05:59.267
• [1.514 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 07/15/23 13:05:59.275
  Jul 15 13:05:59.275: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/15/23 13:05:59.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:05:59.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:05:59.289
  Jul 15 13:05:59.293: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  E0715 13:05:59.496990      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:06:00.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4672" for this suite. @ 07/15/23 13:06:00.319
• [1.051 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 07/15/23 13:06:00.326
  Jul 15 13:06:00.326: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename pod-network-test @ 07/15/23 13:06:00.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:06:00.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:06:00.341
  STEP: Performing setup for networking test in namespace pod-network-test-8463 @ 07/15/23 13:06:00.344
  STEP: creating a selector @ 07/15/23 13:06:00.344
  STEP: Creating the service pods in kubernetes @ 07/15/23 13:06:00.344
  Jul 15 13:06:00.344: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0715 13:06:00.497365      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:01.497524      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:02.498300      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:03.498475      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:04.498825      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:05.499443      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:06.499701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:07.499990      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:08.500295      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:09.500465      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:10.501457      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:11.502382      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 07/15/23 13:06:12.416
  E0715 13:06:12.503002      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:13.503352      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:06:14.431: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 15 13:06:14.431: INFO: Breadth first check of 192.168.33.75 on host 172.31.16.190...
  Jul 15 13:06:14.435: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.33.84:9080/dial?request=hostname&protocol=http&host=192.168.33.75&port=8083&tries=1'] Namespace:pod-network-test-8463 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:06:14.435: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:06:14.436: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:06:14.436: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8463/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.33.84%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.33.75%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 15 13:06:14.490: INFO: Waiting for responses: map[]
  Jul 15 13:06:14.490: INFO: reached 192.168.33.75 after 0/1 tries
  Jul 15 13:06:14.490: INFO: Breadth first check of 192.168.191.212 on host 172.31.42.138...
  Jul 15 13:06:14.494: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.33.84:9080/dial?request=hostname&protocol=http&host=192.168.191.212&port=8083&tries=1'] Namespace:pod-network-test-8463 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:06:14.494: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:06:14.494: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:06:14.494: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8463/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.33.84%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.191.212%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  E0715 13:06:14.503785      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:06:14.552: INFO: Waiting for responses: map[]
  Jul 15 13:06:14.552: INFO: reached 192.168.191.212 after 0/1 tries
  Jul 15 13:06:14.552: INFO: Breadth first check of 192.168.4.50 on host 172.31.84.236...
  Jul 15 13:06:14.556: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.33.84:9080/dial?request=hostname&protocol=http&host=192.168.4.50&port=8083&tries=1'] Namespace:pod-network-test-8463 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:06:14.556: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:06:14.557: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:06:14.557: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8463/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.33.84%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.4.50%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 15 13:06:14.607: INFO: Waiting for responses: map[]
  Jul 15 13:06:14.607: INFO: reached 192.168.4.50 after 0/1 tries
  Jul 15 13:06:14.607: INFO: Going to retry 0 out of 3 pods....
  Jul 15 13:06:14.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-8463" for this suite. @ 07/15/23 13:06:14.611
• [14.290 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 07/15/23 13:06:14.617
  Jul 15 13:06:14.617: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename dns @ 07/15/23 13:06:14.617
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:06:14.631
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:06:14.634
  STEP: Creating a test externalName service @ 07/15/23 13:06:14.636
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9978.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9978.svc.cluster.local; sleep 1; done
   @ 07/15/23 13:06:14.64
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9978.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9978.svc.cluster.local; sleep 1; done
   @ 07/15/23 13:06:14.641
  STEP: creating a pod to probe DNS @ 07/15/23 13:06:14.641
  STEP: submitting the pod to kubernetes @ 07/15/23 13:06:14.641
  E0715 13:06:15.504653      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:16.504822      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/15/23 13:06:16.656
  STEP: looking for the results for each expected name from probers @ 07/15/23 13:06:16.659
  Jul 15 13:06:16.668: INFO: DNS probes using dns-test-43b5d657-a723-4518-8f4d-2f939fa481f9 succeeded

  STEP: changing the externalName to bar.example.com @ 07/15/23 13:06:16.668
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9978.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9978.svc.cluster.local; sleep 1; done
   @ 07/15/23 13:06:16.675
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9978.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9978.svc.cluster.local; sleep 1; done
   @ 07/15/23 13:06:16.675
  STEP: creating a second pod to probe DNS @ 07/15/23 13:06:16.675
  STEP: submitting the pod to kubernetes @ 07/15/23 13:06:16.675
  E0715 13:06:17.505531      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:18.506220      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/15/23 13:06:18.689
  STEP: looking for the results for each expected name from probers @ 07/15/23 13:06:18.691
  Jul 15 13:06:18.698: INFO: File wheezy_udp@dns-test-service-3.dns-9978.svc.cluster.local from pod  dns-9978/dns-test-01cfb65b-5427-4c51-a3a5-eb42dab81d9a contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jul 15 13:06:18.701: INFO: File jessie_udp@dns-test-service-3.dns-9978.svc.cluster.local from pod  dns-9978/dns-test-01cfb65b-5427-4c51-a3a5-eb42dab81d9a contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jul 15 13:06:18.701: INFO: Lookups using dns-9978/dns-test-01cfb65b-5427-4c51-a3a5-eb42dab81d9a failed for: [wheezy_udp@dns-test-service-3.dns-9978.svc.cluster.local jessie_udp@dns-test-service-3.dns-9978.svc.cluster.local]

  E0715 13:06:19.506814      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:20.507844      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:21.508241      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:22.508353      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:23.508568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:06:23.710: INFO: DNS probes using dns-test-01cfb65b-5427-4c51-a3a5-eb42dab81d9a succeeded

  STEP: changing the service to type=ClusterIP @ 07/15/23 13:06:23.71
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9978.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9978.svc.cluster.local; sleep 1; done
   @ 07/15/23 13:06:23.723
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9978.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9978.svc.cluster.local; sleep 1; done
   @ 07/15/23 13:06:23.723
  STEP: creating a third pod to probe DNS @ 07/15/23 13:06:23.723
  STEP: submitting the pod to kubernetes @ 07/15/23 13:06:23.726
  E0715 13:06:24.509471      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:25.509969      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/15/23 13:06:25.74
  STEP: looking for the results for each expected name from probers @ 07/15/23 13:06:25.743
  Jul 15 13:06:25.752: INFO: DNS probes using dns-test-efd8f427-3f49-4eb4-b935-a6b697be3d5e succeeded

  Jul 15 13:06:25.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/15/23 13:06:25.755
  STEP: deleting the pod @ 07/15/23 13:06:25.765
  STEP: deleting the pod @ 07/15/23 13:06:25.774
  STEP: deleting the test externalName service @ 07/15/23 13:06:25.79
  STEP: Destroying namespace "dns-9978" for this suite. @ 07/15/23 13:06:25.807
• [11.194 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 07/15/23 13:06:25.813
  Jul 15 13:06:25.813: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename watch @ 07/15/23 13:06:25.814
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:06:25.825
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:06:25.828
  STEP: getting a starting resourceVersion @ 07/15/23 13:06:25.83
  STEP: starting a background goroutine to produce watch events @ 07/15/23 13:06:25.834
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 07/15/23 13:06:25.834
  E0715 13:06:26.510206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:27.510650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:28.511769      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:06:28.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2031" for this suite. @ 07/15/23 13:06:28.669
• [2.907 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 07/15/23 13:06:28.72
  Jul 15 13:06:28.720: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename statefulset @ 07/15/23 13:06:28.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:06:28.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:06:28.736
  STEP: Creating service test in namespace statefulset-9566 @ 07/15/23 13:06:28.738
  STEP: Creating a new StatefulSet @ 07/15/23 13:06:28.744
  Jul 15 13:06:28.751: INFO: Found 0 stateful pods, waiting for 3
  E0715 13:06:29.512129      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:30.512366      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:31.512564      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:32.513145      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:33.513293      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:34.514137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:35.514667      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:36.515568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:37.516578      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:38.516709      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:06:38.754: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 15 13:06:38.755: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 15 13:06:38.755: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 07/15/23 13:06:38.766
  Jul 15 13:06:38.785: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 07/15/23 13:06:38.785
  E0715 13:06:39.517162      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:40.517597      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:41.517747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:42.517852      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:43.518615      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:44.519578      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:45.520489      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:46.520752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:47.521207      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:48.521294      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 07/15/23 13:06:48.802
  STEP: Performing a canary update @ 07/15/23 13:06:48.803
  Jul 15 13:06:48.823: INFO: Updating stateful set ss2
  Jul 15 13:06:48.830: INFO: Waiting for Pod statefulset-9566/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0715 13:06:49.521983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:50.522198      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:51.522363      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:52.522635      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:53.522759      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:54.523020      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:55.523146      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:56.523474      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:57.523848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:06:58.523978      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 07/15/23 13:06:58.839
  Jul 15 13:06:58.873: INFO: Found 2 stateful pods, waiting for 3
  E0715 13:06:59.524566      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:00.525311      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:01.525389      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:02.526166      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:03.526332      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:04.526573      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:05.526907      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:06.527204      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:07.527422      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:08.527665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:07:08.877: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 15 13:07:08.877: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 15 13:07:08.877: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 07/15/23 13:07:08.884
  Jul 15 13:07:08.904: INFO: Updating stateful set ss2
  Jul 15 13:07:08.911: INFO: Waiting for Pod statefulset-9566/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0715 13:07:09.528702      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:10.528923      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:11.529079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:12.530154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:13.531246      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:14.532181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:15.532762      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:16.533061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:17.533088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:18.534147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:07:18.939: INFO: Updating stateful set ss2
  Jul 15 13:07:18.947: INFO: Waiting for StatefulSet statefulset-9566/ss2 to complete update
  Jul 15 13:07:18.947: INFO: Waiting for Pod statefulset-9566/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0715 13:07:19.535179      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:20.535685      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:21.535709      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:22.536053      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:23.536485      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:24.536784      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:25.537285      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:26.538183      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:27.538410      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:28.538699      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:07:28.956: INFO: Deleting all statefulset in ns statefulset-9566
  Jul 15 13:07:28.960: INFO: Scaling statefulset ss2 to 0
  E0715 13:07:29.539343      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:30.540288      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:31.540915      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:32.541206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:33.541255      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:34.541725      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:35.542261      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:36.542541      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:37.542821      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:38.543105      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:07:38.978: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 15 13:07:38.981: INFO: Deleting statefulset ss2
  Jul 15 13:07:38.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9566" for this suite. @ 07/15/23 13:07:39
• [70.285 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 07/15/23 13:07:39.006
  Jul 15 13:07:39.006: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename ingressclass @ 07/15/23 13:07:39.006
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:07:39.019
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:07:39.021
  STEP: getting /apis @ 07/15/23 13:07:39.024
  STEP: getting /apis/networking.k8s.io @ 07/15/23 13:07:39.027
  STEP: getting /apis/networking.k8s.iov1 @ 07/15/23 13:07:39.028
  STEP: creating @ 07/15/23 13:07:39.029
  STEP: getting @ 07/15/23 13:07:39.041
  STEP: listing @ 07/15/23 13:07:39.044
  STEP: watching @ 07/15/23 13:07:39.047
  Jul 15 13:07:39.048: INFO: starting watch
  STEP: patching @ 07/15/23 13:07:39.049
  STEP: updating @ 07/15/23 13:07:39.052
  Jul 15 13:07:39.057: INFO: waiting for watch events with expected annotations
  Jul 15 13:07:39.057: INFO: saw patched and updated annotations
  STEP: deleting @ 07/15/23 13:07:39.057
  STEP: deleting a collection @ 07/15/23 13:07:39.07
  Jul 15 13:07:39.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-3785" for this suite. @ 07/15/23 13:07:39.089
• [0.089 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 07/15/23 13:07:39.095
  Jul 15 13:07:39.095: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubelet-test @ 07/15/23 13:07:39.096
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:07:39.107
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:07:39.109
  E0715 13:07:39.543238      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:40.543591      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:41.543739      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:42.543988      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:07:43.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-356" for this suite. @ 07/15/23 13:07:43.135
• [4.047 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 07/15/23 13:07:43.143
  Jul 15 13:07:43.143: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename svcaccounts @ 07/15/23 13:07:43.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:07:43.152
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:07:43.155
  Jul 15 13:07:43.170: INFO: created pod
  E0715 13:07:43.544773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:44.545716      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:45.545967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:46.546271      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:07:47.185
  E0715 13:07:47.546893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:48.547144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:49.547456      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:50.547816      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:51.548076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:52.548380      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:53.548609      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:54.548748      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:55.549162      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:56.550210      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:57.550470      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:58.550736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:07:59.551814      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:00.551866      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:01.552850      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:02.553070      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:03.554136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:04.555014      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:05.555140      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:06.555483      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:07.555706      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:08.555960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:09.556079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:10.556386      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:11.556529      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:12.556793      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:13.557057      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:14.557230      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:15.557694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:16.558231      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:08:17.186: INFO: polling logs
  Jul 15 13:08:17.204: INFO: Pod logs: 
  I0715 13:07:43.787169       1 log.go:198] OK: Got token
  I0715 13:07:43.787195       1 log.go:198] validating with in-cluster discovery
  I0715 13:07:43.787402       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0715 13:07:43.787427       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4043:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1689427063, NotBefore:1689426463, IssuedAt:1689426463, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4043", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"287645e4-535f-49d9-8d6e-5f542e6d655b"}}}
  I0715 13:07:43.795404       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0715 13:07:43.800939       1 log.go:198] OK: Validated signature on JWT
  I0715 13:07:43.801020       1 log.go:198] OK: Got valid claims from token!
  I0715 13:07:43.801039       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4043:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1689427063, NotBefore:1689426463, IssuedAt:1689426463, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4043", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"287645e4-535f-49d9-8d6e-5f542e6d655b"}}}

  Jul 15 13:08:17.204: INFO: completed pod
  Jul 15 13:08:17.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4043" for this suite. @ 07/15/23 13:08:17.215
• [34.079 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 07/15/23 13:08:17.222
  Jul 15 13:08:17.222: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename runtimeclass @ 07/15/23 13:08:17.223
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:08:17.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:08:17.236
  Jul 15 13:08:17.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2743" for this suite. @ 07/15/23 13:08:17.267
• [0.051 seconds]
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 07/15/23 13:08:17.273
  Jul 15 13:08:17.273: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename conformance-tests @ 07/15/23 13:08:17.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:08:17.283
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:08:17.286
  STEP: Getting node addresses @ 07/15/23 13:08:17.289
  Jul 15 13:08:17.289: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Jul 15 13:08:17.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-9545" for this suite. @ 07/15/23 13:08:17.296
• [0.030 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 07/15/23 13:08:17.304
  Jul 15 13:08:17.304: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 07/15/23 13:08:17.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:08:17.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:08:17.316
  STEP: creating a target pod @ 07/15/23 13:08:17.319
  E0715 13:08:17.558793      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:18.559004      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 07/15/23 13:08:19.34
  E0715 13:08:19.559493      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:20.560019      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 07/15/23 13:08:21.36
  Jul 15 13:08:21.360: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3784 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:08:21.360: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:08:21.360: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:08:21.360: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-3784/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Jul 15 13:08:21.416: INFO: Exec stderr: ""
  Jul 15 13:08:21.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-3784" for this suite. @ 07/15/23 13:08:21.427
• [4.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 07/15/23 13:08:21.436
  Jul 15 13:08:21.436: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename secrets @ 07/15/23 13:08:21.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:08:21.447
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:08:21.449
  Jul 15 13:08:21.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2845" for this suite. @ 07/15/23 13:08:21.492
• [0.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 07/15/23 13:08:21.501
  Jul 15 13:08:21.501: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename cronjob @ 07/15/23 13:08:21.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:08:21.509
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:08:21.512
  STEP: Creating a suspended cronjob @ 07/15/23 13:08:21.514
  STEP: Ensuring no jobs are scheduled @ 07/15/23 13:08:21.52
  E0715 13:08:21.560393      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:22.561315      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:23.561864      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:24.562536      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:25.562826      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:26.562991      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:27.563660      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:28.563745      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:29.564321      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:30.564640      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:31.564662      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:32.564857      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:33.565258      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:34.566189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:35.567201      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:36.567407      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:37.567638      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:38.567824      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:39.568359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:40.568701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:41.568885      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:42.569156      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:43.570132      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:44.570400      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:45.570373      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:46.570650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:47.571073      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:48.571659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:49.572014      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:50.572152      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:51.573010      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:52.573156      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:53.573276      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:54.574338      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:55.574974      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:56.576084      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:57.576892      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:58.577190      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:08:59.577554      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:00.577995      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:01.578920      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:02.579109      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:03.580030      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:04.580238      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:05.580860      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:06.580960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:07.581923      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:08.582293      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:09.583156      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:10.583244      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:11.583310      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:12.583459      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:13.584313      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:14.584578      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:15.585348      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:16.586217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:17.586512      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:18.586803      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:19.587550      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:20.587726      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:21.588023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:22.588496      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:23.588983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:24.589159      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:25.589659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:26.590185      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:27.590929      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:28.591243      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:29.591448      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:30.591957      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:31.592501      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:32.592768      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:33.593121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:34.593419      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:35.594410      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:36.595039      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:37.595705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:38.595894      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:39.596431      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:40.596878      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:41.597162      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:42.598139      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:43.598904      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:44.599161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:45.599373      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:46.599621      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:47.599721      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:48.599975      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:49.600035      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:50.600364      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:51.601262      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:52.601618      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:53.601937      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:54.602908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:55.602933      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:56.603181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:57.603186      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:58.603456      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:09:59.604049      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:00.604360      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:01.604973      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:02.605085      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:03.605798      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:04.606138      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:05.607035      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:06.607344      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:07.607635      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:08.607907      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:09.608817      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:10.609783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:11.610360      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:12.610450      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:13.610585      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:14.611609      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:15.612216      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:16.612999      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:17.613944      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:18.614209      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:19.614776      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:20.615141      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:21.615588      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:22.615871      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:23.615874      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:24.615989      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:25.616564      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:26.616846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:27.616848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:28.617044      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:29.617235      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:30.618202      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:31.618347      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:32.618567      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:33.619228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:34.619485      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:35.619726      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:36.619957      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:37.620499      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:38.620768      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:39.621805      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:40.621959      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:41.622312      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:42.622538      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:43.623623      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:44.623738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:45.624020      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:46.624274      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:47.624994      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:48.625123      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:49.625484      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:50.625790      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:51.626237      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:52.626447      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:53.626896      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:54.627190      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:55.627512      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:56.627708      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:57.627922      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:58.628122      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:10:59.628492      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:00.628915      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:01.629767      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:02.629877      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:03.630548      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:04.631411      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:05.632038      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:06.632326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:07.633188      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:08.634134      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:09.634859      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:10.634889      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:11.635187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:12.635676      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:13.635774      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:14.636001      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:15.636993      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:16.637067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:17.637215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:18.637307      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:19.637354      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:20.637414      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:21.638170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:22.638429      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:23.638605      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:24.638809      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:25.639228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:26.639474      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:27.639929      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:28.640107      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:29.640469      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:30.640837      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:31.640875      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:32.641074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:33.642146      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:34.642690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:35.643431      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:36.644189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:37.644623      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:38.644821      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:39.645330      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:40.646134      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:41.647217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:42.647383      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:43.647651      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:44.647859      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:45.648061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:46.648242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:47.648908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:48.649172      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:49.650137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:50.650479      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:51.651253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:52.651555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:53.652609      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:54.652803      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:55.653395      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:56.654208      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:57.654918      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:58.655089      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:11:59.655617      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:00.655718      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:01.655848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:02.655955      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:03.655996      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:04.656184      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:05.657267      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:06.658229      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:07.658348      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:08.658597      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:09.659294      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:10.659532      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:11.660180      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:12.660379      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:13.661089      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:14.661427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:15.662148      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:16.662433      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:17.662710      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:18.663150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:19.663975      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:20.664303      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:21.665331      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:22.665488      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:23.666185      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:24.666417      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:25.666506      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:26.666670      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:27.667692      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:28.667978      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:29.668364      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:30.668577      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:31.669092      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:32.670199      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:33.670800      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:34.670894      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:35.671461      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:36.671724      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:37.672499      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:38.673119      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:39.674167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:40.674784      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:41.675345      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:42.675631      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:43.675849      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:44.676160      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:45.676688      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:46.676988      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:47.677910      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:48.678140      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:49.678654      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:50.679062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:51.679788      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:52.679941      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:53.680912      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:54.681174      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:55.681444      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:56.681649      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:57.681944      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:58.682160      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:12:59.682285      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:00.682357      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:01.683362      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:02.683559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:03.684292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:04.684573      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:05.684591      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:06.684841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:07.684999      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:08.685138      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:09.685787      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:10.686093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:11.686521      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:12.686742      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:13.687705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:14.687899      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:15.687973      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:16.688230      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:17.688526      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:18.689391      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:19.690190      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:20.690297      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 07/15/23 13:13:21.528
  STEP: Removing cronjob @ 07/15/23 13:13:21.532
  Jul 15 13:13:21.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8265" for this suite. @ 07/15/23 13:13:21.542
• [300.049 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 07/15/23 13:13:21.55
  Jul 15 13:13:21.550: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename downward-api @ 07/15/23 13:13:21.551
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:13:21.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:13:21.565
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 13:13:21.568
  E0715 13:13:21.691195      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:22.692264      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:23.692982      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:24.693159      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:13:25.59
  Jul 15 13:13:25.593: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-777bf534-af89-4c2a-bca3-6afb41fe7bd5 container client-container: <nil>
  STEP: delete the pod @ 07/15/23 13:13:25.608
  Jul 15 13:13:25.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-440" for this suite. @ 07/15/23 13:13:25.627
• [4.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 07/15/23 13:13:25.634
  Jul 15 13:13:25.634: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename cronjob @ 07/15/23 13:13:25.635
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:13:25.646
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:13:25.648
  STEP: Creating a cronjob @ 07/15/23 13:13:25.651
  STEP: creating @ 07/15/23 13:13:25.651
  STEP: getting @ 07/15/23 13:13:25.656
  STEP: listing @ 07/15/23 13:13:25.658
  STEP: watching @ 07/15/23 13:13:25.662
  Jul 15 13:13:25.662: INFO: starting watch
  STEP: cluster-wide listing @ 07/15/23 13:13:25.664
  STEP: cluster-wide watching @ 07/15/23 13:13:25.667
  Jul 15 13:13:25.667: INFO: starting watch
  STEP: patching @ 07/15/23 13:13:25.668
  STEP: updating @ 07/15/23 13:13:25.672
  Jul 15 13:13:25.682: INFO: waiting for watch events with expected annotations
  Jul 15 13:13:25.682: INFO: saw patched and updated annotations
  STEP: patching /status @ 07/15/23 13:13:25.682
  STEP: updating /status @ 07/15/23 13:13:25.687
  E0715 13:13:25.693560      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get /status @ 07/15/23 13:13:25.694
  STEP: deleting @ 07/15/23 13:13:25.697
  STEP: deleting a collection @ 07/15/23 13:13:25.711
  Jul 15 13:13:25.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2686" for this suite. @ 07/15/23 13:13:25.725
• [0.098 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 07/15/23 13:13:25.733
  Jul 15 13:13:25.733: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename downward-api @ 07/15/23 13:13:25.733
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:13:25.744
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:13:25.747
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 13:13:25.749
  E0715 13:13:26.694208      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:27.694609      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:28.695266      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:29.695409      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:13:29.768
  Jul 15 13:13:29.772: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-a2aa2401-6055-4018-b14f-e1c49de186c8 container client-container: <nil>
  STEP: delete the pod @ 07/15/23 13:13:29.778
  Jul 15 13:13:29.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9405" for this suite. @ 07/15/23 13:13:29.798
• [4.073 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 07/15/23 13:13:29.806
  Jul 15 13:13:29.806: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 13:13:29.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:13:29.817
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:13:29.82
  STEP: Setting up server cert @ 07/15/23 13:13:29.84
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 13:13:30.239
  STEP: Deploying the webhook pod @ 07/15/23 13:13:30.248
  STEP: Wait for the deployment to be ready @ 07/15/23 13:13:30.259
  Jul 15 13:13:30.268: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0715 13:13:30.695610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:31.695956      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/15/23 13:13:32.28
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 13:13:32.289
  E0715 13:13:32.696835      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:13:33.290: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 07/15/23 13:13:33.294
  STEP: create a pod that should be denied by the webhook @ 07/15/23 13:13:33.308
  STEP: create a pod that causes the webhook to hang @ 07/15/23 13:13:33.317
  E0715 13:13:33.697424      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:34.697748      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:35.698298      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:36.698530      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:37.698783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:38.699032      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:39.699256      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:40.699318      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:41.699372      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:42.699547      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 07/15/23 13:13:43.324
  STEP: create a configmap that should be admitted by the webhook @ 07/15/23 13:13:43.331
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 07/15/23 13:13:43.337
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 07/15/23 13:13:43.345
  STEP: create a namespace that bypass the webhook @ 07/15/23 13:13:43.351
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 07/15/23 13:13:43.362
  Jul 15 13:13:43.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3474" for this suite. @ 07/15/23 13:13:43.413
  STEP: Destroying namespace "webhook-markers-5798" for this suite. @ 07/15/23 13:13:43.42
  STEP: Destroying namespace "exempted-namespace-8757" for this suite. @ 07/15/23 13:13:43.426
• [13.625 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 07/15/23 13:13:43.432
  Jul 15 13:13:43.432: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename dns @ 07/15/23 13:13:43.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:13:43.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:13:43.445
  STEP: Creating a test headless service @ 07/15/23 13:13:43.447
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6206.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6206.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6206.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6206.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6206.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6206.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6206.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6206.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6206.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6206.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6206.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6206.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 43.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.43_udp@PTR;check="$$(dig +tcp +noall +answer +search 43.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.43_tcp@PTR;sleep 1; done
   @ 07/15/23 13:13:43.463
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6206.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6206.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6206.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6206.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6206.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6206.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6206.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6206.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6206.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6206.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6206.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6206.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 43.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.43_udp@PTR;check="$$(dig +tcp +noall +answer +search 43.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.43_tcp@PTR;sleep 1; done
   @ 07/15/23 13:13:43.463
  STEP: creating a pod to probe DNS @ 07/15/23 13:13:43.463
  STEP: submitting the pod to kubernetes @ 07/15/23 13:13:43.463
  E0715 13:13:43.699610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:44.699836      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/15/23 13:13:45.481
  STEP: looking for the results for each expected name from probers @ 07/15/23 13:13:45.486
  Jul 15 13:13:45.490: INFO: Unable to read wheezy_udp@dns-test-service.dns-6206.svc.cluster.local from pod dns-6206/dns-test-08b49daf-eeb7-4384-80bf-b2021e345637: the server could not find the requested resource (get pods dns-test-08b49daf-eeb7-4384-80bf-b2021e345637)
  Jul 15 13:13:45.494: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6206.svc.cluster.local from pod dns-6206/dns-test-08b49daf-eeb7-4384-80bf-b2021e345637: the server could not find the requested resource (get pods dns-test-08b49daf-eeb7-4384-80bf-b2021e345637)
  Jul 15 13:13:45.498: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6206.svc.cluster.local from pod dns-6206/dns-test-08b49daf-eeb7-4384-80bf-b2021e345637: the server could not find the requested resource (get pods dns-test-08b49daf-eeb7-4384-80bf-b2021e345637)
  Jul 15 13:13:45.502: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6206.svc.cluster.local from pod dns-6206/dns-test-08b49daf-eeb7-4384-80bf-b2021e345637: the server could not find the requested resource (get pods dns-test-08b49daf-eeb7-4384-80bf-b2021e345637)
  Jul 15 13:13:45.520: INFO: Unable to read jessie_udp@dns-test-service.dns-6206.svc.cluster.local from pod dns-6206/dns-test-08b49daf-eeb7-4384-80bf-b2021e345637: the server could not find the requested resource (get pods dns-test-08b49daf-eeb7-4384-80bf-b2021e345637)
  Jul 15 13:13:45.523: INFO: Unable to read jessie_tcp@dns-test-service.dns-6206.svc.cluster.local from pod dns-6206/dns-test-08b49daf-eeb7-4384-80bf-b2021e345637: the server could not find the requested resource (get pods dns-test-08b49daf-eeb7-4384-80bf-b2021e345637)
  Jul 15 13:13:45.527: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6206.svc.cluster.local from pod dns-6206/dns-test-08b49daf-eeb7-4384-80bf-b2021e345637: the server could not find the requested resource (get pods dns-test-08b49daf-eeb7-4384-80bf-b2021e345637)
  Jul 15 13:13:45.531: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6206.svc.cluster.local from pod dns-6206/dns-test-08b49daf-eeb7-4384-80bf-b2021e345637: the server could not find the requested resource (get pods dns-test-08b49daf-eeb7-4384-80bf-b2021e345637)
  Jul 15 13:13:45.545: INFO: Lookups using dns-6206/dns-test-08b49daf-eeb7-4384-80bf-b2021e345637 failed for: [wheezy_udp@dns-test-service.dns-6206.svc.cluster.local wheezy_tcp@dns-test-service.dns-6206.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6206.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6206.svc.cluster.local jessie_udp@dns-test-service.dns-6206.svc.cluster.local jessie_tcp@dns-test-service.dns-6206.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6206.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6206.svc.cluster.local]

  E0715 13:13:45.700417      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:46.700589      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:47.700687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:48.700891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:49.701088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:13:50.603: INFO: DNS probes using dns-6206/dns-test-08b49daf-eeb7-4384-80bf-b2021e345637 succeeded

  Jul 15 13:13:50.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/15/23 13:13:50.607
  STEP: deleting the test service @ 07/15/23 13:13:50.627
  STEP: deleting the test headless service @ 07/15/23 13:13:50.646
  STEP: Destroying namespace "dns-6206" for this suite. @ 07/15/23 13:13:50.658
• [7.231 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 07/15/23 13:13:50.664
  Jul 15 13:13:50.664: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename crd-webhook @ 07/15/23 13:13:50.665
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:13:50.677
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:13:50.68
  STEP: Setting up server cert @ 07/15/23 13:13:50.683
  E0715 13:13:50.701271      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 07/15/23 13:13:50.935
  STEP: Deploying the custom resource conversion webhook pod @ 07/15/23 13:13:50.939
  STEP: Wait for the deployment to be ready @ 07/15/23 13:13:50.949
  Jul 15 13:13:50.959: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0715 13:13:51.702352      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:52.702464      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/15/23 13:13:52.972
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 13:13:52.982
  E0715 13:13:53.703565      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:13:53.983: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jul 15 13:13:53.988: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  E0715 13:13:54.704171      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:55.704773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 07/15/23 13:13:56.539
  STEP: v2 custom resource should be converted @ 07/15/23 13:13:56.544
  Jul 15 13:13:56.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0715 13:13:56.704779      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-2543" for this suite. @ 07/15/23 13:13:57.099
• [6.443 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 07/15/23 13:13:57.108
  Jul 15 13:13:57.108: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 13:13:57.108
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:13:57.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:13:57.123
  STEP: Creating configMap with name projected-configmap-test-volume-map-db2fbbfc-e3c5-4757-ab96-adef641bd683 @ 07/15/23 13:13:57.126
  STEP: Creating a pod to test consume configMaps @ 07/15/23 13:13:57.13
  E0715 13:13:57.705074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:58.705161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:13:59.706344      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:00.706649      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:14:01.149
  Jul 15 13:14:01.152: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-projected-configmaps-e0c1d100-f8fa-4888-89a3-79f66b417d25 container agnhost-container: <nil>
  STEP: delete the pod @ 07/15/23 13:14:01.158
  Jul 15 13:14:01.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5663" for this suite. @ 07/15/23 13:14:01.179
• [4.078 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 07/15/23 13:14:01.185
  Jul 15 13:14:01.185: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename namespaces @ 07/15/23 13:14:01.186
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:14:01.196
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:14:01.198
  STEP: creating a Namespace @ 07/15/23 13:14:01.201
  STEP: patching the Namespace @ 07/15/23 13:14:01.21
  STEP: get the Namespace and ensuring it has the label @ 07/15/23 13:14:01.215
  Jul 15 13:14:01.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3160" for this suite. @ 07/15/23 13:14:01.22
  STEP: Destroying namespace "nspatchtest-5855673a-7717-4fd4-942c-52a2018b720c-994" for this suite. @ 07/15/23 13:14:01.226
• [0.048 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 07/15/23 13:14:01.233
  Jul 15 13:14:01.234: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename field-validation @ 07/15/23 13:14:01.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:14:01.246
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:14:01.248
  Jul 15 13:14:01.251: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  E0715 13:14:01.706789      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:02.707528      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:03.707708      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0715 13:14:03.789425      23 warnings.go:70] unknown field "alpha"
  W0715 13:14:03.789451      23 warnings.go:70] unknown field "beta"
  W0715 13:14:03.789457      23 warnings.go:70] unknown field "delta"
  W0715 13:14:03.789462      23 warnings.go:70] unknown field "epsilon"
  W0715 13:14:03.789467      23 warnings.go:70] unknown field "gamma"
  Jul 15 13:14:03.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-451" for this suite. @ 07/15/23 13:14:03.819
• [2.593 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 07/15/23 13:14:03.827
  Jul 15 13:14:03.827: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl @ 07/15/23 13:14:03.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:14:03.837
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:14:03.84
  STEP: creating the pod @ 07/15/23 13:14:03.842
  Jul 15 13:14:03.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-4438 create -f -'
  Jul 15 13:14:04.263: INFO: stderr: ""
  Jul 15 13:14:04.263: INFO: stdout: "pod/pause created\n"
  E0715 13:14:04.708150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:05.708728      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 07/15/23 13:14:06.27
  Jul 15 13:14:06.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-4438 label pods pause testing-label=testing-label-value'
  Jul 15 13:14:06.322: INFO: stderr: ""
  Jul 15 13:14:06.322: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 07/15/23 13:14:06.322
  Jul 15 13:14:06.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-4438 get pod pause -L testing-label'
  Jul 15 13:14:06.379: INFO: stderr: ""
  Jul 15 13:14:06.379: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 07/15/23 13:14:06.379
  Jul 15 13:14:06.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-4438 label pods pause testing-label-'
  Jul 15 13:14:06.433: INFO: stderr: ""
  Jul 15 13:14:06.433: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 07/15/23 13:14:06.433
  Jul 15 13:14:06.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-4438 get pod pause -L testing-label'
  Jul 15 13:14:06.478: INFO: stderr: ""
  Jul 15 13:14:06.478: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 07/15/23 13:14:06.478
  Jul 15 13:14:06.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-4438 delete --grace-period=0 --force -f -'
  Jul 15 13:14:06.531: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 15 13:14:06.531: INFO: stdout: "pod \"pause\" force deleted\n"
  Jul 15 13:14:06.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-4438 get rc,svc -l name=pause --no-headers'
  Jul 15 13:14:06.578: INFO: stderr: "No resources found in kubectl-4438 namespace.\n"
  Jul 15 13:14:06.578: INFO: stdout: ""
  Jul 15 13:14:06.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-4438 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul 15 13:14:06.618: INFO: stderr: ""
  Jul 15 13:14:06.618: INFO: stdout: ""
  Jul 15 13:14:06.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4438" for this suite. @ 07/15/23 13:14:06.623
• [2.801 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 07/15/23 13:14:06.629
  Jul 15 13:14:06.629: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename downward-api @ 07/15/23 13:14:06.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:14:06.641
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:14:06.643
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 13:14:06.646
  E0715 13:14:06.709321      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:07.709572      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:08.709826      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:09.710247      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:14:10.667
  Jul 15 13:14:10.670: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-ac007905-6123-4a6f-93f6-77fa1cef6b04 container client-container: <nil>
  STEP: delete the pod @ 07/15/23 13:14:10.677
  Jul 15 13:14:10.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7278" for this suite. @ 07/15/23 13:14:10.694
• [4.071 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 07/15/23 13:14:10.7
  Jul 15 13:14:10.700: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl @ 07/15/23 13:14:10.701
  E0715 13:14:10.711046      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:14:10.712
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:14:10.714
  Jul 15 13:14:10.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-6520 create -f -'
  Jul 15 13:14:11.116: INFO: stderr: ""
  Jul 15 13:14:11.116: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Jul 15 13:14:11.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-6520 create -f -'
  Jul 15 13:14:11.283: INFO: stderr: ""
  Jul 15 13:14:11.283: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/15/23 13:14:11.283
  E0715 13:14:11.711559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:14:12.287: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 15 13:14:12.287: INFO: Found 1 / 1
  Jul 15 13:14:12.287: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jul 15 13:14:12.290: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 15 13:14:12.290: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 15 13:14:12.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-6520 describe pod agnhost-primary-bhcbm'
  Jul 15 13:14:12.343: INFO: stderr: ""
  Jul 15 13:14:12.343: INFO: stdout: "Name:             agnhost-primary-bhcbm\nNamespace:        kubectl-6520\nPriority:         0\nService Account:  default\nNode:             ip-172-31-16-190/172.31.16.190\nStart Time:       Sat, 15 Jul 2023 13:14:11 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.33.104\nIPs:\n  IP:           192.168.33.104\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://d8ccfd747383319db1333f1ed9e78d07168ec7ea6a64a55df47e4595a37b3396\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 15 Jul 2023 13:14:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9nng6 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-9nng6:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-6520/agnhost-primary-bhcbm to ip-172-31-16-190\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Jul 15 13:14:12.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-6520 describe rc agnhost-primary'
  Jul 15 13:14:12.397: INFO: stderr: ""
  Jul 15 13:14:12.397: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6520\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-bhcbm\n"
  Jul 15 13:14:12.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-6520 describe service agnhost-primary'
  Jul 15 13:14:12.450: INFO: stderr: ""
  Jul 15 13:14:12.450: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6520\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.139\nIPs:               10.152.183.139\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.33.104:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Jul 15 13:14:12.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-6520 describe node ip-172-31-16-190'
  Jul 15 13:14:12.531: INFO: stderr: ""
  Jul 15 13:14:12.531: INFO: stdout: "Name:               ip-172-31-16-190\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-worker\n                    juju-charm=kubernetes-worker\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-16-190\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 15 Jul 2023 11:51:55 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-16-190\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 15 Jul 2023 13:14:03 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 15 Jul 2023 13:12:32 +0000   Sat, 15 Jul 2023 11:52:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 15 Jul 2023 13:12:32 +0000   Sat, 15 Jul 2023 11:52:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 15 Jul 2023 13:12:32 +0000   Sat, 15 Jul 2023 11:52:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 15 Jul 2023 13:12:32 +0000   Sat, 15 Jul 2023 11:52:36 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.16.190\n  Hostname:    ip-172-31-16-190\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      16069568Ki\n  example.com/fakecpu:    1k\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 7937088Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    4\n  ephemeral-storage:      14809713845\n  example.com/fakecpu:    1k\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 7834688Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                      ec259d1fc1b965b7eafec5ca35a53254\n  System UUID:                     ec259d1f-c1b9-65b7-eafe-c5ca35a53254\n  Boot ID:                         dd3ad806-1915-4b87-a4eb-cb2ac81f3593\n  Kernel Version:                  5.19.0-1028-aws\n  OS Image:                        Ubuntu 22.04.2 LTS\n  Operating System:                linux\n  Architecture:                    amd64\n  Container Runtime Version:       containerd://1.6.8\n  Kubelet Version:                 v1.27.3\n  Kube-Proxy Version:              v1.27.3\nNon-terminated Pods:               (3 in total)\n  Namespace                        Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                        ----                                                       ------------  ----------  ---------------  -------------  ---\n  ingress-nginx-kubernetes-worker  nginx-ingress-controller-kubernetes-worker-rg5kx           0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m\n  kubectl-6520                     agnhost-primary-bhcbm                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         1s\n  sonobuoy                         sonobuoy-systemd-logs-daemon-set-ff6bac3670ee4930-cffz4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         75m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests  Limits\n  --------               --------  ------\n  cpu                    0 (0%)    0 (0%)\n  memory                 0 (0%)    0 (0%)\n  ephemeral-storage      0 (0%)    0 (0%)\n  hugepages-1Gi          0 (0%)    0 (0%)\n  hugepages-2Mi          0 (0%)    0 (0%)\n  example.com/fakecpu    0         0\n  scheduling.k8s.io/foo  0         0\nEvents:                  <none>\n"
  Jul 15 13:14:12.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-6520 describe namespace kubectl-6520'
  Jul 15 13:14:12.586: INFO: stderr: ""
  Jul 15 13:14:12.586: INFO: stdout: "Name:         kubectl-6520\nLabels:       e2e-framework=kubectl\n              e2e-run=18cdf2d3-4826-4237-a5b2-e9decef30f4d\n              kubernetes.io/metadata.name=kubectl-6520\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Jul 15 13:14:12.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6520" for this suite. @ 07/15/23 13:14:12.591
• [1.898 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 07/15/23 13:14:12.599
  Jul 15 13:14:12.599: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename svcaccounts @ 07/15/23 13:14:12.6
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:14:12.612
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:14:12.614
  STEP: Creating ServiceAccount "e2e-sa-f7n7w"  @ 07/15/23 13:14:12.617
  Jul 15 13:14:12.621: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-f7n7w"  @ 07/15/23 13:14:12.621
  Jul 15 13:14:12.627: INFO: AutomountServiceAccountToken: true
  Jul 15 13:14:12.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4486" for this suite. @ 07/15/23 13:14:12.631
• [0.038 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 07/15/23 13:14:12.638
  Jul 15 13:14:12.638: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl @ 07/15/23 13:14:12.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:14:12.648
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:14:12.651
  STEP: Starting the proxy @ 07/15/23 13:14:12.653
  Jul 15 13:14:12.653: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-7622 proxy --unix-socket=/tmp/kubectl-proxy-unix3505074533/test'
  STEP: retrieving proxy /api/ output @ 07/15/23 13:14:12.683
  Jul 15 13:14:12.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7622" for this suite. @ 07/15/23 13:14:12.688
• [0.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 07/15/23 13:14:12.696
  Jul 15 13:14:12.696: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 13:14:12.696
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:14:12.706
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:14:12.709
  E0715 13:14:12.711862      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 13:14:12.711
  E0715 13:14:13.712847      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:14.713059      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:15.713914      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:16.714173      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:14:16.73
  Jul 15 13:14:16.733: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-73a35a9a-bdab-4361-9bc3-75f900489af8 container client-container: <nil>
  STEP: delete the pod @ 07/15/23 13:14:16.741
  Jul 15 13:14:16.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5436" for this suite. @ 07/15/23 13:14:16.759
• [4.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 07/15/23 13:14:16.767
  Jul 15 13:14:16.767: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename sched-pred @ 07/15/23 13:14:16.767
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:14:16.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:14:16.782
  Jul 15 13:14:16.784: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 15 13:14:16.792: INFO: Waiting for terminating namespaces to be deleted...
  Jul 15 13:14:16.796: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-16-190 before test
  Jul 15 13:14:16.801: INFO: nginx-ingress-controller-kubernetes-worker-rg5kx from ingress-nginx-kubernetes-worker started at 2023-07-15 13:00:41 +0000 UTC (1 container statuses recorded)
  Jul 15 13:14:16.801: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 15 13:14:16.801: INFO: agnhost-primary-bhcbm from kubectl-6520 started at 2023-07-15 13:14:11 +0000 UTC (1 container statuses recorded)
  Jul 15 13:14:16.801: INFO: 	Container agnhost-primary ready: true, restart count 0
  Jul 15 13:14:16.801: INFO: sonobuoy-systemd-logs-daemon-set-ff6bac3670ee4930-cffz4 from sonobuoy started at 2023-07-15 11:58:26 +0000 UTC (2 container statuses recorded)
  Jul 15 13:14:16.801: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 15 13:14:16.801: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 15 13:14:16.801: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-42-138 before test
  Jul 15 13:14:16.805: INFO: nginx-ingress-controller-kubernetes-worker-7nm9g from ingress-nginx-kubernetes-worker started at 2023-07-15 11:50:47 +0000 UTC (1 container statuses recorded)
  Jul 15 13:14:16.805: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 15 13:14:16.805: INFO: calico-kube-controllers-7d466d5f7-7h6g9 from kube-system started at 2023-07-15 13:00:16 +0000 UTC (1 container statuses recorded)
  Jul 15 13:14:16.805: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul 15 13:14:16.805: INFO: sonobuoy from sonobuoy started at 2023-07-15 11:58:24 +0000 UTC (1 container statuses recorded)
  Jul 15 13:14:16.805: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 15 13:14:16.805: INFO: sonobuoy-e2e-job-7bfe7aae7c0f4f05 from sonobuoy started at 2023-07-15 11:58:25 +0000 UTC (2 container statuses recorded)
  Jul 15 13:14:16.805: INFO: 	Container e2e ready: true, restart count 0
  Jul 15 13:14:16.805: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 15 13:14:16.805: INFO: sonobuoy-systemd-logs-daemon-set-ff6bac3670ee4930-rmnwj from sonobuoy started at 2023-07-15 11:58:26 +0000 UTC (2 container statuses recorded)
  Jul 15 13:14:16.805: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 15 13:14:16.805: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 15 13:14:16.805: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-84-236 before test
  Jul 15 13:14:16.810: INFO: default-http-backend-kubernetes-worker-65fc475d49-z5npc from ingress-nginx-kubernetes-worker started at 2023-07-15 11:48:26 +0000 UTC (1 container statuses recorded)
  Jul 15 13:14:16.810: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul 15 13:14:16.810: INFO: nginx-ingress-controller-kubernetes-worker-r7qgp from ingress-nginx-kubernetes-worker started at 2023-07-15 11:48:26 +0000 UTC (1 container statuses recorded)
  Jul 15 13:14:16.810: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 15 13:14:16.810: INFO: coredns-5c7f76ccb8-cn56l from kube-system started at 2023-07-15 11:48:22 +0000 UTC (1 container statuses recorded)
  Jul 15 13:14:16.810: INFO: 	Container coredns ready: true, restart count 0
  Jul 15 13:14:16.810: INFO: kube-state-metrics-5b95b4459c-dcf9f from kube-system started at 2023-07-15 11:48:22 +0000 UTC (1 container statuses recorded)
  Jul 15 13:14:16.810: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul 15 13:14:16.810: INFO: metrics-server-v0.5.2-6cf8c8b69c-kdwqb from kube-system started at 2023-07-15 11:48:22 +0000 UTC (2 container statuses recorded)
  Jul 15 13:14:16.810: INFO: 	Container metrics-server ready: true, restart count 0
  Jul 15 13:14:16.810: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul 15 13:14:16.810: INFO: dashboard-metrics-scraper-6b8586b5c9-h8plq from kubernetes-dashboard started at 2023-07-15 11:48:22 +0000 UTC (1 container statuses recorded)
  Jul 15 13:14:16.810: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul 15 13:14:16.810: INFO: kubernetes-dashboard-6869f4cd5f-2knpl from kubernetes-dashboard started at 2023-07-15 11:48:22 +0000 UTC (1 container statuses recorded)
  Jul 15 13:14:16.810: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul 15 13:14:16.810: INFO: sonobuoy-systemd-logs-daemon-set-ff6bac3670ee4930-k229l from sonobuoy started at 2023-07-15 11:58:26 +0000 UTC (2 container statuses recorded)
  Jul 15 13:14:16.810: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 15 13:14:16.810: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ip-172-31-16-190 @ 07/15/23 13:14:16.824
  STEP: verifying the node has the label node ip-172-31-42-138 @ 07/15/23 13:14:16.835
  STEP: verifying the node has the label node ip-172-31-84-236 @ 07/15/23 13:14:16.849
  Jul 15 13:14:16.859: INFO: Pod default-http-backend-kubernetes-worker-65fc475d49-z5npc requesting resource cpu=10m on Node ip-172-31-84-236
  Jul 15 13:14:16.859: INFO: Pod nginx-ingress-controller-kubernetes-worker-7nm9g requesting resource cpu=0m on Node ip-172-31-42-138
  Jul 15 13:14:16.859: INFO: Pod nginx-ingress-controller-kubernetes-worker-r7qgp requesting resource cpu=0m on Node ip-172-31-84-236
  Jul 15 13:14:16.859: INFO: Pod nginx-ingress-controller-kubernetes-worker-rg5kx requesting resource cpu=0m on Node ip-172-31-16-190
  Jul 15 13:14:16.859: INFO: Pod calico-kube-controllers-7d466d5f7-7h6g9 requesting resource cpu=0m on Node ip-172-31-42-138
  Jul 15 13:14:16.859: INFO: Pod coredns-5c7f76ccb8-cn56l requesting resource cpu=100m on Node ip-172-31-84-236
  Jul 15 13:14:16.859: INFO: Pod kube-state-metrics-5b95b4459c-dcf9f requesting resource cpu=0m on Node ip-172-31-84-236
  Jul 15 13:14:16.859: INFO: Pod metrics-server-v0.5.2-6cf8c8b69c-kdwqb requesting resource cpu=5m on Node ip-172-31-84-236
  Jul 15 13:14:16.859: INFO: Pod agnhost-primary-bhcbm requesting resource cpu=0m on Node ip-172-31-16-190
  Jul 15 13:14:16.859: INFO: Pod dashboard-metrics-scraper-6b8586b5c9-h8plq requesting resource cpu=0m on Node ip-172-31-84-236
  Jul 15 13:14:16.859: INFO: Pod kubernetes-dashboard-6869f4cd5f-2knpl requesting resource cpu=0m on Node ip-172-31-84-236
  Jul 15 13:14:16.859: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-42-138
  Jul 15 13:14:16.859: INFO: Pod sonobuoy-e2e-job-7bfe7aae7c0f4f05 requesting resource cpu=0m on Node ip-172-31-42-138
  Jul 15 13:14:16.859: INFO: Pod sonobuoy-systemd-logs-daemon-set-ff6bac3670ee4930-cffz4 requesting resource cpu=0m on Node ip-172-31-16-190
  Jul 15 13:14:16.859: INFO: Pod sonobuoy-systemd-logs-daemon-set-ff6bac3670ee4930-k229l requesting resource cpu=0m on Node ip-172-31-84-236
  Jul 15 13:14:16.859: INFO: Pod sonobuoy-systemd-logs-daemon-set-ff6bac3670ee4930-rmnwj requesting resource cpu=0m on Node ip-172-31-42-138
  STEP: Starting Pods to consume most of the cluster CPU. @ 07/15/23 13:14:16.859
  Jul 15 13:14:16.859: INFO: Creating a pod which consumes cpu=2800m on Node ip-172-31-16-190
  Jul 15 13:14:16.866: INFO: Creating a pod which consumes cpu=2800m on Node ip-172-31-42-138
  Jul 15 13:14:16.871: INFO: Creating a pod which consumes cpu=2719m on Node ip-172-31-84-236
  E0715 13:14:17.714475      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:18.714655      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 07/15/23 13:14:18.894
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-363d0526-20f8-420f-902d-7508dcc8d937.17720cc67e75a017], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6288/filler-pod-363d0526-20f8-420f-902d-7508dcc8d937 to ip-172-31-84-236] @ 07/15/23 13:14:18.897
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-363d0526-20f8-420f-902d-7508dcc8d937.17720cc69ee134cf], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/15/23 13:14:18.897
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-363d0526-20f8-420f-902d-7508dcc8d937.17720cc69fb07211], Reason = [Created], Message = [Created container filler-pod-363d0526-20f8-420f-902d-7508dcc8d937] @ 07/15/23 13:14:18.897
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-363d0526-20f8-420f-902d-7508dcc8d937.17720cc6a2fce099], Reason = [Started], Message = [Started container filler-pod-363d0526-20f8-420f-902d-7508dcc8d937] @ 07/15/23 13:14:18.897
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b1bf84a7-4124-4fb4-95d1-895af96c801e.17720cc67dd998c7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6288/filler-pod-b1bf84a7-4124-4fb4-95d1-895af96c801e to ip-172-31-16-190] @ 07/15/23 13:14:18.897
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b1bf84a7-4124-4fb4-95d1-895af96c801e.17720cc69da27c2a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/15/23 13:14:18.897
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b1bf84a7-4124-4fb4-95d1-895af96c801e.17720cc69e6d6afe], Reason = [Created], Message = [Created container filler-pod-b1bf84a7-4124-4fb4-95d1-895af96c801e] @ 07/15/23 13:14:18.897
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b1bf84a7-4124-4fb4-95d1-895af96c801e.17720cc6a06a5c85], Reason = [Started], Message = [Started container filler-pod-b1bf84a7-4124-4fb4-95d1-895af96c801e] @ 07/15/23 13:14:18.897
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-bcf46e7d-c9df-43f2-b9f8-9d2339f8efa7.17720cc67dfb9089], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6288/filler-pod-bcf46e7d-c9df-43f2-b9f8-9d2339f8efa7 to ip-172-31-42-138] @ 07/15/23 13:14:18.897
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-bcf46e7d-c9df-43f2-b9f8-9d2339f8efa7.17720cc69efa544f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/15/23 13:14:18.897
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-bcf46e7d-c9df-43f2-b9f8-9d2339f8efa7.17720cc69fec89a8], Reason = [Created], Message = [Created container filler-pod-bcf46e7d-c9df-43f2-b9f8-9d2339f8efa7] @ 07/15/23 13:14:18.897
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-bcf46e7d-c9df-43f2-b9f8-9d2339f8efa7.17720cc6a35521a9], Reason = [Started], Message = [Started container filler-pod-bcf46e7d-c9df-43f2-b9f8-9d2339f8efa7] @ 07/15/23 13:14:18.897
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.17720cc6f6f01cb9], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/5 nodes are available: 2 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] @ 07/15/23 13:14:18.909
  E0715 13:14:19.715199      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node ip-172-31-16-190 @ 07/15/23 13:14:19.91
  STEP: verifying the node doesn't have the label node @ 07/15/23 13:14:19.92
  STEP: removing the label node off the node ip-172-31-42-138 @ 07/15/23 13:14:19.924
  STEP: verifying the node doesn't have the label node @ 07/15/23 13:14:19.933
  STEP: removing the label node off the node ip-172-31-84-236 @ 07/15/23 13:14:19.937
  STEP: verifying the node doesn't have the label node @ 07/15/23 13:14:19.948
  Jul 15 13:14:19.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-6288" for this suite. @ 07/15/23 13:14:19.959
• [3.197 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 07/15/23 13:14:19.965
  Jul 15 13:14:19.965: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename sched-preemption @ 07/15/23 13:14:19.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:14:19.977
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:14:19.979
  Jul 15 13:14:19.994: INFO: Waiting up to 1m0s for all nodes to be ready
  E0715 13:14:20.715595      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:21.715798      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:22.716023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:23.716364      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:24.717189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:25.718166      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:26.718234      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:27.718496      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:28.719261      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:29.719409      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:30.720425      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:31.720527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:32.720898      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:33.721377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:34.721853      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:35.721995      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:36.722700      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:37.723093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:38.723652      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:39.723960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:40.724585      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:41.724884      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:42.725181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:43.726200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:44.727015      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:45.727891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:46.728984      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:47.729059      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:48.729195      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:49.730195      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:50.730486      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:51.730784      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:52.731809      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:53.732544      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:54.733487      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:55.734334      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:56.734616      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:57.734880      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:58.735936      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:14:59.736210      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:00.736848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:01.737076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:02.737979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:03.738167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:04.738509      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:05.738601      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:06.738680      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:07.738967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:08.739299      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:09.739421      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:10.740066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:11.741042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:12.741211      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:13.741215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:14.742199      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:15.742769      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:16.742950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:17.743257      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:18.743317      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:19.743654      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:15:20.012: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 07/15/23 13:15:20.015
  Jul 15 13:15:20.031: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jul 15 13:15:20.036: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jul 15 13:15:20.048: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jul 15 13:15:20.054: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jul 15 13:15:20.066: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jul 15 13:15:20.072: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 07/15/23 13:15:20.072
  E0715 13:15:20.743930      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:21.744141      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 07/15/23 13:15:22.103
  E0715 13:15:22.744537      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:23.744802      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:24.745659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:25.746125      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:15:26.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-364" for this suite. @ 07/15/23 13:15:26.185
• [66.226 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 07/15/23 13:15:26.191
  Jul 15 13:15:26.191: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename deployment @ 07/15/23 13:15:26.192
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:15:26.204
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:15:26.207
  Jul 15 13:15:26.209: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Jul 15 13:15:26.217: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0715 13:15:26.746835      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:27.747215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:28.747537      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:29.747876      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:30.748357      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:15:31.221: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/15/23 13:15:31.221
  Jul 15 13:15:31.221: INFO: Creating deployment "test-rolling-update-deployment"
  Jul 15 13:15:31.226: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Jul 15 13:15:31.232: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0715 13:15:31.749130      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:32.749165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:15:33.239: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Jul 15 13:15:33.241: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Jul 15 13:15:33.253: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1228  b8361590-b6f8-430d-b34a-d92589d8f444 35811 1 2023-07-15 13:15:31 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-07-15 13:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-15 13:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004389ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-15 13:15:31 +0000 UTC,LastTransitionTime:2023-07-15 13:15:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-07-15 13:15:31 +0000 UTC,LastTransitionTime:2023-07-15 13:15:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 15 13:15:33.256: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-1228  1925dca5-0c2e-4ad2-8e89-13cf8edf6031 35798 1 2023-07-15 13:15:31 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b8361590-b6f8-430d-b34a-d92589d8f444 0xc00003b717 0xc00003b718}] [] [{kube-controller-manager Update apps/v1 2023-07-15 13:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8361590-b6f8-430d-b34a-d92589d8f444\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-15 13:15:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00003b7c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 15 13:15:33.256: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Jul 15 13:15:33.256: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1228  1f3d2351-dcfd-46ee-8045-eb62e9494db8 35809 2 2023-07-15 13:15:26 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b8361590-b6f8-430d-b34a-d92589d8f444 0xc00003b5d7 0xc00003b5d8}] [] [{e2e.test Update apps/v1 2023-07-15 13:15:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-15 13:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8361590-b6f8-430d-b34a-d92589d8f444\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-15 13:15:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00003b6a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 15 13:15:33.261: INFO: Pod "test-rolling-update-deployment-656d657cd8-z5qcv" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-z5qcv test-rolling-update-deployment-656d657cd8- deployment-1228  a0b4aebf-18d8-43dd-a680-5b8f750a5c41 35797 0 2023-07-15 13:15:31 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 1925dca5-0c2e-4ad2-8e89-13cf8edf6031 0xc004e47917 0xc004e47918}] [] [{kube-controller-manager Update v1 2023-07-15 13:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1925dca5-0c2e-4ad2-8e89-13cf8edf6031\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-15 13:15:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.33.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vdlwm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vdlwm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-16-190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 13:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 13:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 13:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-15 13:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.16.190,PodIP:192.168.33.93,StartTime:2023-07-15 13:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-15 13:15:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://d783111f5a11ae2f547406102af8383cd123291900dbed54a9b95f29f098a707,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.33.93,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 15 13:15:33.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1228" for this suite. @ 07/15/23 13:15:33.264
• [7.081 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 07/15/23 13:15:33.272
  Jul 15 13:15:33.272: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-probe @ 07/15/23 13:15:33.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:15:33.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:15:33.284
  STEP: Creating pod test-grpc-cfa0b111-9ba8-4177-8ca1-aa6a77724b77 in namespace container-probe-4114 @ 07/15/23 13:15:33.286
  E0715 13:15:33.750155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:34.751088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:15:35.301: INFO: Started pod test-grpc-cfa0b111-9ba8-4177-8ca1-aa6a77724b77 in namespace container-probe-4114
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/15/23 13:15:35.301
  Jul 15 13:15:35.304: INFO: Initial restart count of pod test-grpc-cfa0b111-9ba8-4177-8ca1-aa6a77724b77 is 0
  E0715 13:15:35.751377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:36.751716      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:37.751972      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:38.752425      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:39.753322      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:40.754180      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:41.754516      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:42.754807      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:43.755752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:44.756010      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:45.756732      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:46.757041      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:47.757690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:48.757872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:49.757884      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:50.758297      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:51.759309      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:52.759511      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:53.759704      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:54.760018      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:55.760258      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:56.761206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:57.762129      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:58.762417      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:15:59.762524      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:00.762868      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:01.763754      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:02.763957      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:03.765002      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:04.765194      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:05.766166      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:06.766433      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:07.767353      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:08.767559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:09.767960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:10.768322      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:11.769122      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:12.769194      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:13.769957      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:14.770277      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:15.770584      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:16.770693      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:17.771687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:18.771978      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:19.772615      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:20.773068      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:21.774100      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:22.774357      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:23.774972      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:24.775234      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:25.775608      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:26.775887      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:27.775948      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:28.776149      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:29.776693      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:30.777394      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:31.778222      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:32.778600      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:33.778711      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:34.778745      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:35.779407      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:36.779588      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:37.780089      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:38.780396      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:39.781081      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:40.781148      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:16:41.465: INFO: Restart count of pod container-probe-4114/test-grpc-cfa0b111-9ba8-4177-8ca1-aa6a77724b77 is now 1 (1m6.160430266s elapsed)
  Jul 15 13:16:41.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/15/23 13:16:41.468
  STEP: Destroying namespace "container-probe-4114" for this suite. @ 07/15/23 13:16:41.478
• [68.213 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 07/15/23 13:16:41.486
  Jul 15 13:16:41.486: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename pods @ 07/15/23 13:16:41.486
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:16:41.5
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:16:41.503
  STEP: creating the pod @ 07/15/23 13:16:41.505
  STEP: submitting the pod to kubernetes @ 07/15/23 13:16:41.505
  STEP: verifying QOS class is set on the pod @ 07/15/23 13:16:41.512
  Jul 15 13:16:41.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1546" for this suite. @ 07/15/23 13:16:41.535
• [0.053 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 07/15/23 13:16:41.54
  Jul 15 13:16:41.540: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename resourcequota @ 07/15/23 13:16:41.54
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:16:41.55
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:16:41.552
  STEP: Creating resourceQuota "e2e-rq-status-ld6fd" @ 07/15/23 13:16:41.558
  Jul 15 13:16:41.566: INFO: Resource quota "e2e-rq-status-ld6fd" reports spec: hard cpu limit of 500m
  Jul 15 13:16:41.566: INFO: Resource quota "e2e-rq-status-ld6fd" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-ld6fd" /status @ 07/15/23 13:16:41.566
  STEP: Confirm /status for "e2e-rq-status-ld6fd" resourceQuota via watch @ 07/15/23 13:16:41.578
  Jul 15 13:16:41.580: INFO: observed resourceQuota "e2e-rq-status-ld6fd" in namespace "resourcequota-4521" with hard status: v1.ResourceList(nil)
  Jul 15 13:16:41.580: INFO: Found resourceQuota "e2e-rq-status-ld6fd" in namespace "resourcequota-4521" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jul 15 13:16:41.580: INFO: ResourceQuota "e2e-rq-status-ld6fd" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 07/15/23 13:16:41.582
  Jul 15 13:16:41.588: INFO: Resource quota "e2e-rq-status-ld6fd" reports spec: hard cpu limit of 1
  Jul 15 13:16:41.588: INFO: Resource quota "e2e-rq-status-ld6fd" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-ld6fd" /status @ 07/15/23 13:16:41.588
  STEP: Confirm /status for "e2e-rq-status-ld6fd" resourceQuota via watch @ 07/15/23 13:16:41.592
  Jul 15 13:16:41.594: INFO: observed resourceQuota "e2e-rq-status-ld6fd" in namespace "resourcequota-4521" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jul 15 13:16:41.594: INFO: Found resourceQuota "e2e-rq-status-ld6fd" in namespace "resourcequota-4521" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jul 15 13:16:41.594: INFO: ResourceQuota "e2e-rq-status-ld6fd" /status was patched
  STEP: Get "e2e-rq-status-ld6fd" /status @ 07/15/23 13:16:41.594
  Jul 15 13:16:41.598: INFO: Resourcequota "e2e-rq-status-ld6fd" reports status: hard cpu of 1
  Jul 15 13:16:41.598: INFO: Resourcequota "e2e-rq-status-ld6fd" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-ld6fd" /status before checking Spec is unchanged @ 07/15/23 13:16:41.603
  Jul 15 13:16:41.607: INFO: Resourcequota "e2e-rq-status-ld6fd" reports status: hard cpu of 2
  Jul 15 13:16:41.607: INFO: Resourcequota "e2e-rq-status-ld6fd" reports status: hard memory of 2Gi
  Jul 15 13:16:41.609: INFO: Found resourceQuota "e2e-rq-status-ld6fd" in namespace "resourcequota-4521" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0715 13:16:41.781559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:42.781933      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:43.782491      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:44.782584      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:45.783058      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:46.783381      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:47.783488      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:48.783701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:49.783906      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:50.784329      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:51.784380      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:52.784580      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:53.784782      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:54.784875      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:55.785255      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:56.785951      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:57.786200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:58.786426      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:16:59.786638      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:00.786944      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:01.787312      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:02.787729      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:03.787919      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:04.788405      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:05.788495      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:06.788792      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:07.788967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:08.789062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:09.789155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:10.789259      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:17:11.619: INFO: ResourceQuota "e2e-rq-status-ld6fd" Spec was unchanged and /status reset
  Jul 15 13:17:11.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4521" for this suite. @ 07/15/23 13:17:11.624
• [30.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 07/15/23 13:17:11.634
  Jul 15 13:17:11.634: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename svcaccounts @ 07/15/23 13:17:11.635
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:17:11.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:17:11.648
  E0715 13:17:11.790055      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:12.790619      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 07/15/23 13:17:13.673
  Jul 15 13:17:13.673: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5877 pod-service-account-12794b0f-545e-4484-93e3-a80591007932 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 07/15/23 13:17:13.782
  Jul 15 13:17:13.782: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5877 pod-service-account-12794b0f-545e-4484-93e3-a80591007932 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  E0715 13:17:13.790568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 07/15/23 13:17:13.882
  Jul 15 13:17:13.882: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5877 pod-service-account-12794b0f-545e-4484-93e3-a80591007932 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Jul 15 13:17:13.988: INFO: Got root ca configmap in namespace "svcaccounts-5877"
  Jul 15 13:17:13.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5877" for this suite. @ 07/15/23 13:17:13.995
• [2.369 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 07/15/23 13:17:14.004
  Jul 15 13:17:14.004: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename watch @ 07/15/23 13:17:14.004
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:17:14.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:17:14.016
  STEP: creating a watch on configmaps @ 07/15/23 13:17:14.019
  STEP: creating a new configmap @ 07/15/23 13:17:14.021
  STEP: modifying the configmap once @ 07/15/23 13:17:14.024
  STEP: closing the watch once it receives two notifications @ 07/15/23 13:17:14.031
  Jul 15 13:17:14.031: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-716  65bdfc46-a2f6-49e9-94cd-23389f1ca48b 36175 0 2023-07-15 13:17:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-15 13:17:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 13:17:14.032: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-716  65bdfc46-a2f6-49e9-94cd-23389f1ca48b 36176 0 2023-07-15 13:17:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-15 13:17:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 07/15/23 13:17:14.032
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 07/15/23 13:17:14.039
  STEP: deleting the configmap @ 07/15/23 13:17:14.04
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 07/15/23 13:17:14.045
  Jul 15 13:17:14.045: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-716  65bdfc46-a2f6-49e9-94cd-23389f1ca48b 36177 0 2023-07-15 13:17:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-15 13:17:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 13:17:14.046: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-716  65bdfc46-a2f6-49e9-94cd-23389f1ca48b 36178 0 2023-07-15 13:17:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-15 13:17:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 15 13:17:14.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-716" for this suite. @ 07/15/23 13:17:14.049
• [0.052 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 07/15/23 13:17:14.056
  Jul 15 13:17:14.056: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 13:17:14.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:17:14.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:17:14.07
  STEP: Setting up server cert @ 07/15/23 13:17:14.088
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 13:17:14.276
  STEP: Deploying the webhook pod @ 07/15/23 13:17:14.284
  STEP: Wait for the deployment to be ready @ 07/15/23 13:17:14.296
  Jul 15 13:17:14.305: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0715 13:17:14.790804      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:15.790848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/15/23 13:17:16.316
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 13:17:16.325
  E0715 13:17:16.791290      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:17:17.325: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 07/15/23 13:17:17.392
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/15/23 13:17:17.418
  STEP: Deleting the collection of validation webhooks @ 07/15/23 13:17:17.439
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/15/23 13:17:17.483
  Jul 15 13:17:17.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9927" for this suite. @ 07/15/23 13:17:17.529
  STEP: Destroying namespace "webhook-markers-8432" for this suite. @ 07/15/23 13:17:17.539
• [3.491 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 07/15/23 13:17:17.547
  Jul 15 13:17:17.547: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename field-validation @ 07/15/23 13:17:17.547
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:17:17.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:17:17.561
  Jul 15 13:17:17.563: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  E0715 13:17:17.791505      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:18.791647      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:19.791907      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0715 13:17:20.096771      23 warnings.go:70] unknown field "alpha"
  W0715 13:17:20.096794      23 warnings.go:70] unknown field "beta"
  W0715 13:17:20.096799      23 warnings.go:70] unknown field "delta"
  W0715 13:17:20.096803      23 warnings.go:70] unknown field "epsilon"
  W0715 13:17:20.096807      23 warnings.go:70] unknown field "gamma"
  Jul 15 13:17:20.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1054" for this suite. @ 07/15/23 13:17:20.126
• [2.587 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 07/15/23 13:17:20.134
  Jul 15 13:17:20.134: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 13:17:20.134
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:17:20.144
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:17:20.146
  STEP: Setting up server cert @ 07/15/23 13:17:20.164
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 13:17:20.315
  STEP: Deploying the webhook pod @ 07/15/23 13:17:20.32
  STEP: Wait for the deployment to be ready @ 07/15/23 13:17:20.331
  Jul 15 13:17:20.342: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0715 13:17:20.792010      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:21.792479      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/15/23 13:17:22.354
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 13:17:22.365
  E0715 13:17:22.792840      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:17:23.365: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 07/15/23 13:17:23.37
  STEP: create a pod that should be updated by the webhook @ 07/15/23 13:17:23.384
  Jul 15 13:17:23.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5028" for this suite. @ 07/15/23 13:17:23.448
  STEP: Destroying namespace "webhook-markers-7779" for this suite. @ 07/15/23 13:17:23.454
• [3.325 seconds]
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 07/15/23 13:17:23.459
  Jul 15 13:17:23.459: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename events @ 07/15/23 13:17:23.46
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:17:23.473
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:17:23.476
  STEP: creating a test event @ 07/15/23 13:17:23.478
  STEP: listing events in all namespaces @ 07/15/23 13:17:23.487
  STEP: listing events in test namespace @ 07/15/23 13:17:23.49
  STEP: listing events with field selection filtering on source @ 07/15/23 13:17:23.494
  STEP: listing events with field selection filtering on reportingController @ 07/15/23 13:17:23.498
  STEP: getting the test event @ 07/15/23 13:17:23.5
  STEP: patching the test event @ 07/15/23 13:17:23.503
  STEP: getting the test event @ 07/15/23 13:17:23.512
  STEP: updating the test event @ 07/15/23 13:17:23.514
  STEP: getting the test event @ 07/15/23 13:17:23.521
  STEP: deleting the test event @ 07/15/23 13:17:23.523
  STEP: listing events in all namespaces @ 07/15/23 13:17:23.531
  STEP: listing events in test namespace @ 07/15/23 13:17:23.535
  Jul 15 13:17:23.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1916" for this suite. @ 07/15/23 13:17:23.541
• [0.086 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 07/15/23 13:17:23.546
  Jul 15 13:17:23.546: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename gc @ 07/15/23 13:17:23.547
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:17:23.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:17:23.56
  STEP: create the rc @ 07/15/23 13:17:23.568
  W0715 13:17:23.574219      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0715 13:17:23.793462      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:24.795007      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:25.795814      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:26.795922      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:27.796609      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:28.796693      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 07/15/23 13:17:29.579
  STEP: wait for the rc to be deleted @ 07/15/23 13:17:29.586
  E0715 13:17:29.797713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:17:30.596: INFO: 80 pods remaining
  Jul 15 13:17:30.596: INFO: 80 pods has nil DeletionTimestamp
  Jul 15 13:17:30.596: INFO: 
  E0715 13:17:30.798441      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:17:31.600: INFO: 71 pods remaining
  Jul 15 13:17:31.600: INFO: 70 pods has nil DeletionTimestamp
  Jul 15 13:17:31.600: INFO: 
  E0715 13:17:31.798553      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:17:32.597: INFO: 60 pods remaining
  Jul 15 13:17:32.597: INFO: 60 pods has nil DeletionTimestamp
  Jul 15 13:17:32.597: INFO: 
  E0715 13:17:32.798756      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:17:33.594: INFO: 40 pods remaining
  Jul 15 13:17:33.594: INFO: 40 pods has nil DeletionTimestamp
  Jul 15 13:17:33.594: INFO: 
  E0715 13:17:33.799372      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:17:34.594: INFO: 31 pods remaining
  Jul 15 13:17:34.594: INFO: 30 pods has nil DeletionTimestamp
  Jul 15 13:17:34.594: INFO: 
  E0715 13:17:34.799981      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:17:35.594: INFO: 20 pods remaining
  Jul 15 13:17:35.594: INFO: 20 pods has nil DeletionTimestamp
  Jul 15 13:17:35.594: INFO: 
  E0715 13:17:35.800028      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/15/23 13:17:36.592
  W0715 13:17:36.597537      23 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 15 13:17:36.597: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 15 13:17:36.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-221" for this suite. @ 07/15/23 13:17:36.601
• [13.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 07/15/23 13:17:36.61
  Jul 15 13:17:36.610: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename security-context @ 07/15/23 13:17:36.61
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:17:36.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:17:36.627
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 07/15/23 13:17:36.629
  E0715 13:17:36.800622      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:37.800700      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:38.801627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:39.802255      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:17:40.651
  Jul 15 13:17:40.654: INFO: Trying to get logs from node ip-172-31-16-190 pod security-context-7e54dd9b-787b-4469-b357-3e1a6dd141c1 container test-container: <nil>
  STEP: delete the pod @ 07/15/23 13:17:40.668
  Jul 15 13:17:40.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-9414" for this suite. @ 07/15/23 13:17:40.688
• [4.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 07/15/23 13:17:40.696
  Jul 15 13:17:40.696: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename replicaset @ 07/15/23 13:17:40.696
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:17:40.708
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:17:40.711
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 07/15/23 13:17:40.713
  E0715 13:17:40.802288      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:41.803260      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 07/15/23 13:17:42.733
  STEP: Then the orphan pod is adopted @ 07/15/23 13:17:42.739
  E0715 13:17:42.803689      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 07/15/23 13:17:43.745
  Jul 15 13:17:43.748: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 07/15/23 13:17:43.759
  E0715 13:17:43.804574      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:17:44.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2465" for this suite. @ 07/15/23 13:17:44.769
• [4.080 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 07/15/23 13:17:44.776
  Jul 15 13:17:44.776: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename configmap @ 07/15/23 13:17:44.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:17:44.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:17:44.794
  STEP: Creating configMap with name configmap-test-volume-map-4b3bf0f6-0fb3-4b6b-9cf6-001bb5548fed @ 07/15/23 13:17:44.796
  STEP: Creating a pod to test consume configMaps @ 07/15/23 13:17:44.8
  E0715 13:17:44.805289      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:45.806221      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:46.806504      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:47.807337      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:48.807612      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:17:48.822
  Jul 15 13:17:48.824: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-configmaps-1bd7ebc1-811c-48b0-9264-452b7157df66 container agnhost-container: <nil>
  STEP: delete the pod @ 07/15/23 13:17:48.832
  Jul 15 13:17:48.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3999" for this suite. @ 07/15/23 13:17:48.851
• [4.081 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 07/15/23 13:17:48.857
  Jul 15 13:17:48.857: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/15/23 13:17:48.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:17:48.87
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:17:48.873
  STEP: create the container to handle the HTTPGet hook request. @ 07/15/23 13:17:48.879
  E0715 13:17:49.807767      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:50.808302      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 07/15/23 13:17:50.903
  E0715 13:17:51.808476      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:52.809147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 07/15/23 13:17:52.919
  STEP: delete the pod with lifecycle hook @ 07/15/23 13:17:52.927
  E0715 13:17:53.810163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:54.810485      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:17:54.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-6118" for this suite. @ 07/15/23 13:17:54.945
• [6.095 seconds]
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 07/15/23 13:17:54.953
  Jul 15 13:17:54.953: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename statefulset @ 07/15/23 13:17:54.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:17:54.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:17:54.967
  STEP: Creating service test in namespace statefulset-6306 @ 07/15/23 13:17:54.97
  STEP: Creating stateful set ss in namespace statefulset-6306 @ 07/15/23 13:17:54.977
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6306 @ 07/15/23 13:17:54.984
  Jul 15 13:17:54.988: INFO: Found 0 stateful pods, waiting for 1
  E0715 13:17:55.811473      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:56.811678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:57.811927      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:58.812115      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:17:59.812378      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:00.812788      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:01.813760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:02.814149      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:03.814388      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:04.814709      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:04.992: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 07/15/23 13:18:04.992
  Jul 15 13:18:04.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-6306 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 15 13:18:05.101: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 15 13:18:05.101: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 15 13:18:05.101: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 15 13:18:05.105: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0715 13:18:05.814848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:06.815192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:07.815288      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:08.815588      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:09.815937      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:10.816319      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:11.816467      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:12.816725      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:13.816826      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:14.817098      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:15.111: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 15 13:18:15.111: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 15 13:18:15.126: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Jul 15 13:18:15.126: INFO: ss-0  ip-172-31-16-190  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:17:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:17:55 +0000 UTC  }]
  Jul 15 13:18:15.126: INFO: 
  Jul 15 13:18:15.126: INFO: StatefulSet ss has not reached scale 3, at 1
  E0715 13:18:15.817887      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:16.131: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996710793s
  E0715 13:18:16.818918      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:17.135: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99066945s
  E0715 13:18:17.819397      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:18.139: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987172991s
  E0715 13:18:18.820395      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:19.144: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983162612s
  E0715 13:18:19.820496      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:20.149: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977942555s
  E0715 13:18:20.821009      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:21.153: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973553776s
  E0715 13:18:21.821069      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:22.158: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969473743s
  E0715 13:18:22.821171      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:23.165: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.963316734s
  E0715 13:18:23.822155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:24.169: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.323143ms
  E0715 13:18:24.822258      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6306 @ 07/15/23 13:18:25.17
  Jul 15 13:18:25.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-6306 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 15 13:18:25.277: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 15 13:18:25.277: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 15 13:18:25.277: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 15 13:18:25.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-6306 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 15 13:18:25.388: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jul 15 13:18:25.388: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 15 13:18:25.388: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 15 13:18:25.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-6306 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 15 13:18:25.493: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jul 15 13:18:25.493: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 15 13:18:25.493: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 15 13:18:25.496: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 15 13:18:25.496: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 15 13:18:25.496: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 07/15/23 13:18:25.496
  Jul 15 13:18:25.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-6306 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 15 13:18:25.602: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 15 13:18:25.602: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 15 13:18:25.602: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 15 13:18:25.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-6306 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 15 13:18:25.706: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 15 13:18:25.706: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 15 13:18:25.706: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 15 13:18:25.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-6306 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 15 13:18:25.807: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 15 13:18:25.807: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 15 13:18:25.807: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 15 13:18:25.807: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 15 13:18:25.810: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0715 13:18:25.822393      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:26.822659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:27.822923      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:28.822985      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:29.823105      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:30.823451      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:31.823703      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:32.823990      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:33.824689      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:34.825001      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:35.817: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 15 13:18:35.817: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jul 15 13:18:35.817: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  E0715 13:18:35.825405      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:35.831: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Jul 15 13:18:35.831: INFO: ss-0  ip-172-31-16-190  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:17:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:17:55 +0000 UTC  }]
  Jul 15 13:18:35.831: INFO: ss-1  ip-172-31-42-138  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:15 +0000 UTC  }]
  Jul 15 13:18:35.831: INFO: ss-2  ip-172-31-84-236  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:15 +0000 UTC  }]
  Jul 15 13:18:35.831: INFO: 
  Jul 15 13:18:35.831: INFO: StatefulSet ss has not reached scale 0, at 3
  E0715 13:18:36.826093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:36.836: INFO: POD   NODE              PHASE      GRACE  CONDITIONS
  Jul 15 13:18:36.836: INFO: ss-1  ip-172-31-42-138  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:15 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:26 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:26 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:15 +0000 UTC  }]
  Jul 15 13:18:36.836: INFO: ss-2  ip-172-31-84-236  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:15 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:26 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:26 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-15 13:18:15 +0000 UTC  }]
  Jul 15 13:18:36.836: INFO: 
  Jul 15 13:18:36.836: INFO: StatefulSet ss has not reached scale 0, at 2
  E0715 13:18:37.826685      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:37.840: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990181419s
  E0715 13:18:38.827376      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:38.845: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.986433795s
  E0715 13:18:39.827848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:39.849: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.981020516s
  E0715 13:18:40.828402      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:40.853: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.977413704s
  E0715 13:18:41.828749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:41.857: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.973021971s
  E0715 13:18:42.829089      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:42.862: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.969828326s
  E0715 13:18:43.830165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:43.867: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.963703655s
  E0715 13:18:44.830236      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:44.871: INFO: Verifying statefulset ss doesn't scale past 0 for another 958.634445ms
  E0715 13:18:45.830979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6306 @ 07/15/23 13:18:45.872
  Jul 15 13:18:45.876: INFO: Scaling statefulset ss to 0
  Jul 15 13:18:45.885: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 15 13:18:45.889: INFO: Deleting all statefulset in ns statefulset-6306
  Jul 15 13:18:45.892: INFO: Scaling statefulset ss to 0
  Jul 15 13:18:45.903: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 15 13:18:45.906: INFO: Deleting statefulset ss
  Jul 15 13:18:45.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6306" for this suite. @ 07/15/23 13:18:45.926
• [50.980 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 07/15/23 13:18:45.934
  Jul 15 13:18:45.934: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 13:18:45.934
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:18:45.945
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:18:45.948
  STEP: Creating configMap with name projected-configmap-test-volume-map-0b100881-6b5a-4972-af12-4006ad13d4a5 @ 07/15/23 13:18:45.951
  STEP: Creating a pod to test consume configMaps @ 07/15/23 13:18:45.957
  E0715 13:18:46.831245      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:47.831496      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:48.831793      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:49.832017      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:18:49.976
  Jul 15 13:18:49.979: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-projected-configmaps-6435200e-3663-407c-9eab-77e64e792151 container agnhost-container: <nil>
  STEP: delete the pod @ 07/15/23 13:18:49.99
  Jul 15 13:18:50.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-597" for this suite. @ 07/15/23 13:18:50.013
• [4.086 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 07/15/23 13:18:50.02
  Jul 15 13:18:50.020: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-probe @ 07/15/23 13:18:50.02
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:18:50.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:18:50.034
  STEP: Creating pod liveness-2b394bb2-423a-4321-861b-fafbd873805a in namespace container-probe-3495 @ 07/15/23 13:18:50.036
  E0715 13:18:50.832557      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:51.832833      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:18:52.053: INFO: Started pod liveness-2b394bb2-423a-4321-861b-fafbd873805a in namespace container-probe-3495
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/15/23 13:18:52.053
  Jul 15 13:18:52.057: INFO: Initial restart count of pod liveness-2b394bb2-423a-4321-861b-fafbd873805a is 0
  E0715 13:18:52.833160      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:53.834188      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:54.834360      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:55.834986      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:56.835611      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:57.835992      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:58.836386      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:18:59.836508      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:00.837348      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:01.838234      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:02.838507      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:03.838631      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:04.838979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:05.839701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:06.840222      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:07.840396      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:08.841127      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:09.841289      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:10.842217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:11.842484      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:12.842999      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:13.843266      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:14.843640      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:15.843690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:16.843996      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:17.844389      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:18.844754      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:19.845141      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:20.845200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:21.846228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:22.846771      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:23.847034      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:24.848028      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:25.848117      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:26.848659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:27.848839      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:28.849116      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:29.849382      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:30.849694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:31.849905      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:32.850228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:33.850520      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:34.850582      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:35.850778      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:36.851666      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:37.852514      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:38.853495      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:39.853603      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:40.854121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:41.854424      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:42.854813      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:43.855079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:44.855785      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:45.855942      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:46.856371      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:47.856620      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:48.856689      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:49.856861      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:50.857511      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:51.857631      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:52.858213      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:53.858705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:54.859114      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:55.859693      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:56.860200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:57.860496      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:58.860663      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:19:59.861026      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:00.861118      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:01.861305      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:02.861377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:03.862304      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:04.862496      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:05.863089      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:06.863550      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:07.863814      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:08.864738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:09.865614      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:10.866217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:11.866339      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:12.867432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:13.867699      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:14.867828      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:15.867940      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:16.868362      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:17.868592      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:18.868879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:19.869067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:20.869765      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:21.870255      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:22.871066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:23.871436      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:24.871547      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:25.872143      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:26.872568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:27.872819      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:28.873542      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:29.874270      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:30.874812      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:31.874994      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:32.875128      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:33.875710      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:34.876492      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:35.877090      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:36.877876      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:37.878023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:38.878123      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:39.878431      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:40.879099      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:41.879158      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:42.880137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:43.880252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:44.880733      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:45.881306      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:46.881436      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:47.882217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:48.882260      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:49.882430      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:50.882798      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:51.882979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:52.883148      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:53.883572      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:54.883612      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:55.884102      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:56.884892      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:57.885137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:58.885707      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:20:59.886185      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:00.886772      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:01.887019      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:02.887295      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:03.887463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:04.888306      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:05.888898      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:06.889150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:07.889204      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:08.889782      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:09.890215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:10.890415      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:11.890645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:12.891374      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:13.892283      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:14.893052      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:15.893178      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:16.894116      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:17.894361      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:18.895135      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:19.895309      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:20.895945      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:21.896142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:22.896883      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:23.897713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:24.897815      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:25.898476      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:26.898758      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:27.899013      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:28.899112      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:29.899362      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:30.899997      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:31.900302      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:32.901113      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:33.901309      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:34.901354      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:35.901458      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:36.902309      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:37.902562      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:38.903563      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:39.903657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:40.903771      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:41.904025      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:42.904217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:43.904386      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:44.905286      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:45.905347      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:46.906260      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:47.906464      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:48.907122      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:49.907306      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:50.908258      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:51.908562      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:52.909226      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:53.909323      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:54.909569      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:55.910188      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:56.910989      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:57.911090      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:58.911656      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:21:59.912403      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:00.912788      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:01.913026      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:02.913628      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:03.914175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:04.914804      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:05.915364      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:06.915921      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:07.916196      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:08.916317      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:09.916784      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:10.916732      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:11.917635      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:12.918213      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:13.918417      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:14.918590      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:15.919368      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:16.919720      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:17.919901      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:18.920440      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:19.920677      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:20.920869      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:21.921060      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:22.921736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:23.921957      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:24.922051      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:25.922187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:26.922858      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:27.923115      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:28.923918      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:29.924053      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:30.924834      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:31.925310      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:32.926151      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:33.926380      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:34.926779      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:35.927288      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:36.927938      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:37.928106      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:38.928786      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:39.929094      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:40.929693      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:41.929881      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:42.930153      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:43.930348      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:44.931240      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:45.931707      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:46.932761      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:47.932872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:48.933004      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:49.933164      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:50.933207      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:51.934161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:22:52.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/15/23 13:22:52.651
  STEP: Destroying namespace "container-probe-3495" for this suite. @ 07/15/23 13:22:52.666
• [242.653 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 07/15/23 13:22:52.674
  Jul 15 13:22:52.674: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir @ 07/15/23 13:22:52.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:22:52.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:22:52.689
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 07/15/23 13:22:52.693
  E0715 13:22:52.935050      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:53.935249      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:54.935329      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:55.935970      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:22:56.714
  Jul 15 13:22:56.718: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-0aedb266-7abd-40b0-b346-909319b5f877 container test-container: <nil>
  STEP: delete the pod @ 07/15/23 13:22:56.731
  Jul 15 13:22:56.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9163" for this suite. @ 07/15/23 13:22:56.747
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 07/15/23 13:22:56.754
  Jul 15 13:22:56.754: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename field-validation @ 07/15/23 13:22:56.755
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:22:56.765
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:22:56.767
  STEP: apply creating a deployment @ 07/15/23 13:22:56.77
  Jul 15 13:22:56.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4030" for this suite. @ 07/15/23 13:22:56.789
• [0.041 seconds]
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 07/15/23 13:22:56.796
  Jul 15 13:22:56.796: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename events @ 07/15/23 13:22:56.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:22:56.808
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:22:56.811
  STEP: creating a test event @ 07/15/23 13:22:56.814
  STEP: listing all events in all namespaces @ 07/15/23 13:22:56.818
  STEP: patching the test event @ 07/15/23 13:22:56.823
  STEP: fetching the test event @ 07/15/23 13:22:56.828
  STEP: updating the test event @ 07/15/23 13:22:56.831
  STEP: getting the test event @ 07/15/23 13:22:56.841
  STEP: deleting the test event @ 07/15/23 13:22:56.843
  STEP: listing all events in all namespaces @ 07/15/23 13:22:56.851
  Jul 15 13:22:56.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4863" for this suite. @ 07/15/23 13:22:56.857
• [0.068 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 07/15/23 13:22:56.864
  Jul 15 13:22:56.864: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir-wrapper @ 07/15/23 13:22:56.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:22:56.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:22:56.883
  STEP: Creating 50 configmaps @ 07/15/23 13:22:56.886
  E0715 13:22:56.936905      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 07/15/23 13:22:57.119
  Jul 15 13:22:57.222: INFO: Pod name wrapped-volume-race-8fa7ccbb-9f90-41fe-8425-9dd030f36c1b: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/15/23 13:22:57.222
  E0715 13:22:57.937076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:22:58.937102      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 07/15/23 13:22:59.292
  Jul 15 13:22:59.304: INFO: Pod name wrapped-volume-race-50231f0d-dd0a-433f-8411-eb1d6d0adcb7: Found 0 pods out of 5
  E0715 13:22:59.938001      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:00.938442      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:01.938654      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:02.938905      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:03.938983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:23:04.310: INFO: Pod name wrapped-volume-race-50231f0d-dd0a-433f-8411-eb1d6d0adcb7: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/15/23 13:23:04.31
  STEP: Creating RC which spawns configmap-volume pods @ 07/15/23 13:23:04.33
  Jul 15 13:23:04.343: INFO: Pod name wrapped-volume-race-37e00131-cdda-48d9-abaf-1f8b0d558334: Found 0 pods out of 5
  E0715 13:23:04.939194      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:05.939475      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:06.939727      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:07.940022      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:08.940259      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:23:09.350: INFO: Pod name wrapped-volume-race-37e00131-cdda-48d9-abaf-1f8b0d558334: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/15/23 13:23:09.35
  Jul 15 13:23:09.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-37e00131-cdda-48d9-abaf-1f8b0d558334 in namespace emptydir-wrapper-5519, will wait for the garbage collector to delete the pods @ 07/15/23 13:23:09.373
  Jul 15 13:23:09.434: INFO: Deleting ReplicationController wrapped-volume-race-37e00131-cdda-48d9-abaf-1f8b0d558334 took: 8.271018ms
  Jul 15 13:23:09.535: INFO: Terminating ReplicationController wrapped-volume-race-37e00131-cdda-48d9-abaf-1f8b0d558334 pods took: 100.960305ms
  E0715 13:23:09.941322      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-50231f0d-dd0a-433f-8411-eb1d6d0adcb7 in namespace emptydir-wrapper-5519, will wait for the garbage collector to delete the pods @ 07/15/23 13:23:10.836
  Jul 15 13:23:10.897: INFO: Deleting ReplicationController wrapped-volume-race-50231f0d-dd0a-433f-8411-eb1d6d0adcb7 took: 7.59635ms
  E0715 13:23:10.941404      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:23:10.998: INFO: Terminating ReplicationController wrapped-volume-race-50231f0d-dd0a-433f-8411-eb1d6d0adcb7 pods took: 101.05461ms
  E0715 13:23:11.941704      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-8fa7ccbb-9f90-41fe-8425-9dd030f36c1b in namespace emptydir-wrapper-5519, will wait for the garbage collector to delete the pods @ 07/15/23 13:23:12.599
  Jul 15 13:23:12.661: INFO: Deleting ReplicationController wrapped-volume-race-8fa7ccbb-9f90-41fe-8425-9dd030f36c1b took: 7.932639ms
  Jul 15 13:23:12.861: INFO: Terminating ReplicationController wrapped-volume-race-8fa7ccbb-9f90-41fe-8425-9dd030f36c1b pods took: 200.267766ms
  E0715 13:23:12.942177      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:13.943087      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 07/15/23 13:23:14.562
  STEP: Destroying namespace "emptydir-wrapper-5519" for this suite. @ 07/15/23 13:23:14.848
• [17.990 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 07/15/23 13:23:14.855
  Jul 15 13:23:14.855: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename services @ 07/15/23 13:23:14.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:23:14.865
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:23:14.868
  STEP: creating service nodeport-test with type=NodePort in namespace services-3213 @ 07/15/23 13:23:14.875
  STEP: creating replication controller nodeport-test in namespace services-3213 @ 07/15/23 13:23:14.889
  I0715 13:23:14.895010      23 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-3213, replica count: 2
  E0715 13:23:14.943591      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:15.944418      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:16.944610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:17.944807      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0715 13:23:17.945925      23 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 15 13:23:17.945: INFO: Creating new exec pod
  E0715 13:23:18.945176      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:19.945508      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:20.946226      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:23:20.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-3213 exec execpodb57vr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jul 15 13:23:21.071: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jul 15 13:23:21.071: INFO: stdout: ""
  E0715 13:23:21.947201      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:23:22.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-3213 exec execpodb57vr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jul 15 13:23:22.172: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jul 15 13:23:22.172: INFO: stdout: "nodeport-test-926wk"
  Jul 15 13:23:22.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-3213 exec execpodb57vr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.119 80'
  Jul 15 13:23:22.273: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.119 80\nConnection to 10.152.183.119 80 port [tcp/http] succeeded!\n"
  Jul 15 13:23:22.273: INFO: stdout: ""
  E0715 13:23:22.947367      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:23:23.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-3213 exec execpodb57vr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.119 80'
  Jul 15 13:23:23.381: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.119 80\nConnection to 10.152.183.119 80 port [tcp/http] succeeded!\n"
  Jul 15 13:23:23.381: INFO: stdout: "nodeport-test-pr9hj"
  Jul 15 13:23:23.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-3213 exec execpodb57vr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.84.236 32316'
  Jul 15 13:23:23.484: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.84.236 32316\nConnection to 172.31.84.236 32316 port [tcp/*] succeeded!\n"
  Jul 15 13:23:23.484: INFO: stdout: "nodeport-test-926wk"
  Jul 15 13:23:23.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=services-3213 exec execpodb57vr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.42.138 32316'
  Jul 15 13:23:23.586: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.42.138 32316\nConnection to 172.31.42.138 32316 port [tcp/*] succeeded!\n"
  Jul 15 13:23:23.586: INFO: stdout: "nodeport-test-pr9hj"
  Jul 15 13:23:23.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3213" for this suite. @ 07/15/23 13:23:23.591
• [8.743 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 07/15/23 13:23:23.599
  Jul 15 13:23:23.599: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename gc @ 07/15/23 13:23:23.599
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:23:23.61
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:23:23.613
  STEP: create the rc @ 07/15/23 13:23:23.615
  W0715 13:23:23.622477      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0715 13:23:23.947943      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:24.948386      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:25.948839      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:26.949033      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:27.949165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 07/15/23 13:23:28.627
  STEP: wait for all pods to be garbage collected @ 07/15/23 13:23:28.634
  E0715 13:23:28.949711      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:29.949825      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:30.950257      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:31.950273      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:32.950543      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/15/23 13:23:33.642
  W0715 13:23:33.647067      23 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 15 13:23:33.647: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 15 13:23:33.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3483" for this suite. @ 07/15/23 13:23:33.65
• [10.058 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 07/15/23 13:23:33.657
  Jul 15 13:23:33.657: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename subjectreview @ 07/15/23 13:23:33.658
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:23:33.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:23:33.678
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-337" @ 07/15/23 13:23:33.681
  Jul 15 13:23:33.684: INFO: saUsername: "system:serviceaccount:subjectreview-337:e2e"
  Jul 15 13:23:33.684: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-337"}
  Jul 15 13:23:33.684: INFO: saUID: "686179a1-2d52-4e8c-8f23-c03dca22a6ad"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-337:e2e" @ 07/15/23 13:23:33.684
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-337:e2e" @ 07/15/23 13:23:33.684
  Jul 15 13:23:33.685: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-337:e2e" api 'list' configmaps in "subjectreview-337" namespace @ 07/15/23 13:23:33.686
  Jul 15 13:23:33.687: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-337:e2e" @ 07/15/23 13:23:33.687
  Jul 15 13:23:33.688: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Jul 15 13:23:33.688: INFO: LocalSubjectAccessReview has been verified
  Jul 15 13:23:33.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-337" for this suite. @ 07/15/23 13:23:33.691
• [0.040 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 07/15/23 13:23:33.698
  Jul 15 13:23:33.698: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename services @ 07/15/23 13:23:33.699
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:23:33.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:23:33.714
  STEP: fetching services @ 07/15/23 13:23:33.716
  Jul 15 13:23:33.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1994" for this suite. @ 07/15/23 13:23:33.724
• [0.033 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 07/15/23 13:23:33.732
  Jul 15 13:23:33.732: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename disruption @ 07/15/23 13:23:33.732
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:23:33.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:23:33.745
  STEP: Waiting for the pdb to be processed @ 07/15/23 13:23:33.754
  E0715 13:23:33.951306      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:34.951856      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 07/15/23 13:23:35.788
  Jul 15 13:23:35.792: INFO: running pods: 0 < 3
  E0715 13:23:35.952771      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:36.953059      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:23:37.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4799" for this suite. @ 07/15/23 13:23:37.803
• [4.078 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 07/15/23 13:23:37.811
  Jul 15 13:23:37.811: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 13:23:37.811
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:23:37.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:23:37.826
  STEP: Creating the pod @ 07/15/23 13:23:37.829
  E0715 13:23:37.953548      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:38.953625      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:39.953900      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:23:40.368: INFO: Successfully updated pod "labelsupdatee62f7536-d01a-46d9-8abc-9b415180faf2"
  E0715 13:23:40.954602      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:41.954796      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:42.955083      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:43.955283      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:23:44.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5168" for this suite. @ 07/15/23 13:23:44.397
• [6.592 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 07/15/23 13:23:44.405
  Jul 15 13:23:44.405: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename secrets @ 07/15/23 13:23:44.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:23:44.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:23:44.421
  STEP: Creating secret with name secret-test-45f99c5c-84bc-4888-9e76-c4111e600c70 @ 07/15/23 13:23:44.423
  STEP: Creating a pod to test consume secrets @ 07/15/23 13:23:44.427
  E0715 13:23:44.956203      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:45.956592      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:46.956992      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:47.957219      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:23:48.449
  Jul 15 13:23:48.452: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-secrets-dcd8ef17-caca-498a-8019-77a816cd3658 container secret-env-test: <nil>
  STEP: delete the pod @ 07/15/23 13:23:48.459
  Jul 15 13:23:48.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1162" for this suite. @ 07/15/23 13:23:48.478
• [4.080 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 07/15/23 13:23:48.486
  Jul 15 13:23:48.486: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename downward-api @ 07/15/23 13:23:48.487
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:23:48.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:23:48.499
  STEP: Creating a pod to test downward API volume plugin @ 07/15/23 13:23:48.501
  E0715 13:23:48.957909      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:49.958201      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:50.958385      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:51.958504      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:23:52.524
  Jul 15 13:23:52.527: INFO: Trying to get logs from node ip-172-31-16-190 pod downwardapi-volume-9c5bfb03-b10e-45ae-9cba-836bb78b83c9 container client-container: <nil>
  STEP: delete the pod @ 07/15/23 13:23:52.538
  Jul 15 13:23:52.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2105" for this suite. @ 07/15/23 13:23:52.557
• [4.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 07/15/23 13:23:52.564
  Jul 15 13:23:52.564: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename security-context @ 07/15/23 13:23:52.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:23:52.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:23:52.577
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 07/15/23 13:23:52.58
  E0715 13:23:52.958606      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:53.958843      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:54.959533      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:55.959755      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:23:56.602
  Jul 15 13:23:56.605: INFO: Trying to get logs from node ip-172-31-16-190 pod security-context-fec4b1d6-bc9f-4858-ad4f-c9a3324f6b8b container test-container: <nil>
  STEP: delete the pod @ 07/15/23 13:23:56.612
  Jul 15 13:23:56.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-7605" for this suite. @ 07/15/23 13:23:56.631
• [4.074 seconds]
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 07/15/23 13:23:56.638
  Jul 15 13:23:56.638: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename security-context-test @ 07/15/23 13:23:56.639
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:23:56.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:23:56.652
  E0715 13:23:56.960603      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:57.961037      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:58.961938      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:23:59.962294      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:24:00.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6762" for this suite. @ 07/15/23 13:24:00.682
• [4.049 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 07/15/23 13:24:00.687
  Jul 15 13:24:00.687: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/15/23 13:24:00.688
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:24:00.701
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:24:00.703
  STEP: create the container to handle the HTTPGet hook request. @ 07/15/23 13:24:00.71
  E0715 13:24:00.962918      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:01.963157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 07/15/23 13:24:02.733
  E0715 13:24:02.963216      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:03.963387      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 07/15/23 13:24:04.748
  E0715 13:24:04.964475      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:05.964674      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 07/15/23 13:24:06.762
  Jul 15 13:24:06.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-2389" for this suite. @ 07/15/23 13:24:06.772
• [6.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 07/15/23 13:24:06.779
  Jul 15 13:24:06.779: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename var-expansion @ 07/15/23 13:24:06.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:24:06.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:24:06.794
  STEP: creating the pod @ 07/15/23 13:24:06.797
  STEP: waiting for pod running @ 07/15/23 13:24:06.807
  E0715 13:24:06.965391      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:07.966400      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 07/15/23 13:24:08.814
  Jul 15 13:24:08.817: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7571 PodName:var-expansion-ef22f2ea-95a4-4402-9901-1f3260dde661 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:24:08.817: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:24:08.817: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:24:08.817: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-7571/pods/var-expansion-ef22f2ea-95a4-4402-9901-1f3260dde661/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 07/15/23 13:24:08.872
  Jul 15 13:24:08.876: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7571 PodName:var-expansion-ef22f2ea-95a4-4402-9901-1f3260dde661 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:24:08.876: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:24:08.876: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:24:08.876: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-7571/pods/var-expansion-ef22f2ea-95a4-4402-9901-1f3260dde661/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 07/15/23 13:24:08.933
  E0715 13:24:08.967344      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:24:09.443: INFO: Successfully updated pod "var-expansion-ef22f2ea-95a4-4402-9901-1f3260dde661"
  STEP: waiting for annotated pod running @ 07/15/23 13:24:09.443
  STEP: deleting the pod gracefully @ 07/15/23 13:24:09.447
  Jul 15 13:24:09.447: INFO: Deleting pod "var-expansion-ef22f2ea-95a4-4402-9901-1f3260dde661" in namespace "var-expansion-7571"
  Jul 15 13:24:09.454: INFO: Wait up to 5m0s for pod "var-expansion-ef22f2ea-95a4-4402-9901-1f3260dde661" to be fully deleted
  E0715 13:24:09.967873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:10.967992      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:11.968860      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:12.968992      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:13.969098      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:14.969689      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:15.969926      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:16.970121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:17.970234      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:18.971280      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:19.971490      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:20.971699      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:21.972789      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:22.973110      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:23.973909      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:24.974367      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:25.974835      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:26.974940      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:27.975035      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:28.975228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:29.975903      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:30.976088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:31.976186      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:32.976372      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:33.977069      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:34.977439      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:35.978151      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:36.978515      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:37.979165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:38.979533      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:39.980582      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:40.981520      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:24:41.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7571" for this suite. @ 07/15/23 13:24:41.537
• [34.765 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 07/15/23 13:24:41.544
  Jul 15 13:24:41.544: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename container-probe @ 07/15/23 13:24:41.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:24:41.556
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:24:41.559
  STEP: Creating pod busybox-7e8fdab6-6a09-4c14-b183-83dbd694583c in namespace container-probe-9380 @ 07/15/23 13:24:41.562
  E0715 13:24:41.981811      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:42.982156      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:24:43.578: INFO: Started pod busybox-7e8fdab6-6a09-4c14-b183-83dbd694583c in namespace container-probe-9380
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/15/23 13:24:43.578
  Jul 15 13:24:43.582: INFO: Initial restart count of pod busybox-7e8fdab6-6a09-4c14-b183-83dbd694583c is 0
  E0715 13:24:43.982482      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:44.983167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:45.983764      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:46.984051      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:47.985012      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:48.985076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:49.986026      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:50.986247      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:51.986887      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:52.987095      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:53.987356      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:54.987814      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:55.988528      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:56.988737      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:57.989515      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:58.989693      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:24:59.990125      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:00.990341      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:01.991157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:02.991397      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:03.991455      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:04.992041      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:05.992348      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:06.992522      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:07.992615      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:08.992805      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:09.993439      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:10.994135      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:11.995085      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:12.995191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:13.995230      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:14.995781      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:15.996109      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:16.996294      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:17.996653      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:18.996826      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:19.997616      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:20.998141      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:21.998228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:22.998486      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:23.999111      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:24.999551      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:26.000024      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:27.000228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:28.000801      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:29.001011      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:30.001353      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:31.001555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:32.002142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:33.002365      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:34.003140      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:35.003614      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:36.004281      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:37.004449      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:38.005352      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:39.006128      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:40.006478      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:41.006723      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:42.006932      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:43.007128      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:44.007727      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:45.008124      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:46.008353      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:47.008445      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:48.009424      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:49.009519      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:50.010536      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:51.010636      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:52.011292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:53.012199      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:54.013250      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:55.013648      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:56.013755      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:57.013966      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:58.014079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:25:59.014250      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:00.014971      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:01.015146      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:02.015189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:03.015357      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:04.015457      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:05.015881      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:06.016428      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:07.016654      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:08.016725      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:09.016909      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:10.017862      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:11.018046      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:12.018150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:13.018330      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:14.019385      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:15.019824      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:16.020548      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:17.020664      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:18.020838      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:19.020967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:20.021650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:21.021838      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:22.022588      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:23.022766      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:24.023242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:25.023661      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:26.024377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:27.024655      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:28.024836      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:29.024954      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:30.025870      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:31.026129      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:32.026568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:33.026672      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:34.026825      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:35.027792      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:36.028559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:37.028739      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:38.029139      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:39.030138      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:40.030983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:41.031176      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:42.032010      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:43.032178      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:44.032309      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:45.032679      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:46.033485      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:47.033604      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:48.034118      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:49.034946      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:50.035703      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:51.035842      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:52.036398      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:53.036700      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:54.037103      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:55.037519      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:56.037860      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:57.038243      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:58.039207      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:26:59.039338      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:00.039872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:01.040121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:02.040286      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:03.041023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:04.041252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:05.041628      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:06.042612      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:07.042811      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:08.043819      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:09.044137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:10.044556      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:11.044771      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:12.045435      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:13.046178      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:14.046488      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:15.046875      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:16.047844      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:17.048029      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:18.048911      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:19.049128      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:20.050165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:21.050433      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:22.050965      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:23.051164      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:24.052127      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:25.053081      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:26.053858      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:27.054513      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:28.055166      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:29.055572      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:30.055796      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:31.055985      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:32.056968      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:33.057067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:34.058007      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:35.058524      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:36.059284      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:37.059448      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:38.060163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:39.060428      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:40.060557      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:41.060602      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:42.060853      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:43.061136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:44.061624      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:45.062232      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:46.062832      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:47.063019      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:48.064032      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:49.064238      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:50.065176      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:51.065253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:52.066097      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:53.066384      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:54.067340      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:55.067451      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:56.067877      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:57.068128      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:58.068997      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:27:59.069155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:00.070151      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:01.070413      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:02.070634      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:03.071524      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:04.072194      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:05.072456      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:06.073309      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:07.074165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:08.074705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:09.074956      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:10.074963      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:11.075085      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:12.075354      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:13.075465      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:14.076082      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:15.076592      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:16.076821      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:17.077671      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:18.078022      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:19.078600      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:20.079264      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:21.079403      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:22.079635      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:23.080674      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:24.081026      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:25.081990      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:26.082755      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:27.083243      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:28.083441      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:29.083911      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:30.084384      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:31.084493      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:32.084831      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:33.085170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:34.086174      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:35.087193      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:36.087487      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:37.088090      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:38.088448      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:39.089110      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:40.089365      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:41.090193      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:42.090425      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:43.090567      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:44.090999      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:28:44.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/15/23 13:28:44.16
  STEP: Destroying namespace "container-probe-9380" for this suite. @ 07/15/23 13:28:44.171
• [242.634 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 07/15/23 13:28:44.179
  Jul 15 13:28:44.179: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename statefulset @ 07/15/23 13:28:44.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:28:44.193
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:28:44.196
  STEP: Creating service test in namespace statefulset-9109 @ 07/15/23 13:28:44.199
  STEP: Creating a new StatefulSet @ 07/15/23 13:28:44.204
  Jul 15 13:28:44.215: INFO: Found 0 stateful pods, waiting for 3
  E0715 13:28:45.091702      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:46.092446      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:47.093437      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:48.094174      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:49.094298      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:50.094713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:51.094887      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:52.095118      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:53.095408      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:54.095617      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:28:54.219: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 15 13:28:54.219: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 15 13:28:54.219: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Jul 15 13:28:54.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-9109 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 15 13:28:54.335: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 15 13:28:54.335: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 15 13:28:54.335: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0715 13:28:55.095762      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:56.095997      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:57.096294      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:58.096991      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:28:59.097138      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:00.097369      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:01.097634      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:02.098136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:03.098515      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:04.098555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 07/15/23 13:29:04.35
  Jul 15 13:29:04.372: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 07/15/23 13:29:04.372
  E0715 13:29:05.099392      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:06.099646      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:07.099907      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:08.099967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:09.100095      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:10.100370      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:11.100631      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:12.100898      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:13.101142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:14.101326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 07/15/23 13:29:14.39
  Jul 15 13:29:14.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-9109 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 15 13:29:14.500: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 15 13:29:14.500: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 15 13:29:14.500: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0715 13:29:15.102345      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:16.102599      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:17.102951      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:18.103522      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:19.103921      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:20.104877      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:21.105174      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:22.105206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:23.106144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:24.106320      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 07/15/23 13:29:24.522
  Jul 15 13:29:24.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-9109 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 15 13:29:24.621: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 15 13:29:24.621: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 15 13:29:24.621: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0715 13:29:25.106612      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:26.106806      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:27.107116      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:28.107297      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:29.107406      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:30.107838      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:31.108487      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:32.108601      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:33.108826      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:34.109119      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:29:34.657: INFO: Updating stateful set ss2
  E0715 13:29:35.109677      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:36.110356      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:37.110562      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:38.110776      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:39.111721      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:40.111760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:41.112409      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:42.113274      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:43.114224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:44.114526      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 07/15/23 13:29:44.673
  Jul 15 13:29:44.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=statefulset-9109 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 15 13:29:44.784: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 15 13:29:44.784: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 15 13:29:44.784: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0715 13:29:45.115021      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:46.115170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:47.115430      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:48.115749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:49.116077      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:50.116427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:51.116536      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:52.116668      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:53.116794      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:54.117045      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:29:54.805: INFO: Deleting all statefulset in ns statefulset-9109
  Jul 15 13:29:54.808: INFO: Scaling statefulset ss2 to 0
  E0715 13:29:55.117466      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:56.118161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:57.118419      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:58.118701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:29:59.118816      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:00.119168      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:01.119362      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:02.119421      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:03.119617      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:04.119769      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:30:04.827: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 15 13:30:04.830: INFO: Deleting statefulset ss2
  Jul 15 13:30:04.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9109" for this suite. @ 07/15/23 13:30:04.846
• [80.674 seconds]
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 07/15/23 13:30:04.853
  Jul 15 13:30:04.853: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename svcaccounts @ 07/15/23 13:30:04.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:30:04.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:30:04.87
  STEP: creating a ServiceAccount @ 07/15/23 13:30:04.872
  STEP: watching for the ServiceAccount to be added @ 07/15/23 13:30:04.879
  STEP: patching the ServiceAccount @ 07/15/23 13:30:04.882
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 07/15/23 13:30:04.904
  STEP: deleting the ServiceAccount @ 07/15/23 13:30:04.907
  Jul 15 13:30:04.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8185" for this suite. @ 07/15/23 13:30:04.923
• [0.075 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 07/15/23 13:30:04.928
  Jul 15 13:30:04.928: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename limitrange @ 07/15/23 13:30:04.929
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:30:04.94
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:30:04.942
  STEP: Creating a LimitRange @ 07/15/23 13:30:04.946
  STEP: Setting up watch @ 07/15/23 13:30:04.946
  STEP: Submitting a LimitRange @ 07/15/23 13:30:05.051
  STEP: Verifying LimitRange creation was observed @ 07/15/23 13:30:05.056
  STEP: Fetching the LimitRange to ensure it has proper values @ 07/15/23 13:30:05.056
  Jul 15 13:30:05.059: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jul 15 13:30:05.059: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 07/15/23 13:30:05.059
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 07/15/23 13:30:05.064
  Jul 15 13:30:05.067: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jul 15 13:30:05.067: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 07/15/23 13:30:05.067
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 07/15/23 13:30:05.073
  Jul 15 13:30:05.076: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Jul 15 13:30:05.076: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 07/15/23 13:30:05.076
  STEP: Failing to create a Pod with more than max resources @ 07/15/23 13:30:05.077
  STEP: Updating a LimitRange @ 07/15/23 13:30:05.079
  STEP: Verifying LimitRange updating is effective @ 07/15/23 13:30:05.084
  E0715 13:30:05.119927      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:06.120301      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 07/15/23 13:30:07.088
  STEP: Failing to create a Pod with more than max resources @ 07/15/23 13:30:07.094
  STEP: Deleting a LimitRange @ 07/15/23 13:30:07.096
  STEP: Verifying the LimitRange was deleted @ 07/15/23 13:30:07.103
  E0715 13:30:07.120616      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:08.120864      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:09.121189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:10.121183      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:11.122171      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:30:12.107: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 07/15/23 13:30:12.107
  Jul 15 13:30:12.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0715 13:30:12.122509      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "limitrange-8424" for this suite. @ 07/15/23 13:30:12.123
• [7.201 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 07/15/23 13:30:12.129
  Jul 15 13:30:12.129: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename services @ 07/15/23 13:30:12.13
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:30:12.142
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:30:12.144
  STEP: creating an Endpoint @ 07/15/23 13:30:12.151
  STEP: waiting for available Endpoint @ 07/15/23 13:30:12.155
  STEP: listing all Endpoints @ 07/15/23 13:30:12.156
  STEP: updating the Endpoint @ 07/15/23 13:30:12.159
  STEP: fetching the Endpoint @ 07/15/23 13:30:12.166
  STEP: patching the Endpoint @ 07/15/23 13:30:12.169
  STEP: fetching the Endpoint @ 07/15/23 13:30:12.175
  STEP: deleting the Endpoint by Collection @ 07/15/23 13:30:12.179
  STEP: waiting for Endpoint deletion @ 07/15/23 13:30:12.193
  STEP: fetching the Endpoint @ 07/15/23 13:30:12.194
  Jul 15 13:30:12.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2354" for this suite. @ 07/15/23 13:30:12.2
• [0.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 07/15/23 13:30:12.207
  Jul 15 13:30:12.207: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename configmap @ 07/15/23 13:30:12.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:30:12.218
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:30:12.221
  STEP: Creating configMap with name configmap-test-volume-3fd29e76-8127-4f02-be7a-b4c5704ff684 @ 07/15/23 13:30:12.223
  STEP: Creating a pod to test consume configMaps @ 07/15/23 13:30:12.228
  E0715 13:30:13.122756      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:14.122784      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:15.123040      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:16.123331      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:30:16.25
  Jul 15 13:30:16.252: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-configmaps-9459eea9-8782-46a9-b2c2-fbd0d3527ad1 container configmap-volume-test: <nil>
  STEP: delete the pod @ 07/15/23 13:30:16.27
  Jul 15 13:30:16.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4267" for this suite. @ 07/15/23 13:30:16.289
• [4.088 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 07/15/23 13:30:16.296
  Jul 15 13:30:16.296: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 13:30:16.296
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:30:16.308
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:30:16.31
  STEP: Setting up server cert @ 07/15/23 13:30:16.329
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 13:30:16.521
  STEP: Deploying the webhook pod @ 07/15/23 13:30:16.53
  STEP: Wait for the deployment to be ready @ 07/15/23 13:30:16.54
  Jul 15 13:30:16.546: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0715 13:30:17.124319      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:18.124684      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/15/23 13:30:18.556
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 13:30:18.566
  E0715 13:30:19.125000      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:30:19.566: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 15 13:30:19.571: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7614-crds.webhook.example.com via the AdmissionRegistration API @ 07/15/23 13:30:20.083
  Jul 15 13:30:20.098: INFO: Waiting for webhook configuration to be ready...
  E0715 13:30:20.125450      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource that should be mutated by the webhook @ 07/15/23 13:30:20.206
  E0715 13:30:21.126241      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:22.126287      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:30:22.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8140" for this suite. @ 07/15/23 13:30:22.795
  STEP: Destroying namespace "webhook-markers-5590" for this suite. @ 07/15/23 13:30:22.801
• [6.511 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 07/15/23 13:30:22.807
  Jul 15 13:30:22.807: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename endpointslice @ 07/15/23 13:30:22.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:30:22.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:30:22.822
  Jul 15 13:30:22.832: INFO: Endpoints addresses: [172.31.2.164 172.31.95.215] , ports: [6443]
  Jul 15 13:30:22.832: INFO: EndpointSlices addresses: [172.31.2.164 172.31.95.215] , ports: [6443]
  Jul 15 13:30:22.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4267" for this suite. @ 07/15/23 13:30:22.837
• [0.036 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 07/15/23 13:30:22.844
  Jul 15 13:30:22.844: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 13:30:22.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:30:22.855
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:30:22.858
  STEP: Creating projection with secret that has name projected-secret-test-5f9b76c0-35e9-4273-839b-e298550f24fe @ 07/15/23 13:30:22.86
  STEP: Creating a pod to test consume secrets @ 07/15/23 13:30:22.864
  E0715 13:30:23.126886      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:24.127010      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:30:24.883
  Jul 15 13:30:24.886: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-projected-secrets-b2c89fa6-fce3-4ff6-8782-115142b1970e container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/15/23 13:30:24.893
  Jul 15 13:30:24.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2995" for this suite. @ 07/15/23 13:30:24.91
• [2.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 07/15/23 13:30:24.916
  Jul 15 13:30:24.916: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename kubectl @ 07/15/23 13:30:24.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:30:24.929
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:30:24.931
  Jul 15 13:30:24.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-608581196 --namespace=kubectl-8329 version'
  Jul 15 13:30:24.972: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Jul 15 13:30:24.972: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.3\", GitCommit:\"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:53:42Z\", GoVersion:\"go1.20.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.3\", GitCommit:\"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", GitTreeState:\"clean\", BuildDate:\"2023-06-15T02:06:40Z\", GoVersion:\"go1.20.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Jul 15 13:30:24.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8329" for this suite. @ 07/15/23 13:30:24.976
• [0.067 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 07/15/23 13:30:24.982
  Jul 15 13:30:24.982: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir @ 07/15/23 13:30:24.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:30:24.995
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:30:24.997
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 07/15/23 13:30:25
  E0715 13:30:25.127934      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:26.128659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:27.129335      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:28.129421      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:30:29.025
  Jul 15 13:30:29.028: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-c824a9ce-0392-4dd4-9450-15bcf85fec09 container test-container: <nil>
  STEP: delete the pod @ 07/15/23 13:30:29.035
  Jul 15 13:30:29.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4072" for this suite. @ 07/15/23 13:30:29.057
• [4.081 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 07/15/23 13:30:29.064
  Jul 15 13:30:29.064: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename projected @ 07/15/23 13:30:29.065
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:30:29.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:30:29.077
  STEP: Creating configMap with name projected-configmap-test-volume-map-341ac54f-7abc-4dee-a07c-d2353040d806 @ 07/15/23 13:30:29.082
  STEP: Creating a pod to test consume configMaps @ 07/15/23 13:30:29.086
  E0715 13:30:29.130251      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:30.130606      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:31.131021      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:32.132086      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:30:33.104
  Jul 15 13:30:33.108: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-projected-configmaps-b6d1e03d-631d-4054-b42e-7aeb87da337c container agnhost-container: <nil>
  STEP: delete the pod @ 07/15/23 13:30:33.114
  Jul 15 13:30:33.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1774" for this suite. @ 07/15/23 13:30:33.13
  E0715 13:30:33.132703      23 retrywatcher.go:130] "Watch failed" err="context canceled"
• [4.071 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 07/15/23 13:30:33.136
  Jul 15 13:30:33.136: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename emptydir @ 07/15/23 13:30:33.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:30:33.15
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:30:33.152
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 07/15/23 13:30:33.156
  E0715 13:30:34.133150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:35.133792      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:36.134761      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:37.134960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/15/23 13:30:37.18
  Jul 15 13:30:37.183: INFO: Trying to get logs from node ip-172-31-16-190 pod pod-28428f95-7a39-4442-a636-c064fce0764d container test-container: <nil>
  STEP: delete the pod @ 07/15/23 13:30:37.19
  Jul 15 13:30:37.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7637" for this suite. @ 07/15/23 13:30:37.209
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 07/15/23 13:30:37.218
  Jul 15 13:30:37.218: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename sched-pred @ 07/15/23 13:30:37.218
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:30:37.231
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:30:37.234
  Jul 15 13:30:37.236: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 15 13:30:37.243: INFO: Waiting for terminating namespaces to be deleted...
  Jul 15 13:30:37.246: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-16-190 before test
  Jul 15 13:30:37.249: INFO: nginx-ingress-controller-kubernetes-worker-rg5kx from ingress-nginx-kubernetes-worker started at 2023-07-15 13:00:41 +0000 UTC (1 container statuses recorded)
  Jul 15 13:30:37.249: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 15 13:30:37.249: INFO: sonobuoy-systemd-logs-daemon-set-ff6bac3670ee4930-cffz4 from sonobuoy started at 2023-07-15 11:58:26 +0000 UTC (2 container statuses recorded)
  Jul 15 13:30:37.249: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 15 13:30:37.249: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 15 13:30:37.249: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-42-138 before test
  Jul 15 13:30:37.255: INFO: nginx-ingress-controller-kubernetes-worker-7nm9g from ingress-nginx-kubernetes-worker started at 2023-07-15 11:50:47 +0000 UTC (1 container statuses recorded)
  Jul 15 13:30:37.255: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 15 13:30:37.255: INFO: calico-kube-controllers-7d466d5f7-7h6g9 from kube-system started at 2023-07-15 13:00:16 +0000 UTC (1 container statuses recorded)
  Jul 15 13:30:37.255: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul 15 13:30:37.255: INFO: sonobuoy from sonobuoy started at 2023-07-15 11:58:24 +0000 UTC (1 container statuses recorded)
  Jul 15 13:30:37.255: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 15 13:30:37.255: INFO: sonobuoy-e2e-job-7bfe7aae7c0f4f05 from sonobuoy started at 2023-07-15 11:58:25 +0000 UTC (2 container statuses recorded)
  Jul 15 13:30:37.255: INFO: 	Container e2e ready: true, restart count 0
  Jul 15 13:30:37.255: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 15 13:30:37.255: INFO: sonobuoy-systemd-logs-daemon-set-ff6bac3670ee4930-rmnwj from sonobuoy started at 2023-07-15 11:58:26 +0000 UTC (2 container statuses recorded)
  Jul 15 13:30:37.255: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 15 13:30:37.255: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 15 13:30:37.255: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-84-236 before test
  Jul 15 13:30:37.259: INFO: default-http-backend-kubernetes-worker-65fc475d49-z5npc from ingress-nginx-kubernetes-worker started at 2023-07-15 11:48:26 +0000 UTC (1 container statuses recorded)
  Jul 15 13:30:37.259: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul 15 13:30:37.259: INFO: nginx-ingress-controller-kubernetes-worker-r7qgp from ingress-nginx-kubernetes-worker started at 2023-07-15 11:48:26 +0000 UTC (1 container statuses recorded)
  Jul 15 13:30:37.259: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 15 13:30:37.259: INFO: coredns-5c7f76ccb8-cn56l from kube-system started at 2023-07-15 11:48:22 +0000 UTC (1 container statuses recorded)
  Jul 15 13:30:37.259: INFO: 	Container coredns ready: true, restart count 0
  Jul 15 13:30:37.259: INFO: kube-state-metrics-5b95b4459c-dcf9f from kube-system started at 2023-07-15 11:48:22 +0000 UTC (1 container statuses recorded)
  Jul 15 13:30:37.259: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul 15 13:30:37.259: INFO: metrics-server-v0.5.2-6cf8c8b69c-kdwqb from kube-system started at 2023-07-15 11:48:22 +0000 UTC (2 container statuses recorded)
  Jul 15 13:30:37.259: INFO: 	Container metrics-server ready: true, restart count 0
  Jul 15 13:30:37.259: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul 15 13:30:37.259: INFO: dashboard-metrics-scraper-6b8586b5c9-h8plq from kubernetes-dashboard started at 2023-07-15 11:48:22 +0000 UTC (1 container statuses recorded)
  Jul 15 13:30:37.259: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul 15 13:30:37.259: INFO: kubernetes-dashboard-6869f4cd5f-2knpl from kubernetes-dashboard started at 2023-07-15 11:48:22 +0000 UTC (1 container statuses recorded)
  Jul 15 13:30:37.259: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul 15 13:30:37.259: INFO: sonobuoy-systemd-logs-daemon-set-ff6bac3670ee4930-k229l from sonobuoy started at 2023-07-15 11:58:26 +0000 UTC (2 container statuses recorded)
  Jul 15 13:30:37.259: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 15 13:30:37.259: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/15/23 13:30:37.259
  E0715 13:30:38.135156      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:39.135192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/15/23 13:30:39.28
  STEP: Trying to apply a random label on the found node. @ 07/15/23 13:30:39.293
  STEP: verifying the node has the label kubernetes.io/e2e-9f9294f4-a2fb-4e86-9e7e-87d7eae6ffff 95 @ 07/15/23 13:30:39.3
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 07/15/23 13:30:39.304
  E0715 13:30:40.135826      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:41.136110      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.16.190 on the node which pod4 resides and expect not scheduled @ 07/15/23 13:30:41.32
  E0715 13:30:42.136239      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:43.136545      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:44.137091      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:45.137643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:46.138514      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:47.138717      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:48.139641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:49.139914      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:50.140247      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:51.140537      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:52.140712      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:53.140851      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:54.140906      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:55.141496      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:56.142269      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:57.142470      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:58.143244      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:30:59.143521      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:00.143653      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:01.143954      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:02.144972      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:03.145065      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:04.145185      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:05.145344      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:06.145988      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:07.146213      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:08.146353      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:09.146587      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:10.146738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:11.147003      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:12.147114      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:13.147234      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:14.148155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:15.148713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:16.149077      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:17.149080      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:18.149477      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:19.149640      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:20.149989      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:21.150106      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:22.151103      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:23.151363      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:24.151745      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:25.152422      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:26.152499      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:27.152718      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:28.153581      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:29.154187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:30.154605      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:31.154874      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:32.154999      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:33.155305      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:34.155837      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:35.156510      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:36.157504      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:37.157722      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:38.158459      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:39.158594      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:40.159601      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:41.159731      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:42.160059      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:43.160314      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:44.160832      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:45.161597      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:46.162224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:47.162505      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:48.163367      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:49.163618      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:50.164678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:51.164979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:52.165741      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:53.166211      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:54.166763      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:55.167369      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:56.167529      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:57.167688      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:58.167904      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:31:59.167992      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:00.168362      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:01.168633      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:02.168830      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:03.169062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:04.170138      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:05.170715      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:06.170703      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:07.171000      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:08.171016      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:09.171150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:10.171873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:11.171985      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:12.172362      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:13.172485      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:14.173008      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:15.173665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:16.173861      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:17.174198      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:18.174781      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:19.175034      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:20.175362      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:21.175624      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:22.175977      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:23.176212      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:24.176502      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:25.177379      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:26.178149      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:27.178315      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:28.179189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:29.179378      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:30.179599      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:31.179779      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:32.179832      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:33.179948      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:34.180876      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:35.181523      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:36.182196      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:37.182425      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:38.183447      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:39.183629      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:40.184056      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:41.184117      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:42.184216      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:43.184392      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:44.185444      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:45.185894      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:46.185924      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:47.186158      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:48.187065      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:49.187302      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:50.187902      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:51.188140      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:52.189031      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:53.189070      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:54.189962      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:55.190645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:56.191424      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:57.191580      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:58.191647      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:32:59.191833      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:00.191947      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:01.192388      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:02.192567      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:03.192961      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:04.193678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:05.194154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:06.194262      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:07.194540      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:08.194677      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:09.194705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:10.195396      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:11.196067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:12.196348      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:13.196629      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:14.197032      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:15.197538      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:16.198178      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:17.198327      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:18.198432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:19.198568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:20.199077      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:21.199293      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:22.199906      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:23.200115      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:24.200309      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:25.201066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:26.202062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:27.202301      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:28.202395      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:29.202719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:30.203522      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:31.203645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:32.204040      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:33.204385      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:34.204655      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:35.205614      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:36.206144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:37.207125      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:38.207232      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:39.207538      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:40.207531      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:41.207771      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:42.207983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:43.208247      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:44.208518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:45.208983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:46.210015      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:47.210137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:48.210788      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:49.211406      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:50.211502      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:51.211702      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:52.212063      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:53.212189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:54.212332      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:55.212581      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:56.213500      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:57.213668      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:58.214133      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:33:59.214256      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:00.215122      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:01.215276      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:02.216315      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:03.216398      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:04.216718      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:05.217615      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:06.218431      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:07.218683      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:08.219427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:09.219724      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:10.219870      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:11.220122      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:12.220773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:13.221003      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:14.221180      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:15.222145      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:16.222923      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:17.223192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:18.223332      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:19.223511      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:20.224149      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:21.224368      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:22.225146      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:23.226223      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:24.226607      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:25.227162      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:26.227273      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:27.227466      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:28.228079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:29.228296      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:30.228465      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:31.228653      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:32.229712      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:33.229896      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:34.230701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:35.231182      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:36.232127      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:37.232279      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:38.232827      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:39.233953      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:40.234043      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:41.234157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:42.234956      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:43.235234      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:44.235486      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:45.235831      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:46.235984      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:47.236131      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:48.236667      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:49.237684      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:50.237965      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:51.238166      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:52.238294      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:53.239080      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:54.239288      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:55.240025      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:56.240063      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:57.240366      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:58.241066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:34:59.242209      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:00.242593      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:01.242837      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:02.242920      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:03.243184      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:04.243205      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:05.243795      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:06.244028      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:07.244342      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:08.244398      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:09.244690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:10.245336      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:11.245506      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:12.245750      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:13.246219      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:14.247225      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:15.247770      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:16.247876      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:17.248165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:18.248728      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:19.249023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:20.249065      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:21.249227      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:22.250222      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:23.250577      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:24.250772      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:25.250833      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:26.251490      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:27.251748      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:28.252726      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:29.253183      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:30.253967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:31.254217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:32.254525      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:33.254880      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:34.254907      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:35.255464      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:36.255548      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:37.255594      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:38.256514      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:39.256778      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:40.257435      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:41.258222      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-9f9294f4-a2fb-4e86-9e7e-87d7eae6ffff off the node ip-172-31-16-190 @ 07/15/23 13:35:41.328
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-9f9294f4-a2fb-4e86-9e7e-87d7eae6ffff @ 07/15/23 13:35:41.341
  Jul 15 13:35:41.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-7554" for this suite. @ 07/15/23 13:35:41.351
• [304.140 seconds]
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 07/15/23 13:35:41.358
  Jul 15 13:35:41.358: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename dns @ 07/15/23 13:35:41.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:35:41.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:35:41.371
  STEP: Creating a test headless service @ 07/15/23 13:35:41.374
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9127.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9127.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9127.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9127.svc.cluster.local;sleep 1; done
   @ 07/15/23 13:35:41.379
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9127.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9127.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9127.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9127.svc.cluster.local;sleep 1; done
   @ 07/15/23 13:35:41.379
  STEP: creating a pod to probe DNS @ 07/15/23 13:35:41.379
  STEP: submitting the pod to kubernetes @ 07/15/23 13:35:41.379
  E0715 13:35:42.258652      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:43.259088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/15/23 13:35:43.397
  STEP: looking for the results for each expected name from probers @ 07/15/23 13:35:43.4
  Jul 15 13:35:43.406: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9127.svc.cluster.local from pod dns-9127/dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a: the server could not find the requested resource (get pods dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a)
  Jul 15 13:35:43.409: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9127.svc.cluster.local from pod dns-9127/dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a: the server could not find the requested resource (get pods dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a)
  Jul 15 13:35:43.413: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9127.svc.cluster.local from pod dns-9127/dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a: the server could not find the requested resource (get pods dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a)
  Jul 15 13:35:43.417: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9127.svc.cluster.local from pod dns-9127/dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a: the server could not find the requested resource (get pods dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a)
  Jul 15 13:35:43.420: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9127.svc.cluster.local from pod dns-9127/dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a: the server could not find the requested resource (get pods dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a)
  Jul 15 13:35:43.424: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9127.svc.cluster.local from pod dns-9127/dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a: the server could not find the requested resource (get pods dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a)
  Jul 15 13:35:43.428: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9127.svc.cluster.local from pod dns-9127/dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a: the server could not find the requested resource (get pods dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a)
  Jul 15 13:35:43.432: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9127.svc.cluster.local from pod dns-9127/dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a: the server could not find the requested resource (get pods dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a)
  Jul 15 13:35:43.432: INFO: Lookups using dns-9127/dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9127.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9127.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9127.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9127.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9127.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9127.svc.cluster.local jessie_udp@dns-test-service-2.dns-9127.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9127.svc.cluster.local]

  E0715 13:35:44.259751      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:45.260276      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:46.260410      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:47.261162      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:48.262158      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:35:48.463: INFO: DNS probes using dns-9127/dns-test-a95f9aaa-72ee-4e0d-943b-7c86a2e9f14a succeeded

  Jul 15 13:35:48.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/15/23 13:35:48.466
  STEP: deleting the test headless service @ 07/15/23 13:35:48.481
  STEP: Destroying namespace "dns-9127" for this suite. @ 07/15/23 13:35:48.494
• [7.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 07/15/23 13:35:48.503
  Jul 15 13:35:48.503: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 07/15/23 13:35:48.503
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:35:48.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:35:48.516
  STEP: Setting up the test @ 07/15/23 13:35:48.519
  STEP: Creating hostNetwork=false pod @ 07/15/23 13:35:48.519
  E0715 13:35:49.262810      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:50.263706      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 07/15/23 13:35:50.543
  E0715 13:35:51.264081      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:52.264367      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 07/15/23 13:35:52.562
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 07/15/23 13:35:52.562
  Jul 15 13:35:52.562: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5045 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:35:52.562: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:35:52.563: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:35:52.563: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5045/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 15 13:35:52.628: INFO: Exec stderr: ""
  Jul 15 13:35:52.628: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5045 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:35:52.628: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:35:52.629: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:35:52.629: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5045/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 15 13:35:52.688: INFO: Exec stderr: ""
  Jul 15 13:35:52.688: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5045 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:35:52.688: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:35:52.689: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:35:52.689: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5045/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 15 13:35:52.725: INFO: Exec stderr: ""
  Jul 15 13:35:52.725: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5045 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:35:52.725: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:35:52.725: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:35:52.725: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5045/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 15 13:35:52.780: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 07/15/23 13:35:52.78
  Jul 15 13:35:52.780: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5045 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:35:52.780: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:35:52.780: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:35:52.780: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5045/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jul 15 13:35:52.816: INFO: Exec stderr: ""
  Jul 15 13:35:52.816: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5045 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:35:52.816: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:35:52.816: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:35:52.816: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5045/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jul 15 13:35:52.884: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 07/15/23 13:35:52.884
  Jul 15 13:35:52.884: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5045 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:35:52.884: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:35:52.884: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:35:52.884: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5045/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 15 13:35:52.917: INFO: Exec stderr: ""
  Jul 15 13:35:52.917: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5045 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:35:52.917: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:35:52.918: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:35:52.918: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5045/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 15 13:35:52.972: INFO: Exec stderr: ""
  Jul 15 13:35:52.972: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5045 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:35:52.972: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:35:52.972: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:35:52.972: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5045/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 15 13:35:53.011: INFO: Exec stderr: ""
  Jul 15 13:35:53.011: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5045 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 15 13:35:53.011: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:35:53.011: INFO: ExecWithOptions: Clientset creation
  Jul 15 13:35:53.011: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5045/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 15 13:35:53.064: INFO: Exec stderr: ""
  Jul 15 13:35:53.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-5045" for this suite. @ 07/15/23 13:35:53.069
• [4.574 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 07/15/23 13:35:53.079
  Jul 15 13:35:53.079: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename init-container @ 07/15/23 13:35:53.079
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:35:53.091
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:35:53.093
  STEP: creating the pod @ 07/15/23 13:35:53.096
  Jul 15 13:35:53.096: INFO: PodSpec: initContainers in spec.initContainers
  E0715 13:35:53.264650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:54.265563      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:55.266570      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:56.266882      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:57.267000      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:58.267726      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:35:59.267982      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:00.269080      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:01.269071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:02.269164      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:03.270152      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:04.270832      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:05.271344      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:06.272109      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:07.273014      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:08.273183      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:09.274181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:10.275005      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:11.275630      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:12.275917      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:13.276847      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:14.277089      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:15.277622      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:16.278135      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:17.278332      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:18.278524      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:19.279218      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:20.280339      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:21.280611      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:22.280841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:23.281892      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:24.282078      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:25.282452      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:26.282559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:27.282747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:28.282904      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:29.283094      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:30.283330      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:31.283440      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:32.283630      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:33.283741      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:34.283916      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:35.284533      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:36.284732      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:36:37.014: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8f0cc691-01f9-4064-b6d2-d4ab498d0000", GenerateName:"", Namespace:"init-container-6484", SelfLink:"", UID:"4d962d33-06be-49ec-a731-3d48ec00d20a", ResourceVersion:"43758", Generation:0, CreationTimestamp:time.Date(2023, time.July, 15, 13, 35, 53, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"96383764"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 15, 13, 35, 53, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000dff968), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 15, 13, 36, 37, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000dffa10), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-pnjwc", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003b43160), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pnjwc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pnjwc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pnjwc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004452e68), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-42-138", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0004711f0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004452ef0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004452f10)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004452f18), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004452f1c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004d01580), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 15, 13, 35, 53, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 15, 13, 35, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 15, 13, 35, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 15, 13, 35, 53, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.42.138", PodIP:"192.168.191.237", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.191.237"}}, StartTime:time.Date(2023, time.July, 15, 13, 35, 53, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0004715e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000471650)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://302bece47921535cd2ca76f4f03fdcd385dc086c2e1d2e55cdbb5beb31c88523", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003b431e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003b431c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004452f94), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Jul 15 13:36:37.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6484" for this suite. @ 07/15/23 13:36:37.02
• [43.949 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 07/15/23 13:36:37.028
  Jul 15 13:36:37.028: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename webhook @ 07/15/23 13:36:37.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:36:37.04
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:36:37.043
  STEP: Setting up server cert @ 07/15/23 13:36:37.063
  E0715 13:36:37.285432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/15/23 13:36:37.318
  STEP: Deploying the webhook pod @ 07/15/23 13:36:37.326
  STEP: Wait for the deployment to be ready @ 07/15/23 13:36:37.336
  Jul 15 13:36:37.343: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0715 13:36:38.286163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:39.286365      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/15/23 13:36:39.356
  STEP: Verifying the service has paired with the endpoint @ 07/15/23 13:36:39.366
  E0715 13:36:40.286970      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:36:40.367: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 07/15/23 13:36:40.37
  STEP: Creating a custom resource definition that should be denied by the webhook @ 07/15/23 13:36:40.384
  Jul 15 13:36:40.384: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  Jul 15 13:36:40.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6795" for this suite. @ 07/15/23 13:36:40.439
  STEP: Destroying namespace "webhook-markers-9138" for this suite. @ 07/15/23 13:36:40.445
• [3.425 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 07/15/23 13:36:40.454
  Jul 15 13:36:40.454: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/15/23 13:36:40.455
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:36:40.466
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:36:40.468
  Jul 15 13:36:40.471: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  E0715 13:36:41.287755      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:42.287875      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0715 13:36:43.289002      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 15 13:36:43.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4380" for this suite. @ 07/15/23 13:36:43.555
• [3.109 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 07/15/23 13:36:43.563
  Jul 15 13:36:43.563: INFO: >>> kubeConfig: /tmp/kubeconfig-608581196
  STEP: Building a namespace api object, basename configmap @ 07/15/23 13:36:43.564
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/15/23 13:36:43.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/15/23 13:36:43.577
  STEP: creating a ConfigMap @ 07/15/23 13:36:43.58
  STEP: fetching the ConfigMap @ 07/15/23 13:36:43.584
  STEP: patching the ConfigMap @ 07/15/23 13:36:43.587
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 07/15/23 13:36:43.591
  STEP: deleting the ConfigMap by collection with a label selector @ 07/15/23 13:36:43.595
  STEP: listing all ConfigMaps in test namespace @ 07/15/23 13:36:43.603
  Jul 15 13:36:43.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3043" for this suite. @ 07/15/23 13:36:43.61
• [0.054 seconds]
------------------------------
SSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Jul 15 13:36:43.617: INFO: Running AfterSuite actions on node 1
  Jul 15 13:36:43.617: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.032 seconds]
------------------------------

Ran 378 of 7207 Specs in 5888.545 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h38m8.753786872s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

