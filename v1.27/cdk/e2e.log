  I0617 12:02:40.775433      19 e2e.go:117] Starting e2e run "0dfee738-58d2-42c6-8483-6cb3be66b003" on Ginkgo node 1
  Jun 17 12:02:40.802: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1687003360 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Jun 17 12:02:41.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:02:41.150: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Jun 17 12:02:41.205: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Jun 17 12:02:41.210: INFO: e2e test version: v1.27.3
  Jun 17 12:02:41.212: INFO: kube-apiserver version: v1.27.3
  Jun 17 12:02:41.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:02:41.219: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.077 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 06/17/23 12:02:41.801
  Jun 17 12:02:41.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 12:02:41.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:02:41.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:02:41.841
  STEP: Setting up server cert @ 06/17/23 12:02:41.893
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 12:02:42.702
  STEP: Deploying the webhook pod @ 06/17/23 12:02:42.715
  STEP: Wait for the deployment to be ready @ 06/17/23 12:02:42.731
  Jun 17 12:02:42.739: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  Jun 17 12:02:44.752: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 17, 12, 2, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 12, 2, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 12, 2, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 12, 2, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 06/17/23 12:02:46.757
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 12:02:46.775
  Jun 17 12:02:47.775: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 06/17/23 12:02:47.78
  STEP: create a namespace for the webhook @ 06/17/23 12:02:47.799
  STEP: create a configmap should be unconditionally rejected by the webhook @ 06/17/23 12:02:47.819
  Jun 17 12:02:47.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8003" for this suite. @ 06/17/23 12:02:48.018
  STEP: Destroying namespace "webhook-markers-3932" for this suite. @ 06/17/23 12:02:48.042
  STEP: Destroying namespace "fail-closed-namespace-1170" for this suite. @ 06/17/23 12:02:48.051
• [6.261 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 06/17/23 12:02:48.062
  Jun 17 12:02:48.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename field-validation @ 06/17/23 12:02:48.063
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:02:48.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:02:48.096
  Jun 17 12:02:48.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  W0617 12:02:50.684278      19 warnings.go:70] unknown field "alpha"
  W0617 12:02:50.684299      19 warnings.go:70] unknown field "beta"
  W0617 12:02:50.684305      19 warnings.go:70] unknown field "delta"
  W0617 12:02:50.684311      19 warnings.go:70] unknown field "epsilon"
  W0617 12:02:50.684317      19 warnings.go:70] unknown field "gamma"
  Jun 17 12:02:50.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-503" for this suite. @ 06/17/23 12:02:50.719
• [2.663 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 06/17/23 12:02:50.725
  Jun 17 12:02:50.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename resourcequota @ 06/17/23 12:02:50.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:02:50.744
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:02:50.747
  STEP: Counting existing ResourceQuota @ 06/17/23 12:02:50.751
  STEP: Creating a ResourceQuota @ 06/17/23 12:02:55.755
  STEP: Ensuring resource quota status is calculated @ 06/17/23 12:02:55.762
  STEP: Creating a Service @ 06/17/23 12:02:57.768
  STEP: Creating a NodePort Service @ 06/17/23 12:02:57.788
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 06/17/23 12:02:57.817
  STEP: Ensuring resource quota status captures service creation @ 06/17/23 12:02:57.841
  STEP: Deleting Services @ 06/17/23 12:02:59.847
  STEP: Ensuring resource quota status released usage @ 06/17/23 12:02:59.899
  Jun 17 12:03:01.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5495" for this suite. @ 06/17/23 12:03:01.91
• [11.194 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 06/17/23 12:03:01.92
  Jun 17 12:03:01.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename var-expansion @ 06/17/23 12:03:01.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:03:01.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:03:01.942
  Jun 17 12:03:03.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 17 12:03:03.973: INFO: Deleting pod "var-expansion-3968f33c-e270-478b-aa2f-25f2c7884bc5" in namespace "var-expansion-7915"
  Jun 17 12:03:03.981: INFO: Wait up to 5m0s for pod "var-expansion-3968f33c-e270-478b-aa2f-25f2c7884bc5" to be fully deleted
  STEP: Destroying namespace "var-expansion-7915" for this suite. @ 06/17/23 12:03:05.992
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 06/17/23 12:03:06.005
  Jun 17 12:03:06.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename job @ 06/17/23 12:03:06.005
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:03:06.022
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:03:06.026
  STEP: Creating a suspended job @ 06/17/23 12:03:06.033
  STEP: Patching the Job @ 06/17/23 12:03:06.04
  STEP: Watching for Job to be patched @ 06/17/23 12:03:06.05
  Jun 17 12:03:06.053: INFO: Event ADDED observed for Job e2e-8qtcz in namespace job-9270 with labels: map[e2e-job-label:e2e-8qtcz] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jun 17 12:03:06.053: INFO: Event MODIFIED found for Job e2e-8qtcz in namespace job-9270 with labels: map[e2e-8qtcz:patched e2e-job-label:e2e-8qtcz] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 06/17/23 12:03:06.053
  STEP: Watching for Job to be updated @ 06/17/23 12:03:06.09
  Jun 17 12:03:06.092: INFO: Event MODIFIED found for Job e2e-8qtcz in namespace job-9270 with labels: map[e2e-8qtcz:patched e2e-job-label:e2e-8qtcz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 17 12:03:06.092: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 06/17/23 12:03:06.092
  Jun 17 12:03:06.098: INFO: Job: e2e-8qtcz as labels: map[e2e-8qtcz:patched e2e-job-label:e2e-8qtcz]
  STEP: Waiting for job to complete @ 06/17/23 12:03:06.098
  STEP: Delete a job collection with a labelselector @ 06/17/23 12:03:16.103
  STEP: Watching for Job to be deleted @ 06/17/23 12:03:16.113
  Jun 17 12:03:16.115: INFO: Event MODIFIED observed for Job e2e-8qtcz in namespace job-9270 with labels: map[e2e-8qtcz:patched e2e-job-label:e2e-8qtcz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 17 12:03:16.115: INFO: Event MODIFIED observed for Job e2e-8qtcz in namespace job-9270 with labels: map[e2e-8qtcz:patched e2e-job-label:e2e-8qtcz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 17 12:03:16.115: INFO: Event MODIFIED observed for Job e2e-8qtcz in namespace job-9270 with labels: map[e2e-8qtcz:patched e2e-job-label:e2e-8qtcz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 17 12:03:16.116: INFO: Event MODIFIED observed for Job e2e-8qtcz in namespace job-9270 with labels: map[e2e-8qtcz:patched e2e-job-label:e2e-8qtcz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 17 12:03:16.116: INFO: Event MODIFIED observed for Job e2e-8qtcz in namespace job-9270 with labels: map[e2e-8qtcz:patched e2e-job-label:e2e-8qtcz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 17 12:03:16.116: INFO: Event MODIFIED observed for Job e2e-8qtcz in namespace job-9270 with labels: map[e2e-8qtcz:patched e2e-job-label:e2e-8qtcz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 17 12:03:16.116: INFO: Event MODIFIED observed for Job e2e-8qtcz in namespace job-9270 with labels: map[e2e-8qtcz:patched e2e-job-label:e2e-8qtcz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 17 12:03:16.116: INFO: Event MODIFIED observed for Job e2e-8qtcz in namespace job-9270 with labels: map[e2e-8qtcz:patched e2e-job-label:e2e-8qtcz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 17 12:03:16.116: INFO: Event MODIFIED observed for Job e2e-8qtcz in namespace job-9270 with labels: map[e2e-8qtcz:patched e2e-job-label:e2e-8qtcz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 17 12:03:16.116: INFO: Event MODIFIED observed for Job e2e-8qtcz in namespace job-9270 with labels: map[e2e-8qtcz:patched e2e-job-label:e2e-8qtcz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 17 12:03:16.116: INFO: Event MODIFIED observed for Job e2e-8qtcz in namespace job-9270 with labels: map[e2e-8qtcz:patched e2e-job-label:e2e-8qtcz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 17 12:03:16.116: INFO: Event DELETED found for Job e2e-8qtcz in namespace job-9270 with labels: map[e2e-8qtcz:patched e2e-job-label:e2e-8qtcz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 06/17/23 12:03:16.116
  Jun 17 12:03:16.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9270" for this suite. @ 06/17/23 12:03:16.134
• [10.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 06/17/23 12:03:16.149
  Jun 17 12:03:16.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 12:03:16.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:03:16.17
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:03:16.173
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 12:03:16.177
  STEP: Saw pod success @ 06/17/23 12:03:20.204
  Jun 17 12:03:20.207: INFO: Trying to get logs from node ip-172-31-25-17 pod downwardapi-volume-26f79aba-f572-4b71-ae7e-80727045a767 container client-container: <nil>
  STEP: delete the pod @ 06/17/23 12:03:20.23
  Jun 17 12:03:20.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1637" for this suite. @ 06/17/23 12:03:20.253
• [4.111 seconds]
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 06/17/23 12:03:20.26
  Jun 17 12:03:20.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl @ 06/17/23 12:03:20.261
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:03:20.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:03:20.281
  STEP: creating all guestbook components @ 06/17/23 12:03:20.289
  Jun 17 12:03:20.289: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Jun 17 12:03:20.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7844 create -f -'
  Jun 17 12:03:21.001: INFO: stderr: ""
  Jun 17 12:03:21.001: INFO: stdout: "service/agnhost-replica created\n"
  Jun 17 12:03:21.001: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Jun 17 12:03:21.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7844 create -f -'
  Jun 17 12:03:21.292: INFO: stderr: ""
  Jun 17 12:03:21.292: INFO: stdout: "service/agnhost-primary created\n"
  Jun 17 12:03:21.292: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Jun 17 12:03:21.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7844 create -f -'
  Jun 17 12:03:21.563: INFO: stderr: ""
  Jun 17 12:03:21.563: INFO: stdout: "service/frontend created\n"
  Jun 17 12:03:21.563: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Jun 17 12:03:21.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7844 create -f -'
  Jun 17 12:03:21.812: INFO: stderr: ""
  Jun 17 12:03:21.812: INFO: stdout: "deployment.apps/frontend created\n"
  Jun 17 12:03:21.812: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jun 17 12:03:21.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7844 create -f -'
  Jun 17 12:03:22.605: INFO: stderr: ""
  Jun 17 12:03:22.605: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Jun 17 12:03:22.605: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jun 17 12:03:22.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7844 create -f -'
  Jun 17 12:03:22.872: INFO: stderr: ""
  Jun 17 12:03:22.872: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 06/17/23 12:03:22.872
  Jun 17 12:03:22.872: INFO: Waiting for all frontend pods to be Running.
  Jun 17 12:03:27.923: INFO: Waiting for frontend to serve content.
  Jun 17 12:03:27.935: INFO: Trying to add a new entry to the guestbook.
  Jun 17 12:03:27.947: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 06/17/23 12:03:27.958
  Jun 17 12:03:27.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7844 delete --grace-period=0 --force -f -'
  Jun 17 12:03:28.047: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 17 12:03:28.047: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 06/17/23 12:03:28.047
  Jun 17 12:03:28.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7844 delete --grace-period=0 --force -f -'
  Jun 17 12:03:28.138: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 17 12:03:28.138: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 06/17/23 12:03:28.138
  Jun 17 12:03:28.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7844 delete --grace-period=0 --force -f -'
  Jun 17 12:03:28.216: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 17 12:03:28.216: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 06/17/23 12:03:28.216
  Jun 17 12:03:28.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7844 delete --grace-period=0 --force -f -'
  Jun 17 12:03:28.285: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 17 12:03:28.285: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 06/17/23 12:03:28.285
  Jun 17 12:03:28.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7844 delete --grace-period=0 --force -f -'
  Jun 17 12:03:28.366: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 17 12:03:28.366: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 06/17/23 12:03:28.366
  Jun 17 12:03:28.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7844 delete --grace-period=0 --force -f -'
  Jun 17 12:03:28.444: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 17 12:03:28.444: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Jun 17 12:03:28.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7844" for this suite. @ 06/17/23 12:03:28.449
• [8.199 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 06/17/23 12:03:28.459
  Jun 17 12:03:28.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir @ 06/17/23 12:03:28.461
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:03:28.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:03:28.485
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 06/17/23 12:03:28.489
  STEP: Saw pod success @ 06/17/23 12:03:32.515
  Jun 17 12:03:32.518: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-f68d99df-9de0-41f1-a793-9989939a3edc container test-container: <nil>
  STEP: delete the pod @ 06/17/23 12:03:32.537
  Jun 17 12:03:32.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-658" for this suite. @ 06/17/23 12:03:32.565
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 06/17/23 12:03:32.575
  Jun 17 12:03:32.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir @ 06/17/23 12:03:32.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:03:32.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:03:32.594
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 06/17/23 12:03:32.598
  STEP: Saw pod success @ 06/17/23 12:03:36.633
  Jun 17 12:03:36.636: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-3aead75b-051a-4cc1-8138-8a97706b6a80 container test-container: <nil>
  STEP: delete the pod @ 06/17/23 12:03:36.644
  Jun 17 12:03:36.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9267" for this suite. @ 06/17/23 12:03:36.679
• [4.113 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 06/17/23 12:03:36.688
  Jun 17 12:03:36.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename statefulset @ 06/17/23 12:03:36.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:03:36.706
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:03:36.71
  STEP: Creating service test in namespace statefulset-983 @ 06/17/23 12:03:36.713
  STEP: Creating a new StatefulSet @ 06/17/23 12:03:36.722
  Jun 17 12:03:36.734: INFO: Found 0 stateful pods, waiting for 3
  Jun 17 12:03:46.740: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 17 12:03:46.740: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 17 12:03:46.740: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
  Jun 17 12:03:56.739: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 17 12:03:56.739: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 17 12:03:56.739: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Jun 17 12:03:56.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-983 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 17 12:03:56.887: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 17 12:03:56.887: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 17 12:03:56.887: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 06/17/23 12:04:06.906
  Jun 17 12:04:06.926: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 06/17/23 12:04:06.926
  STEP: Updating Pods in reverse ordinal order @ 06/17/23 12:04:16.944
  Jun 17 12:04:16.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-983 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 17 12:04:17.089: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 17 12:04:17.089: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 17 12:04:17.089: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  STEP: Rolling back to a previous revision @ 06/17/23 12:04:37.124
  Jun 17 12:04:37.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-983 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 17 12:04:37.247: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 17 12:04:37.247: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 17 12:04:37.247: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 17 12:04:47.287: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 06/17/23 12:04:57.314
  Jun 17 12:04:57.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-983 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 17 12:04:57.445: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 17 12:04:57.445: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 17 12:04:57.445: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 17 12:05:07.471: INFO: Deleting all statefulset in ns statefulset-983
  Jun 17 12:05:07.474: INFO: Scaling statefulset ss2 to 0
  Jun 17 12:05:17.496: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 17 12:05:17.500: INFO: Deleting statefulset ss2
  Jun 17 12:05:17.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-983" for this suite. @ 06/17/23 12:05:17.519
• [100.850 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 06/17/23 12:05:17.54
  Jun 17 12:05:17.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 12:05:17.541
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:05:17.562
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:05:17.566
  STEP: Creating configMap with name projected-configmap-test-volume-fb46c460-ed5e-4a52-8c34-4d77324fa06c @ 06/17/23 12:05:17.57
  STEP: Creating a pod to test consume configMaps @ 06/17/23 12:05:17.574
  STEP: Saw pod success @ 06/17/23 12:05:21.599
  Jun 17 12:05:21.603: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-projected-configmaps-08649233-3533-448d-86c5-97301fd1a880 container agnhost-container: <nil>
  STEP: delete the pod @ 06/17/23 12:05:21.619
  Jun 17 12:05:21.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5851" for this suite. @ 06/17/23 12:05:21.647
• [4.114 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 06/17/23 12:05:21.655
  Jun 17 12:05:21.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubelet-test @ 06/17/23 12:05:21.656
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:05:21.671
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:05:21.679
  Jun 17 12:05:23.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3411" for this suite. @ 06/17/23 12:05:23.719
• [2.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 06/17/23 12:05:23.731
  Jun 17 12:05:23.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename services @ 06/17/23 12:05:23.732
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:05:23.752
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:05:23.756
  STEP: creating a Service @ 06/17/23 12:05:23.767
  STEP: watching for the Service to be added @ 06/17/23 12:05:23.782
  Jun 17 12:05:23.784: INFO: Found Service test-service-vxmmr in namespace services-4171 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Jun 17 12:05:23.784: INFO: Service test-service-vxmmr created
  STEP: Getting /status @ 06/17/23 12:05:23.784
  Jun 17 12:05:23.790: INFO: Service test-service-vxmmr has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 06/17/23 12:05:23.79
  STEP: watching for the Service to be patched @ 06/17/23 12:05:23.799
  Jun 17 12:05:23.801: INFO: observed Service test-service-vxmmr in namespace services-4171 with annotations: map[] & LoadBalancer: {[]}
  Jun 17 12:05:23.801: INFO: Found Service test-service-vxmmr in namespace services-4171 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Jun 17 12:05:23.801: INFO: Service test-service-vxmmr has service status patched
  STEP: updating the ServiceStatus @ 06/17/23 12:05:23.801
  Jun 17 12:05:23.814: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 06/17/23 12:05:23.815
  Jun 17 12:05:23.816: INFO: Observed Service test-service-vxmmr in namespace services-4171 with annotations: map[] & Conditions: {[]}
  Jun 17 12:05:23.817: INFO: Observed event: &Service{ObjectMeta:{test-service-vxmmr  services-4171  553a473c-5c4b-4704-b9bd-1b937d643eeb 4019 0 2023-06-17 12:05:23 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-06-17 12:05:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-06-17 12:05:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.97,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.97],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Jun 17 12:05:23.817: INFO: Found Service test-service-vxmmr in namespace services-4171 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jun 17 12:05:23.817: INFO: Service test-service-vxmmr has service status updated
  STEP: patching the service @ 06/17/23 12:05:23.817
  STEP: watching for the Service to be patched @ 06/17/23 12:05:23.829
  Jun 17 12:05:23.832: INFO: observed Service test-service-vxmmr in namespace services-4171 with labels: map[test-service-static:true]
  Jun 17 12:05:23.832: INFO: observed Service test-service-vxmmr in namespace services-4171 with labels: map[test-service-static:true]
  Jun 17 12:05:23.832: INFO: observed Service test-service-vxmmr in namespace services-4171 with labels: map[test-service-static:true]
  Jun 17 12:05:23.832: INFO: Found Service test-service-vxmmr in namespace services-4171 with labels: map[test-service:patched test-service-static:true]
  Jun 17 12:05:23.832: INFO: Service test-service-vxmmr patched
  STEP: deleting the service @ 06/17/23 12:05:23.832
  STEP: watching for the Service to be deleted @ 06/17/23 12:05:23.846
  Jun 17 12:05:23.848: INFO: Observed event: ADDED
  Jun 17 12:05:23.848: INFO: Observed event: MODIFIED
  Jun 17 12:05:23.848: INFO: Observed event: MODIFIED
  Jun 17 12:05:23.848: INFO: Observed event: MODIFIED
  Jun 17 12:05:23.849: INFO: Found Service test-service-vxmmr in namespace services-4171 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Jun 17 12:05:23.849: INFO: Service test-service-vxmmr deleted
  Jun 17 12:05:23.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4171" for this suite. @ 06/17/23 12:05:23.853
• [0.129 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 06/17/23 12:05:23.861
  Jun 17 12:05:23.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename pod-network-test @ 06/17/23 12:05:23.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:05:23.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:05:23.885
  STEP: Performing setup for networking test in namespace pod-network-test-7268 @ 06/17/23 12:05:23.889
  STEP: creating a selector @ 06/17/23 12:05:23.889
  STEP: Creating the service pods in kubernetes @ 06/17/23 12:05:23.889
  Jun 17 12:05:23.889: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 06/17/23 12:05:35.984
  Jun 17 12:05:38.015: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jun 17 12:05:38.015: INFO: Breadth first check of 192.168.178.146 on host 172.31.25.17...
  Jun 17 12:05:38.020: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.75:9080/dial?request=hostname&protocol=http&host=192.168.178.146&port=8083&tries=1'] Namespace:pod-network-test-7268 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 12:05:38.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:05:38.021: INFO: ExecWithOptions: Clientset creation
  Jun 17 12:05:38.021: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-7268/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.2.75%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.178.146%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 17 12:05:38.096: INFO: Waiting for responses: map[]
  Jun 17 12:05:38.096: INFO: reached 192.168.178.146 after 0/1 tries
  Jun 17 12:05:38.096: INFO: Breadth first check of 192.168.91.139 on host 172.31.68.253...
  Jun 17 12:05:38.101: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.75:9080/dial?request=hostname&protocol=http&host=192.168.91.139&port=8083&tries=1'] Namespace:pod-network-test-7268 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 12:05:38.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:05:38.101: INFO: ExecWithOptions: Clientset creation
  Jun 17 12:05:38.101: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-7268/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.2.75%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.91.139%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 17 12:05:38.168: INFO: Waiting for responses: map[]
  Jun 17 12:05:38.168: INFO: reached 192.168.91.139 after 0/1 tries
  Jun 17 12:05:38.168: INFO: Breadth first check of 192.168.2.74 on host 172.31.86.18...
  Jun 17 12:05:38.173: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.75:9080/dial?request=hostname&protocol=http&host=192.168.2.74&port=8083&tries=1'] Namespace:pod-network-test-7268 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 12:05:38.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:05:38.173: INFO: ExecWithOptions: Clientset creation
  Jun 17 12:05:38.174: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-7268/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.2.75%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.2.74%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 17 12:05:38.235: INFO: Waiting for responses: map[]
  Jun 17 12:05:38.235: INFO: reached 192.168.2.74 after 0/1 tries
  Jun 17 12:05:38.235: INFO: Going to retry 0 out of 3 pods....
  Jun 17 12:05:38.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7268" for this suite. @ 06/17/23 12:05:38.241
• [14.387 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 06/17/23 12:05:38.25
  Jun 17 12:05:38.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl @ 06/17/23 12:05:38.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:05:38.27
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:05:38.273
  STEP: creating a replication controller @ 06/17/23 12:05:38.277
  Jun 17 12:05:38.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 create -f -'
  Jun 17 12:05:38.548: INFO: stderr: ""
  Jun 17 12:05:38.548: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 06/17/23 12:05:38.548
  Jun 17 12:05:38.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 17 12:05:38.636: INFO: stderr: ""
  Jun 17 12:05:38.636: INFO: stdout: "update-demo-nautilus-pt89x update-demo-nautilus-tgdgr "
  Jun 17 12:05:38.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods update-demo-nautilus-pt89x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 17 12:05:38.701: INFO: stderr: ""
  Jun 17 12:05:38.701: INFO: stdout: ""
  Jun 17 12:05:38.701: INFO: update-demo-nautilus-pt89x is created but not running
  Jun 17 12:05:43.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 17 12:05:43.779: INFO: stderr: ""
  Jun 17 12:05:43.779: INFO: stdout: "update-demo-nautilus-pt89x update-demo-nautilus-tgdgr "
  Jun 17 12:05:43.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods update-demo-nautilus-pt89x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 17 12:05:43.847: INFO: stderr: ""
  Jun 17 12:05:43.847: INFO: stdout: "true"
  Jun 17 12:05:43.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods update-demo-nautilus-pt89x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 17 12:05:43.915: INFO: stderr: ""
  Jun 17 12:05:43.915: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 17 12:05:43.915: INFO: validating pod update-demo-nautilus-pt89x
  Jun 17 12:05:43.922: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 17 12:05:43.923: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 17 12:05:43.923: INFO: update-demo-nautilus-pt89x is verified up and running
  Jun 17 12:05:43.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods update-demo-nautilus-tgdgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 17 12:05:43.992: INFO: stderr: ""
  Jun 17 12:05:43.992: INFO: stdout: "true"
  Jun 17 12:05:43.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods update-demo-nautilus-tgdgr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 17 12:05:44.065: INFO: stderr: ""
  Jun 17 12:05:44.065: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 17 12:05:44.065: INFO: validating pod update-demo-nautilus-tgdgr
  Jun 17 12:05:44.073: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 17 12:05:44.073: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 17 12:05:44.073: INFO: update-demo-nautilus-tgdgr is verified up and running
  STEP: scaling down the replication controller @ 06/17/23 12:05:44.073
  Jun 17 12:05:44.074: INFO: scanned /root for discovery docs: <nil>
  Jun 17 12:05:44.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  Jun 17 12:05:45.241: INFO: stderr: ""
  Jun 17 12:05:45.241: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 06/17/23 12:05:45.241
  Jun 17 12:05:45.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 17 12:05:45.340: INFO: stderr: ""
  Jun 17 12:05:45.340: INFO: stdout: "update-demo-nautilus-tgdgr "
  Jun 17 12:05:45.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods update-demo-nautilus-tgdgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 17 12:05:45.455: INFO: stderr: ""
  Jun 17 12:05:45.455: INFO: stdout: "true"
  Jun 17 12:05:45.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods update-demo-nautilus-tgdgr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 17 12:05:45.549: INFO: stderr: ""
  Jun 17 12:05:45.549: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 17 12:05:45.549: INFO: validating pod update-demo-nautilus-tgdgr
  Jun 17 12:05:45.553: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 17 12:05:45.554: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 17 12:05:45.554: INFO: update-demo-nautilus-tgdgr is verified up and running
  STEP: scaling up the replication controller @ 06/17/23 12:05:45.554
  Jun 17 12:05:45.555: INFO: scanned /root for discovery docs: <nil>
  Jun 17 12:05:45.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  Jun 17 12:05:46.703: INFO: stderr: ""
  Jun 17 12:05:46.703: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 06/17/23 12:05:46.703
  Jun 17 12:05:46.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 17 12:05:46.772: INFO: stderr: ""
  Jun 17 12:05:46.772: INFO: stdout: "update-demo-nautilus-dd4nw update-demo-nautilus-tgdgr "
  Jun 17 12:05:46.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods update-demo-nautilus-dd4nw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 17 12:05:46.839: INFO: stderr: ""
  Jun 17 12:05:46.839: INFO: stdout: ""
  Jun 17 12:05:46.839: INFO: update-demo-nautilus-dd4nw is created but not running
  Jun 17 12:05:51.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 17 12:05:51.908: INFO: stderr: ""
  Jun 17 12:05:51.908: INFO: stdout: "update-demo-nautilus-dd4nw update-demo-nautilus-tgdgr "
  Jun 17 12:05:51.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods update-demo-nautilus-dd4nw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 17 12:05:51.977: INFO: stderr: ""
  Jun 17 12:05:51.977: INFO: stdout: "true"
  Jun 17 12:05:51.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods update-demo-nautilus-dd4nw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 17 12:05:52.043: INFO: stderr: ""
  Jun 17 12:05:52.043: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 17 12:05:52.043: INFO: validating pod update-demo-nautilus-dd4nw
  Jun 17 12:05:52.050: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 17 12:05:52.050: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 17 12:05:52.050: INFO: update-demo-nautilus-dd4nw is verified up and running
  Jun 17 12:05:52.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods update-demo-nautilus-tgdgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 17 12:05:52.116: INFO: stderr: ""
  Jun 17 12:05:52.116: INFO: stdout: "true"
  Jun 17 12:05:52.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods update-demo-nautilus-tgdgr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 17 12:05:52.182: INFO: stderr: ""
  Jun 17 12:05:52.182: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 17 12:05:52.182: INFO: validating pod update-demo-nautilus-tgdgr
  Jun 17 12:05:52.188: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 17 12:05:52.188: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 17 12:05:52.188: INFO: update-demo-nautilus-tgdgr is verified up and running
  STEP: using delete to clean up resources @ 06/17/23 12:05:52.188
  Jun 17 12:05:52.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 delete --grace-period=0 --force -f -'
  Jun 17 12:05:52.254: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 17 12:05:52.254: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jun 17 12:05:52.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get rc,svc -l name=update-demo --no-headers'
  Jun 17 12:05:52.361: INFO: stderr: "No resources found in kubectl-7995 namespace.\n"
  Jun 17 12:05:52.361: INFO: stdout: ""
  Jun 17 12:05:52.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7995 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jun 17 12:05:52.463: INFO: stderr: ""
  Jun 17 12:05:52.463: INFO: stdout: ""
  Jun 17 12:05:52.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7995" for this suite. @ 06/17/23 12:05:52.468
• [14.227 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 06/17/23 12:05:52.477
  Jun 17 12:05:52.477: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename deployment @ 06/17/23 12:05:52.479
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:05:52.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:05:52.501
  Jun 17 12:05:52.513: INFO: Pod name rollover-pod: Found 0 pods out of 1
  Jun 17 12:05:57.519: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/17/23 12:05:57.519
  Jun 17 12:05:57.520: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  Jun 17 12:05:59.524: INFO: Creating deployment "test-rollover-deployment"
  Jun 17 12:05:59.533: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  Jun 17 12:06:01.541: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Jun 17 12:06:01.550: INFO: Ensure that both replica sets have 1 created replica
  Jun 17 12:06:01.557: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Jun 17 12:06:01.568: INFO: Updating deployment test-rollover-deployment
  Jun 17 12:06:01.568: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  Jun 17 12:06:03.577: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Jun 17 12:06:03.585: INFO: Make sure deployment "test-rollover-deployment" is complete
  Jun 17 12:06:03.594: INFO: all replica sets need to contain the pod-template-hash label
  Jun 17 12:06:03.594: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 12, 5, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 12, 5, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 12, 6, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 12, 5, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 17 12:06:05.603: INFO: all replica sets need to contain the pod-template-hash label
  Jun 17 12:06:05.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 12, 5, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 12, 5, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 12, 6, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 12, 5, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 17 12:06:07.603: INFO: all replica sets need to contain the pod-template-hash label
  Jun 17 12:06:07.604: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 12, 5, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 12, 5, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 12, 6, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 12, 5, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 17 12:06:09.602: INFO: all replica sets need to contain the pod-template-hash label
  Jun 17 12:06:09.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 12, 5, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 12, 5, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 12, 6, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 12, 5, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 17 12:06:11.603: INFO: all replica sets need to contain the pod-template-hash label
  Jun 17 12:06:11.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 12, 5, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 12, 5, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 12, 6, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 12, 5, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 17 12:06:13.603: INFO: 
  Jun 17 12:06:13.603: INFO: Ensure that both old replica sets have no replicas
  Jun 17 12:06:13.615: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-463  a24bf4ef-d250-46bd-808b-757dacab3f2f 4465 2 2023-06-17 12:05:59 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-17 12:06:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:06:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00418c378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-17 12:05:59 +0000 UTC,LastTransitionTime:2023-06-17 12:05:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-06-17 12:06:13 +0000 UTC,LastTransitionTime:2023-06-17 12:05:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun 17 12:06:13.619: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-463  e519679a-8cf4-4b77-89d9-d49e19bb3688 4455 2 2023-06-17 12:06:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment a24bf4ef-d250-46bd-808b-757dacab3f2f 0xc004120f57 0xc004120f58}] [] [{kube-controller-manager Update apps/v1 2023-06-17 12:06:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a24bf4ef-d250-46bd-808b-757dacab3f2f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:06:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004121008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun 17 12:06:13.619: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Jun 17 12:06:13.619: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-463  d413566e-4672-467f-8d94-bed4d77f1da3 4464 2 2023-06-17 12:05:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment a24bf4ef-d250-46bd-808b-757dacab3f2f 0xc004120e37 0xc004120e38}] [] [{e2e.test Update apps/v1 2023-06-17 12:05:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:06:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a24bf4ef-d250-46bd-808b-757dacab3f2f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:06:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004120ef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 17 12:06:13.619: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-463  c99b6fc8-152f-4385-b341-290938d6e0ee 4411 2 2023-06-17 12:05:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment a24bf4ef-d250-46bd-808b-757dacab3f2f 0xc004121067 0xc004121068}] [] [{kube-controller-manager Update apps/v1 2023-06-17 12:06:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a24bf4ef-d250-46bd-808b-757dacab3f2f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:06:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004121118 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 17 12:06:13.624: INFO: Pod "test-rollover-deployment-57777854c9-drjdw" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-drjdw test-rollover-deployment-57777854c9- deployment-463  2bdfecfd-06a2-437a-94dd-d3eac6f553b3 4431 0 2023-06-17 12:06:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 e519679a-8cf4-4b77-89d9-d49e19bb3688 0xc00418c737 0xc00418c738}] [] [{kube-controller-manager Update v1 2023-06-17 12:06:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e519679a-8cf4-4b77-89d9-d49e19bb3688\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 12:06:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kp4hf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kp4hf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:06:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:06:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:06:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:06:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.18,PodIP:192.168.2.77,StartTime:2023-06-17 12:06:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-17 12:06:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://1c95aacec7aea207fc4cf024cef0d573dd75f70ddbd32ec13a5401c624ca297a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.77,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 12:06:13.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-463" for this suite. @ 06/17/23 12:06:13.628
• [21.160 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 06/17/23 12:06:13.638
  Jun 17 12:06:13.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename cronjob @ 06/17/23 12:06:13.639
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:06:13.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:06:13.671
  STEP: Creating a cronjob @ 06/17/23 12:06:13.674
  STEP: creating @ 06/17/23 12:06:13.674
  STEP: getting @ 06/17/23 12:06:13.682
  STEP: listing @ 06/17/23 12:06:13.686
  STEP: watching @ 06/17/23 12:06:13.689
  Jun 17 12:06:13.689: INFO: starting watch
  STEP: cluster-wide listing @ 06/17/23 12:06:13.691
  STEP: cluster-wide watching @ 06/17/23 12:06:13.695
  Jun 17 12:06:13.695: INFO: starting watch
  STEP: patching @ 06/17/23 12:06:13.697
  STEP: updating @ 06/17/23 12:06:13.705
  Jun 17 12:06:13.716: INFO: waiting for watch events with expected annotations
  Jun 17 12:06:13.716: INFO: saw patched and updated annotations
  STEP: patching /status @ 06/17/23 12:06:13.716
  STEP: updating /status @ 06/17/23 12:06:13.728
  STEP: get /status @ 06/17/23 12:06:13.736
  STEP: deleting @ 06/17/23 12:06:13.74
  STEP: deleting a collection @ 06/17/23 12:06:13.755
  Jun 17 12:06:13.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8457" for this suite. @ 06/17/23 12:06:13.773
• [0.141 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 06/17/23 12:06:13.78
  Jun 17 12:06:13.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename security-context-test @ 06/17/23 12:06:13.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:06:13.799
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:06:13.803
  Jun 17 12:06:17.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8082" for this suite. @ 06/17/23 12:06:17.844
• [4.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 06/17/23 12:06:17.856
  Jun 17 12:06:17.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename secrets @ 06/17/23 12:06:17.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:06:17.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:06:17.877
  STEP: Creating secret with name secret-test-dce0e306-77db-4994-858d-c254f266d9e7 @ 06/17/23 12:06:17.907
  STEP: Creating a pod to test consume secrets @ 06/17/23 12:06:17.912
  STEP: Saw pod success @ 06/17/23 12:06:21.935
  Jun 17 12:06:21.939: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-secrets-26fdd85d-5fce-45fb-a203-3f519c33c527 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/17/23 12:06:21.947
  Jun 17 12:06:21.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9808" for this suite. @ 06/17/23 12:06:21.97
  STEP: Destroying namespace "secret-namespace-3308" for this suite. @ 06/17/23 12:06:21.976
• [4.128 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 06/17/23 12:06:21.985
  Jun 17 12:06:21.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename security-context @ 06/17/23 12:06:21.986
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:06:22.003
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:06:22.008
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 06/17/23 12:06:22.012
  STEP: Saw pod success @ 06/17/23 12:06:26.038
  Jun 17 12:06:26.041: INFO: Trying to get logs from node ip-172-31-25-17 pod security-context-e5737bd4-cb1d-449b-814f-e7d3b7432151 container test-container: <nil>
  STEP: delete the pod @ 06/17/23 12:06:26.049
  Jun 17 12:06:26.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-4923" for this suite. @ 06/17/23 12:06:26.074
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 06/17/23 12:06:26.084
  Jun 17 12:06:26.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename runtimeclass @ 06/17/23 12:06:26.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:06:26.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:06:26.114
  Jun 17 12:06:28.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6523" for this suite. @ 06/17/23 12:06:28.158
• [2.081 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 06/17/23 12:06:28.165
  Jun 17 12:06:28.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir @ 06/17/23 12:06:28.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:06:28.185
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:06:28.188
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 06/17/23 12:06:28.192
  STEP: Saw pod success @ 06/17/23 12:06:32.225
  Jun 17 12:06:32.229: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-e5bc309e-5a65-467f-8059-b5ebfa7b7f44 container test-container: <nil>
  STEP: delete the pod @ 06/17/23 12:06:32.237
  Jun 17 12:06:32.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8306" for this suite. @ 06/17/23 12:06:32.259
• [4.102 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 06/17/23 12:06:32.269
  Jun 17 12:06:32.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename downward-api @ 06/17/23 12:06:32.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:06:32.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:06:32.291
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 12:06:32.298
  STEP: Saw pod success @ 06/17/23 12:06:36.329
  Jun 17 12:06:36.333: INFO: Trying to get logs from node ip-172-31-25-17 pod downwardapi-volume-dc358848-a6da-4656-9f64-d5b8a7c88100 container client-container: <nil>
  STEP: delete the pod @ 06/17/23 12:06:36.341
  Jun 17 12:06:36.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7577" for this suite. @ 06/17/23 12:06:36.366
• [4.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 06/17/23 12:06:36.375
  Jun 17 12:06:36.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename downward-api @ 06/17/23 12:06:36.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:06:36.394
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:06:36.397
  STEP: Creating the pod @ 06/17/23 12:06:36.402
  Jun 17 12:06:38.946: INFO: Successfully updated pod "annotationupdate704d2124-229d-4f90-87a5-86f4a8c402a6"
  Jun 17 12:06:40.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5552" for this suite. @ 06/17/23 12:06:40.973
• [4.606 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 06/17/23 12:06:40.982
  Jun 17 12:06:40.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename svcaccounts @ 06/17/23 12:06:40.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:06:41.004
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:06:41.008
  STEP: Creating a pod to test service account token:  @ 06/17/23 12:06:41.012
  STEP: Saw pod success @ 06/17/23 12:06:45.036
  Jun 17 12:06:45.041: INFO: Trying to get logs from node ip-172-31-25-17 pod test-pod-17ad2c5e-d6c9-4fec-95ae-a06ee4d84656 container agnhost-container: <nil>
  STEP: delete the pod @ 06/17/23 12:06:45.049
  Jun 17 12:06:45.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7482" for this suite. @ 06/17/23 12:06:45.072
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 06/17/23 12:06:45.083
  Jun 17 12:06:45.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename secrets @ 06/17/23 12:06:45.084
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:06:45.104
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:06:45.108
  STEP: Creating secret with name s-test-opt-del-dc6a8f1c-0410-4dc6-8e0c-cd37a1a7883e @ 06/17/23 12:06:45.115
  STEP: Creating secret with name s-test-opt-upd-88e2d761-a6c4-4f1a-a08d-95b41bf117b4 @ 06/17/23 12:06:45.12
  STEP: Creating the pod @ 06/17/23 12:06:45.125
  STEP: Deleting secret s-test-opt-del-dc6a8f1c-0410-4dc6-8e0c-cd37a1a7883e @ 06/17/23 12:06:47.196
  STEP: Updating secret s-test-opt-upd-88e2d761-a6c4-4f1a-a08d-95b41bf117b4 @ 06/17/23 12:06:47.204
  STEP: Creating secret with name s-test-opt-create-82bef9ab-ca8f-45bf-b3ed-b26fb6b49c9c @ 06/17/23 12:06:47.209
  STEP: waiting to observe update in volume @ 06/17/23 12:06:47.214
  Jun 17 12:08:07.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3809" for this suite. @ 06/17/23 12:08:07.635
• [82.560 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 06/17/23 12:08:07.643
  Jun 17 12:08:07.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename services @ 06/17/23 12:08:07.644
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:08:07.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:08:07.672
  STEP: creating service in namespace services-6136 @ 06/17/23 12:08:07.676
  STEP: creating service affinity-nodeport in namespace services-6136 @ 06/17/23 12:08:07.676
  STEP: creating replication controller affinity-nodeport in namespace services-6136 @ 06/17/23 12:08:07.697
  I0617 12:08:07.708749      19 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-6136, replica count: 3
  I0617 12:08:10.760661      19 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 17 12:08:10.773: INFO: Creating new exec pod
  Jun 17 12:08:13.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-6136 exec execpod-affinityhqbmj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Jun 17 12:08:13.931: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Jun 17 12:08:13.931: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 12:08:13.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-6136 exec execpod-affinityhqbmj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.251 80'
  Jun 17 12:08:14.066: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.251 80\nConnection to 10.152.183.251 80 port [tcp/http] succeeded!\n"
  Jun 17 12:08:14.066: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 12:08:14.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-6136 exec execpod-affinityhqbmj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.68.253 31734'
  Jun 17 12:08:14.219: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.68.253 31734\nConnection to 172.31.68.253 31734 port [tcp/*] succeeded!\n"
  Jun 17 12:08:14.219: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 12:08:14.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-6136 exec execpod-affinityhqbmj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.86.18 31734'
  Jun 17 12:08:14.342: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.86.18 31734\nConnection to 172.31.86.18 31734 port [tcp/*] succeeded!\n"
  Jun 17 12:08:14.342: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 12:08:14.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-6136 exec execpod-affinityhqbmj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.25.17:31734/ ; done'
  Jun 17 12:08:14.572: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:31734/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:31734/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:31734/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:31734/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:31734/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:31734/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:31734/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:31734/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:31734/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:31734/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:31734/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:31734/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:31734/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:31734/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:31734/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:31734/\n"
  Jun 17 12:08:14.573: INFO: stdout: "\naffinity-nodeport-fjmxd\naffinity-nodeport-fjmxd\naffinity-nodeport-fjmxd\naffinity-nodeport-fjmxd\naffinity-nodeport-fjmxd\naffinity-nodeport-fjmxd\naffinity-nodeport-fjmxd\naffinity-nodeport-fjmxd\naffinity-nodeport-fjmxd\naffinity-nodeport-fjmxd\naffinity-nodeport-fjmxd\naffinity-nodeport-fjmxd\naffinity-nodeport-fjmxd\naffinity-nodeport-fjmxd\naffinity-nodeport-fjmxd\naffinity-nodeport-fjmxd"
  Jun 17 12:08:14.573: INFO: Received response from host: affinity-nodeport-fjmxd
  Jun 17 12:08:14.573: INFO: Received response from host: affinity-nodeport-fjmxd
  Jun 17 12:08:14.573: INFO: Received response from host: affinity-nodeport-fjmxd
  Jun 17 12:08:14.573: INFO: Received response from host: affinity-nodeport-fjmxd
  Jun 17 12:08:14.573: INFO: Received response from host: affinity-nodeport-fjmxd
  Jun 17 12:08:14.573: INFO: Received response from host: affinity-nodeport-fjmxd
  Jun 17 12:08:14.573: INFO: Received response from host: affinity-nodeport-fjmxd
  Jun 17 12:08:14.573: INFO: Received response from host: affinity-nodeport-fjmxd
  Jun 17 12:08:14.573: INFO: Received response from host: affinity-nodeport-fjmxd
  Jun 17 12:08:14.573: INFO: Received response from host: affinity-nodeport-fjmxd
  Jun 17 12:08:14.573: INFO: Received response from host: affinity-nodeport-fjmxd
  Jun 17 12:08:14.573: INFO: Received response from host: affinity-nodeport-fjmxd
  Jun 17 12:08:14.573: INFO: Received response from host: affinity-nodeport-fjmxd
  Jun 17 12:08:14.573: INFO: Received response from host: affinity-nodeport-fjmxd
  Jun 17 12:08:14.573: INFO: Received response from host: affinity-nodeport-fjmxd
  Jun 17 12:08:14.573: INFO: Received response from host: affinity-nodeport-fjmxd
  Jun 17 12:08:14.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 17 12:08:14.578: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-6136, will wait for the garbage collector to delete the pods @ 06/17/23 12:08:14.594
  Jun 17 12:08:14.659: INFO: Deleting ReplicationController affinity-nodeport took: 9.369867ms
  Jun 17 12:08:14.759: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.283798ms
  STEP: Destroying namespace "services-6136" for this suite. @ 06/17/23 12:08:17.087
• [9.450 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 06/17/23 12:08:17.096
  Jun 17 12:08:17.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-probe @ 06/17/23 12:08:17.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:08:17.112
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:08:17.12
  STEP: Creating pod test-webserver-ce33e1cc-11f5-4b65-857c-87cc36e39a83 in namespace container-probe-3195 @ 06/17/23 12:08:17.123
  Jun 17 12:08:19.145: INFO: Started pod test-webserver-ce33e1cc-11f5-4b65-857c-87cc36e39a83 in namespace container-probe-3195
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/17/23 12:08:19.145
  Jun 17 12:08:19.149: INFO: Initial restart count of pod test-webserver-ce33e1cc-11f5-4b65-857c-87cc36e39a83 is 0
  Jun 17 12:12:19.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/17/23 12:12:19.767
  STEP: Destroying namespace "container-probe-3195" for this suite. @ 06/17/23 12:12:19.782
• [242.697 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 06/17/23 12:12:19.796
  Jun 17 12:12:19.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename configmap @ 06/17/23 12:12:19.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:12:19.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:12:19.823
  STEP: Creating configMap with name configmap-test-volume-map-4b6cac1f-6f3e-4e7d-863d-1fe8bc894e82 @ 06/17/23 12:12:19.826
  STEP: Creating a pod to test consume configMaps @ 06/17/23 12:12:19.831
  STEP: Saw pod success @ 06/17/23 12:12:23.854
  Jun 17 12:12:23.858: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-configmaps-3da458fd-6561-440f-810a-bb8b3c01c6a2 container agnhost-container: <nil>
  STEP: delete the pod @ 06/17/23 12:12:23.88
  Jun 17 12:12:23.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2677" for this suite. @ 06/17/23 12:12:23.902
• [4.115 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 06/17/23 12:12:23.911
  Jun 17 12:12:23.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename var-expansion @ 06/17/23 12:12:23.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:12:23.932
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:12:23.936
  STEP: Creating a pod to test substitution in volume subpath @ 06/17/23 12:12:23.94
  STEP: Saw pod success @ 06/17/23 12:12:27.965
  Jun 17 12:12:27.968: INFO: Trying to get logs from node ip-172-31-25-17 pod var-expansion-192b7939-b386-4094-af3f-eed6668687d6 container dapi-container: <nil>
  STEP: delete the pod @ 06/17/23 12:12:27.977
  Jun 17 12:12:27.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5932" for this suite. @ 06/17/23 12:12:28
• [4.097 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 06/17/23 12:12:28.009
  Jun 17 12:12:28.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename replicaset @ 06/17/23 12:12:28.01
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:12:28.028
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:12:28.032
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 06/17/23 12:12:28.035
  Jun 17 12:12:28.045: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jun 17 12:12:33.051: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/17/23 12:12:33.051
  STEP: getting scale subresource @ 06/17/23 12:12:33.051
  STEP: updating a scale subresource @ 06/17/23 12:12:33.055
  STEP: verifying the replicaset Spec.Replicas was modified @ 06/17/23 12:12:33.062
  STEP: Patch a scale subresource @ 06/17/23 12:12:33.066
  Jun 17 12:12:33.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2248" for this suite. @ 06/17/23 12:12:33.087
• [5.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 06/17/23 12:12:33.101
  Jun 17 12:12:33.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename containers @ 06/17/23 12:12:33.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:12:33.123
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:12:33.129
  STEP: Creating a pod to test override all @ 06/17/23 12:12:33.134
  STEP: Saw pod success @ 06/17/23 12:12:37.165
  Jun 17 12:12:37.170: INFO: Trying to get logs from node ip-172-31-25-17 pod client-containers-b393cc35-f1f5-410c-9ce1-a45f76911295 container agnhost-container: <nil>
  STEP: delete the pod @ 06/17/23 12:12:37.177
  Jun 17 12:12:37.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-530" for this suite. @ 06/17/23 12:12:37.203
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 06/17/23 12:12:37.217
  Jun 17 12:12:37.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 12:12:37.218
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:12:37.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:12:37.24
  STEP: Creating configMap with name projected-configmap-test-volume-map-8e5ceec3-4608-4a28-b5a7-fc522d3a89ac @ 06/17/23 12:12:37.244
  STEP: Creating a pod to test consume configMaps @ 06/17/23 12:12:37.25
  STEP: Saw pod success @ 06/17/23 12:12:41.275
  Jun 17 12:12:41.278: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-projected-configmaps-0d2cf8b6-572c-4188-bb32-730b47eaffc9 container agnhost-container: <nil>
  STEP: delete the pod @ 06/17/23 12:12:41.288
  Jun 17 12:12:41.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6483" for this suite. @ 06/17/23 12:12:41.312
• [4.104 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 06/17/23 12:12:41.321
  Jun 17 12:12:41.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename secrets @ 06/17/23 12:12:41.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:12:41.34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:12:41.343
  STEP: Creating secret with name secret-test-map-665721a2-4da2-40d8-875f-689038c691eb @ 06/17/23 12:12:41.351
  STEP: Creating a pod to test consume secrets @ 06/17/23 12:12:41.356
  STEP: Saw pod success @ 06/17/23 12:12:45.385
  Jun 17 12:12:45.388: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-secrets-3f51f04b-22bb-4d08-87fe-e2328be93c62 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/17/23 12:12:45.396
  Jun 17 12:12:45.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2958" for this suite. @ 06/17/23 12:12:45.42
• [4.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 06/17/23 12:12:45.43
  Jun 17 12:12:45.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename resourcequota @ 06/17/23 12:12:45.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:12:45.451
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:12:45.454
  STEP: Counting existing ResourceQuota @ 06/17/23 12:13:02.464
  STEP: Creating a ResourceQuota @ 06/17/23 12:13:07.468
  STEP: Ensuring resource quota status is calculated @ 06/17/23 12:13:07.473
  STEP: Creating a ConfigMap @ 06/17/23 12:13:09.478
  STEP: Ensuring resource quota status captures configMap creation @ 06/17/23 12:13:09.492
  STEP: Deleting a ConfigMap @ 06/17/23 12:13:11.497
  STEP: Ensuring resource quota status released usage @ 06/17/23 12:13:11.503
  Jun 17 12:13:13.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6733" for this suite. @ 06/17/23 12:13:13.514
• [28.091 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 06/17/23 12:13:13.521
  Jun 17 12:13:13.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename secrets @ 06/17/23 12:13:13.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:13:13.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:13:13.545
  STEP: Creating secret with name secret-test-bb3cb341-1a4c-4d88-a7f2-c15ec60b7828 @ 06/17/23 12:13:13.549
  STEP: Creating a pod to test consume secrets @ 06/17/23 12:13:13.554
  STEP: Saw pod success @ 06/17/23 12:13:17.58
  Jun 17 12:13:17.583: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-secrets-6f40fc27-f61f-4d77-adf2-1a28afc9aa0a container secret-volume-test: <nil>
  STEP: delete the pod @ 06/17/23 12:13:17.593
  Jun 17 12:13:17.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1467" for this suite. @ 06/17/23 12:13:17.617
• [4.103 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 06/17/23 12:13:17.625
  Jun 17 12:13:17.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename job @ 06/17/23 12:13:17.626
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:13:17.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:13:17.647
  STEP: Creating a job @ 06/17/23 12:13:17.654
  STEP: Ensuring active pods == parallelism @ 06/17/23 12:13:17.661
  STEP: delete a job @ 06/17/23 12:13:19.668
  STEP: deleting Job.batch foo in namespace job-9121, will wait for the garbage collector to delete the pods @ 06/17/23 12:13:19.668
  Jun 17 12:13:19.737: INFO: Deleting Job.batch foo took: 12.889441ms
  Jun 17 12:13:19.838: INFO: Terminating Job.batch foo pods took: 100.780732ms
  STEP: Ensuring job was deleted @ 06/17/23 12:13:51.138
  Jun 17 12:13:51.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9121" for this suite. @ 06/17/23 12:13:51.147
• [33.529 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 06/17/23 12:13:51.155
  Jun 17 12:13:51.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl @ 06/17/23 12:13:51.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:13:51.173
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:13:51.177
  STEP: creating a replication controller @ 06/17/23 12:13:51.181
  Jun 17 12:13:51.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-6287 create -f -'
  Jun 17 12:13:51.437: INFO: stderr: ""
  Jun 17 12:13:51.437: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 06/17/23 12:13:51.437
  Jun 17 12:13:51.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-6287 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 17 12:13:51.509: INFO: stderr: ""
  Jun 17 12:13:51.509: INFO: stdout: "update-demo-nautilus-bb59s update-demo-nautilus-z76dr "
  Jun 17 12:13:51.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-6287 get pods update-demo-nautilus-bb59s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 17 12:13:51.571: INFO: stderr: ""
  Jun 17 12:13:51.571: INFO: stdout: ""
  Jun 17 12:13:51.571: INFO: update-demo-nautilus-bb59s is created but not running
  Jun 17 12:13:56.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-6287 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 17 12:13:56.640: INFO: stderr: ""
  Jun 17 12:13:56.640: INFO: stdout: "update-demo-nautilus-bb59s update-demo-nautilus-z76dr "
  Jun 17 12:13:56.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-6287 get pods update-demo-nautilus-bb59s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 17 12:13:56.705: INFO: stderr: ""
  Jun 17 12:13:56.705: INFO: stdout: "true"
  Jun 17 12:13:56.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-6287 get pods update-demo-nautilus-bb59s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 17 12:13:56.772: INFO: stderr: ""
  Jun 17 12:13:56.772: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 17 12:13:56.772: INFO: validating pod update-demo-nautilus-bb59s
  Jun 17 12:13:56.778: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 17 12:13:56.778: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 17 12:13:56.778: INFO: update-demo-nautilus-bb59s is verified up and running
  Jun 17 12:13:56.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-6287 get pods update-demo-nautilus-z76dr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 17 12:13:56.847: INFO: stderr: ""
  Jun 17 12:13:56.847: INFO: stdout: "true"
  Jun 17 12:13:56.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-6287 get pods update-demo-nautilus-z76dr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 17 12:13:56.912: INFO: stderr: ""
  Jun 17 12:13:56.912: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 17 12:13:56.912: INFO: validating pod update-demo-nautilus-z76dr
  Jun 17 12:13:56.919: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 17 12:13:56.919: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 17 12:13:56.919: INFO: update-demo-nautilus-z76dr is verified up and running
  STEP: using delete to clean up resources @ 06/17/23 12:13:56.919
  Jun 17 12:13:56.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-6287 delete --grace-period=0 --force -f -'
  Jun 17 12:13:56.988: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 17 12:13:56.988: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jun 17 12:13:56.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-6287 get rc,svc -l name=update-demo --no-headers'
  Jun 17 12:13:57.083: INFO: stderr: "No resources found in kubectl-6287 namespace.\n"
  Jun 17 12:13:57.084: INFO: stdout: ""
  Jun 17 12:13:57.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-6287 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jun 17 12:13:57.198: INFO: stderr: ""
  Jun 17 12:13:57.198: INFO: stdout: ""
  Jun 17 12:13:57.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6287" for this suite. @ 06/17/23 12:13:57.203
• [6.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 06/17/23 12:13:57.214
  Jun 17 12:13:57.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename disruption @ 06/17/23 12:13:57.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:13:57.233
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:13:57.238
  STEP: Creating a kubernetes client @ 06/17/23 12:13:57.241
  Jun 17 12:13:57.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename disruption-2 @ 06/17/23 12:13:57.243
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:13:57.259
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:13:57.262
  STEP: Waiting for the pdb to be processed @ 06/17/23 12:13:57.278
  STEP: Waiting for the pdb to be processed @ 06/17/23 12:13:59.295
  STEP: Waiting for the pdb to be processed @ 06/17/23 12:14:01.314
  STEP: listing a collection of PDBs across all namespaces @ 06/17/23 12:14:03.326
  STEP: listing a collection of PDBs in namespace disruption-998 @ 06/17/23 12:14:03.33
  STEP: deleting a collection of PDBs @ 06/17/23 12:14:03.334
  STEP: Waiting for the PDB collection to be deleted @ 06/17/23 12:14:03.349
  Jun 17 12:14:03.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 17 12:14:03.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-3622" for this suite. @ 06/17/23 12:14:03.362
  STEP: Destroying namespace "disruption-998" for this suite. @ 06/17/23 12:14:03.37
• [6.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 06/17/23 12:14:03.381
  Jun 17 12:14:03.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename configmap @ 06/17/23 12:14:03.381
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:14:03.396
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:14:03.399
  STEP: creating a ConfigMap @ 06/17/23 12:14:03.446
  STEP: fetching the ConfigMap @ 06/17/23 12:14:03.452
  STEP: patching the ConfigMap @ 06/17/23 12:14:03.456
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 06/17/23 12:14:03.463
  STEP: deleting the ConfigMap by collection with a label selector @ 06/17/23 12:14:03.468
  STEP: listing all ConfigMaps in test namespace @ 06/17/23 12:14:03.477
  Jun 17 12:14:03.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6048" for this suite. @ 06/17/23 12:14:03.484
• [0.113 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 06/17/23 12:14:03.495
  Jun 17 12:14:03.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename replicaset @ 06/17/23 12:14:03.495
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:14:03.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:14:03.517
  STEP: Create a ReplicaSet @ 06/17/23 12:14:03.52
  STEP: Verify that the required pods have come up @ 06/17/23 12:14:03.526
  Jun 17 12:14:03.530: INFO: Pod name sample-pod: Found 0 pods out of 3
  Jun 17 12:14:08.537: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 06/17/23 12:14:08.537
  Jun 17 12:14:08.543: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 06/17/23 12:14:08.543
  STEP: DeleteCollection of the ReplicaSets @ 06/17/23 12:14:08.548
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 06/17/23 12:14:08.559
  Jun 17 12:14:08.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4127" for this suite. @ 06/17/23 12:14:08.575
• [5.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 06/17/23 12:14:08.597
  Jun 17 12:14:08.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename statefulset @ 06/17/23 12:14:08.598
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:14:08.618
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:14:08.626
  STEP: Creating service test in namespace statefulset-9864 @ 06/17/23 12:14:08.633
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 06/17/23 12:14:08.641
  STEP: Creating stateful set ss in namespace statefulset-9864 @ 06/17/23 12:14:08.653
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9864 @ 06/17/23 12:14:08.66
  Jun 17 12:14:08.664: INFO: Found 0 stateful pods, waiting for 1
  Jun 17 12:14:18.671: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 06/17/23 12:14:18.671
  Jun 17 12:14:18.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-9864 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 17 12:14:18.811: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 17 12:14:18.811: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 17 12:14:18.811: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 17 12:14:18.816: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Jun 17 12:14:28.820: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jun 17 12:14:28.820: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 17 12:14:28.840: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999761s
  Jun 17 12:14:29.844: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995611118s
  Jun 17 12:14:30.850: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990744911s
  Jun 17 12:14:31.854: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985890045s
  Jun 17 12:14:32.860: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.980245734s
  Jun 17 12:14:33.866: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974728207s
  Jun 17 12:14:34.870: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.969906353s
  Jun 17 12:14:35.875: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.964535626s
  Jun 17 12:14:36.880: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.960147009s
  Jun 17 12:14:37.886: INFO: Verifying statefulset ss doesn't scale past 1 for another 955.149118ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9864 @ 06/17/23 12:14:38.886
  Jun 17 12:14:38.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-9864 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 17 12:14:39.021: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 17 12:14:39.021: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 17 12:14:39.021: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 17 12:14:39.025: INFO: Found 1 stateful pods, waiting for 3
  Jun 17 12:14:49.032: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 17 12:14:49.032: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 17 12:14:49.032: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 06/17/23 12:14:49.032
  STEP: Scale down will halt with unhealthy stateful pod @ 06/17/23 12:14:49.032
  Jun 17 12:14:49.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-9864 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 17 12:14:49.167: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 17 12:14:49.167: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 17 12:14:49.167: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 17 12:14:49.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-9864 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 17 12:14:49.307: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 17 12:14:49.308: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 17 12:14:49.308: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 17 12:14:49.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-9864 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 17 12:14:49.442: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 17 12:14:49.442: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 17 12:14:49.442: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 17 12:14:49.442: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 17 12:14:49.447: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  Jun 17 12:14:59.460: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jun 17 12:14:59.460: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jun 17 12:14:59.460: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jun 17 12:14:59.476: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999706s
  Jun 17 12:15:00.481: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995579688s
  Jun 17 12:15:01.486: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990325012s
  Jun 17 12:15:02.492: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984005244s
  Jun 17 12:15:03.496: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979262496s
  Jun 17 12:15:04.501: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97495464s
  Jun 17 12:15:05.507: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969365517s
  Jun 17 12:15:06.512: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96386121s
  Jun 17 12:15:07.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.958859512s
  Jun 17 12:15:08.523: INFO: Verifying statefulset ss doesn't scale past 3 for another 953.021166ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9864 @ 06/17/23 12:15:09.523
  Jun 17 12:15:09.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-9864 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 17 12:15:09.662: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 17 12:15:09.662: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 17 12:15:09.662: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 17 12:15:09.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-9864 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 17 12:15:09.789: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 17 12:15:09.789: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 17 12:15:09.789: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 17 12:15:09.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-9864 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 17 12:15:09.911: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 17 12:15:09.911: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 17 12:15:09.911: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 17 12:15:09.911: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 06/17/23 12:15:19.931
  Jun 17 12:15:19.931: INFO: Deleting all statefulset in ns statefulset-9864
  Jun 17 12:15:19.934: INFO: Scaling statefulset ss to 0
  Jun 17 12:15:19.948: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 17 12:15:19.951: INFO: Deleting statefulset ss
  Jun 17 12:15:19.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9864" for this suite. @ 06/17/23 12:15:19.975
• [71.386 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 06/17/23 12:15:19.985
  Jun 17 12:15:19.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename pods @ 06/17/23 12:15:19.986
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:15:20.004
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:15:20.007
  STEP: creating the pod @ 06/17/23 12:15:20.015
  STEP: setting up watch @ 06/17/23 12:15:20.015
  STEP: submitting the pod to kubernetes @ 06/17/23 12:15:20.119
  STEP: verifying the pod is in kubernetes @ 06/17/23 12:15:20.132
  STEP: verifying pod creation was observed @ 06/17/23 12:15:20.137
  STEP: deleting the pod gracefully @ 06/17/23 12:15:22.151
  STEP: verifying pod deletion was observed @ 06/17/23 12:15:22.16
  Jun 17 12:15:23.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4457" for this suite. @ 06/17/23 12:15:23.226
• [3.250 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 06/17/23 12:15:23.236
  Jun 17 12:15:23.236: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename crd-watch @ 06/17/23 12:15:23.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:15:23.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:15:23.258
  Jun 17 12:15:23.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Creating first CR  @ 06/17/23 12:15:25.81
  Jun 17 12:15:25.816: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-17T12:15:25Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-17T12:15:25Z]] name:name1 resourceVersion:6954 uid:a1d796a3-4aa0-486a-964e-73d758898d63] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 06/17/23 12:15:35.816
  Jun 17 12:15:35.822: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-17T12:15:35Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-17T12:15:35Z]] name:name2 resourceVersion:6986 uid:bbe48da8-3e0b-4a4e-b1dc-5e7b25471153] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 06/17/23 12:15:45.823
  Jun 17 12:15:45.832: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-17T12:15:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-17T12:15:45Z]] name:name1 resourceVersion:7006 uid:a1d796a3-4aa0-486a-964e-73d758898d63] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 06/17/23 12:15:55.832
  Jun 17 12:15:55.841: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-17T12:15:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-17T12:15:55Z]] name:name2 resourceVersion:7025 uid:bbe48da8-3e0b-4a4e-b1dc-5e7b25471153] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 06/17/23 12:16:05.841
  Jun 17 12:16:05.850: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-17T12:15:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-17T12:15:45Z]] name:name1 resourceVersion:7047 uid:a1d796a3-4aa0-486a-964e-73d758898d63] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 06/17/23 12:16:15.851
  Jun 17 12:16:15.859: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-17T12:15:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-17T12:15:55Z]] name:name2 resourceVersion:7067 uid:bbe48da8-3e0b-4a4e-b1dc-5e7b25471153] num:map[num1:9223372036854775807 num2:1000000]]}
  Jun 17 12:16:26.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-5629" for this suite. @ 06/17/23 12:16:26.384
• [63.157 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 06/17/23 12:16:26.393
  Jun 17 12:16:26.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 12:16:26.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:16:26.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:16:26.418
  STEP: Setting up server cert @ 06/17/23 12:16:26.441
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 12:16:26.839
  STEP: Deploying the webhook pod @ 06/17/23 12:16:26.847
  STEP: Wait for the deployment to be ready @ 06/17/23 12:16:26.861
  Jun 17 12:16:26.870: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/17/23 12:16:28.882
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 12:16:28.895
  Jun 17 12:16:29.895: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun 17 12:16:29.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-889-crds.webhook.example.com via the AdmissionRegistration API @ 06/17/23 12:16:30.411
  STEP: Creating a custom resource that should be mutated by the webhook @ 06/17/23 12:16:30.43
  Jun 17 12:16:32.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3829" for this suite. @ 06/17/23 12:16:33.062
  STEP: Destroying namespace "webhook-markers-2710" for this suite. @ 06/17/23 12:16:33.069
• [6.684 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 06/17/23 12:16:33.08
  Jun 17 12:16:33.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename namespaces @ 06/17/23 12:16:33.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:16:33.096
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:16:33.1
  STEP: Creating namespace "e2e-ns-mbffj" @ 06/17/23 12:16:33.104
  Jun 17 12:16:33.119: INFO: Namespace "e2e-ns-mbffj-9595" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-mbffj-9595" @ 06/17/23 12:16:33.119
  Jun 17 12:16:33.127: INFO: Namespace "e2e-ns-mbffj-9595" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-mbffj-9595" @ 06/17/23 12:16:33.127
  Jun 17 12:16:33.138: INFO: Namespace "e2e-ns-mbffj-9595" has []v1.FinalizerName{"kubernetes"}
  Jun 17 12:16:33.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9858" for this suite. @ 06/17/23 12:16:33.143
  STEP: Destroying namespace "e2e-ns-mbffj-9595" for this suite. @ 06/17/23 12:16:33.15
• [0.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 06/17/23 12:16:33.16
  Jun 17 12:16:33.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename daemonsets @ 06/17/23 12:16:33.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:16:33.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:16:33.181
  Jun 17 12:16:33.208: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 06/17/23 12:16:33.215
  Jun 17 12:16:33.220: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 12:16:33.220: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 06/17/23 12:16:33.22
  Jun 17 12:16:33.243: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 12:16:33.243: INFO: Node ip-172-31-86-18 is running 0 daemon pod, expected 1
  Jun 17 12:16:34.248: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 17 12:16:34.248: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 06/17/23 12:16:34.252
  Jun 17 12:16:34.274: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 17 12:16:34.274: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  Jun 17 12:16:35.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 12:16:35.280: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 06/17/23 12:16:35.28
  Jun 17 12:16:35.294: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 12:16:35.294: INFO: Node ip-172-31-86-18 is running 0 daemon pod, expected 1
  Jun 17 12:16:36.301: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 12:16:36.301: INFO: Node ip-172-31-86-18 is running 0 daemon pod, expected 1
  Jun 17 12:16:37.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 17 12:16:37.299: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 06/17/23 12:16:37.307
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9251, will wait for the garbage collector to delete the pods @ 06/17/23 12:16:37.307
  Jun 17 12:16:37.369: INFO: Deleting DaemonSet.extensions daemon-set took: 7.022062ms
  Jun 17 12:16:37.469: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.401731ms
  Jun 17 12:16:39.175: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 12:16:39.175: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 17 12:16:39.180: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7306"},"items":null}

  Jun 17 12:16:39.184: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7306"},"items":null}

  Jun 17 12:16:39.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9251" for this suite. @ 06/17/23 12:16:39.217
• [6.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 06/17/23 12:16:39.232
  Jun 17 12:16:39.232: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-runtime @ 06/17/23 12:16:39.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:16:39.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:16:39.254
  STEP: create the container @ 06/17/23 12:16:39.258
  W0617 12:16:39.267923      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 06/17/23 12:16:39.268
  STEP: get the container status @ 06/17/23 12:16:42.292
  STEP: the container should be terminated @ 06/17/23 12:16:42.296
  STEP: the termination message should be set @ 06/17/23 12:16:42.296
  Jun 17 12:16:42.296: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 06/17/23 12:16:42.296
  Jun 17 12:16:42.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6370" for this suite. @ 06/17/23 12:16:42.323
• [3.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 06/17/23 12:16:42.332
  Jun 17 12:16:42.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename svcaccounts @ 06/17/23 12:16:42.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:16:42.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:16:42.353
  STEP: creating a ServiceAccount @ 06/17/23 12:16:42.357
  STEP: watching for the ServiceAccount to be added @ 06/17/23 12:16:42.377
  STEP: patching the ServiceAccount @ 06/17/23 12:16:42.381
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 06/17/23 12:16:42.387
  STEP: deleting the ServiceAccount @ 06/17/23 12:16:42.391
  Jun 17 12:16:42.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1466" for this suite. @ 06/17/23 12:16:42.413
• [0.092 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 06/17/23 12:16:42.424
  Jun 17 12:16:42.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir-wrapper @ 06/17/23 12:16:42.425
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:16:42.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:16:42.442
  STEP: Creating 50 configmaps @ 06/17/23 12:16:42.446
  STEP: Creating RC which spawns configmap-volume pods @ 06/17/23 12:16:42.707
  Jun 17 12:16:42.827: INFO: Pod name wrapped-volume-race-40c066a6-ed77-4e01-9f8b-13c4c23e455c: Found 3 pods out of 5
  Jun 17 12:16:47.837: INFO: Pod name wrapped-volume-race-40c066a6-ed77-4e01-9f8b-13c4c23e455c: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 06/17/23 12:16:47.837
  STEP: Creating RC which spawns configmap-volume pods @ 06/17/23 12:16:47.863
  Jun 17 12:16:47.879: INFO: Pod name wrapped-volume-race-adbde27c-d64e-4d68-8cb6-39a0e9b1ca15: Found 0 pods out of 5
  Jun 17 12:16:52.888: INFO: Pod name wrapped-volume-race-adbde27c-d64e-4d68-8cb6-39a0e9b1ca15: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 06/17/23 12:16:52.888
  STEP: Creating RC which spawns configmap-volume pods @ 06/17/23 12:16:52.913
  Jun 17 12:16:52.931: INFO: Pod name wrapped-volume-race-3e71fdd7-906a-4d93-af0e-ee811a8859ae: Found 0 pods out of 5
  Jun 17 12:16:57.940: INFO: Pod name wrapped-volume-race-3e71fdd7-906a-4d93-af0e-ee811a8859ae: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 06/17/23 12:16:57.941
  Jun 17 12:16:57.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-3e71fdd7-906a-4d93-af0e-ee811a8859ae in namespace emptydir-wrapper-8972, will wait for the garbage collector to delete the pods @ 06/17/23 12:16:57.965
  Jun 17 12:16:58.028: INFO: Deleting ReplicationController wrapped-volume-race-3e71fdd7-906a-4d93-af0e-ee811a8859ae took: 7.362394ms
  Jun 17 12:16:58.128: INFO: Terminating ReplicationController wrapped-volume-race-3e71fdd7-906a-4d93-af0e-ee811a8859ae pods took: 100.760907ms
  STEP: deleting ReplicationController wrapped-volume-race-adbde27c-d64e-4d68-8cb6-39a0e9b1ca15 in namespace emptydir-wrapper-8972, will wait for the garbage collector to delete the pods @ 06/17/23 12:17:00.63
  Jun 17 12:17:00.694: INFO: Deleting ReplicationController wrapped-volume-race-adbde27c-d64e-4d68-8cb6-39a0e9b1ca15 took: 7.843945ms
  Jun 17 12:17:00.794: INFO: Terminating ReplicationController wrapped-volume-race-adbde27c-d64e-4d68-8cb6-39a0e9b1ca15 pods took: 100.090143ms
  STEP: deleting ReplicationController wrapped-volume-race-40c066a6-ed77-4e01-9f8b-13c4c23e455c in namespace emptydir-wrapper-8972, will wait for the garbage collector to delete the pods @ 06/17/23 12:17:02.795
  Jun 17 12:17:02.858: INFO: Deleting ReplicationController wrapped-volume-race-40c066a6-ed77-4e01-9f8b-13c4c23e455c took: 7.693622ms
  Jun 17 12:17:02.959: INFO: Terminating ReplicationController wrapped-volume-race-40c066a6-ed77-4e01-9f8b-13c4c23e455c pods took: 100.392939ms
  STEP: Cleaning up the configMaps @ 06/17/23 12:17:04.86
  STEP: Destroying namespace "emptydir-wrapper-8972" for this suite. @ 06/17/23 12:17:05.204
• [22.789 seconds]
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 06/17/23 12:17:05.213
  Jun 17 12:17:05.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename services @ 06/17/23 12:17:05.214
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:17:05.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:17:05.233
  STEP: fetching services @ 06/17/23 12:17:05.237
  Jun 17 12:17:05.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3839" for this suite. @ 06/17/23 12:17:05.245
• [0.039 seconds]
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 06/17/23 12:17:05.253
  Jun 17 12:17:05.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename downward-api @ 06/17/23 12:17:05.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:17:05.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:17:05.274
  STEP: Creating a pod to test downward api env vars @ 06/17/23 12:17:05.278
  STEP: Saw pod success @ 06/17/23 12:17:09.304
  Jun 17 12:17:09.308: INFO: Trying to get logs from node ip-172-31-25-17 pod downward-api-225578f9-edff-4092-b10d-c74db1b7e87c container dapi-container: <nil>
  STEP: delete the pod @ 06/17/23 12:17:09.326
  Jun 17 12:17:09.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4197" for this suite. @ 06/17/23 12:17:09.349
• [4.103 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 06/17/23 12:17:09.356
  Jun 17 12:17:09.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 12:17:09.357
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:17:09.374
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:17:09.378
  STEP: Creating configMap with name cm-test-opt-del-073f7045-a178-4c37-bb3c-e4a11a7d3bb8 @ 06/17/23 12:17:09.387
  STEP: Creating configMap with name cm-test-opt-upd-359e6d27-aae2-4039-91b6-6c4f90fa0793 @ 06/17/23 12:17:09.393
  STEP: Creating the pod @ 06/17/23 12:17:09.398
  STEP: Deleting configmap cm-test-opt-del-073f7045-a178-4c37-bb3c-e4a11a7d3bb8 @ 06/17/23 12:17:11.444
  STEP: Updating configmap cm-test-opt-upd-359e6d27-aae2-4039-91b6-6c4f90fa0793 @ 06/17/23 12:17:11.451
  STEP: Creating configMap with name cm-test-opt-create-00837c97-d590-41dd-b443-f1e4ce7ca26f @ 06/17/23 12:17:11.457
  STEP: waiting to observe update in volume @ 06/17/23 12:17:11.461
  Jun 17 12:18:15.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2064" for this suite. @ 06/17/23 12:18:15.814
• [66.465 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 06/17/23 12:18:15.821
  Jun 17 12:18:15.821: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl @ 06/17/23 12:18:15.822
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:18:15.843
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:18:15.846
  Jun 17 12:18:15.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-1325 create -f -'
  Jun 17 12:18:16.585: INFO: stderr: ""
  Jun 17 12:18:16.585: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Jun 17 12:18:16.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-1325 create -f -'
  Jun 17 12:18:16.864: INFO: stderr: ""
  Jun 17 12:18:16.864: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 06/17/23 12:18:16.864
  Jun 17 12:18:17.869: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 17 12:18:17.869: INFO: Found 0 / 1
  Jun 17 12:18:18.870: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 17 12:18:18.870: INFO: Found 1 / 1
  Jun 17 12:18:18.870: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jun 17 12:18:18.874: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 17 12:18:18.874: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jun 17 12:18:18.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-1325 describe pod agnhost-primary-xl4rm'
  Jun 17 12:18:18.951: INFO: stderr: ""
  Jun 17 12:18:18.952: INFO: stdout: "Name:             agnhost-primary-xl4rm\nNamespace:        kubectl-1325\nPriority:         0\nService Account:  default\nNode:             ip-172-31-86-18/172.31.86.18\nStart Time:       Sat, 17 Jun 2023 12:18:16 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.2.86\nIPs:\n  IP:           192.168.2.86\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://fb0d082a018d7e479ecb713f30bd6dc34096e3091e0b4ec61c7dd1516e9216c0\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 17 Jun 2023 12:18:17 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tn5s2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-tn5s2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-1325/agnhost-primary-xl4rm to ip-172-31-86-18\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Jun 17 12:18:18.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-1325 describe rc agnhost-primary'
  Jun 17 12:18:19.035: INFO: stderr: ""
  Jun 17 12:18:19.035: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1325\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-xl4rm\n"
  Jun 17 12:18:19.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-1325 describe service agnhost-primary'
  Jun 17 12:18:19.112: INFO: stderr: ""
  Jun 17 12:18:19.112: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1325\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.81\nIPs:               10.152.183.81\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.2.86:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Jun 17 12:18:19.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-1325 describe node ip-172-31-25-17'
  Jun 17 12:18:19.228: INFO: stderr: ""
  Jun 17 12:18:19.228: INFO: stdout: "Name:               ip-172-31-25-17\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-worker\n                    juju-charm=kubernetes-worker\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-25-17\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 17 Jun 2023 11:55:42 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-25-17\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 17 Jun 2023 12:18:15 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 17 Jun 2023 12:15:57 +0000   Sat, 17 Jun 2023 11:55:42 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 17 Jun 2023 12:15:57 +0000   Sat, 17 Jun 2023 11:55:42 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 17 Jun 2023 12:15:57 +0000   Sat, 17 Jun 2023 11:55:42 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 17 Jun 2023 12:15:57 +0000   Sat, 17 Jun 2023 11:56:23 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.25.17\n  Hostname:    ip-172-31-25-17\nCapacity:\n  cpu:                2\n  ephemeral-storage:  16069568Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16070184Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  14809713845\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15967784Ki\n  pods:               110\nSystem Info:\n  Machine ID:                      ec2ae4aa70fa38df3323514dc74bc4aa\n  System UUID:                     ec2ae4aa-70fa-38df-3323-514dc74bc4aa\n  Boot ID:                         d6703e09-fa6e-4844-a1ba-0e4898cff12a\n  Kernel Version:                  5.19.0-1027-aws\n  OS Image:                        Ubuntu 22.04.2 LTS\n  Operating System:                linux\n  Architecture:                    amd64\n  Container Runtime Version:       containerd://1.6.8\n  Kubelet Version:                 v1.27.3\n  Kube-Proxy Version:              v1.27.3\nNon-terminated Pods:               (4 in total)\n  Namespace                        Name                                                             CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                        ----                                                             ------------  ----------  ---------------  -------------  ---\n  ingress-nginx-kubernetes-worker  nginx-ingress-controller-kubernetes-worker-wgt59                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         21m\n  projected-2064                   pod-projected-configmaps-c1c67f88-9abb-43be-91c5-1e32cb8b514b    0 (0%)        0 (0%)      0 (0%)           0 (0%)         70s\n  sonobuoy                         sonobuoy                                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\n  sonobuoy                         sonobuoy-systemd-logs-daemon-set-1a2f55ba8cf84413-46gwp          0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests  Limits\n  --------           --------  ------\n  cpu                0 (0%)    0 (0%)\n  memory             0 (0%)    0 (0%)\n  ephemeral-storage  0 (0%)    0 (0%)\n  hugepages-1Gi      0 (0%)    0 (0%)\n  hugepages-2Mi      0 (0%)    0 (0%)\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 22m                kube-proxy       \n  Normal   Starting                 22m                kubelet          Starting kubelet.\n  Normal   NodeHasSufficientMemory  22m                kubelet          Node ip-172-31-25-17 status is now: NodeHasSufficientMemory\n  Normal   NodeHasSufficientPID     22m (x2 over 22m)  kubelet          Node ip-172-31-25-17 status is now: NodeHasSufficientPID\n  Normal   RegisteredNode           22m                node-controller  Node ip-172-31-25-17 event: Registered Node ip-172-31-25-17 in Controller\n  Normal   Starting                 21m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      21m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  21m (x5 over 21m)  kubelet          Node ip-172-31-25-17 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    21m (x5 over 21m)  kubelet          Node ip-172-31-25-17 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     21m (x5 over 21m)  kubelet          Node ip-172-31-25-17 status is now: NodeHasSufficientPID\n  Normal   NodeReady                21m (x2 over 21m)  kubelet          Node ip-172-31-25-17 status is now: NodeReady\n"
  Jun 17 12:18:19.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-1325 describe namespace kubectl-1325'
  Jun 17 12:18:19.304: INFO: stderr: ""
  Jun 17 12:18:19.304: INFO: stdout: "Name:         kubectl-1325\nLabels:       e2e-framework=kubectl\n              e2e-run=0dfee738-58d2-42c6-8483-6cb3be66b003\n              kubernetes.io/metadata.name=kubectl-1325\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Jun 17 12:18:19.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1325" for this suite. @ 06/17/23 12:18:19.309
• [3.495 seconds]
------------------------------
SSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 06/17/23 12:18:19.317
  Jun 17 12:18:19.317: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename dns @ 06/17/23 12:18:19.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:18:19.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:18:19.34
  STEP: Creating a test headless service @ 06/17/23 12:18:19.343
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2762.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2762.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2762.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2762.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2762.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2762.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2762.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2762.svc.cluster.local;sleep 1; done
   @ 06/17/23 12:18:19.351
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2762.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2762.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2762.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2762.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2762.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2762.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2762.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2762.svc.cluster.local;sleep 1; done
   @ 06/17/23 12:18:19.351
  STEP: creating a pod to probe DNS @ 06/17/23 12:18:19.351
  STEP: submitting the pod to kubernetes @ 06/17/23 12:18:19.351
  STEP: retrieving the pod @ 06/17/23 12:18:27.394
  STEP: looking for the results for each expected name from probers @ 06/17/23 12:18:27.398
  Jun 17 12:18:27.404: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2762.svc.cluster.local from pod dns-2762/dns-test-b64bc4f4-e588-42e3-aa01-00f663027321: the server could not find the requested resource (get pods dns-test-b64bc4f4-e588-42e3-aa01-00f663027321)
  Jun 17 12:18:27.409: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2762.svc.cluster.local from pod dns-2762/dns-test-b64bc4f4-e588-42e3-aa01-00f663027321: the server could not find the requested resource (get pods dns-test-b64bc4f4-e588-42e3-aa01-00f663027321)
  Jun 17 12:18:27.413: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2762.svc.cluster.local from pod dns-2762/dns-test-b64bc4f4-e588-42e3-aa01-00f663027321: the server could not find the requested resource (get pods dns-test-b64bc4f4-e588-42e3-aa01-00f663027321)
  Jun 17 12:18:27.418: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2762.svc.cluster.local from pod dns-2762/dns-test-b64bc4f4-e588-42e3-aa01-00f663027321: the server could not find the requested resource (get pods dns-test-b64bc4f4-e588-42e3-aa01-00f663027321)
  Jun 17 12:18:27.423: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2762.svc.cluster.local from pod dns-2762/dns-test-b64bc4f4-e588-42e3-aa01-00f663027321: the server could not find the requested resource (get pods dns-test-b64bc4f4-e588-42e3-aa01-00f663027321)
  Jun 17 12:18:27.427: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2762.svc.cluster.local from pod dns-2762/dns-test-b64bc4f4-e588-42e3-aa01-00f663027321: the server could not find the requested resource (get pods dns-test-b64bc4f4-e588-42e3-aa01-00f663027321)
  Jun 17 12:18:27.432: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2762.svc.cluster.local from pod dns-2762/dns-test-b64bc4f4-e588-42e3-aa01-00f663027321: the server could not find the requested resource (get pods dns-test-b64bc4f4-e588-42e3-aa01-00f663027321)
  Jun 17 12:18:27.436: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2762.svc.cluster.local from pod dns-2762/dns-test-b64bc4f4-e588-42e3-aa01-00f663027321: the server could not find the requested resource (get pods dns-test-b64bc4f4-e588-42e3-aa01-00f663027321)
  Jun 17 12:18:27.436: INFO: Lookups using dns-2762/dns-test-b64bc4f4-e588-42e3-aa01-00f663027321 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2762.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2762.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2762.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2762.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2762.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2762.svc.cluster.local jessie_udp@dns-test-service-2.dns-2762.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2762.svc.cluster.local]

  Jun 17 12:18:32.475: INFO: DNS probes using dns-2762/dns-test-b64bc4f4-e588-42e3-aa01-00f663027321 succeeded

  Jun 17 12:18:32.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/17/23 12:18:32.48
  STEP: deleting the test headless service @ 06/17/23 12:18:32.496
  STEP: Destroying namespace "dns-2762" for this suite. @ 06/17/23 12:18:32.513
• [13.205 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 06/17/23 12:18:32.525
  Jun 17 12:18:32.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 12:18:32.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:18:32.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:18:32.549
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 12:18:32.553
  STEP: Saw pod success @ 06/17/23 12:18:36.587
  Jun 17 12:18:36.591: INFO: Trying to get logs from node ip-172-31-25-17 pod downwardapi-volume-de644940-f9d7-4809-bcaa-7c51bc3f240d container client-container: <nil>
  STEP: delete the pod @ 06/17/23 12:18:36.599
  Jun 17 12:18:36.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8258" for this suite. @ 06/17/23 12:18:36.619
• [4.101 seconds]
------------------------------
S
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 06/17/23 12:18:36.627
  Jun 17 12:18:36.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename downward-api @ 06/17/23 12:18:36.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:18:36.642
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:18:36.646
  STEP: Creating a pod to test downward api env vars @ 06/17/23 12:18:36.65
  STEP: Saw pod success @ 06/17/23 12:18:40.678
  Jun 17 12:18:40.683: INFO: Trying to get logs from node ip-172-31-25-17 pod downward-api-497147da-2ffe-4d39-b445-766b7d40aacb container dapi-container: <nil>
  STEP: delete the pod @ 06/17/23 12:18:40.691
  Jun 17 12:18:40.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1678" for this suite. @ 06/17/23 12:18:40.717
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 06/17/23 12:18:40.729
  Jun 17 12:18:40.729: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename field-validation @ 06/17/23 12:18:40.73
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:18:40.746
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:18:40.749
  Jun 17 12:18:40.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:18:43.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6579" for this suite. @ 06/17/23 12:18:43.314
• [2.591 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 06/17/23 12:18:43.321
  Jun 17 12:18:43.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename svcaccounts @ 06/17/23 12:18:43.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:18:43.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:18:43.347
  Jun 17 12:18:43.370: INFO: created pod pod-service-account-defaultsa
  Jun 17 12:18:43.370: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Jun 17 12:18:43.380: INFO: created pod pod-service-account-mountsa
  Jun 17 12:18:43.380: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Jun 17 12:18:43.392: INFO: created pod pod-service-account-nomountsa
  Jun 17 12:18:43.392: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Jun 17 12:18:43.402: INFO: created pod pod-service-account-defaultsa-mountspec
  Jun 17 12:18:43.402: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Jun 17 12:18:43.409: INFO: created pod pod-service-account-mountsa-mountspec
  Jun 17 12:18:43.409: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Jun 17 12:18:43.420: INFO: created pod pod-service-account-nomountsa-mountspec
  Jun 17 12:18:43.421: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Jun 17 12:18:43.431: INFO: created pod pod-service-account-defaultsa-nomountspec
  Jun 17 12:18:43.431: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Jun 17 12:18:43.440: INFO: created pod pod-service-account-mountsa-nomountspec
  Jun 17 12:18:43.440: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Jun 17 12:18:43.449: INFO: created pod pod-service-account-nomountsa-nomountspec
  Jun 17 12:18:43.449: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Jun 17 12:18:43.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1207" for this suite. @ 06/17/23 12:18:43.463
• [0.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 06/17/23 12:18:43.476
  Jun 17 12:18:43.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename secrets @ 06/17/23 12:18:43.478
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:18:43.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:18:43.502
  STEP: creating secret secrets-4468/secret-test-f44d1a6e-b889-486b-ab91-640d5158e92d @ 06/17/23 12:18:43.506
  STEP: Creating a pod to test consume secrets @ 06/17/23 12:18:43.511
  STEP: Saw pod success @ 06/17/23 12:18:47.535
  Jun 17 12:18:47.540: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-configmaps-62152d47-a043-485a-951e-c39f3bf938cc container env-test: <nil>
  STEP: delete the pod @ 06/17/23 12:18:47.548
  Jun 17 12:18:47.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4468" for this suite. @ 06/17/23 12:18:47.572
• [4.102 seconds]
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 06/17/23 12:18:47.579
  Jun 17 12:18:47.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename sched-pred @ 06/17/23 12:18:47.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:18:47.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:18:47.601
  Jun 17 12:18:47.605: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jun 17 12:18:47.613: INFO: Waiting for terminating namespaces to be deleted...
  Jun 17 12:18:47.617: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-25-17 before test
  Jun 17 12:18:47.623: INFO: nginx-ingress-controller-kubernetes-worker-wgt59 from ingress-nginx-kubernetes-worker started at 2023-06-17 11:56:23 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.623: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 17 12:18:47.623: INFO: sonobuoy from sonobuoy started at 2023-06-17 12:02:27 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.623: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jun 17 12:18:47.623: INFO: sonobuoy-systemd-logs-daemon-set-1a2f55ba8cf84413-46gwp from sonobuoy started at 2023-06-17 12:02:29 +0000 UTC (2 container statuses recorded)
  Jun 17 12:18:47.623: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 17 12:18:47.623: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 17 12:18:47.623: INFO: pod-service-account-defaultsa from svcaccounts-1207 started at 2023-06-17 12:18:43 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.623: INFO: 	Container token-test ready: false, restart count 0
  Jun 17 12:18:47.623: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-1207 started at 2023-06-17 12:18:43 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.623: INFO: 	Container token-test ready: false, restart count 0
  Jun 17 12:18:47.623: INFO: pod-service-account-defaultsa-nomountspec from svcaccounts-1207 started at 2023-06-17 12:18:43 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.623: INFO: 	Container token-test ready: true, restart count 0
  Jun 17 12:18:47.623: INFO: pod-service-account-mountsa-nomountspec from svcaccounts-1207 started at 2023-06-17 12:18:43 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.623: INFO: 	Container token-test ready: true, restart count 0
  Jun 17 12:18:47.623: INFO: pod-service-account-nomountsa from svcaccounts-1207 started at 2023-06-17 12:18:43 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.623: INFO: 	Container token-test ready: true, restart count 0
  Jun 17 12:18:47.623: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-68-253 before test
  Jun 17 12:18:47.629: INFO: default-http-backend-kubernetes-worker-65fc475d49-7hp8p from ingress-nginx-kubernetes-worker started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.629: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jun 17 12:18:47.629: INFO: nginx-ingress-controller-kubernetes-worker-hfc8s from ingress-nginx-kubernetes-worker started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.629: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 17 12:18:47.629: INFO: calico-kube-controllers-867f6b8548-wrvps from kube-system started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.629: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jun 17 12:18:47.629: INFO: coredns-5c7f76ccb8-gc7gk from kube-system started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.629: INFO: 	Container coredns ready: true, restart count 0
  Jun 17 12:18:47.629: INFO: kube-state-metrics-5b95b4459c-dgvxs from kube-system started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.629: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jun 17 12:18:47.629: INFO: metrics-server-v0.5.2-6cf8c8b69c-z9drm from kube-system started at 2023-06-17 11:49:37 +0000 UTC (2 container statuses recorded)
  Jun 17 12:18:47.629: INFO: 	Container metrics-server ready: true, restart count 0
  Jun 17 12:18:47.629: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jun 17 12:18:47.629: INFO: dashboard-metrics-scraper-6b8586b5c9-bpx9l from kubernetes-dashboard started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.629: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jun 17 12:18:47.629: INFO: kubernetes-dashboard-6869f4cd5f-blwp9 from kubernetes-dashboard started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.629: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jun 17 12:18:47.629: INFO: sonobuoy-systemd-logs-daemon-set-1a2f55ba8cf84413-dk2pv from sonobuoy started at 2023-06-17 12:02:29 +0000 UTC (2 container statuses recorded)
  Jun 17 12:18:47.629: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 17 12:18:47.629: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 17 12:18:47.629: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-86-18 before test
  Jun 17 12:18:47.636: INFO: nginx-ingress-controller-kubernetes-worker-jqhpt from ingress-nginx-kubernetes-worker started at 2023-06-17 11:58:10 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.636: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 17 12:18:47.636: INFO: sonobuoy-e2e-job-c63a99f81b61472e from sonobuoy started at 2023-06-17 12:02:29 +0000 UTC (2 container statuses recorded)
  Jun 17 12:18:47.636: INFO: 	Container e2e ready: true, restart count 0
  Jun 17 12:18:47.636: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 17 12:18:47.636: INFO: sonobuoy-systemd-logs-daemon-set-1a2f55ba8cf84413-9ngvl from sonobuoy started at 2023-06-17 12:02:29 +0000 UTC (2 container statuses recorded)
  Jun 17 12:18:47.636: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 17 12:18:47.636: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 17 12:18:47.636: INFO: pod-service-account-mountsa from svcaccounts-1207 started at 2023-06-17 12:18:43 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.636: INFO: 	Container token-test ready: false, restart count 0
  Jun 17 12:18:47.636: INFO: pod-service-account-mountsa-mountspec from svcaccounts-1207 started at 2023-06-17 12:18:43 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.636: INFO: 	Container token-test ready: false, restart count 0
  Jun 17 12:18:47.636: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-1207 started at 2023-06-17 12:18:43 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.636: INFO: 	Container token-test ready: false, restart count 0
  Jun 17 12:18:47.636: INFO: pod-service-account-nomountsa-nomountspec from svcaccounts-1207 started at 2023-06-17 12:18:43 +0000 UTC (1 container statuses recorded)
  Jun 17 12:18:47.636: INFO: 	Container token-test ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 06/17/23 12:18:47.636
  STEP: Explicitly delete pod here to free the resource it takes. @ 06/17/23 12:18:49.657
  STEP: Trying to apply a random label on the found node. @ 06/17/23 12:18:49.674
  STEP: verifying the node has the label kubernetes.io/e2e-70a859ac-0e8b-4124-97c8-f95e6554b832 95 @ 06/17/23 12:18:49.683
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 06/17/23 12:18:49.689
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.86.18 on the node which pod4 resides and expect not scheduled @ 06/17/23 12:18:51.708
  STEP: removing the label kubernetes.io/e2e-70a859ac-0e8b-4124-97c8-f95e6554b832 off the node ip-172-31-86-18 @ 06/17/23 12:23:51.722
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-70a859ac-0e8b-4124-97c8-f95e6554b832 @ 06/17/23 12:23:51.736
  Jun 17 12:23:51.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9004" for this suite. @ 06/17/23 12:23:51.747
• [304.174 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 06/17/23 12:23:51.756
  Jun 17 12:23:51.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename namespaces @ 06/17/23 12:23:51.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:23:51.774
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:23:51.778
  STEP: Read namespace status @ 06/17/23 12:23:51.782
  Jun 17 12:23:51.786: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 06/17/23 12:23:51.786
  Jun 17 12:23:51.793: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 06/17/23 12:23:51.793
  Jun 17 12:23:51.802: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Jun 17 12:23:51.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7009" for this suite. @ 06/17/23 12:23:51.807
• [0.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 06/17/23 12:23:51.817
  Jun 17 12:23:51.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename services @ 06/17/23 12:23:51.818
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:23:51.836
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:23:51.841
  STEP: creating service in namespace services-7306 @ 06/17/23 12:23:51.844
  STEP: creating service affinity-clusterip-transition in namespace services-7306 @ 06/17/23 12:23:51.844
  STEP: creating replication controller affinity-clusterip-transition in namespace services-7306 @ 06/17/23 12:23:51.855
  I0617 12:23:51.867972      19 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-7306, replica count: 3
  I0617 12:23:54.919011      19 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 17 12:23:54.928: INFO: Creating new exec pod
  Jun 17 12:23:57.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-7306 exec execpod-affinityqbftb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Jun 17 12:23:58.074: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Jun 17 12:23:58.074: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 12:23:58.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-7306 exec execpod-affinityqbftb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.147 80'
  Jun 17 12:23:58.212: INFO: stderr: "+ nc -v -t -w 2 10.152.183.147 80\n+ echo hostName\nConnection to 10.152.183.147 80 port [tcp/http] succeeded!\n"
  Jun 17 12:23:58.212: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 12:23:58.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-7306 exec execpod-affinityqbftb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.147:80/ ; done'
  Jun 17 12:23:58.419: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n"
  Jun 17 12:23:58.420: INFO: stdout: "\naffinity-clusterip-transition-mcd9z\naffinity-clusterip-transition-l2tnm\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-mcd9z\naffinity-clusterip-transition-mcd9z\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-l2tnm\naffinity-clusterip-transition-mcd9z\naffinity-clusterip-transition-l2tnm\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-l2tnm\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-mcd9z\naffinity-clusterip-transition-l2tnm\naffinity-clusterip-transition-mcd9z\naffinity-clusterip-transition-l2tnm"
  Jun 17 12:23:58.420: INFO: Received response from host: affinity-clusterip-transition-mcd9z
  Jun 17 12:23:58.420: INFO: Received response from host: affinity-clusterip-transition-l2tnm
  Jun 17 12:23:58.420: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.420: INFO: Received response from host: affinity-clusterip-transition-mcd9z
  Jun 17 12:23:58.420: INFO: Received response from host: affinity-clusterip-transition-mcd9z
  Jun 17 12:23:58.420: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.420: INFO: Received response from host: affinity-clusterip-transition-l2tnm
  Jun 17 12:23:58.420: INFO: Received response from host: affinity-clusterip-transition-mcd9z
  Jun 17 12:23:58.420: INFO: Received response from host: affinity-clusterip-transition-l2tnm
  Jun 17 12:23:58.420: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.420: INFO: Received response from host: affinity-clusterip-transition-l2tnm
  Jun 17 12:23:58.420: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.420: INFO: Received response from host: affinity-clusterip-transition-mcd9z
  Jun 17 12:23:58.420: INFO: Received response from host: affinity-clusterip-transition-l2tnm
  Jun 17 12:23:58.420: INFO: Received response from host: affinity-clusterip-transition-mcd9z
  Jun 17 12:23:58.420: INFO: Received response from host: affinity-clusterip-transition-l2tnm
  Jun 17 12:23:58.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-7306 exec execpod-affinityqbftb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.147:80/ ; done'
  Jun 17 12:23:58.627: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.147:80/\n"
  Jun 17 12:23:58.627: INFO: stdout: "\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-cc2r6\naffinity-clusterip-transition-cc2r6"
  Jun 17 12:23:58.627: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.627: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.627: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.627: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.627: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.627: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.627: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.627: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.627: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.627: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.627: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.627: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.627: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.627: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.627: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.627: INFO: Received response from host: affinity-clusterip-transition-cc2r6
  Jun 17 12:23:58.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 17 12:23:58.631: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7306, will wait for the garbage collector to delete the pods @ 06/17/23 12:23:58.65
  Jun 17 12:23:58.711: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.951043ms
  Jun 17 12:23:58.812: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.37515ms
  STEP: Destroying namespace "services-7306" for this suite. @ 06/17/23 12:24:01.032
• [9.223 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 06/17/23 12:24:01.041
  Jun 17 12:24:01.041: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename crd-webhook @ 06/17/23 12:24:01.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:24:01.058
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:24:01.067
  STEP: Setting up server cert @ 06/17/23 12:24:01.071
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 06/17/23 12:24:01.498
  STEP: Deploying the custom resource conversion webhook pod @ 06/17/23 12:24:01.508
  STEP: Wait for the deployment to be ready @ 06/17/23 12:24:01.522
  Jun 17 12:24:01.531: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/17/23 12:24:03.544
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 12:24:03.562
  Jun 17 12:24:04.562: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jun 17 12:24:04.567: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Creating a v1 custom resource @ 06/17/23 12:24:07.164
  STEP: Create a v2 custom resource @ 06/17/23 12:24:07.183
  STEP: List CRs in v1 @ 06/17/23 12:24:07.297
  STEP: List CRs in v2 @ 06/17/23 12:24:07.304
  Jun 17 12:24:07.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-6799" for this suite. @ 06/17/23 12:24:07.878
• [6.848 seconds]
------------------------------
S
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 06/17/23 12:24:07.89
  Jun 17 12:24:07.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename cronjob @ 06/17/23 12:24:07.892
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:24:07.911
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:24:07.914
  STEP: Creating a ForbidConcurrent cronjob @ 06/17/23 12:24:07.921
  STEP: Ensuring a job is scheduled @ 06/17/23 12:24:07.928
  STEP: Ensuring exactly one is scheduled @ 06/17/23 12:25:01.932
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 06/17/23 12:25:01.936
  STEP: Ensuring no more jobs are scheduled @ 06/17/23 12:25:01.94
  STEP: Removing cronjob @ 06/17/23 12:30:01.949
  Jun 17 12:30:01.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3768" for this suite. @ 06/17/23 12:30:01.961
• [354.078 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 06/17/23 12:30:01.969
  Jun 17 12:30:01.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename deployment @ 06/17/23 12:30:01.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:30:01.99
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:30:01.996
  STEP: creating a Deployment @ 06/17/23 12:30:02.008
  STEP: waiting for Deployment to be created @ 06/17/23 12:30:02.015
  STEP: waiting for all Replicas to be Ready @ 06/17/23 12:30:02.017
  Jun 17 12:30:02.019: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 17 12:30:02.019: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 17 12:30:02.029: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 17 12:30:02.029: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 17 12:30:02.049: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 17 12:30:02.049: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 17 12:30:02.080: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 17 12:30:02.080: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 17 12:30:02.891: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jun 17 12:30:02.891: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jun 17 12:30:03.543: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 06/17/23 12:30:03.543
  W0617 12:30:03.554617      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jun 17 12:30:03.557: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 06/17/23 12:30:03.557
  Jun 17 12:30:03.559: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 0
  Jun 17 12:30:03.559: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 0
  Jun 17 12:30:03.559: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 0
  Jun 17 12:30:03.559: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 0
  Jun 17 12:30:03.559: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 0
  Jun 17 12:30:03.559: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 0
  Jun 17 12:30:03.560: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 0
  Jun 17 12:30:03.560: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 0
  Jun 17 12:30:03.560: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1
  Jun 17 12:30:03.560: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1
  Jun 17 12:30:03.560: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2
  Jun 17 12:30:03.560: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2
  Jun 17 12:30:03.560: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2
  Jun 17 12:30:03.560: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2
  Jun 17 12:30:03.572: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2
  Jun 17 12:30:03.572: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2
  Jun 17 12:30:03.600: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2
  Jun 17 12:30:03.600: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2
  Jun 17 12:30:03.625: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1
  Jun 17 12:30:03.625: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1
  Jun 17 12:30:03.637: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1
  Jun 17 12:30:03.637: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1
  Jun 17 12:30:04.898: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2
  Jun 17 12:30:04.898: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2
  Jun 17 12:30:04.932: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1
  STEP: listing Deployments @ 06/17/23 12:30:04.932
  Jun 17 12:30:04.937: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 06/17/23 12:30:04.937
  Jun 17 12:30:04.949: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 06/17/23 12:30:04.949
  Jun 17 12:30:04.960: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 17 12:30:04.964: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 17 12:30:05.002: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 17 12:30:05.017: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 17 12:30:05.900: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 17 12:30:05.923: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 17 12:30:05.931: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 17 12:30:05.948: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 17 12:30:05.965: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 17 12:30:07.582: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 06/17/23 12:30:07.618
  STEP: fetching the DeploymentStatus @ 06/17/23 12:30:07.625
  Jun 17 12:30:07.631: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1
  Jun 17 12:30:07.631: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1
  Jun 17 12:30:07.631: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1
  Jun 17 12:30:07.631: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 1
  Jun 17 12:30:07.631: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2
  Jun 17 12:30:07.631: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2
  Jun 17 12:30:07.631: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2
  Jun 17 12:30:07.631: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2
  Jun 17 12:30:07.631: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 2
  Jun 17 12:30:07.632: INFO: observed Deployment test-deployment in namespace deployment-5706 with ReadyReplicas 3
  STEP: deleting the Deployment @ 06/17/23 12:30:07.632
  Jun 17 12:30:07.643: INFO: observed event type MODIFIED
  Jun 17 12:30:07.643: INFO: observed event type MODIFIED
  Jun 17 12:30:07.643: INFO: observed event type MODIFIED
  Jun 17 12:30:07.643: INFO: observed event type MODIFIED
  Jun 17 12:30:07.644: INFO: observed event type MODIFIED
  Jun 17 12:30:07.644: INFO: observed event type MODIFIED
  Jun 17 12:30:07.644: INFO: observed event type MODIFIED
  Jun 17 12:30:07.644: INFO: observed event type MODIFIED
  Jun 17 12:30:07.644: INFO: observed event type MODIFIED
  Jun 17 12:30:07.644: INFO: observed event type MODIFIED
  Jun 17 12:30:07.644: INFO: observed event type MODIFIED
  Jun 17 12:30:07.649: INFO: Log out all the ReplicaSets if there is no deployment created
  Jun 17 12:30:07.656: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-5706  988ae475-c0d3-45db-b435-473d5dd01d24 10367 3 2023-06-17 12:30:02 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment f31472fe-3d28-4181-964c-85a477bf16f0 0xc004e42b87 0xc004e42b88}] [] [{kube-controller-manager Update apps/v1 2023-06-17 12:30:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f31472fe-3d28-4181-964c-85a477bf16f0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:30:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e42c10 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jun 17 12:30:07.662: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-5706  99367a65-a9cf-4bf8-9625-72e3f27edb12 10478 4 2023-06-17 12:30:03 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment f31472fe-3d28-4181-964c-85a477bf16f0 0xc004e42c77 0xc004e42c78}] [] [{kube-controller-manager Update apps/v1 2023-06-17 12:30:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f31472fe-3d28-4181-964c-85a477bf16f0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:30:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e42d00 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jun 17 12:30:07.668: INFO: pod: "test-deployment-5b5dcbcd95-jw2kw":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-jw2kw test-deployment-5b5dcbcd95- deployment-5706  467c8387-1bdd-4028-adf3-c0f673cacba5 10474 0 2023-06-17 12:30:03 +0000 UTC 2023-06-17 12:30:08 +0000 UTC 0xc004e432e8 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 99367a65-a9cf-4bf8-9625-72e3f27edb12 0xc004e43317 0xc004e43318}] [] [{kube-controller-manager Update v1 2023-06-17 12:30:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99367a65-a9cf-4bf8-9625-72e3f27edb12\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 12:30:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.178.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4p9q8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4p9q8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:30:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:30:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:30:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:30:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:192.168.178.141,StartTime:2023-06-17 12:30:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-17 12:30:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://cffddf52902309d9efa7adb29a12419f64f19038274faad986abc1b274d0b599,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.178.141,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jun 17 12:30:07.668: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-5706  ff92eaa2-e92a-4123-8b33-e55e9c23a87f 10468 2 2023-06-17 12:30:04 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment f31472fe-3d28-4181-964c-85a477bf16f0 0xc004e42d67 0xc004e42d68}] [] [{kube-controller-manager Update apps/v1 2023-06-17 12:30:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f31472fe-3d28-4181-964c-85a477bf16f0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:30:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e42df0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Jun 17 12:30:07.676: INFO: pod: "test-deployment-6fc78d85c6-brlql":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-brlql test-deployment-6fc78d85c6- deployment-5706  b372063b-c987-4d6d-8826-81cf692f84c3 10484 0 2023-06-17 12:30:05 +0000 UTC 2023-06-17 12:30:08 +0000 UTC 0xc004e43f18 map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 ff92eaa2-e92a-4123-8b33-e55e9c23a87f 0xc004e43f47 0xc004e43f48}] [] [{kube-controller-manager Update v1 2023-06-17 12:30:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff92eaa2-e92a-4123-8b33-e55e9c23a87f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 12:30:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wrjbv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wrjbv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:30:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:30:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:30:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:30:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.18,PodIP:192.168.2.94,StartTime:2023-06-17 12:30:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-17 12:30:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5aed581ac86741b95ffd13fb8dbbbea000bc87e7bfb756bc18f7576ad6a2e215,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.94,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jun 17 12:30:07.676: INFO: pod: "test-deployment-6fc78d85c6-n6857":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-n6857 test-deployment-6fc78d85c6- deployment-5706  17849939-101b-47d1-a22f-7c2d4eb57ad7 10413 0 2023-06-17 12:30:04 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 ff92eaa2-e92a-4123-8b33-e55e9c23a87f 0xc004deee77 0xc004deee78}] [] [{kube-controller-manager Update v1 2023-06-17 12:30:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff92eaa2-e92a-4123-8b33-e55e9c23a87f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 12:30:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.178.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jxrtv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jxrtv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:30:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:30:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:30:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:30:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:192.168.178.142,StartTime:2023-06-17 12:30:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-17 12:30:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2243f2159c84a4a77838bc57aa43088940e32943a6e8797869213d52c41367a7,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.178.142,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jun 17 12:30:07.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5706" for this suite. @ 06/17/23 12:30:07.68
• [5.721 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 06/17/23 12:30:07.692
  Jun 17 12:30:07.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename pods @ 06/17/23 12:30:07.694
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:30:07.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:30:07.711
  STEP: Saw pod success @ 06/17/23 12:30:13.784
  Jun 17 12:30:13.788: INFO: Trying to get logs from node ip-172-31-25-17 pod client-envvars-137b407b-9795-4de0-bf03-a1bb2f092318 container env3cont: <nil>
  STEP: delete the pod @ 06/17/23 12:30:13.81
  Jun 17 12:30:13.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4546" for this suite. @ 06/17/23 12:30:13.838
• [6.153 seconds]
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 06/17/23 12:30:13.845
  Jun 17 12:30:13.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename lease-test @ 06/17/23 12:30:13.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:30:13.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:30:13.873
  Jun 17 12:30:13.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-9064" for this suite. @ 06/17/23 12:30:13.945
• [0.106 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 06/17/23 12:30:13.952
  Jun 17 12:30:13.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename configmap @ 06/17/23 12:30:13.953
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:30:13.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:30:13.97
  STEP: Creating configMap with name configmap-test-upd-5d8cf115-e6d7-4550-921f-4b18233780c2 @ 06/17/23 12:30:13.978
  STEP: Creating the pod @ 06/17/23 12:30:13.983
  STEP: Updating configmap configmap-test-upd-5d8cf115-e6d7-4550-921f-4b18233780c2 @ 06/17/23 12:30:16.015
  STEP: waiting to observe update in volume @ 06/17/23 12:30:16.02
  Jun 17 12:31:34.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4906" for this suite. @ 06/17/23 12:31:34.404
• [80.458 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 06/17/23 12:31:34.411
  Jun 17 12:31:34.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename pods @ 06/17/23 12:31:34.411
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:31:34.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:31:34.431
  STEP: creating the pod @ 06/17/23 12:31:34.438
  STEP: submitting the pod to kubernetes @ 06/17/23 12:31:34.438
  STEP: verifying QOS class is set on the pod @ 06/17/23 12:31:34.448
  Jun 17 12:31:34.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5180" for this suite. @ 06/17/23 12:31:34.458
• [0.055 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 06/17/23 12:31:34.466
  Jun 17 12:31:34.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename events @ 06/17/23 12:31:34.467
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:31:34.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:31:34.5
  STEP: Create set of events @ 06/17/23 12:31:34.504
  Jun 17 12:31:34.511: INFO: created test-event-1
  Jun 17 12:31:34.516: INFO: created test-event-2
  Jun 17 12:31:34.521: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 06/17/23 12:31:34.521
  STEP: delete collection of events @ 06/17/23 12:31:34.525
  Jun 17 12:31:34.525: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 06/17/23 12:31:34.548
  Jun 17 12:31:34.548: INFO: requesting list of events to confirm quantity
  Jun 17 12:31:34.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7850" for this suite. @ 06/17/23 12:31:34.555
• [0.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 06/17/23 12:31:34.564
  Jun 17 12:31:34.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename security-context-test @ 06/17/23 12:31:34.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:31:34.581
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:31:34.585
  Jun 17 12:31:38.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-223" for this suite. @ 06/17/23 12:31:38.616
• [4.062 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 06/17/23 12:31:38.626
  Jun 17 12:31:38.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename field-validation @ 06/17/23 12:31:38.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:31:38.648
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:31:38.653
  STEP: apply creating a deployment @ 06/17/23 12:31:38.658
  Jun 17 12:31:38.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6665" for this suite. @ 06/17/23 12:31:38.682
• [0.066 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 06/17/23 12:31:38.692
  Jun 17 12:31:38.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir @ 06/17/23 12:31:38.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:31:38.717
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:31:38.72
  STEP: Creating a pod to test emptydir volume type on node default medium @ 06/17/23 12:31:38.726
  STEP: Saw pod success @ 06/17/23 12:31:42.754
  Jun 17 12:31:42.759: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-f55ca9de-6a46-4801-87f9-9aa08ba6fa37 container test-container: <nil>
  STEP: delete the pod @ 06/17/23 12:31:42.781
  Jun 17 12:31:42.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8310" for this suite. @ 06/17/23 12:31:42.805
• [4.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 06/17/23 12:31:42.819
  Jun 17 12:31:42.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename field-validation @ 06/17/23 12:31:42.82
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:31:42.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:31:42.845
  STEP: apply creating a deployment @ 06/17/23 12:31:42.85
  Jun 17 12:31:42.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1174" for this suite. @ 06/17/23 12:31:42.873
• [0.063 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 06/17/23 12:31:42.883
  Jun 17 12:31:42.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl @ 06/17/23 12:31:42.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:31:42.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:31:42.909
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 06/17/23 12:31:42.913
  Jun 17 12:31:42.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-5316 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jun 17 12:31:42.994: INFO: stderr: ""
  Jun 17 12:31:42.994: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 06/17/23 12:31:42.994
  Jun 17 12:31:42.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-5316 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Jun 17 12:31:43.073: INFO: stderr: ""
  Jun 17 12:31:43.073: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 06/17/23 12:31:43.073
  Jun 17 12:31:43.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-5316 delete pods e2e-test-httpd-pod'
  Jun 17 12:31:44.748: INFO: stderr: ""
  Jun 17 12:31:44.748: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jun 17 12:31:44.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5316" for this suite. @ 06/17/23 12:31:44.754
• [1.882 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 06/17/23 12:31:44.766
  Jun 17 12:31:44.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 12:31:44.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:31:44.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:31:44.793
  STEP: Setting up server cert @ 06/17/23 12:31:44.839
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 12:31:45.105
  STEP: Deploying the webhook pod @ 06/17/23 12:31:45.115
  STEP: Wait for the deployment to be ready @ 06/17/23 12:31:45.13
  Jun 17 12:31:45.142: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/17/23 12:31:47.156
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 12:31:47.169
  Jun 17 12:31:48.170: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 06/17/23 12:31:48.271
  STEP: Creating a configMap that should be mutated @ 06/17/23 12:31:48.288
  STEP: Deleting the collection of validation webhooks @ 06/17/23 12:31:48.328
  STEP: Creating a configMap that should not be mutated @ 06/17/23 12:31:48.389
  Jun 17 12:31:48.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5044" for this suite. @ 06/17/23 12:31:48.468
  STEP: Destroying namespace "webhook-markers-9819" for this suite. @ 06/17/23 12:31:48.477
• [3.725 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 06/17/23 12:31:48.491
  Jun 17 12:31:48.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/17/23 12:31:48.493
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:31:48.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:31:48.517
  Jun 17 12:31:48.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 06/17/23 12:31:49.962
  Jun 17 12:31:49.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-1483 --namespace=crd-publish-openapi-1483 create -f -'
  Jun 17 12:31:50.690: INFO: stderr: ""
  Jun 17 12:31:50.690: INFO: stdout: "e2e-test-crd-publish-openapi-8952-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jun 17 12:31:50.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-1483 --namespace=crd-publish-openapi-1483 delete e2e-test-crd-publish-openapi-8952-crds test-cr'
  Jun 17 12:31:50.773: INFO: stderr: ""
  Jun 17 12:31:50.773: INFO: stdout: "e2e-test-crd-publish-openapi-8952-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Jun 17 12:31:50.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-1483 --namespace=crd-publish-openapi-1483 apply -f -'
  Jun 17 12:31:50.977: INFO: stderr: ""
  Jun 17 12:31:50.977: INFO: stdout: "e2e-test-crd-publish-openapi-8952-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jun 17 12:31:50.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-1483 --namespace=crd-publish-openapi-1483 delete e2e-test-crd-publish-openapi-8952-crds test-cr'
  Jun 17 12:31:51.046: INFO: stderr: ""
  Jun 17 12:31:51.046: INFO: stdout: "e2e-test-crd-publish-openapi-8952-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 06/17/23 12:31:51.046
  Jun 17 12:31:51.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-1483 explain e2e-test-crd-publish-openapi-8952-crds'
  Jun 17 12:31:51.263: INFO: stderr: ""
  Jun 17 12:31:51.263: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-8952-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Jun 17 12:31:53.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1483" for this suite. @ 06/17/23 12:31:53.195
• [4.709 seconds]
------------------------------
SS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 06/17/23 12:31:53.201
  Jun 17 12:31:53.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename runtimeclass @ 06/17/23 12:31:53.202
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:31:53.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:31:53.223
  STEP: Deleting RuntimeClass runtimeclass-9033-delete-me @ 06/17/23 12:31:53.229
  STEP: Waiting for the RuntimeClass to disappear @ 06/17/23 12:31:53.235
  Jun 17 12:31:53.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9033" for this suite. @ 06/17/23 12:31:53.247
• [0.052 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 06/17/23 12:31:53.253
  Jun 17 12:31:53.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename dns @ 06/17/23 12:31:53.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:31:53.272
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:31:53.275
  STEP: Creating a test externalName service @ 06/17/23 12:31:53.278
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5752.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5752.svc.cluster.local; sleep 1; done
   @ 06/17/23 12:31:53.283
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5752.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5752.svc.cluster.local; sleep 1; done
   @ 06/17/23 12:31:53.283
  STEP: creating a pod to probe DNS @ 06/17/23 12:31:53.283
  STEP: submitting the pod to kubernetes @ 06/17/23 12:31:53.283
  STEP: retrieving the pod @ 06/17/23 12:31:55.299
  STEP: looking for the results for each expected name from probers @ 06/17/23 12:31:55.302
  Jun 17 12:31:55.310: INFO: DNS probes using dns-test-f289c9ef-e01c-4eee-9c90-c3f7799a2de5 succeeded

  STEP: changing the externalName to bar.example.com @ 06/17/23 12:31:55.31
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5752.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5752.svc.cluster.local; sleep 1; done
   @ 06/17/23 12:31:55.319
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5752.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5752.svc.cluster.local; sleep 1; done
   @ 06/17/23 12:31:55.319
  STEP: creating a second pod to probe DNS @ 06/17/23 12:31:55.319
  STEP: submitting the pod to kubernetes @ 06/17/23 12:31:55.319
  STEP: retrieving the pod @ 06/17/23 12:32:01.339
  STEP: looking for the results for each expected name from probers @ 06/17/23 12:32:01.342
  Jun 17 12:32:01.350: INFO: DNS probes using dns-test-1d5ca299-aa22-486b-8ea4-2b8c40ac1b95 succeeded

  STEP: changing the service to type=ClusterIP @ 06/17/23 12:32:01.35
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5752.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5752.svc.cluster.local; sleep 1; done
   @ 06/17/23 12:32:01.363
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5752.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5752.svc.cluster.local; sleep 1; done
   @ 06/17/23 12:32:01.363
  STEP: creating a third pod to probe DNS @ 06/17/23 12:32:01.363
  STEP: submitting the pod to kubernetes @ 06/17/23 12:32:01.367
  STEP: retrieving the pod @ 06/17/23 12:32:03.383
  STEP: looking for the results for each expected name from probers @ 06/17/23 12:32:03.386
  Jun 17 12:32:03.394: INFO: DNS probes using dns-test-b9d44566-95e1-4a35-bdf2-875c90292862 succeeded

  Jun 17 12:32:03.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/17/23 12:32:03.397
  STEP: deleting the pod @ 06/17/23 12:32:03.411
  STEP: deleting the pod @ 06/17/23 12:32:03.424
  STEP: deleting the test externalName service @ 06/17/23 12:32:03.44
  STEP: Destroying namespace "dns-5752" for this suite. @ 06/17/23 12:32:03.455
• [10.208 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 06/17/23 12:32:03.462
  Jun 17 12:32:03.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename replication-controller @ 06/17/23 12:32:03.463
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:32:03.491
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:32:03.493
  Jun 17 12:32:03.497: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 06/17/23 12:32:04.507
  STEP: Checking rc "condition-test" has the desired failure condition set @ 06/17/23 12:32:04.512
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 06/17/23 12:32:05.519
  Jun 17 12:32:05.530: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 06/17/23 12:32:05.53
  Jun 17 12:32:06.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5204" for this suite. @ 06/17/23 12:32:06.542
• [3.085 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 06/17/23 12:32:06.549
  Jun 17 12:32:06.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename resourcequota @ 06/17/23 12:32:06.55
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:32:06.57
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:32:06.573
  STEP: Creating a ResourceQuota with best effort scope @ 06/17/23 12:32:06.576
  STEP: Ensuring ResourceQuota status is calculated @ 06/17/23 12:32:06.581
  STEP: Creating a ResourceQuota with not best effort scope @ 06/17/23 12:32:08.584
  STEP: Ensuring ResourceQuota status is calculated @ 06/17/23 12:32:08.589
  STEP: Creating a best-effort pod @ 06/17/23 12:32:10.592
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 06/17/23 12:32:10.605
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 06/17/23 12:32:12.608
  STEP: Deleting the pod @ 06/17/23 12:32:14.613
  STEP: Ensuring resource quota status released the pod usage @ 06/17/23 12:32:14.622
  STEP: Creating a not best-effort pod @ 06/17/23 12:32:16.626
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 06/17/23 12:32:16.635
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 06/17/23 12:32:18.639
  STEP: Deleting the pod @ 06/17/23 12:32:20.644
  STEP: Ensuring resource quota status released the pod usage @ 06/17/23 12:32:20.659
  Jun 17 12:32:22.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4980" for this suite. @ 06/17/23 12:32:22.667
• [16.124 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 06/17/23 12:32:22.673
  Jun 17 12:32:22.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename statefulset @ 06/17/23 12:32:22.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:32:22.694
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:32:22.696
  STEP: Creating service test in namespace statefulset-9032 @ 06/17/23 12:32:22.699
  STEP: Creating a new StatefulSet @ 06/17/23 12:32:22.705
  Jun 17 12:32:22.715: INFO: Found 0 stateful pods, waiting for 3
  Jun 17 12:32:32.719: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 17 12:32:32.719: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 17 12:32:32.719: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 06/17/23 12:32:32.728
  Jun 17 12:32:32.748: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 06/17/23 12:32:32.748
  STEP: Not applying an update when the partition is greater than the number of replicas @ 06/17/23 12:32:42.762
  STEP: Performing a canary update @ 06/17/23 12:32:42.762
  Jun 17 12:32:42.780: INFO: Updating stateful set ss2
  Jun 17 12:32:42.785: INFO: Waiting for Pod statefulset-9032/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 06/17/23 12:32:52.793
  Jun 17 12:32:52.823: INFO: Found 1 stateful pods, waiting for 3
  Jun 17 12:33:02.829: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 17 12:33:02.829: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 17 12:33:02.829: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 06/17/23 12:33:02.834
  Jun 17 12:33:02.853: INFO: Updating stateful set ss2
  Jun 17 12:33:02.860: INFO: Waiting for Pod statefulset-9032/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jun 17 12:33:12.885: INFO: Updating stateful set ss2
  Jun 17 12:33:12.891: INFO: Waiting for StatefulSet statefulset-9032/ss2 to complete update
  Jun 17 12:33:12.891: INFO: Waiting for Pod statefulset-9032/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jun 17 12:33:22.898: INFO: Deleting all statefulset in ns statefulset-9032
  Jun 17 12:33:22.901: INFO: Scaling statefulset ss2 to 0
  Jun 17 12:33:32.917: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 17 12:33:32.920: INFO: Deleting statefulset ss2
  Jun 17 12:33:32.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9032" for this suite. @ 06/17/23 12:33:32.936
• [70.269 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 06/17/23 12:33:32.944
  Jun 17 12:33:32.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl @ 06/17/23 12:33:32.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:33:32.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:33:32.965
  STEP: Starting the proxy @ 06/17/23 12:33:32.968
  Jun 17 12:33:32.968: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-1744 proxy --unix-socket=/tmp/kubectl-proxy-unix3012824328/test'
  STEP: retrieving proxy /api/ output @ 06/17/23 12:33:33.018
  Jun 17 12:33:33.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1744" for this suite. @ 06/17/23 12:33:33.022
• [0.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 06/17/23 12:33:33.031
  Jun 17 12:33:33.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename secrets @ 06/17/23 12:33:33.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:33:33.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:33:33.052
  STEP: Creating projection with secret that has name secret-emptykey-test-62e641e6-e296-4547-9412-f1b741e3cf47 @ 06/17/23 12:33:33.055
  Jun 17 12:33:33.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-264" for this suite. @ 06/17/23 12:33:33.06
• [0.035 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 06/17/23 12:33:33.067
  Jun 17 12:33:33.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename discovery @ 06/17/23 12:33:33.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:33:33.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:33:33.087
  STEP: Setting up server cert @ 06/17/23 12:33:33.091
  Jun 17 12:33:33.349: INFO: Checking APIGroup: apiregistration.k8s.io
  Jun 17 12:33:33.350: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Jun 17 12:33:33.350: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Jun 17 12:33:33.350: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Jun 17 12:33:33.350: INFO: Checking APIGroup: apps
  Jun 17 12:33:33.351: INFO: PreferredVersion.GroupVersion: apps/v1
  Jun 17 12:33:33.352: INFO: Versions found [{apps/v1 v1}]
  Jun 17 12:33:33.352: INFO: apps/v1 matches apps/v1
  Jun 17 12:33:33.352: INFO: Checking APIGroup: events.k8s.io
  Jun 17 12:33:33.353: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Jun 17 12:33:33.353: INFO: Versions found [{events.k8s.io/v1 v1}]
  Jun 17 12:33:33.353: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Jun 17 12:33:33.353: INFO: Checking APIGroup: authentication.k8s.io
  Jun 17 12:33:33.354: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Jun 17 12:33:33.354: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Jun 17 12:33:33.354: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Jun 17 12:33:33.354: INFO: Checking APIGroup: authorization.k8s.io
  Jun 17 12:33:33.355: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Jun 17 12:33:33.355: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Jun 17 12:33:33.355: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Jun 17 12:33:33.355: INFO: Checking APIGroup: autoscaling
  Jun 17 12:33:33.356: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Jun 17 12:33:33.356: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Jun 17 12:33:33.356: INFO: autoscaling/v2 matches autoscaling/v2
  Jun 17 12:33:33.356: INFO: Checking APIGroup: batch
  Jun 17 12:33:33.358: INFO: PreferredVersion.GroupVersion: batch/v1
  Jun 17 12:33:33.358: INFO: Versions found [{batch/v1 v1}]
  Jun 17 12:33:33.358: INFO: batch/v1 matches batch/v1
  Jun 17 12:33:33.358: INFO: Checking APIGroup: certificates.k8s.io
  Jun 17 12:33:33.359: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Jun 17 12:33:33.359: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Jun 17 12:33:33.359: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Jun 17 12:33:33.359: INFO: Checking APIGroup: networking.k8s.io
  Jun 17 12:33:33.360: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Jun 17 12:33:33.360: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Jun 17 12:33:33.360: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Jun 17 12:33:33.360: INFO: Checking APIGroup: policy
  Jun 17 12:33:33.361: INFO: PreferredVersion.GroupVersion: policy/v1
  Jun 17 12:33:33.361: INFO: Versions found [{policy/v1 v1}]
  Jun 17 12:33:33.361: INFO: policy/v1 matches policy/v1
  Jun 17 12:33:33.361: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Jun 17 12:33:33.362: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Jun 17 12:33:33.362: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Jun 17 12:33:33.362: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Jun 17 12:33:33.362: INFO: Checking APIGroup: storage.k8s.io
  Jun 17 12:33:33.363: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Jun 17 12:33:33.363: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Jun 17 12:33:33.363: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Jun 17 12:33:33.363: INFO: Checking APIGroup: admissionregistration.k8s.io
  Jun 17 12:33:33.364: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Jun 17 12:33:33.364: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Jun 17 12:33:33.364: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Jun 17 12:33:33.364: INFO: Checking APIGroup: apiextensions.k8s.io
  Jun 17 12:33:33.365: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Jun 17 12:33:33.365: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Jun 17 12:33:33.365: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Jun 17 12:33:33.365: INFO: Checking APIGroup: scheduling.k8s.io
  Jun 17 12:33:33.366: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Jun 17 12:33:33.366: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Jun 17 12:33:33.366: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Jun 17 12:33:33.366: INFO: Checking APIGroup: coordination.k8s.io
  Jun 17 12:33:33.367: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Jun 17 12:33:33.367: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Jun 17 12:33:33.367: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Jun 17 12:33:33.367: INFO: Checking APIGroup: node.k8s.io
  Jun 17 12:33:33.368: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Jun 17 12:33:33.368: INFO: Versions found [{node.k8s.io/v1 v1}]
  Jun 17 12:33:33.368: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Jun 17 12:33:33.368: INFO: Checking APIGroup: discovery.k8s.io
  Jun 17 12:33:33.369: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Jun 17 12:33:33.369: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Jun 17 12:33:33.369: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Jun 17 12:33:33.369: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Jun 17 12:33:33.371: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Jun 17 12:33:33.371: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Jun 17 12:33:33.371: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Jun 17 12:33:33.371: INFO: Checking APIGroup: metrics.k8s.io
  Jun 17 12:33:33.372: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Jun 17 12:33:33.372: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Jun 17 12:33:33.372: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Jun 17 12:33:33.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-3481" for this suite. @ 06/17/23 12:33:33.376
• [0.316 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 06/17/23 12:33:33.384
  Jun 17 12:33:33.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename namespaces @ 06/17/23 12:33:33.385
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:33:33.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:33:33.406
  STEP: Updating Namespace "namespaces-293" @ 06/17/23 12:33:33.409
  Jun 17 12:33:33.416: INFO: Namespace "namespaces-293" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"0dfee738-58d2-42c6-8483-6cb3be66b003", "kubernetes.io/metadata.name":"namespaces-293", "namespaces-293":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Jun 17 12:33:33.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-293" for this suite. @ 06/17/23 12:33:33.42
• [0.041 seconds]
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 06/17/23 12:33:33.426
  Jun 17 12:33:33.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename limitrange @ 06/17/23 12:33:33.426
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:33:33.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:33:33.447
  STEP: Creating LimitRange "e2e-limitrange-cls4c" in namespace "limitrange-5381" @ 06/17/23 12:33:33.449
  STEP: Creating another limitRange in another namespace @ 06/17/23 12:33:33.453
  Jun 17 12:33:33.469: INFO: Namespace "e2e-limitrange-cls4c-6202" created
  Jun 17 12:33:33.469: INFO: Creating LimitRange "e2e-limitrange-cls4c" in namespace "e2e-limitrange-cls4c-6202"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-cls4c" @ 06/17/23 12:33:33.474
  Jun 17 12:33:33.476: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-cls4c" in "limitrange-5381" namespace @ 06/17/23 12:33:33.476
  Jun 17 12:33:33.482: INFO: LimitRange "e2e-limitrange-cls4c" has been patched
  STEP: Delete LimitRange "e2e-limitrange-cls4c" by Collection with labelSelector: "e2e-limitrange-cls4c=patched" @ 06/17/23 12:33:33.482
  STEP: Confirm that the limitRange "e2e-limitrange-cls4c" has been deleted @ 06/17/23 12:33:33.488
  Jun 17 12:33:33.488: INFO: Requesting list of LimitRange to confirm quantity
  Jun 17 12:33:33.491: INFO: Found 0 LimitRange with label "e2e-limitrange-cls4c=patched"
  Jun 17 12:33:33.491: INFO: LimitRange "e2e-limitrange-cls4c" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-cls4c" @ 06/17/23 12:33:33.491
  Jun 17 12:33:33.494: INFO: Found 1 limitRange
  Jun 17 12:33:33.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-5381" for this suite. @ 06/17/23 12:33:33.497
  STEP: Destroying namespace "e2e-limitrange-cls4c-6202" for this suite. @ 06/17/23 12:33:33.504
• [0.084 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 06/17/23 12:33:33.51
  Jun 17 12:33:33.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename services @ 06/17/23 12:33:33.511
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:33:33.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:33:33.531
  STEP: creating service in namespace services-5804 @ 06/17/23 12:33:33.534
  STEP: creating service affinity-nodeport-transition in namespace services-5804 @ 06/17/23 12:33:33.534
  STEP: creating replication controller affinity-nodeport-transition in namespace services-5804 @ 06/17/23 12:33:33.546
  I0617 12:33:33.553586      19 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-5804, replica count: 3
  I0617 12:33:36.605219      19 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 17 12:33:36.614: INFO: Creating new exec pod
  Jun 17 12:33:39.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-5804 exec execpod-affinityhxnlr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Jun 17 12:33:39.763: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Jun 17 12:33:39.763: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 12:33:39.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-5804 exec execpod-affinityhxnlr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.53 80'
  Jun 17 12:33:39.899: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.53 80\nConnection to 10.152.183.53 80 port [tcp/http] succeeded!\n"
  Jun 17 12:33:39.899: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 12:33:39.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-5804 exec execpod-affinityhxnlr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.25.17 30673'
  Jun 17 12:33:40.026: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.25.17 30673\nConnection to 172.31.25.17 30673 port [tcp/*] succeeded!\n"
  Jun 17 12:33:40.026: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 12:33:40.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-5804 exec execpod-affinityhxnlr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.86.18 30673'
  Jun 17 12:33:40.150: INFO: stderr: "+ nc -v -t -w 2 172.31.86.18 30673\n+ echo hostName\nConnection to 172.31.86.18 30673 port [tcp/*] succeeded!\n"
  Jun 17 12:33:40.150: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 12:33:40.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-5804 exec execpod-affinityhxnlr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.25.17:30673/ ; done'
  Jun 17 12:33:40.404: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n"
  Jun 17 12:33:40.404: INFO: stdout: "\naffinity-nodeport-transition-4sf9x\naffinity-nodeport-transition-lkvfv\naffinity-nodeport-transition-lkvfv\naffinity-nodeport-transition-lkvfv\naffinity-nodeport-transition-lkvfv\naffinity-nodeport-transition-4sf9x\naffinity-nodeport-transition-lkvfv\naffinity-nodeport-transition-lkvfv\naffinity-nodeport-transition-4sf9x\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-lkvfv\naffinity-nodeport-transition-4sf9x\naffinity-nodeport-transition-4sf9x\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-4sf9x\naffinity-nodeport-transition-984qk"
  Jun 17 12:33:40.404: INFO: Received response from host: affinity-nodeport-transition-4sf9x
  Jun 17 12:33:40.404: INFO: Received response from host: affinity-nodeport-transition-lkvfv
  Jun 17 12:33:40.404: INFO: Received response from host: affinity-nodeport-transition-lkvfv
  Jun 17 12:33:40.404: INFO: Received response from host: affinity-nodeport-transition-lkvfv
  Jun 17 12:33:40.404: INFO: Received response from host: affinity-nodeport-transition-lkvfv
  Jun 17 12:33:40.404: INFO: Received response from host: affinity-nodeport-transition-4sf9x
  Jun 17 12:33:40.404: INFO: Received response from host: affinity-nodeport-transition-lkvfv
  Jun 17 12:33:40.404: INFO: Received response from host: affinity-nodeport-transition-lkvfv
  Jun 17 12:33:40.404: INFO: Received response from host: affinity-nodeport-transition-4sf9x
  Jun 17 12:33:40.404: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.404: INFO: Received response from host: affinity-nodeport-transition-lkvfv
  Jun 17 12:33:40.404: INFO: Received response from host: affinity-nodeport-transition-4sf9x
  Jun 17 12:33:40.405: INFO: Received response from host: affinity-nodeport-transition-4sf9x
  Jun 17 12:33:40.405: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.405: INFO: Received response from host: affinity-nodeport-transition-4sf9x
  Jun 17 12:33:40.405: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-5804 exec execpod-affinityhxnlr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.25.17:30673/ ; done'
  Jun 17 12:33:40.634: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.25.17:30673/\n"
  Jun 17 12:33:40.634: INFO: stdout: "\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-984qk\naffinity-nodeport-transition-984qk"
  Jun 17 12:33:40.634: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.634: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.634: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.634: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.634: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.634: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.634: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.634: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.634: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.634: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.634: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.634: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.634: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.634: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.634: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.634: INFO: Received response from host: affinity-nodeport-transition-984qk
  Jun 17 12:33:40.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 17 12:33:40.638: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5804, will wait for the garbage collector to delete the pods @ 06/17/23 12:33:40.65
  Jun 17 12:33:40.710: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.067648ms
  Jun 17 12:33:40.811: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.325078ms
  STEP: Destroying namespace "services-5804" for this suite. @ 06/17/23 12:33:43.135
• [9.631 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 06/17/23 12:33:43.141
  Jun 17 12:33:43.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename sched-pred @ 06/17/23 12:33:43.142
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:33:43.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:33:43.161
  Jun 17 12:33:43.164: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jun 17 12:33:43.174: INFO: Waiting for terminating namespaces to be deleted...
  Jun 17 12:33:43.176: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-25-17 before test
  Jun 17 12:33:43.181: INFO: nginx-ingress-controller-kubernetes-worker-wgt59 from ingress-nginx-kubernetes-worker started at 2023-06-17 11:56:23 +0000 UTC (1 container statuses recorded)
  Jun 17 12:33:43.181: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 17 12:33:43.181: INFO: sonobuoy from sonobuoy started at 2023-06-17 12:02:27 +0000 UTC (1 container statuses recorded)
  Jun 17 12:33:43.181: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jun 17 12:33:43.181: INFO: sonobuoy-systemd-logs-daemon-set-1a2f55ba8cf84413-46gwp from sonobuoy started at 2023-06-17 12:02:29 +0000 UTC (2 container statuses recorded)
  Jun 17 12:33:43.182: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 17 12:33:43.182: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 17 12:33:43.182: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-68-253 before test
  Jun 17 12:33:43.188: INFO: default-http-backend-kubernetes-worker-65fc475d49-7hp8p from ingress-nginx-kubernetes-worker started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:33:43.188: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jun 17 12:33:43.188: INFO: nginx-ingress-controller-kubernetes-worker-hfc8s from ingress-nginx-kubernetes-worker started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:33:43.188: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 17 12:33:43.188: INFO: calico-kube-controllers-867f6b8548-wrvps from kube-system started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:33:43.188: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jun 17 12:33:43.188: INFO: coredns-5c7f76ccb8-gc7gk from kube-system started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:33:43.188: INFO: 	Container coredns ready: true, restart count 0
  Jun 17 12:33:43.188: INFO: kube-state-metrics-5b95b4459c-dgvxs from kube-system started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:33:43.189: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jun 17 12:33:43.189: INFO: metrics-server-v0.5.2-6cf8c8b69c-z9drm from kube-system started at 2023-06-17 11:49:37 +0000 UTC (2 container statuses recorded)
  Jun 17 12:33:43.189: INFO: 	Container metrics-server ready: true, restart count 0
  Jun 17 12:33:43.189: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jun 17 12:33:43.189: INFO: dashboard-metrics-scraper-6b8586b5c9-bpx9l from kubernetes-dashboard started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:33:43.189: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jun 17 12:33:43.189: INFO: kubernetes-dashboard-6869f4cd5f-blwp9 from kubernetes-dashboard started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:33:43.189: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jun 17 12:33:43.189: INFO: sonobuoy-systemd-logs-daemon-set-1a2f55ba8cf84413-dk2pv from sonobuoy started at 2023-06-17 12:02:29 +0000 UTC (2 container statuses recorded)
  Jun 17 12:33:43.189: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 17 12:33:43.189: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 17 12:33:43.189: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-86-18 before test
  Jun 17 12:33:43.193: INFO: nginx-ingress-controller-kubernetes-worker-jqhpt from ingress-nginx-kubernetes-worker started at 2023-06-17 11:58:10 +0000 UTC (1 container statuses recorded)
  Jun 17 12:33:43.193: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 17 12:33:43.193: INFO: sonobuoy-e2e-job-c63a99f81b61472e from sonobuoy started at 2023-06-17 12:02:29 +0000 UTC (2 container statuses recorded)
  Jun 17 12:33:43.193: INFO: 	Container e2e ready: true, restart count 0
  Jun 17 12:33:43.194: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 17 12:33:43.194: INFO: sonobuoy-systemd-logs-daemon-set-1a2f55ba8cf84413-9ngvl from sonobuoy started at 2023-06-17 12:02:29 +0000 UTC (2 container statuses recorded)
  Jun 17 12:33:43.194: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 17 12:33:43.194: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 06/17/23 12:33:43.194
  STEP: Explicitly delete pod here to free the resource it takes. @ 06/17/23 12:33:45.211
  STEP: Trying to apply a random label on the found node. @ 06/17/23 12:33:45.223
  STEP: verifying the node has the label kubernetes.io/e2e-0d6928f7-93b5-46e4-a17f-04377ddf1e09 42 @ 06/17/23 12:33:45.23
  STEP: Trying to relaunch the pod, now with labels. @ 06/17/23 12:33:45.233
  STEP: removing the label kubernetes.io/e2e-0d6928f7-93b5-46e4-a17f-04377ddf1e09 off the node ip-172-31-25-17 @ 06/17/23 12:33:47.248
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-0d6928f7-93b5-46e4-a17f-04377ddf1e09 @ 06/17/23 12:33:47.258
  Jun 17 12:33:47.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-162" for this suite. @ 06/17/23 12:33:47.266
• [4.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 06/17/23 12:33:47.272
  Jun 17 12:33:47.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename daemonsets @ 06/17/23 12:33:47.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:33:47.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:33:47.293
  Jun 17 12:33:47.312: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/17/23 12:33:47.318
  Jun 17 12:33:47.321: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:47.321: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:47.324: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 12:33:47.324: INFO: Node ip-172-31-25-17 is running 0 daemon pod, expected 1
  Jun 17 12:33:48.329: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:48.329: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:48.332: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 17 12:33:48.332: INFO: Node ip-172-31-68-253 is running 0 daemon pod, expected 1
  Jun 17 12:33:49.328: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:49.328: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:49.331: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 17 12:33:49.331: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 06/17/23 12:33:49.342
  STEP: Check that daemon pods images are updated. @ 06/17/23 12:33:49.353
  Jun 17 12:33:49.357: INFO: Wrong image for pod: daemon-set-nkv6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 17 12:33:49.357: INFO: Wrong image for pod: daemon-set-w9xfb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 17 12:33:49.357: INFO: Wrong image for pod: daemon-set-xzjcr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 17 12:33:49.360: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:49.360: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:50.365: INFO: Wrong image for pod: daemon-set-w9xfb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 17 12:33:50.365: INFO: Wrong image for pod: daemon-set-xzjcr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 17 12:33:50.368: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:50.368: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:51.364: INFO: Pod daemon-set-ml8t6 is not available
  Jun 17 12:33:51.364: INFO: Wrong image for pod: daemon-set-w9xfb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 17 12:33:51.364: INFO: Wrong image for pod: daemon-set-xzjcr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 17 12:33:51.367: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:51.367: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:52.364: INFO: Wrong image for pod: daemon-set-w9xfb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 17 12:33:52.367: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:52.367: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:53.366: INFO: Pod daemon-set-s55ct is not available
  Jun 17 12:33:53.366: INFO: Wrong image for pod: daemon-set-w9xfb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 17 12:33:53.372: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:53.373: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:54.367: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:54.368: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:55.364: INFO: Pod daemon-set-hhv82 is not available
  Jun 17 12:33:55.367: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:55.367: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 06/17/23 12:33:55.367
  Jun 17 12:33:55.371: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:55.371: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:55.374: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 17 12:33:55.374: INFO: Node ip-172-31-86-18 is running 0 daemon pod, expected 1
  Jun 17 12:33:56.378: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:56.378: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:33:56.381: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 17 12:33:56.381: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 06/17/23 12:33:56.395
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7603, will wait for the garbage collector to delete the pods @ 06/17/23 12:33:56.395
  Jun 17 12:33:56.455: INFO: Deleting DaemonSet.extensions daemon-set took: 5.578732ms
  Jun 17 12:33:56.555: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.395974ms
  Jun 17 12:33:58.160: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 12:33:58.160: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 17 12:33:58.163: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12478"},"items":null}

  Jun 17 12:33:58.166: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12478"},"items":null}

  Jun 17 12:33:58.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7603" for this suite. @ 06/17/23 12:33:58.184
• [10.917 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 06/17/23 12:33:58.191
  Jun 17 12:33:58.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir @ 06/17/23 12:33:58.192
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:33:58.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:33:58.213
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 06/17/23 12:33:58.217
  STEP: Saw pod success @ 06/17/23 12:34:02.235
  Jun 17 12:34:02.238: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-de754861-4652-4c2c-b426-b2c7614274a5 container test-container: <nil>
  STEP: delete the pod @ 06/17/23 12:34:02.252
  Jun 17 12:34:02.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-488" for this suite. @ 06/17/23 12:34:02.272
• [4.087 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 06/17/23 12:34:02.278
  Jun 17 12:34:02.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl @ 06/17/23 12:34:02.279
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:34:02.298
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:34:02.3
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 06/17/23 12:34:02.303
  Jun 17 12:34:02.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-5487 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Jun 17 12:34:02.378: INFO: stderr: ""
  Jun 17 12:34:02.378: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 06/17/23 12:34:02.378
  Jun 17 12:34:02.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-5487 delete pods e2e-test-httpd-pod'
  Jun 17 12:34:04.366: INFO: stderr: ""
  Jun 17 12:34:04.366: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jun 17 12:34:04.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5487" for this suite. @ 06/17/23 12:34:04.37
• [2.101 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 06/17/23 12:34:04.379
  Jun 17 12:34:04.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 12:34:04.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:34:04.4
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:34:04.403
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 12:34:04.406
  STEP: Saw pod success @ 06/17/23 12:34:08.43
  Jun 17 12:34:08.433: INFO: Trying to get logs from node ip-172-31-25-17 pod downwardapi-volume-c5482093-7865-4ca3-9c11-18b558708f9d container client-container: <nil>
  STEP: delete the pod @ 06/17/23 12:34:08.44
  Jun 17 12:34:08.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9063" for this suite. @ 06/17/23 12:34:08.46
• [4.087 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 06/17/23 12:34:08.466
  Jun 17 12:34:08.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 12:34:08.467
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:34:08.485
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:34:08.488
  STEP: Creating configMap with name configmap-projected-all-test-volume-261e8bb8-b551-45e4-9ff2-51242fd9eba0 @ 06/17/23 12:34:08.49
  STEP: Creating secret with name secret-projected-all-test-volume-b1f635d1-ba42-4057-8b27-7000b00e771d @ 06/17/23 12:34:08.494
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 06/17/23 12:34:08.498
  STEP: Saw pod success @ 06/17/23 12:34:12.517
  Jun 17 12:34:12.521: INFO: Trying to get logs from node ip-172-31-25-17 pod projected-volume-e31a2c9a-bed1-4f20-a94c-2bf22179f5a2 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 06/17/23 12:34:12.527
  Jun 17 12:34:12.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3944" for this suite. @ 06/17/23 12:34:12.543
• [4.083 seconds]
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 06/17/23 12:34:12.549
  Jun 17 12:34:12.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename events @ 06/17/23 12:34:12.55
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:34:12.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:34:12.57
  STEP: creating a test event @ 06/17/23 12:34:12.573
  STEP: listing all events in all namespaces @ 06/17/23 12:34:12.578
  STEP: patching the test event @ 06/17/23 12:34:12.584
  STEP: fetching the test event @ 06/17/23 12:34:12.589
  STEP: updating the test event @ 06/17/23 12:34:12.592
  STEP: getting the test event @ 06/17/23 12:34:12.601
  STEP: deleting the test event @ 06/17/23 12:34:12.603
  STEP: listing all events in all namespaces @ 06/17/23 12:34:12.609
  Jun 17 12:34:12.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8284" for this suite. @ 06/17/23 12:34:12.618
• [0.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 06/17/23 12:34:12.626
  Jun 17 12:34:12.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-probe @ 06/17/23 12:34:12.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:34:12.646
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:34:12.649
  STEP: Creating pod liveness-c3c85378-9c99-46e4-8edd-ff6f555dff30 in namespace container-probe-1180 @ 06/17/23 12:34:12.652
  Jun 17 12:34:14.668: INFO: Started pod liveness-c3c85378-9c99-46e4-8edd-ff6f555dff30 in namespace container-probe-1180
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/17/23 12:34:14.668
  Jun 17 12:34:14.671: INFO: Initial restart count of pod liveness-c3c85378-9c99-46e4-8edd-ff6f555dff30 is 0
  Jun 17 12:38:15.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/17/23 12:38:15.184
  STEP: Destroying namespace "container-probe-1180" for this suite. @ 06/17/23 12:38:15.198
• [242.579 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 06/17/23 12:38:15.206
  Jun 17 12:38:15.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/17/23 12:38:15.206
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:38:15.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:38:15.228
  STEP: set up a multi version CRD @ 06/17/23 12:38:15.233
  Jun 17 12:38:15.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: rename a version @ 06/17/23 12:38:18.724
  STEP: check the new version name is served @ 06/17/23 12:38:18.737
  STEP: check the old version name is removed @ 06/17/23 12:38:20.114
  STEP: check the other version is not changed @ 06/17/23 12:38:20.804
  Jun 17 12:38:23.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5892" for this suite. @ 06/17/23 12:38:23.531
• [8.334 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 06/17/23 12:38:23.54
  Jun 17 12:38:23.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename containers @ 06/17/23 12:38:23.541
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:38:23.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:38:23.564
  Jun 17 12:38:25.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7617" for this suite. @ 06/17/23 12:38:25.61
• [2.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 06/17/23 12:38:25.619
  Jun 17 12:38:25.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename statefulset @ 06/17/23 12:38:25.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:38:25.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:38:25.642
  STEP: Creating service test in namespace statefulset-4299 @ 06/17/23 12:38:25.646
  Jun 17 12:38:25.666: INFO: Found 0 stateful pods, waiting for 1
  Jun 17 12:38:35.675: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 06/17/23 12:38:35.682
  W0617 12:38:35.693060      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jun 17 12:38:35.701: INFO: Found 1 stateful pods, waiting for 2
  Jun 17 12:38:45.706: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 17 12:38:45.706: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 06/17/23 12:38:45.715
  STEP: Delete all of the StatefulSets @ 06/17/23 12:38:45.719
  STEP: Verify that StatefulSets have been deleted @ 06/17/23 12:38:45.73
  Jun 17 12:38:45.738: INFO: Deleting all statefulset in ns statefulset-4299
  Jun 17 12:38:45.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4299" for this suite. @ 06/17/23 12:38:45.781
• [20.171 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 06/17/23 12:38:45.792
  Jun 17 12:38:45.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename deployment @ 06/17/23 12:38:45.795
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:38:45.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:38:45.817
  Jun 17 12:38:45.821: INFO: Creating deployment "test-recreate-deployment"
  Jun 17 12:38:45.827: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Jun 17 12:38:45.834: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  Jun 17 12:38:47.843: INFO: Waiting deployment "test-recreate-deployment" to complete
  Jun 17 12:38:47.847: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Jun 17 12:38:47.857: INFO: Updating deployment test-recreate-deployment
  Jun 17 12:38:47.857: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Jun 17 12:38:47.960: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-3211  3e4f00e6-9d8a-45f7-90aa-0d29c7400c9e 13480 2 2023-06-17 12:38:45 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-17 12:38:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:38:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e45a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-17 12:38:47 +0000 UTC,LastTransitionTime:2023-06-17 12:38:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-06-17 12:38:47 +0000 UTC,LastTransitionTime:2023-06-17 12:38:45 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Jun 17 12:38:47.963: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-3211  449169e3-c086-43a2-a963-598bb652e56a 13478 1 2023-06-17 12:38:47 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 3e4f00e6-9d8a-45f7-90aa-0d29c7400c9e 0xc004deedb7 0xc004deedb8}] [] [{kube-controller-manager Update apps/v1 2023-06-17 12:38:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3e4f00e6-9d8a-45f7-90aa-0d29c7400c9e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:38:47 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004deee58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 17 12:38:47.963: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Jun 17 12:38:47.963: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-3211  5c9a0c26-b065-4308-b841-3ccdd50e1102 13468 2 2023-06-17 12:38:45 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 3e4f00e6-9d8a-45f7-90aa-0d29c7400c9e 0xc004deeec7 0xc004deeec8}] [] [{kube-controller-manager Update apps/v1 2023-06-17 12:38:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3e4f00e6-9d8a-45f7-90aa-0d29c7400c9e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:38:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004deef78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 17 12:38:47.967: INFO: Pod "test-recreate-deployment-54757ffd6c-lhn5t" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-lhn5t test-recreate-deployment-54757ffd6c- deployment-3211  782a2a4c-a522-42de-92de-709f0d7976d5 13479 0 2023-06-17 12:38:47 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 449169e3-c086-43a2-a963-598bb652e56a 0xc004def407 0xc004def408}] [] [{kube-controller-manager Update v1 2023-06-17 12:38:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"449169e3-c086-43a2-a963-598bb652e56a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 12:38:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bfrcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bfrcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:38:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:38:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:38:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:38:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:,StartTime:2023-06-17 12:38:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 12:38:47.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3211" for this suite. @ 06/17/23 12:38:47.971
• [2.185 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 06/17/23 12:38:47.978
  Jun 17 12:38:47.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 12:38:47.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:38:47.996
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:38:48
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 12:38:48.004
  STEP: Saw pod success @ 06/17/23 12:38:52.031
  Jun 17 12:38:52.034: INFO: Trying to get logs from node ip-172-31-25-17 pod downwardapi-volume-d046b1b7-c5aa-4a1f-8217-6bf111f8f6de container client-container: <nil>
  STEP: delete the pod @ 06/17/23 12:38:52.043
  Jun 17 12:38:52.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6616" for this suite. @ 06/17/23 12:38:52.067
• [4.097 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 06/17/23 12:38:52.075
  Jun 17 12:38:52.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 12:38:52.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:38:52.093
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:38:52.096
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-2c5ee094-0725-4e12-839c-de7316a10b54 @ 06/17/23 12:38:52.104
  STEP: Creating the pod @ 06/17/23 12:38:52.11
  STEP: Updating configmap projected-configmap-test-upd-2c5ee094-0725-4e12-839c-de7316a10b54 @ 06/17/23 12:38:54.144
  STEP: waiting to observe update in volume @ 06/17/23 12:38:54.149
  Jun 17 12:38:58.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5422" for this suite. @ 06/17/23 12:38:58.183
• [6.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 06/17/23 12:38:58.193
  Jun 17 12:38:58.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename watch @ 06/17/23 12:38:58.193
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:38:58.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:38:58.213
  STEP: getting a starting resourceVersion @ 06/17/23 12:38:58.217
  STEP: starting a background goroutine to produce watch events @ 06/17/23 12:38:58.221
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 06/17/23 12:38:58.221
  Jun 17 12:39:01.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4948" for this suite. @ 06/17/23 12:39:01.049
• [2.910 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 06/17/23 12:39:01.103
  Jun 17 12:39:01.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename sched-pred @ 06/17/23 12:39:01.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:01.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:01.124
  Jun 17 12:39:01.128: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jun 17 12:39:01.136: INFO: Waiting for terminating namespaces to be deleted...
  Jun 17 12:39:01.140: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-25-17 before test
  Jun 17 12:39:01.146: INFO: nginx-ingress-controller-kubernetes-worker-wgt59 from ingress-nginx-kubernetes-worker started at 2023-06-17 11:56:23 +0000 UTC (1 container statuses recorded)
  Jun 17 12:39:01.146: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 17 12:39:01.146: INFO: pod-projected-configmaps-f6c66ec3-aaf5-4087-a6bc-6d4b5f2827db from projected-5422 started at 2023-06-17 12:38:52 +0000 UTC (1 container statuses recorded)
  Jun 17 12:39:01.146: INFO: 	Container agnhost-container ready: true, restart count 0
  Jun 17 12:39:01.146: INFO: sonobuoy from sonobuoy started at 2023-06-17 12:02:27 +0000 UTC (1 container statuses recorded)
  Jun 17 12:39:01.146: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jun 17 12:39:01.146: INFO: sonobuoy-systemd-logs-daemon-set-1a2f55ba8cf84413-46gwp from sonobuoy started at 2023-06-17 12:02:29 +0000 UTC (2 container statuses recorded)
  Jun 17 12:39:01.146: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 17 12:39:01.146: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 17 12:39:01.146: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-68-253 before test
  Jun 17 12:39:01.152: INFO: default-http-backend-kubernetes-worker-65fc475d49-7hp8p from ingress-nginx-kubernetes-worker started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:39:01.152: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jun 17 12:39:01.152: INFO: nginx-ingress-controller-kubernetes-worker-hfc8s from ingress-nginx-kubernetes-worker started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:39:01.152: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 17 12:39:01.152: INFO: calico-kube-controllers-867f6b8548-wrvps from kube-system started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:39:01.152: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jun 17 12:39:01.152: INFO: coredns-5c7f76ccb8-gc7gk from kube-system started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:39:01.152: INFO: 	Container coredns ready: true, restart count 0
  Jun 17 12:39:01.152: INFO: kube-state-metrics-5b95b4459c-dgvxs from kube-system started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:39:01.152: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jun 17 12:39:01.152: INFO: metrics-server-v0.5.2-6cf8c8b69c-z9drm from kube-system started at 2023-06-17 11:49:37 +0000 UTC (2 container statuses recorded)
  Jun 17 12:39:01.152: INFO: 	Container metrics-server ready: true, restart count 0
  Jun 17 12:39:01.152: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jun 17 12:39:01.152: INFO: dashboard-metrics-scraper-6b8586b5c9-bpx9l from kubernetes-dashboard started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:39:01.152: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jun 17 12:39:01.152: INFO: kubernetes-dashboard-6869f4cd5f-blwp9 from kubernetes-dashboard started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 12:39:01.152: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jun 17 12:39:01.152: INFO: sonobuoy-systemd-logs-daemon-set-1a2f55ba8cf84413-dk2pv from sonobuoy started at 2023-06-17 12:02:29 +0000 UTC (2 container statuses recorded)
  Jun 17 12:39:01.152: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 17 12:39:01.152: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 17 12:39:01.152: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-86-18 before test
  Jun 17 12:39:01.158: INFO: nginx-ingress-controller-kubernetes-worker-jqhpt from ingress-nginx-kubernetes-worker started at 2023-06-17 11:58:10 +0000 UTC (1 container statuses recorded)
  Jun 17 12:39:01.158: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 17 12:39:01.158: INFO: sonobuoy-e2e-job-c63a99f81b61472e from sonobuoy started at 2023-06-17 12:02:29 +0000 UTC (2 container statuses recorded)
  Jun 17 12:39:01.158: INFO: 	Container e2e ready: true, restart count 0
  Jun 17 12:39:01.158: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 17 12:39:01.158: INFO: sonobuoy-systemd-logs-daemon-set-1a2f55ba8cf84413-9ngvl from sonobuoy started at 2023-06-17 12:02:29 +0000 UTC (2 container statuses recorded)
  Jun 17 12:39:01.158: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 17 12:39:01.158: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ip-172-31-25-17 @ 06/17/23 12:39:01.173
  STEP: verifying the node has the label node ip-172-31-68-253 @ 06/17/23 12:39:01.188
  STEP: verifying the node has the label node ip-172-31-86-18 @ 06/17/23 12:39:01.208
  Jun 17 12:39:01.226: INFO: Pod default-http-backend-kubernetes-worker-65fc475d49-7hp8p requesting resource cpu=10m on Node ip-172-31-68-253
  Jun 17 12:39:01.226: INFO: Pod nginx-ingress-controller-kubernetes-worker-hfc8s requesting resource cpu=0m on Node ip-172-31-68-253
  Jun 17 12:39:01.226: INFO: Pod nginx-ingress-controller-kubernetes-worker-jqhpt requesting resource cpu=0m on Node ip-172-31-86-18
  Jun 17 12:39:01.226: INFO: Pod nginx-ingress-controller-kubernetes-worker-wgt59 requesting resource cpu=0m on Node ip-172-31-25-17
  Jun 17 12:39:01.226: INFO: Pod calico-kube-controllers-867f6b8548-wrvps requesting resource cpu=0m on Node ip-172-31-68-253
  Jun 17 12:39:01.226: INFO: Pod coredns-5c7f76ccb8-gc7gk requesting resource cpu=100m on Node ip-172-31-68-253
  Jun 17 12:39:01.226: INFO: Pod kube-state-metrics-5b95b4459c-dgvxs requesting resource cpu=0m on Node ip-172-31-68-253
  Jun 17 12:39:01.226: INFO: Pod metrics-server-v0.5.2-6cf8c8b69c-z9drm requesting resource cpu=5m on Node ip-172-31-68-253
  Jun 17 12:39:01.226: INFO: Pod dashboard-metrics-scraper-6b8586b5c9-bpx9l requesting resource cpu=0m on Node ip-172-31-68-253
  Jun 17 12:39:01.226: INFO: Pod kubernetes-dashboard-6869f4cd5f-blwp9 requesting resource cpu=0m on Node ip-172-31-68-253
  Jun 17 12:39:01.226: INFO: Pod pod-projected-configmaps-f6c66ec3-aaf5-4087-a6bc-6d4b5f2827db requesting resource cpu=0m on Node ip-172-31-25-17
  Jun 17 12:39:01.226: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-25-17
  Jun 17 12:39:01.226: INFO: Pod sonobuoy-e2e-job-c63a99f81b61472e requesting resource cpu=0m on Node ip-172-31-86-18
  Jun 17 12:39:01.226: INFO: Pod sonobuoy-systemd-logs-daemon-set-1a2f55ba8cf84413-46gwp requesting resource cpu=0m on Node ip-172-31-25-17
  Jun 17 12:39:01.226: INFO: Pod sonobuoy-systemd-logs-daemon-set-1a2f55ba8cf84413-9ngvl requesting resource cpu=0m on Node ip-172-31-86-18
  Jun 17 12:39:01.226: INFO: Pod sonobuoy-systemd-logs-daemon-set-1a2f55ba8cf84413-dk2pv requesting resource cpu=0m on Node ip-172-31-68-253
  STEP: Starting Pods to consume most of the cluster CPU. @ 06/17/23 12:39:01.226
  Jun 17 12:39:01.226: INFO: Creating a pod which consumes cpu=1319m on Node ip-172-31-68-253
  Jun 17 12:39:01.238: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-86-18
  Jun 17 12:39:01.246: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-25-17
  STEP: Creating another pod that requires unavailable amount of CPU. @ 06/17/23 12:39:03.277
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-20414d6d-a7a9-4f9a-9c7f-166679c6f2ed.1769729a04d78137], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7418/filler-pod-20414d6d-a7a9-4f9a-9c7f-166679c6f2ed to ip-172-31-86-18] @ 06/17/23 12:39:03.281
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-20414d6d-a7a9-4f9a-9c7f-166679c6f2ed.1769729a2a9add7f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 06/17/23 12:39:03.281
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-20414d6d-a7a9-4f9a-9c7f-166679c6f2ed.1769729a2b9b39e7], Reason = [Created], Message = [Created container filler-pod-20414d6d-a7a9-4f9a-9c7f-166679c6f2ed] @ 06/17/23 12:39:03.281
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-20414d6d-a7a9-4f9a-9c7f-166679c6f2ed.1769729a2f6edd79], Reason = [Started], Message = [Started container filler-pod-20414d6d-a7a9-4f9a-9c7f-166679c6f2ed] @ 06/17/23 12:39:03.281
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-570dabcc-1c62-478d-aa49-b34b155b79c6.1769729a0623ead6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7418/filler-pod-570dabcc-1c62-478d-aa49-b34b155b79c6 to ip-172-31-25-17] @ 06/17/23 12:39:03.282
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-570dabcc-1c62-478d-aa49-b34b155b79c6.1769729a2bb585f2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 06/17/23 12:39:03.282
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-570dabcc-1c62-478d-aa49-b34b155b79c6.1769729a2ce0a573], Reason = [Created], Message = [Created container filler-pod-570dabcc-1c62-478d-aa49-b34b155b79c6] @ 06/17/23 12:39:03.282
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-570dabcc-1c62-478d-aa49-b34b155b79c6.1769729a3006c9f7], Reason = [Started], Message = [Started container filler-pod-570dabcc-1c62-478d-aa49-b34b155b79c6] @ 06/17/23 12:39:03.282
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d9dc87a1-b810-462d-b602-7fb73c9ef129.1769729a04112a02], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7418/filler-pod-d9dc87a1-b810-462d-b602-7fb73c9ef129 to ip-172-31-68-253] @ 06/17/23 12:39:03.282
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d9dc87a1-b810-462d-b602-7fb73c9ef129.1769729a29922c4c], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] @ 06/17/23 12:39:03.282
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d9dc87a1-b810-462d-b602-7fb73c9ef129.1769729a3c6eb5ea], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 316.390955ms (316.427891ms including waiting)] @ 06/17/23 12:39:03.282
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d9dc87a1-b810-462d-b602-7fb73c9ef129.1769729a3d52e7ec], Reason = [Created], Message = [Created container filler-pod-d9dc87a1-b810-462d-b602-7fb73c9ef129] @ 06/17/23 12:39:03.282
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d9dc87a1-b810-462d-b602-7fb73c9ef129.1769729a40e5ad63], Reason = [Started], Message = [Started container filler-pod-d9dc87a1-b810-462d-b602-7fb73c9ef129] @ 06/17/23 12:39:03.282
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.1769729a7e0c2e1b], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/5 nodes are available: 2 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] @ 06/17/23 12:39:03.301
  STEP: removing the label node off the node ip-172-31-25-17 @ 06/17/23 12:39:04.299
  STEP: verifying the node doesn't have the label node @ 06/17/23 12:39:04.312
  STEP: removing the label node off the node ip-172-31-68-253 @ 06/17/23 12:39:04.316
  STEP: verifying the node doesn't have the label node @ 06/17/23 12:39:04.329
  STEP: removing the label node off the node ip-172-31-86-18 @ 06/17/23 12:39:04.332
  STEP: verifying the node doesn't have the label node @ 06/17/23 12:39:04.348
  Jun 17 12:39:04.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-7418" for this suite. @ 06/17/23 12:39:04.36
• [3.265 seconds]
------------------------------
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 06/17/23 12:39:04.368
  Jun 17 12:39:04.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 06/17/23 12:39:04.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:04.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:04.389
  STEP: Setting up the test @ 06/17/23 12:39:04.397
  STEP: Creating hostNetwork=false pod @ 06/17/23 12:39:04.397
  STEP: Creating hostNetwork=true pod @ 06/17/23 12:39:06.422
  STEP: Running the test @ 06/17/23 12:39:08.441
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 06/17/23 12:39:08.441
  Jun 17 12:39:08.441: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-743 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 12:39:08.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:39:08.442: INFO: ExecWithOptions: Clientset creation
  Jun 17 12:39:08.442: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jun 17 12:39:08.503: INFO: Exec stderr: ""
  Jun 17 12:39:08.504: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-743 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 12:39:08.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:39:08.504: INFO: ExecWithOptions: Clientset creation
  Jun 17 12:39:08.504: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jun 17 12:39:08.563: INFO: Exec stderr: ""
  Jun 17 12:39:08.563: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-743 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 12:39:08.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:39:08.564: INFO: ExecWithOptions: Clientset creation
  Jun 17 12:39:08.564: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jun 17 12:39:08.623: INFO: Exec stderr: ""
  Jun 17 12:39:08.623: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-743 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 12:39:08.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:39:08.624: INFO: ExecWithOptions: Clientset creation
  Jun 17 12:39:08.624: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jun 17 12:39:08.703: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 06/17/23 12:39:08.703
  Jun 17 12:39:08.703: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-743 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 12:39:08.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:39:08.704: INFO: ExecWithOptions: Clientset creation
  Jun 17 12:39:08.704: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jun 17 12:39:08.759: INFO: Exec stderr: ""
  Jun 17 12:39:08.759: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-743 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 12:39:08.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:39:08.760: INFO: ExecWithOptions: Clientset creation
  Jun 17 12:39:08.760: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jun 17 12:39:08.825: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 06/17/23 12:39:08.825
  Jun 17 12:39:08.825: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-743 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 12:39:08.825: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:39:08.825: INFO: ExecWithOptions: Clientset creation
  Jun 17 12:39:08.825: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-743/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jun 17 12:39:08.885: INFO: Exec stderr: ""
  Jun 17 12:39:08.885: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-743 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 12:39:08.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:39:08.886: INFO: ExecWithOptions: Clientset creation
  Jun 17 12:39:08.886: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-743/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jun 17 12:39:08.954: INFO: Exec stderr: ""
  Jun 17 12:39:08.954: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-743 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 12:39:08.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:39:08.955: INFO: ExecWithOptions: Clientset creation
  Jun 17 12:39:08.955: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-743/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jun 17 12:39:09.019: INFO: Exec stderr: ""
  Jun 17 12:39:09.019: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-743 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 12:39:09.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:39:09.021: INFO: ExecWithOptions: Clientset creation
  Jun 17 12:39:09.021: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-743/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jun 17 12:39:09.085: INFO: Exec stderr: ""
  Jun 17 12:39:09.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-743" for this suite. @ 06/17/23 12:39:09.091
• [4.729 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 06/17/23 12:39:09.098
  Jun 17 12:39:09.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 12:39:09.099
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:09.116
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:09.121
  STEP: Creating configMap with name projected-configmap-test-volume-fcc7ca2b-fd68-4014-b153-bd405753218c @ 06/17/23 12:39:09.124
  STEP: Creating a pod to test consume configMaps @ 06/17/23 12:39:09.13
  STEP: Saw pod success @ 06/17/23 12:39:13.153
  Jun 17 12:39:13.158: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-projected-configmaps-6625187b-a1e5-43ee-957d-5ed3ad870e32 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 06/17/23 12:39:13.165
  Jun 17 12:39:13.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9638" for this suite. @ 06/17/23 12:39:13.189
• [4.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 06/17/23 12:39:13.198
  Jun 17 12:39:13.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename services @ 06/17/23 12:39:13.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:13.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:13.219
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3848 @ 06/17/23 12:39:13.223
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 06/17/23 12:39:13.234
  STEP: creating service externalsvc in namespace services-3848 @ 06/17/23 12:39:13.235
  STEP: creating replication controller externalsvc in namespace services-3848 @ 06/17/23 12:39:13.252
  I0617 12:39:13.261993      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-3848, replica count: 2
  I0617 12:39:16.312568      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 06/17/23 12:39:16.317
  Jun 17 12:39:16.333: INFO: Creating new exec pod
  Jun 17 12:39:18.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-3848 exec execpodk2wnm -- /bin/sh -x -c nslookup clusterip-service.services-3848.svc.cluster.local'
  Jun 17 12:39:18.492: INFO: stderr: "+ nslookup clusterip-service.services-3848.svc.cluster.local\n"
  Jun 17 12:39:18.492: INFO: stdout: "Server:\t\t10.152.183.101\nAddress:\t10.152.183.101#53\n\nclusterip-service.services-3848.svc.cluster.local\tcanonical name = externalsvc.services-3848.svc.cluster.local.\nName:\texternalsvc.services-3848.svc.cluster.local\nAddress: 10.152.183.79\n\n"
  Jun 17 12:39:18.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-3848, will wait for the garbage collector to delete the pods @ 06/17/23 12:39:18.498
  Jun 17 12:39:18.561: INFO: Deleting ReplicationController externalsvc took: 7.544846ms
  Jun 17 12:39:18.661: INFO: Terminating ReplicationController externalsvc pods took: 100.854973ms
  Jun 17 12:39:21.081: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-3848" for this suite. @ 06/17/23 12:39:21.094
• [7.905 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 06/17/23 12:39:21.104
  Jun 17 12:39:21.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 12:39:21.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:21.121
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:21.125
  STEP: Setting up server cert @ 06/17/23 12:39:21.155
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 12:39:21.401
  STEP: Deploying the webhook pod @ 06/17/23 12:39:21.41
  STEP: Wait for the deployment to be ready @ 06/17/23 12:39:21.425
  Jun 17 12:39:21.432: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 06/17/23 12:39:23.445
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 12:39:23.464
  Jun 17 12:39:24.464: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun 17 12:39:24.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 06/17/23 12:39:24.981
  STEP: Creating a custom resource that should be denied by the webhook @ 06/17/23 12:39:25
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 06/17/23 12:39:27.028
  STEP: Updating the custom resource with disallowed data should be denied @ 06/17/23 12:39:27.037
  STEP: Deleting the custom resource should be denied @ 06/17/23 12:39:27.047
  STEP: Remove the offending key and value from the custom resource data @ 06/17/23 12:39:27.055
  STEP: Deleting the updated custom resource should be successful @ 06/17/23 12:39:27.068
  Jun 17 12:39:27.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-969" for this suite. @ 06/17/23 12:39:27.679
  STEP: Destroying namespace "webhook-markers-429" for this suite. @ 06/17/23 12:39:27.687
• [6.591 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 06/17/23 12:39:27.695
  Jun 17 12:39:27.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename configmap @ 06/17/23 12:39:27.696
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:27.712
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:27.716
  Jun 17 12:39:27.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7810" for this suite. @ 06/17/23 12:39:27.768
• [0.079 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 06/17/23 12:39:27.775
  Jun 17 12:39:27.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename secrets @ 06/17/23 12:39:27.776
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:27.793
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:27.796
  Jun 17 12:39:27.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9082" for this suite. @ 06/17/23 12:39:27.847
• [0.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 06/17/23 12:39:27.858
  Jun 17 12:39:27.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl @ 06/17/23 12:39:27.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:27.882
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:27.891
  STEP: create deployment with httpd image @ 06/17/23 12:39:27.897
  Jun 17 12:39:27.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-9809 create -f -'
  Jun 17 12:39:29.103: INFO: stderr: ""
  Jun 17 12:39:29.103: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 06/17/23 12:39:29.103
  Jun 17 12:39:29.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-9809 diff -f -'
  Jun 17 12:39:29.781: INFO: rc: 1
  Jun 17 12:39:29.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-9809 delete -f -'
  Jun 17 12:39:29.846: INFO: stderr: ""
  Jun 17 12:39:29.846: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Jun 17 12:39:29.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9809" for this suite. @ 06/17/23 12:39:29.851
• [2.004 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 06/17/23 12:39:29.863
  Jun 17 12:39:29.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename namespaces @ 06/17/23 12:39:29.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:29.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:29.889
  STEP: creating a Namespace @ 06/17/23 12:39:29.892
  STEP: patching the Namespace @ 06/17/23 12:39:29.906
  STEP: get the Namespace and ensuring it has the label @ 06/17/23 12:39:29.913
  Jun 17 12:39:29.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-2768" for this suite. @ 06/17/23 12:39:29.922
  STEP: Destroying namespace "nspatchtest-7f042ac0-b715-4b5b-8af3-71c71fd07f18-2317" for this suite. @ 06/17/23 12:39:29.929
• [0.076 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 06/17/23 12:39:29.941
  Jun 17 12:39:29.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename configmap @ 06/17/23 12:39:29.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:29.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:29.962
  STEP: Creating configMap with name configmap-test-volume-map-49840caa-6697-4ad8-8845-fef32b3be3a7 @ 06/17/23 12:39:29.967
  STEP: Creating a pod to test consume configMaps @ 06/17/23 12:39:29.974
  STEP: Saw pod success @ 06/17/23 12:39:33.999
  Jun 17 12:39:34.003: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-configmaps-ac291694-bae1-478b-a466-8cfb66420aa5 container agnhost-container: <nil>
  STEP: delete the pod @ 06/17/23 12:39:34.023
  Jun 17 12:39:34.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6405" for this suite. @ 06/17/23 12:39:34.046
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 06/17/23 12:39:34.057
  Jun 17 12:39:34.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename services @ 06/17/23 12:39:34.058
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:34.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:34.079
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-5207 @ 06/17/23 12:39:34.082
  STEP: changing the ExternalName service to type=NodePort @ 06/17/23 12:39:34.087
  STEP: creating replication controller externalname-service in namespace services-5207 @ 06/17/23 12:39:34.111
  I0617 12:39:34.120817      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-5207, replica count: 2
  I0617 12:39:37.171813      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 17 12:39:37.171: INFO: Creating new exec pod
  Jun 17 12:39:40.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-5207 exec execpodc5lsl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun 17 12:39:40.328: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun 17 12:39:40.328: INFO: stdout: "externalname-service-pk9lm"
  Jun 17 12:39:40.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-5207 exec execpodc5lsl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.233 80'
  Jun 17 12:39:40.468: INFO: stderr: "+ nc -v -t -w 2 10.152.183.233 80\nConnection to 10.152.183.233 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun 17 12:39:40.468: INFO: stdout: ""
  Jun 17 12:39:41.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-5207 exec execpodc5lsl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.233 80'
  Jun 17 12:39:41.591: INFO: stderr: "+ nc -v -t -w 2 10.152.183.233 80\nConnection to 10.152.183.233 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun 17 12:39:41.591: INFO: stdout: ""
  Jun 17 12:39:42.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-5207 exec execpodc5lsl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.233 80'
  Jun 17 12:39:42.596: INFO: stderr: "+ nc -v -t -w 2 10.152.183.233 80\nConnection to 10.152.183.233 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun 17 12:39:42.596: INFO: stdout: "externalname-service-pk9lm"
  Jun 17 12:39:42.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-5207 exec execpodc5lsl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.86.18 30507'
  Jun 17 12:39:42.727: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.86.18 30507\nConnection to 172.31.86.18 30507 port [tcp/*] succeeded!\n"
  Jun 17 12:39:42.727: INFO: stdout: "externalname-service-pv8mq"
  Jun 17 12:39:42.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-5207 exec execpodc5lsl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.68.253 30507'
  Jun 17 12:39:42.873: INFO: stderr: "+ nc -v -t -w 2 172.31.68.253 30507\n+ echo hostName\nConnection to 172.31.68.253 30507 port [tcp/*] succeeded!\n"
  Jun 17 12:39:42.873: INFO: stdout: "externalname-service-pk9lm"
  Jun 17 12:39:42.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 17 12:39:42.878: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-5207" for this suite. @ 06/17/23 12:39:42.927
• [8.878 seconds]
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 06/17/23 12:39:42.935
  Jun 17 12:39:42.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename job @ 06/17/23 12:39:42.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:42.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:42.955
  STEP: Creating a job @ 06/17/23 12:39:42.962
  STEP: Ensuring job reaches completions @ 06/17/23 12:39:42.969
  Jun 17 12:39:52.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1266" for this suite. @ 06/17/23 12:39:52.98
• [10.053 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 06/17/23 12:39:52.987
  Jun 17 12:39:52.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 12:39:52.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:53.004
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:53.008
  STEP: Creating projection with secret that has name projected-secret-test-01f8aead-2e27-4bce-8ab5-e57f7211c5f5 @ 06/17/23 12:39:53.012
  STEP: Creating a pod to test consume secrets @ 06/17/23 12:39:53.016
  STEP: Saw pod success @ 06/17/23 12:39:57.041
  Jun 17 12:39:57.045: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-projected-secrets-7b5dd9a9-bb32-4e84-9b26-311f9d40d7a3 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/17/23 12:39:57.054
  Jun 17 12:39:57.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3233" for this suite. @ 06/17/23 12:39:57.076
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 06/17/23 12:39:57.085
  Jun 17 12:39:57.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename ingress @ 06/17/23 12:39:57.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:57.102
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:57.106
  STEP: getting /apis @ 06/17/23 12:39:57.113
  STEP: getting /apis/networking.k8s.io @ 06/17/23 12:39:57.117
  STEP: getting /apis/networking.k8s.iov1 @ 06/17/23 12:39:57.119
  STEP: creating @ 06/17/23 12:39:57.121
  STEP: getting @ 06/17/23 12:39:57.15
  STEP: listing @ 06/17/23 12:39:57.155
  STEP: watching @ 06/17/23 12:39:57.162
  Jun 17 12:39:57.162: INFO: starting watch
  STEP: cluster-wide listing @ 06/17/23 12:39:57.164
  STEP: cluster-wide watching @ 06/17/23 12:39:57.168
  Jun 17 12:39:57.168: INFO: starting watch
  STEP: patching @ 06/17/23 12:39:57.17
  STEP: updating @ 06/17/23 12:39:57.178
  Jun 17 12:39:57.194: INFO: waiting for watch events with expected annotations
  Jun 17 12:39:57.194: INFO: saw patched and updated annotations
  STEP: patching /status @ 06/17/23 12:39:57.195
  STEP: updating /status @ 06/17/23 12:39:57.206
  STEP: get /status @ 06/17/23 12:39:57.222
  STEP: deleting @ 06/17/23 12:39:57.232
  STEP: deleting a collection @ 06/17/23 12:39:57.254
  Jun 17 12:39:57.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-6179" for this suite. @ 06/17/23 12:39:57.28
• [0.208 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 06/17/23 12:39:57.302
  Jun 17 12:39:57.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename configmap @ 06/17/23 12:39:57.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:57.321
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:57.326
  STEP: Creating configMap that has name configmap-test-emptyKey-70a16eb5-5eb1-4b56-a7d6-ecf51e50c198 @ 06/17/23 12:39:57.329
  Jun 17 12:39:57.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6317" for this suite. @ 06/17/23 12:39:57.336
• [0.041 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 06/17/23 12:39:57.343
  Jun 17 12:39:57.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename proxy @ 06/17/23 12:39:57.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:57.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:57.375
  Jun 17 12:39:57.378: INFO: Creating pod...
  Jun 17 12:39:59.397: INFO: Creating service...
  Jun 17 12:39:59.412: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-436/pods/agnhost/proxy/some/path/with/DELETE
  Jun 17 12:39:59.420: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jun 17 12:39:59.420: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-436/pods/agnhost/proxy/some/path/with/GET
  Jun 17 12:39:59.427: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jun 17 12:39:59.427: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-436/pods/agnhost/proxy/some/path/with/HEAD
  Jun 17 12:39:59.432: INFO: http.Client request:HEAD | StatusCode:200
  Jun 17 12:39:59.432: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-436/pods/agnhost/proxy/some/path/with/OPTIONS
  Jun 17 12:39:59.436: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jun 17 12:39:59.436: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-436/pods/agnhost/proxy/some/path/with/PATCH
  Jun 17 12:39:59.442: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jun 17 12:39:59.442: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-436/pods/agnhost/proxy/some/path/with/POST
  Jun 17 12:39:59.447: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jun 17 12:39:59.447: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-436/pods/agnhost/proxy/some/path/with/PUT
  Jun 17 12:39:59.451: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jun 17 12:39:59.451: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-436/services/test-service/proxy/some/path/with/DELETE
  Jun 17 12:39:59.458: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jun 17 12:39:59.458: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-436/services/test-service/proxy/some/path/with/GET
  Jun 17 12:39:59.464: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jun 17 12:39:59.464: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-436/services/test-service/proxy/some/path/with/HEAD
  Jun 17 12:39:59.470: INFO: http.Client request:HEAD | StatusCode:200
  Jun 17 12:39:59.470: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-436/services/test-service/proxy/some/path/with/OPTIONS
  Jun 17 12:39:59.477: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jun 17 12:39:59.477: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-436/services/test-service/proxy/some/path/with/PATCH
  Jun 17 12:39:59.483: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jun 17 12:39:59.483: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-436/services/test-service/proxy/some/path/with/POST
  Jun 17 12:39:59.489: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jun 17 12:39:59.489: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-436/services/test-service/proxy/some/path/with/PUT
  Jun 17 12:39:59.496: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jun 17 12:39:59.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-436" for this suite. @ 06/17/23 12:39:59.5
• [2.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 06/17/23 12:39:59.509
  Jun 17 12:39:59.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename podtemplate @ 06/17/23 12:39:59.51
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:59.527
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:59.531
  STEP: Create a pod template @ 06/17/23 12:39:59.534
  STEP: Replace a pod template @ 06/17/23 12:39:59.54
  Jun 17 12:39:59.550: INFO: Found updated podtemplate annotation: "true"

  Jun 17 12:39:59.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-3507" for this suite. @ 06/17/23 12:39:59.554
• [0.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 06/17/23 12:39:59.565
  Jun 17 12:39:59.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename replicaset @ 06/17/23 12:39:59.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:39:59.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:39:59.591
  Jun 17 12:39:59.609: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jun 17 12:40:04.615: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/17/23 12:40:04.615
  STEP: Scaling up "test-rs" replicaset  @ 06/17/23 12:40:04.615
  Jun 17 12:40:04.628: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 06/17/23 12:40:04.628
  W0617 12:40:04.635704      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jun 17 12:40:04.637: INFO: observed ReplicaSet test-rs in namespace replicaset-2296 with ReadyReplicas 1, AvailableReplicas 1
  Jun 17 12:40:04.664: INFO: observed ReplicaSet test-rs in namespace replicaset-2296 with ReadyReplicas 1, AvailableReplicas 1
  Jun 17 12:40:04.700: INFO: observed ReplicaSet test-rs in namespace replicaset-2296 with ReadyReplicas 1, AvailableReplicas 1
  Jun 17 12:40:04.728: INFO: observed ReplicaSet test-rs in namespace replicaset-2296 with ReadyReplicas 1, AvailableReplicas 1
  Jun 17 12:40:05.535: INFO: observed ReplicaSet test-rs in namespace replicaset-2296 with ReadyReplicas 2, AvailableReplicas 2
  Jun 17 12:40:06.132: INFO: observed Replicaset test-rs in namespace replicaset-2296 with ReadyReplicas 3 found true
  Jun 17 12:40:06.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2296" for this suite. @ 06/17/23 12:40:06.137
• [6.580 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 06/17/23 12:40:06.146
  Jun 17 12:40:06.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename security-context-test @ 06/17/23 12:40:06.147
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:40:06.165
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:40:06.169
  Jun 17 12:40:10.203: INFO: Got logs for pod "busybox-privileged-false-f9875a70-22fb-4a22-b215-bef1aae5d278": "ip: RTNETLINK answers: Operation not permitted\n"
  Jun 17 12:40:10.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-2690" for this suite. @ 06/17/23 12:40:10.208
• [4.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 06/17/23 12:40:10.219
  Jun 17 12:40:10.219: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename resourcequota @ 06/17/23 12:40:10.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:40:10.233
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:40:10.236
  STEP: Discovering how many secrets are in namespace by default @ 06/17/23 12:40:10.246
  STEP: Counting existing ResourceQuota @ 06/17/23 12:40:15.252
  STEP: Creating a ResourceQuota @ 06/17/23 12:40:20.257
  STEP: Ensuring resource quota status is calculated @ 06/17/23 12:40:20.264
  STEP: Creating a Secret @ 06/17/23 12:40:22.27
  STEP: Ensuring resource quota status captures secret creation @ 06/17/23 12:40:22.283
  STEP: Deleting a secret @ 06/17/23 12:40:24.288
  STEP: Ensuring resource quota status released usage @ 06/17/23 12:40:24.295
  Jun 17 12:40:26.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3726" for this suite. @ 06/17/23 12:40:26.304
• [16.092 seconds]
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 06/17/23 12:40:26.311
  Jun 17 12:40:26.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 12:40:26.312
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:40:26.33
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:40:26.334
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 12:40:26.337
  STEP: Saw pod success @ 06/17/23 12:40:30.365
  Jun 17 12:40:30.369: INFO: Trying to get logs from node ip-172-31-25-17 pod downwardapi-volume-a4400a12-7e6a-431c-9bb4-951df24e11f4 container client-container: <nil>
  STEP: delete the pod @ 06/17/23 12:40:30.377
  Jun 17 12:40:30.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7359" for this suite. @ 06/17/23 12:40:30.404
• [4.100 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 06/17/23 12:40:30.412
  Jun 17 12:40:30.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/17/23 12:40:30.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:40:30.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:40:30.436
  STEP: fetching the /apis discovery document @ 06/17/23 12:40:30.444
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 06/17/23 12:40:30.445
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 06/17/23 12:40:30.445
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 06/17/23 12:40:30.445
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 06/17/23 12:40:30.447
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 06/17/23 12:40:30.447
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 06/17/23 12:40:30.448
  Jun 17 12:40:30.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2947" for this suite. @ 06/17/23 12:40:30.453
• [0.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 06/17/23 12:40:30.461
  Jun 17 12:40:30.461: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl @ 06/17/23 12:40:30.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:40:30.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:40:30.48
  STEP: validating cluster-info @ 06/17/23 12:40:30.484
  Jun 17 12:40:30.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-2341 cluster-info'
  Jun 17 12:40:30.553: INFO: stderr: ""
  Jun 17 12:40:30.553: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Jun 17 12:40:30.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2341" for this suite. @ 06/17/23 12:40:30.557
• [0.105 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 06/17/23 12:40:30.567
  Jun 17 12:40:30.567: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename deployment @ 06/17/23 12:40:30.568
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:40:30.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:40:30.594
  STEP: creating a Deployment @ 06/17/23 12:40:30.602
  Jun 17 12:40:30.602: INFO: Creating simple deployment test-deployment-frrgb
  Jun 17 12:40:30.619: INFO: deployment "test-deployment-frrgb" doesn't have the required revision set
  STEP: Getting /status @ 06/17/23 12:40:32.634
  Jun 17 12:40:32.639: INFO: Deployment test-deployment-frrgb has Conditions: [{Available True 2023-06-17 12:40:32 +0000 UTC 2023-06-17 12:40:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-06-17 12:40:32 +0000 UTC 2023-06-17 12:40:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-frrgb-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 06/17/23 12:40:32.639
  Jun 17 12:40:32.650: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 12, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 12, 40, 32, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 12, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 12, 40, 30, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-frrgb-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 06/17/23 12:40:32.65
  Jun 17 12:40:32.652: INFO: Observed &Deployment event: ADDED
  Jun 17 12:40:32.652: INFO: Observed Deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-17 12:40:30 +0000 UTC 2023-06-17 12:40:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-frrgb-5994cf9475"}
  Jun 17 12:40:32.652: INFO: Observed &Deployment event: MODIFIED
  Jun 17 12:40:32.652: INFO: Observed Deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-17 12:40:30 +0000 UTC 2023-06-17 12:40:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-frrgb-5994cf9475"}
  Jun 17 12:40:32.652: INFO: Observed Deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-17 12:40:30 +0000 UTC 2023-06-17 12:40:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jun 17 12:40:32.652: INFO: Observed &Deployment event: MODIFIED
  Jun 17 12:40:32.652: INFO: Observed Deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-17 12:40:30 +0000 UTC 2023-06-17 12:40:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jun 17 12:40:32.652: INFO: Observed Deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-17 12:40:30 +0000 UTC 2023-06-17 12:40:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-frrgb-5994cf9475" is progressing.}
  Jun 17 12:40:32.653: INFO: Observed &Deployment event: MODIFIED
  Jun 17 12:40:32.653: INFO: Observed Deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-17 12:40:32 +0000 UTC 2023-06-17 12:40:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jun 17 12:40:32.653: INFO: Observed Deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-17 12:40:32 +0000 UTC 2023-06-17 12:40:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-frrgb-5994cf9475" has successfully progressed.}
  Jun 17 12:40:32.653: INFO: Observed &Deployment event: MODIFIED
  Jun 17 12:40:32.653: INFO: Observed Deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-17 12:40:32 +0000 UTC 2023-06-17 12:40:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jun 17 12:40:32.653: INFO: Observed Deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-17 12:40:32 +0000 UTC 2023-06-17 12:40:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-frrgb-5994cf9475" has successfully progressed.}
  Jun 17 12:40:32.653: INFO: Found Deployment test-deployment-frrgb in namespace deployment-2279 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 17 12:40:32.653: INFO: Deployment test-deployment-frrgb has an updated status
  STEP: patching the Statefulset Status @ 06/17/23 12:40:32.653
  Jun 17 12:40:32.653: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jun 17 12:40:32.661: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 06/17/23 12:40:32.661
  Jun 17 12:40:32.663: INFO: Observed &Deployment event: ADDED
  Jun 17 12:40:32.664: INFO: Observed deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-17 12:40:30 +0000 UTC 2023-06-17 12:40:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-frrgb-5994cf9475"}
  Jun 17 12:40:32.664: INFO: Observed &Deployment event: MODIFIED
  Jun 17 12:40:32.664: INFO: Observed deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-17 12:40:30 +0000 UTC 2023-06-17 12:40:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-frrgb-5994cf9475"}
  Jun 17 12:40:32.664: INFO: Observed deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-17 12:40:30 +0000 UTC 2023-06-17 12:40:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jun 17 12:40:32.664: INFO: Observed &Deployment event: MODIFIED
  Jun 17 12:40:32.664: INFO: Observed deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-17 12:40:30 +0000 UTC 2023-06-17 12:40:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jun 17 12:40:32.664: INFO: Observed deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-17 12:40:30 +0000 UTC 2023-06-17 12:40:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-frrgb-5994cf9475" is progressing.}
  Jun 17 12:40:32.664: INFO: Observed &Deployment event: MODIFIED
  Jun 17 12:40:32.664: INFO: Observed deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-17 12:40:32 +0000 UTC 2023-06-17 12:40:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jun 17 12:40:32.664: INFO: Observed deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-17 12:40:32 +0000 UTC 2023-06-17 12:40:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-frrgb-5994cf9475" has successfully progressed.}
  Jun 17 12:40:32.665: INFO: Observed &Deployment event: MODIFIED
  Jun 17 12:40:32.665: INFO: Observed deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-17 12:40:32 +0000 UTC 2023-06-17 12:40:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jun 17 12:40:32.665: INFO: Observed deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-17 12:40:32 +0000 UTC 2023-06-17 12:40:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-frrgb-5994cf9475" has successfully progressed.}
  Jun 17 12:40:32.665: INFO: Observed deployment test-deployment-frrgb in namespace deployment-2279 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 17 12:40:32.665: INFO: Observed &Deployment event: MODIFIED
  Jun 17 12:40:32.665: INFO: Found deployment test-deployment-frrgb in namespace deployment-2279 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Jun 17 12:40:32.665: INFO: Deployment test-deployment-frrgb has a patched status
  Jun 17 12:40:32.670: INFO: Deployment "test-deployment-frrgb":
  &Deployment{ObjectMeta:{test-deployment-frrgb  deployment-2279  77e8a8d3-8461-4366-a310-3a6a9ebe76ff 15117 1 2023-06-17 12:40:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-17 12:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-06-17 12:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-06-17 12:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00455bf98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-frrgb-5994cf9475",LastUpdateTime:2023-06-17 12:40:32 +0000 UTC,LastTransitionTime:2023-06-17 12:40:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun 17 12:40:32.675: INFO: New ReplicaSet "test-deployment-frrgb-5994cf9475" of Deployment "test-deployment-frrgb":
  &ReplicaSet{ObjectMeta:{test-deployment-frrgb-5994cf9475  deployment-2279  00a588ec-bec5-4a14-a91b-96e7d8428ef8 15113 1 2023-06-17 12:40:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-frrgb 77e8a8d3-8461-4366-a310-3a6a9ebe76ff 0xc0045d81e0 0xc0045d81e1}] [] [{kube-controller-manager Update apps/v1 2023-06-17 12:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77e8a8d3-8461-4366-a310-3a6a9ebe76ff\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:40:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045d8288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun 17 12:40:32.679: INFO: Pod "test-deployment-frrgb-5994cf9475-xl2gz" is available:
  &Pod{ObjectMeta:{test-deployment-frrgb-5994cf9475-xl2gz test-deployment-frrgb-5994cf9475- deployment-2279  dc454b80-45db-4a20-ac36-61037a66b5ae 15112 0 2023-06-17 12:40:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-frrgb-5994cf9475 00a588ec-bec5-4a14-a91b-96e7d8428ef8 0xc00463e390 0xc00463e391}] [] [{kube-controller-manager Update v1 2023-06-17 12:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00a588ec-bec5-4a14-a91b-96e7d8428ef8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 12:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.178.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-br4mp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-br4mp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:40:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:40:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:192.168.178.183,StartTime:2023-06-17 12:40:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-17 12:40:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cab6227e2147e511f498b47993e8a5ec1f0bcf078aae8f456bbabd27a04a470b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.178.183,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 12:40:32.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2279" for this suite. @ 06/17/23 12:40:32.684
• [2.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 06/17/23 12:40:32.696
  Jun 17 12:40:32.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 06/17/23 12:40:32.696
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:40:32.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:40:32.717
  STEP: creating a target pod @ 06/17/23 12:40:32.721
  STEP: adding an ephemeral container @ 06/17/23 12:40:34.745
  STEP: checking pod container endpoints @ 06/17/23 12:40:36.766
  Jun 17 12:40:36.766: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5173 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 12:40:36.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:40:36.767: INFO: ExecWithOptions: Clientset creation
  Jun 17 12:40:36.767: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-5173/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Jun 17 12:40:36.830: INFO: Exec stderr: ""
  Jun 17 12:40:36.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-5173" for this suite. @ 06/17/23 12:40:36.843
• [4.154 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 06/17/23 12:40:36.85
  Jun 17 12:40:36.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir @ 06/17/23 12:40:36.851
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:40:36.871
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:40:36.874
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 06/17/23 12:40:36.878
  STEP: Saw pod success @ 06/17/23 12:40:40.902
  Jun 17 12:40:40.906: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-0f8359a0-ce6d-47fd-b118-dd221144a3c7 container test-container: <nil>
  STEP: delete the pod @ 06/17/23 12:40:40.914
  Jun 17 12:40:40.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8993" for this suite. @ 06/17/23 12:40:40.933
• [4.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 06/17/23 12:40:40.942
  Jun 17 12:40:40.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename resourcequota @ 06/17/23 12:40:40.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:40:40.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:40:40.965
  STEP: Creating a ResourceQuota with terminating scope @ 06/17/23 12:40:40.969
  STEP: Ensuring ResourceQuota status is calculated @ 06/17/23 12:40:40.975
  STEP: Creating a ResourceQuota with not terminating scope @ 06/17/23 12:40:42.98
  STEP: Ensuring ResourceQuota status is calculated @ 06/17/23 12:40:42.987
  STEP: Creating a long running pod @ 06/17/23 12:40:44.992
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 06/17/23 12:40:45.007
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 06/17/23 12:40:47.012
  STEP: Deleting the pod @ 06/17/23 12:40:49.018
  STEP: Ensuring resource quota status released the pod usage @ 06/17/23 12:40:49.035
  STEP: Creating a terminating pod @ 06/17/23 12:40:51.04
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 06/17/23 12:40:51.052
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 06/17/23 12:40:53.057
  STEP: Deleting the pod @ 06/17/23 12:40:55.063
  STEP: Ensuring resource quota status released the pod usage @ 06/17/23 12:40:55.079
  Jun 17 12:40:57.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4824" for this suite. @ 06/17/23 12:40:57.089
• [16.154 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 06/17/23 12:40:57.098
  Jun 17 12:40:57.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename ingressclass @ 06/17/23 12:40:57.099
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:40:57.115
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:40:57.119
  STEP: getting /apis @ 06/17/23 12:40:57.122
  STEP: getting /apis/networking.k8s.io @ 06/17/23 12:40:57.127
  STEP: getting /apis/networking.k8s.iov1 @ 06/17/23 12:40:57.128
  STEP: creating @ 06/17/23 12:40:57.13
  STEP: getting @ 06/17/23 12:40:57.146
  STEP: listing @ 06/17/23 12:40:57.15
  STEP: watching @ 06/17/23 12:40:57.153
  Jun 17 12:40:57.153: INFO: starting watch
  STEP: patching @ 06/17/23 12:40:57.155
  STEP: updating @ 06/17/23 12:40:57.161
  Jun 17 12:40:57.167: INFO: waiting for watch events with expected annotations
  Jun 17 12:40:57.167: INFO: saw patched and updated annotations
  STEP: deleting @ 06/17/23 12:40:57.167
  STEP: deleting a collection @ 06/17/23 12:40:57.181
  Jun 17 12:40:57.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-3943" for this suite. @ 06/17/23 12:40:57.202
• [0.111 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 06/17/23 12:40:57.209
  Jun 17 12:40:57.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/17/23 12:40:57.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:40:57.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:40:57.229
  Jun 17 12:40:57.232: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:41:03.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1919" for this suite. @ 06/17/23 12:41:03.485
• [6.283 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 06/17/23 12:41:03.494
  Jun 17 12:41:03.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename services @ 06/17/23 12:41:03.495
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:41:03.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:41:03.515
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-4475 @ 06/17/23 12:41:03.519
  STEP: changing the ExternalName service to type=ClusterIP @ 06/17/23 12:41:03.527
  STEP: creating replication controller externalname-service in namespace services-4475 @ 06/17/23 12:41:03.546
  I0617 12:41:03.554553      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-4475, replica count: 2
  I0617 12:41:06.606360      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 17 12:41:06.606: INFO: Creating new exec pod
  Jun 17 12:41:09.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-4475 exec execpod6fkbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun 17 12:41:09.789: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun 17 12:41:09.789: INFO: stdout: "externalname-service-h9lpn"
  Jun 17 12:41:09.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-4475 exec execpod6fkbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.162 80'
  Jun 17 12:41:09.917: INFO: stderr: "+ nc -v -t -w 2 10.152.183.162 80\n+ echo hostName\nConnection to 10.152.183.162 80 port [tcp/http] succeeded!\n"
  Jun 17 12:41:09.917: INFO: stdout: "externalname-service-2g9q5"
  Jun 17 12:41:09.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 17 12:41:09.922: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-4475" for this suite. @ 06/17/23 12:41:09.945
• [6.457 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 06/17/23 12:41:09.951
  Jun 17 12:41:09.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename field-validation @ 06/17/23 12:41:09.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:41:09.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:41:09.974
  Jun 17 12:41:09.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  W0617 12:41:12.536473      19 warnings.go:70] unknown field "alpha"
  W0617 12:41:12.536495      19 warnings.go:70] unknown field "beta"
  W0617 12:41:12.536500      19 warnings.go:70] unknown field "delta"
  W0617 12:41:12.536507      19 warnings.go:70] unknown field "epsilon"
  W0617 12:41:12.536513      19 warnings.go:70] unknown field "gamma"
  Jun 17 12:41:12.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5052" for this suite. @ 06/17/23 12:41:12.575
• [2.630 seconds]
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 06/17/23 12:41:12.582
  Jun 17 12:41:12.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename subpath @ 06/17/23 12:41:12.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:41:12.602
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:41:12.611
  STEP: Setting up data @ 06/17/23 12:41:12.615
  STEP: Creating pod pod-subpath-test-secret-9nb4 @ 06/17/23 12:41:12.624
  STEP: Creating a pod to test atomic-volume-subpath @ 06/17/23 12:41:12.624
  STEP: Saw pod success @ 06/17/23 12:41:36.701
  Jun 17 12:41:36.705: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-subpath-test-secret-9nb4 container test-container-subpath-secret-9nb4: <nil>
  STEP: delete the pod @ 06/17/23 12:41:36.713
  STEP: Deleting pod pod-subpath-test-secret-9nb4 @ 06/17/23 12:41:36.731
  Jun 17 12:41:36.731: INFO: Deleting pod "pod-subpath-test-secret-9nb4" in namespace "subpath-92"
  Jun 17 12:41:36.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-92" for this suite. @ 06/17/23 12:41:36.739
• [24.164 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 06/17/23 12:41:36.747
  Jun 17 12:41:36.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename downward-api @ 06/17/23 12:41:36.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:41:36.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:41:36.767
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 12:41:36.77
  STEP: Saw pod success @ 06/17/23 12:41:40.794
  Jun 17 12:41:40.798: INFO: Trying to get logs from node ip-172-31-25-17 pod downwardapi-volume-5fbc460d-8dd5-440a-b9bf-8f9112f7d1ce container client-container: <nil>
  STEP: delete the pod @ 06/17/23 12:41:40.806
  Jun 17 12:41:40.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7663" for this suite. @ 06/17/23 12:41:40.826
• [4.087 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 06/17/23 12:41:40.835
  Jun 17 12:41:40.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir @ 06/17/23 12:41:40.836
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:41:40.85
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:41:40.859
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 06/17/23 12:41:40.863
  STEP: Saw pod success @ 06/17/23 12:41:44.888
  Jun 17 12:41:44.892: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-9581b9a9-68d3-4320-94eb-1eb7cb0a9884 container test-container: <nil>
  STEP: delete the pod @ 06/17/23 12:41:44.901
  Jun 17 12:41:44.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7723" for this suite. @ 06/17/23 12:41:44.928
• [4.103 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 06/17/23 12:41:44.938
  Jun 17 12:41:44.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename downward-api @ 06/17/23 12:41:44.939
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:41:44.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:41:44.961
  STEP: Creating a pod to test downward api env vars @ 06/17/23 12:41:44.968
  STEP: Saw pod success @ 06/17/23 12:41:48.993
  Jun 17 12:41:48.998: INFO: Trying to get logs from node ip-172-31-25-17 pod downward-api-ae1a469f-8ce9-41e0-a674-219efc34ada5 container dapi-container: <nil>
  STEP: delete the pod @ 06/17/23 12:41:49.007
  Jun 17 12:41:49.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3113" for this suite. @ 06/17/23 12:41:49.035
• [4.104 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 06/17/23 12:41:49.043
  Jun 17 12:41:49.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename gc @ 06/17/23 12:41:49.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:41:49.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:41:49.075
  STEP: create the rc @ 06/17/23 12:41:49.085
  W0617 12:41:49.092660      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 06/17/23 12:41:55.098
  STEP: wait for the rc to be deleted @ 06/17/23 12:41:55.107
  Jun 17 12:41:56.128: INFO: 80 pods remaining
  Jun 17 12:41:56.128: INFO: 80 pods has nil DeletionTimestamp
  Jun 17 12:41:56.128: INFO: 
  Jun 17 12:41:57.127: INFO: 71 pods remaining
  Jun 17 12:41:57.127: INFO: 71 pods has nil DeletionTimestamp
  Jun 17 12:41:57.128: INFO: 
  Jun 17 12:41:58.120: INFO: 60 pods remaining
  Jun 17 12:41:58.121: INFO: 60 pods has nil DeletionTimestamp
  Jun 17 12:41:58.121: INFO: 
  Jun 17 12:41:59.120: INFO: 40 pods remaining
  Jun 17 12:41:59.120: INFO: 40 pods has nil DeletionTimestamp
  Jun 17 12:41:59.120: INFO: 
  Jun 17 12:42:00.129: INFO: 31 pods remaining
  Jun 17 12:42:00.129: INFO: 31 pods has nil DeletionTimestamp
  Jun 17 12:42:00.129: INFO: 
  Jun 17 12:42:01.118: INFO: 20 pods remaining
  Jun 17 12:42:01.119: INFO: 20 pods has nil DeletionTimestamp
  Jun 17 12:42:01.119: INFO: 
  STEP: Gathering metrics @ 06/17/23 12:42:02.115
  W0617 12:42:02.121050      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 17 12:42:02.121: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 17 12:42:02.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-709" for this suite. @ 06/17/23 12:42:02.129
• [13.096 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 06/17/23 12:42:02.14
  Jun 17 12:42:02.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename services @ 06/17/23 12:42:02.142
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:42:02.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:42:02.164
  STEP: creating service multi-endpoint-test in namespace services-5709 @ 06/17/23 12:42:02.168
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5709 to expose endpoints map[] @ 06/17/23 12:42:02.179
  Jun 17 12:42:02.205: INFO: successfully validated that service multi-endpoint-test in namespace services-5709 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-5709 @ 06/17/23 12:42:02.206
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5709 to expose endpoints map[pod1:[100]] @ 06/17/23 12:42:10.25
  Jun 17 12:42:10.265: INFO: successfully validated that service multi-endpoint-test in namespace services-5709 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-5709 @ 06/17/23 12:42:10.265
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5709 to expose endpoints map[pod1:[100] pod2:[101]] @ 06/17/23 12:42:16.304
  Jun 17 12:42:16.321: INFO: successfully validated that service multi-endpoint-test in namespace services-5709 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 06/17/23 12:42:16.321
  Jun 17 12:42:16.321: INFO: Creating new exec pod
  Jun 17 12:42:19.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-5709 exec execpodd6jwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Jun 17 12:42:19.460: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Jun 17 12:42:19.460: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 12:42:19.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-5709 exec execpodd6jwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.23 80'
  Jun 17 12:42:19.588: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.23 80\nConnection to 10.152.183.23 80 port [tcp/http] succeeded!\n"
  Jun 17 12:42:19.588: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 12:42:19.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-5709 exec execpodd6jwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Jun 17 12:42:19.709: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Jun 17 12:42:19.709: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 12:42:19.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-5709 exec execpodd6jwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.23 81'
  Jun 17 12:42:19.837: INFO: stderr: "+ nc -v -t -w 2 10.152.183.23 81\n+ echo hostName\nConnection to 10.152.183.23 81 port [tcp/*] succeeded!\n"
  Jun 17 12:42:19.837: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-5709 @ 06/17/23 12:42:19.837
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5709 to expose endpoints map[pod2:[101]] @ 06/17/23 12:42:19.862
  Jun 17 12:42:20.889: INFO: successfully validated that service multi-endpoint-test in namespace services-5709 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-5709 @ 06/17/23 12:42:20.889
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5709 to expose endpoints map[] @ 06/17/23 12:42:20.909
  Jun 17 12:42:21.931: INFO: successfully validated that service multi-endpoint-test in namespace services-5709 exposes endpoints map[]
  Jun 17 12:42:21.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5709" for this suite. @ 06/17/23 12:42:21.953
• [19.820 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 06/17/23 12:42:21.96
  Jun 17 12:42:21.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/17/23 12:42:21.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:42:21.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:42:21.983
  STEP: set up a multi version CRD @ 06/17/23 12:42:21.987
  Jun 17 12:42:21.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: mark a version not serverd @ 06/17/23 12:42:25.561
  STEP: check the unserved version gets removed @ 06/17/23 12:42:25.578
  STEP: check the other version is not changed @ 06/17/23 12:42:27.038
  Jun 17 12:42:29.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9020" for this suite. @ 06/17/23 12:42:29.833
• [7.880 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 06/17/23 12:42:29.843
  Jun 17 12:42:29.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl @ 06/17/23 12:42:29.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:42:29.858
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:42:29.861
  STEP: creating the pod @ 06/17/23 12:42:29.865
  Jun 17 12:42:29.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-9586 create -f -'
  Jun 17 12:42:30.553: INFO: stderr: ""
  Jun 17 12:42:30.553: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 06/17/23 12:42:32.56
  Jun 17 12:42:32.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-9586 label pods pause testing-label=testing-label-value'
  Jun 17 12:42:32.632: INFO: stderr: ""
  Jun 17 12:42:32.632: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 06/17/23 12:42:32.632
  Jun 17 12:42:32.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-9586 get pod pause -L testing-label'
  Jun 17 12:42:32.699: INFO: stderr: ""
  Jun 17 12:42:32.699: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 06/17/23 12:42:32.699
  Jun 17 12:42:32.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-9586 label pods pause testing-label-'
  Jun 17 12:42:32.772: INFO: stderr: ""
  Jun 17 12:42:32.772: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 06/17/23 12:42:32.772
  Jun 17 12:42:32.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-9586 get pod pause -L testing-label'
  Jun 17 12:42:32.838: INFO: stderr: ""
  Jun 17 12:42:32.838: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 06/17/23 12:42:32.838
  Jun 17 12:42:32.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-9586 delete --grace-period=0 --force -f -'
  Jun 17 12:42:32.910: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 17 12:42:32.910: INFO: stdout: "pod \"pause\" force deleted\n"
  Jun 17 12:42:32.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-9586 get rc,svc -l name=pause --no-headers'
  Jun 17 12:42:32.988: INFO: stderr: "No resources found in kubectl-9586 namespace.\n"
  Jun 17 12:42:32.988: INFO: stdout: ""
  Jun 17 12:42:32.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-9586 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jun 17 12:42:33.053: INFO: stderr: ""
  Jun 17 12:42:33.053: INFO: stdout: ""
  Jun 17 12:42:33.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9586" for this suite. @ 06/17/23 12:42:33.058
• [3.221 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 06/17/23 12:42:33.066
  Jun 17 12:42:33.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename secrets @ 06/17/23 12:42:33.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:42:33.086
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:42:33.094
  STEP: Creating secret with name secret-test-392b4eac-a080-49f7-a9d7-e7d31857304c @ 06/17/23 12:42:33.103
  STEP: Creating a pod to test consume secrets @ 06/17/23 12:42:33.109
  STEP: Saw pod success @ 06/17/23 12:42:37.131
  Jun 17 12:42:37.136: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-secrets-4af9ca93-5e64-4799-8dd8-822e55ec81be container secret-volume-test: <nil>
  STEP: delete the pod @ 06/17/23 12:42:37.152
  Jun 17 12:42:37.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4945" for this suite. @ 06/17/23 12:42:37.17
• [4.110 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 06/17/23 12:42:37.176
  Jun 17 12:42:37.176: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename cronjob @ 06/17/23 12:42:37.177
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:42:37.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:42:37.199
  STEP: Creating a ReplaceConcurrent cronjob @ 06/17/23 12:42:37.203
  STEP: Ensuring a job is scheduled @ 06/17/23 12:42:37.209
  STEP: Ensuring exactly one is scheduled @ 06/17/23 12:43:01.214
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 06/17/23 12:43:01.217
  STEP: Ensuring the job is replaced with a new one @ 06/17/23 12:43:01.22
  STEP: Removing cronjob @ 06/17/23 12:44:01.223
  Jun 17 12:44:01.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2907" for this suite. @ 06/17/23 12:44:01.234
• [84.064 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 06/17/23 12:44:01.242
  Jun 17 12:44:01.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename daemonsets @ 06/17/23 12:44:01.242
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:01.268
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:01.271
  STEP: Creating simple DaemonSet "daemon-set" @ 06/17/23 12:44:01.295
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/17/23 12:44:01.301
  Jun 17 12:44:01.304: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:01.304: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:01.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 12:44:01.307: INFO: Node ip-172-31-25-17 is running 0 daemon pod, expected 1
  Jun 17 12:44:02.311: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:02.311: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:02.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 12:44:02.314: INFO: Node ip-172-31-25-17 is running 0 daemon pod, expected 1
  Jun 17 12:44:03.312: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:03.312: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:03.315: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 17 12:44:03.315: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 06/17/23 12:44:03.318
  Jun 17 12:44:03.322: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 06/17/23 12:44:03.322
  Jun 17 12:44:03.331: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 06/17/23 12:44:03.331
  Jun 17 12:44:03.333: INFO: Observed &DaemonSet event: ADDED
  Jun 17 12:44:03.333: INFO: Observed &DaemonSet event: MODIFIED
  Jun 17 12:44:03.333: INFO: Observed &DaemonSet event: MODIFIED
  Jun 17 12:44:03.333: INFO: Observed &DaemonSet event: MODIFIED
  Jun 17 12:44:03.333: INFO: Observed &DaemonSet event: MODIFIED
  Jun 17 12:44:03.334: INFO: Found daemon set daemon-set in namespace daemonsets-3141 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jun 17 12:44:03.334: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 06/17/23 12:44:03.334
  STEP: watching for the daemon set status to be patched @ 06/17/23 12:44:03.341
  Jun 17 12:44:03.343: INFO: Observed &DaemonSet event: ADDED
  Jun 17 12:44:03.343: INFO: Observed &DaemonSet event: MODIFIED
  Jun 17 12:44:03.343: INFO: Observed &DaemonSet event: MODIFIED
  Jun 17 12:44:03.344: INFO: Observed &DaemonSet event: MODIFIED
  Jun 17 12:44:03.344: INFO: Observed &DaemonSet event: MODIFIED
  Jun 17 12:44:03.344: INFO: Observed daemon set daemon-set in namespace daemonsets-3141 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jun 17 12:44:03.344: INFO: Observed &DaemonSet event: MODIFIED
  Jun 17 12:44:03.344: INFO: Found daemon set daemon-set in namespace daemonsets-3141 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Jun 17 12:44:03.345: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 06/17/23 12:44:03.349
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3141, will wait for the garbage collector to delete the pods @ 06/17/23 12:44:03.349
  Jun 17 12:44:03.408: INFO: Deleting DaemonSet.extensions daemon-set took: 5.68776ms
  Jun 17 12:44:03.509: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.161568ms
  Jun 17 12:44:04.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 12:44:04.913: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 17 12:44:04.916: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18342"},"items":null}

  Jun 17 12:44:04.919: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18342"},"items":null}

  Jun 17 12:44:04.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3141" for this suite. @ 06/17/23 12:44:04.936
• [3.702 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 06/17/23 12:44:04.944
  Jun 17 12:44:04.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename pods @ 06/17/23 12:44:04.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:04.976
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:04.978
  Jun 17 12:44:04.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: creating the pod @ 06/17/23 12:44:04.982
  STEP: submitting the pod to kubernetes @ 06/17/23 12:44:04.982
  Jun 17 12:44:07.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1506" for this suite. @ 06/17/23 12:44:07.076
• [2.137 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 06/17/23 12:44:07.082
  Jun 17 12:44:07.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename svcaccounts @ 06/17/23 12:44:07.083
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:07.1
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:07.103
  Jun 17 12:44:07.109: INFO: Got root ca configmap in namespace "svcaccounts-7826"
  Jun 17 12:44:07.115: INFO: Deleted root ca configmap in namespace "svcaccounts-7826"
  STEP: waiting for a new root ca configmap created @ 06/17/23 12:44:07.615
  Jun 17 12:44:07.619: INFO: Recreated root ca configmap in namespace "svcaccounts-7826"
  Jun 17 12:44:07.623: INFO: Updated root ca configmap in namespace "svcaccounts-7826"
  STEP: waiting for the root ca configmap reconciled @ 06/17/23 12:44:08.123
  Jun 17 12:44:08.127: INFO: Reconciled root ca configmap in namespace "svcaccounts-7826"
  Jun 17 12:44:08.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7826" for this suite. @ 06/17/23 12:44:08.131
• [1.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 06/17/23 12:44:08.14
  Jun 17 12:44:08.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir @ 06/17/23 12:44:08.141
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:08.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:08.164
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 06/17/23 12:44:08.171
  STEP: Saw pod success @ 06/17/23 12:44:12.198
  Jun 17 12:44:12.201: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-8df9e704-4ddd-4956-a272-e5c93e6e07ce container test-container: <nil>
  STEP: delete the pod @ 06/17/23 12:44:12.215
  Jun 17 12:44:12.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7340" for this suite. @ 06/17/23 12:44:12.231
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 06/17/23 12:44:12.239
  Jun 17 12:44:12.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubelet-test @ 06/17/23 12:44:12.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:12.26
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:12.267
  Jun 17 12:44:14.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7062" for this suite. @ 06/17/23 12:44:14.304
• [2.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 06/17/23 12:44:14.312
  Jun 17 12:44:14.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename daemonsets @ 06/17/23 12:44:14.313
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:14.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:14.334
  STEP: Creating simple DaemonSet "daemon-set" @ 06/17/23 12:44:14.352
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/17/23 12:44:14.358
  Jun 17 12:44:14.361: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:14.361: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:14.364: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 12:44:14.364: INFO: Node ip-172-31-25-17 is running 0 daemon pod, expected 1
  Jun 17 12:44:15.368: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:15.368: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:15.371: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 12:44:15.371: INFO: Node ip-172-31-25-17 is running 0 daemon pod, expected 1
  Jun 17 12:44:16.368: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:16.369: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:16.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 17 12:44:16.372: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 06/17/23 12:44:16.374
  Jun 17 12:44:16.387: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:16.387: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:16.390: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 17 12:44:16.390: INFO: Node ip-172-31-25-17 is running 0 daemon pod, expected 1
  Jun 17 12:44:17.395: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:17.395: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:17.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 17 12:44:17.398: INFO: Node ip-172-31-25-17 is running 0 daemon pod, expected 1
  Jun 17 12:44:18.394: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:18.394: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:18.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 17 12:44:18.397: INFO: Node ip-172-31-25-17 is running 0 daemon pod, expected 1
  Jun 17 12:44:19.394: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:19.394: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:44:19.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 17 12:44:19.397: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 06/17/23 12:44:19.4
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8862, will wait for the garbage collector to delete the pods @ 06/17/23 12:44:19.4
  Jun 17 12:44:19.458: INFO: Deleting DaemonSet.extensions daemon-set took: 5.205177ms
  Jun 17 12:44:19.559: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.973114ms
  Jun 17 12:44:20.963: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 12:44:20.963: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 17 12:44:20.966: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18619"},"items":null}

  Jun 17 12:44:20.968: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18619"},"items":null}

  Jun 17 12:44:20.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8862" for this suite. @ 06/17/23 12:44:20.983
• [6.678 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 06/17/23 12:44:20.991
  Jun 17 12:44:20.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl @ 06/17/23 12:44:20.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:21.008
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:21.01
  STEP: creating Agnhost RC @ 06/17/23 12:44:21.013
  Jun 17 12:44:21.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-9639 create -f -'
  Jun 17 12:44:21.283: INFO: stderr: ""
  Jun 17 12:44:21.283: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 06/17/23 12:44:21.283
  Jun 17 12:44:22.288: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 17 12:44:22.288: INFO: Found 0 / 1
  Jun 17 12:44:23.287: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 17 12:44:23.287: INFO: Found 1 / 1
  Jun 17 12:44:23.287: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 06/17/23 12:44:23.287
  Jun 17 12:44:23.290: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 17 12:44:23.290: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jun 17 12:44:23.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-9639 patch pod agnhost-primary-kln4c -p {"metadata":{"annotations":{"x":"y"}}}'
  Jun 17 12:44:23.362: INFO: stderr: ""
  Jun 17 12:44:23.362: INFO: stdout: "pod/agnhost-primary-kln4c patched\n"
  STEP: checking annotations @ 06/17/23 12:44:23.362
  Jun 17 12:44:23.365: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 17 12:44:23.365: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jun 17 12:44:23.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9639" for this suite. @ 06/17/23 12:44:23.369
• [2.384 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 06/17/23 12:44:23.375
  Jun 17 12:44:23.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir @ 06/17/23 12:44:23.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:23.394
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:23.396
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 06/17/23 12:44:23.399
  STEP: Saw pod success @ 06/17/23 12:44:27.419
  Jun 17 12:44:27.422: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-4922cd71-b45f-46d4-96f9-f42b5f430354 container test-container: <nil>
  STEP: delete the pod @ 06/17/23 12:44:27.429
  Jun 17 12:44:27.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1375" for this suite. @ 06/17/23 12:44:27.447
• [4.078 seconds]
------------------------------
S
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 06/17/23 12:44:27.453
  Jun 17 12:44:27.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename job @ 06/17/23 12:44:27.454
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:27.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:27.473
  STEP: Creating a job @ 06/17/23 12:44:27.476
  STEP: Ensure pods equal to parallelism count is attached to the job @ 06/17/23 12:44:27.482
  STEP: patching /status @ 06/17/23 12:44:29.487
  STEP: updating /status @ 06/17/23 12:44:29.494
  STEP: get /status @ 06/17/23 12:44:29.524
  Jun 17 12:44:29.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7083" for this suite. @ 06/17/23 12:44:29.532
• [2.085 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 06/17/23 12:44:29.539
  Jun 17 12:44:29.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename downward-api @ 06/17/23 12:44:29.54
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:29.56
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:29.567
  STEP: Creating a pod to test downward api env vars @ 06/17/23 12:44:29.57
  STEP: Saw pod success @ 06/17/23 12:44:33.595
  Jun 17 12:44:33.598: INFO: Trying to get logs from node ip-172-31-86-18 pod downward-api-a749c990-4710-46ea-86ee-41b1b7b85b8d container dapi-container: <nil>
  STEP: delete the pod @ 06/17/23 12:44:33.605
  Jun 17 12:44:33.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9670" for this suite. @ 06/17/23 12:44:33.623
• [4.091 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 06/17/23 12:44:33.63
  Jun 17 12:44:33.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename replicaset @ 06/17/23 12:44:33.631
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:33.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:33.654
  Jun 17 12:44:33.657: INFO: Creating ReplicaSet my-hostname-basic-3dcb4f7c-ef1d-42ee-96f3-beb9d47f5482
  Jun 17 12:44:33.665: INFO: Pod name my-hostname-basic-3dcb4f7c-ef1d-42ee-96f3-beb9d47f5482: Found 0 pods out of 1
  Jun 17 12:44:38.670: INFO: Pod name my-hostname-basic-3dcb4f7c-ef1d-42ee-96f3-beb9d47f5482: Found 1 pods out of 1
  Jun 17 12:44:38.670: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3dcb4f7c-ef1d-42ee-96f3-beb9d47f5482" is running
  Jun 17 12:44:38.673: INFO: Pod "my-hostname-basic-3dcb4f7c-ef1d-42ee-96f3-beb9d47f5482-f48r4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-17 12:44:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-17 12:44:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-17 12:44:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-17 12:44:33 +0000 UTC Reason: Message:}])
  Jun 17 12:44:38.673: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 06/17/23 12:44:38.673
  Jun 17 12:44:38.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9716" for this suite. @ 06/17/23 12:44:38.686
• [5.063 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 06/17/23 12:44:38.693
  Jun 17 12:44:38.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/17/23 12:44:38.694
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:38.715
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:38.718
  Jun 17 12:44:38.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 06/17/23 12:44:40.19
  Jun 17 12:44:40.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9026 --namespace=crd-publish-openapi-9026 create -f -'
  Jun 17 12:44:40.836: INFO: stderr: ""
  Jun 17 12:44:40.836: INFO: stdout: "e2e-test-crd-publish-openapi-2241-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jun 17 12:44:40.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9026 --namespace=crd-publish-openapi-9026 delete e2e-test-crd-publish-openapi-2241-crds test-foo'
  Jun 17 12:44:40.910: INFO: stderr: ""
  Jun 17 12:44:40.910: INFO: stdout: "e2e-test-crd-publish-openapi-2241-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Jun 17 12:44:40.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9026 --namespace=crd-publish-openapi-9026 apply -f -'
  Jun 17 12:44:41.143: INFO: stderr: ""
  Jun 17 12:44:41.143: INFO: stdout: "e2e-test-crd-publish-openapi-2241-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jun 17 12:44:41.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9026 --namespace=crd-publish-openapi-9026 delete e2e-test-crd-publish-openapi-2241-crds test-foo'
  Jun 17 12:44:41.217: INFO: stderr: ""
  Jun 17 12:44:41.217: INFO: stdout: "e2e-test-crd-publish-openapi-2241-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 06/17/23 12:44:41.217
  Jun 17 12:44:41.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9026 --namespace=crd-publish-openapi-9026 create -f -'
  Jun 17 12:44:41.422: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 06/17/23 12:44:41.422
  Jun 17 12:44:41.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9026 --namespace=crd-publish-openapi-9026 create -f -'
  Jun 17 12:44:41.631: INFO: rc: 1
  Jun 17 12:44:41.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9026 --namespace=crd-publish-openapi-9026 apply -f -'
  Jun 17 12:44:41.847: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 06/17/23 12:44:41.848
  Jun 17 12:44:41.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9026 --namespace=crd-publish-openapi-9026 create -f -'
  Jun 17 12:44:42.069: INFO: rc: 1
  Jun 17 12:44:42.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9026 --namespace=crd-publish-openapi-9026 apply -f -'
  Jun 17 12:44:42.296: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 06/17/23 12:44:42.296
  Jun 17 12:44:42.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9026 explain e2e-test-crd-publish-openapi-2241-crds'
  Jun 17 12:44:42.514: INFO: stderr: ""
  Jun 17 12:44:42.514: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-2241-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 06/17/23 12:44:42.515
  Jun 17 12:44:42.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9026 explain e2e-test-crd-publish-openapi-2241-crds.metadata'
  Jun 17 12:44:43.033: INFO: stderr: ""
  Jun 17 12:44:43.033: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-2241-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Jun 17 12:44:43.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9026 explain e2e-test-crd-publish-openapi-2241-crds.spec'
  Jun 17 12:44:43.250: INFO: stderr: ""
  Jun 17 12:44:43.250: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-2241-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Jun 17 12:44:43.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9026 explain e2e-test-crd-publish-openapi-2241-crds.spec.bars'
  Jun 17 12:44:43.461: INFO: stderr: ""
  Jun 17 12:44:43.461: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-2241-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 06/17/23 12:44:43.461
  Jun 17 12:44:43.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9026 explain e2e-test-crd-publish-openapi-2241-crds.spec.bars2'
  Jun 17 12:44:43.671: INFO: rc: 1
  Jun 17 12:44:45.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9026" for this suite. @ 06/17/23 12:44:45.067
• [6.382 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 06/17/23 12:44:45.076
  Jun 17 12:44:45.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename resourcequota @ 06/17/23 12:44:45.077
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:45.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:45.097
  STEP: Counting existing ResourceQuota @ 06/17/23 12:44:45.101
  STEP: Creating a ResourceQuota @ 06/17/23 12:44:50.104
  STEP: Ensuring resource quota status is calculated @ 06/17/23 12:44:50.109
  Jun 17 12:44:52.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-977" for this suite. @ 06/17/23 12:44:52.116
• [7.048 seconds]
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 06/17/23 12:44:52.123
  Jun 17 12:44:52.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename podtemplate @ 06/17/23 12:44:52.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:52.141
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:52.144
  Jun 17 12:44:52.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-9796" for this suite. @ 06/17/23 12:44:52.176
• [0.059 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 06/17/23 12:44:52.183
  Jun 17 12:44:52.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-runtime @ 06/17/23 12:44:52.184
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:52.2
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:52.203
  STEP: create the container @ 06/17/23 12:44:52.21
  W0617 12:44:52.222843      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 06/17/23 12:44:52.223
  STEP: get the container status @ 06/17/23 12:44:55.239
  STEP: the container should be terminated @ 06/17/23 12:44:55.241
  STEP: the termination message should be set @ 06/17/23 12:44:55.241
  Jun 17 12:44:55.241: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 06/17/23 12:44:55.242
  Jun 17 12:44:55.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9184" for this suite. @ 06/17/23 12:44:55.26
• [3.084 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 06/17/23 12:44:55.268
  Jun 17 12:44:55.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename secrets @ 06/17/23 12:44:55.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:55.294
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:55.296
  STEP: Creating secret with name secret-test-map-ec5126ea-c907-43d7-afe4-b2257e9065ee @ 06/17/23 12:44:55.299
  STEP: Creating a pod to test consume secrets @ 06/17/23 12:44:55.303
  STEP: Saw pod success @ 06/17/23 12:44:59.321
  Jun 17 12:44:59.324: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-secrets-47b7cd42-3a65-4430-a5d4-607fbbbafbe4 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/17/23 12:44:59.33
  Jun 17 12:44:59.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-658" for this suite. @ 06/17/23 12:44:59.35
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 06/17/23 12:44:59.36
  Jun 17 12:44:59.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename resourcequota @ 06/17/23 12:44:59.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:44:59.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:44:59.381
  STEP: Counting existing ResourceQuota @ 06/17/23 12:44:59.384
  STEP: Creating a ResourceQuota @ 06/17/23 12:45:04.388
  STEP: Ensuring resource quota status is calculated @ 06/17/23 12:45:04.394
  STEP: Creating a ReplicationController @ 06/17/23 12:45:06.398
  STEP: Ensuring resource quota status captures replication controller creation @ 06/17/23 12:45:06.41
  STEP: Deleting a ReplicationController @ 06/17/23 12:45:08.414
  STEP: Ensuring resource quota status released usage @ 06/17/23 12:45:08.419
  Jun 17 12:45:10.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3494" for this suite. @ 06/17/23 12:45:10.427
• [11.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 06/17/23 12:45:10.436
  Jun 17 12:45:10.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 06/17/23 12:45:10.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:45:10.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:45:10.462
  STEP: create the container to handle the HTTPGet hook request. @ 06/17/23 12:45:10.468
  STEP: create the pod with lifecycle hook @ 06/17/23 12:45:12.487
  STEP: delete the pod with lifecycle hook @ 06/17/23 12:45:14.504
  STEP: check prestop hook @ 06/17/23 12:45:16.517
  Jun 17 12:45:16.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-6670" for this suite. @ 06/17/23 12:45:16.527
• [6.097 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 06/17/23 12:45:16.534
  Jun 17 12:45:16.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-probe @ 06/17/23 12:45:16.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:45:16.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:45:16.555
  Jun 17 12:46:16.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-7836" for this suite. @ 06/17/23 12:46:16.574
• [60.045 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 06/17/23 12:46:16.579
  Jun 17 12:46:16.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename services @ 06/17/23 12:46:16.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:46:16.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:46:16.602
  STEP: creating service in namespace services-2771 @ 06/17/23 12:46:16.605
  STEP: creating service affinity-clusterip in namespace services-2771 @ 06/17/23 12:46:16.606
  STEP: creating replication controller affinity-clusterip in namespace services-2771 @ 06/17/23 12:46:16.615
  I0617 12:46:16.624794      19 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-2771, replica count: 3
  I0617 12:46:19.675579      19 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 17 12:46:19.681: INFO: Creating new exec pod
  Jun 17 12:46:22.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-2771 exec execpod-affinityp2d97 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Jun 17 12:46:22.832: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Jun 17 12:46:22.832: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 12:46:22.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-2771 exec execpod-affinityp2d97 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.37 80'
  Jun 17 12:46:22.964: INFO: stderr: "+ nc -v -t -w 2 10.152.183.37 80\n+ echo hostName\nConnection to 10.152.183.37 80 port [tcp/http] succeeded!\n"
  Jun 17 12:46:22.964: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 12:46:22.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-2771 exec execpod-affinityp2d97 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.37:80/ ; done'
  Jun 17 12:46:23.163: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.37:80/\n"
  Jun 17 12:46:23.163: INFO: stdout: "\naffinity-clusterip-rk94f\naffinity-clusterip-rk94f\naffinity-clusterip-rk94f\naffinity-clusterip-rk94f\naffinity-clusterip-rk94f\naffinity-clusterip-rk94f\naffinity-clusterip-rk94f\naffinity-clusterip-rk94f\naffinity-clusterip-rk94f\naffinity-clusterip-rk94f\naffinity-clusterip-rk94f\naffinity-clusterip-rk94f\naffinity-clusterip-rk94f\naffinity-clusterip-rk94f\naffinity-clusterip-rk94f\naffinity-clusterip-rk94f"
  Jun 17 12:46:23.163: INFO: Received response from host: affinity-clusterip-rk94f
  Jun 17 12:46:23.163: INFO: Received response from host: affinity-clusterip-rk94f
  Jun 17 12:46:23.163: INFO: Received response from host: affinity-clusterip-rk94f
  Jun 17 12:46:23.163: INFO: Received response from host: affinity-clusterip-rk94f
  Jun 17 12:46:23.163: INFO: Received response from host: affinity-clusterip-rk94f
  Jun 17 12:46:23.163: INFO: Received response from host: affinity-clusterip-rk94f
  Jun 17 12:46:23.163: INFO: Received response from host: affinity-clusterip-rk94f
  Jun 17 12:46:23.163: INFO: Received response from host: affinity-clusterip-rk94f
  Jun 17 12:46:23.163: INFO: Received response from host: affinity-clusterip-rk94f
  Jun 17 12:46:23.163: INFO: Received response from host: affinity-clusterip-rk94f
  Jun 17 12:46:23.163: INFO: Received response from host: affinity-clusterip-rk94f
  Jun 17 12:46:23.163: INFO: Received response from host: affinity-clusterip-rk94f
  Jun 17 12:46:23.163: INFO: Received response from host: affinity-clusterip-rk94f
  Jun 17 12:46:23.163: INFO: Received response from host: affinity-clusterip-rk94f
  Jun 17 12:46:23.163: INFO: Received response from host: affinity-clusterip-rk94f
  Jun 17 12:46:23.163: INFO: Received response from host: affinity-clusterip-rk94f
  Jun 17 12:46:23.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 17 12:46:23.167: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-2771, will wait for the garbage collector to delete the pods @ 06/17/23 12:46:23.18
  Jun 17 12:46:23.240: INFO: Deleting ReplicationController affinity-clusterip took: 5.509193ms
  Jun 17 12:46:23.341: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.991284ms
  STEP: Destroying namespace "services-2771" for this suite. @ 06/17/23 12:46:25.157
• [8.585 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 06/17/23 12:46:25.166
  Jun 17 12:46:25.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename downward-api @ 06/17/23 12:46:25.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:46:25.185
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:46:25.187
  STEP: Creating a pod to test downward api env vars @ 06/17/23 12:46:25.19
  STEP: Saw pod success @ 06/17/23 12:46:29.211
  Jun 17 12:46:29.214: INFO: Trying to get logs from node ip-172-31-25-17 pod downward-api-1fd115fe-ad78-4378-beea-5303ea4a10d6 container dapi-container: <nil>
  STEP: delete the pod @ 06/17/23 12:46:29.221
  Jun 17 12:46:29.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1372" for this suite. @ 06/17/23 12:46:29.243
• [4.083 seconds]
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 06/17/23 12:46:29.249
  Jun 17 12:46:29.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename svcaccounts @ 06/17/23 12:46:29.25
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:46:29.268
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:46:29.271
  Jun 17 12:46:29.289: INFO: created pod
  STEP: Saw pod success @ 06/17/23 12:46:33.299
  Jun 17 12:47:03.299: INFO: polling logs
  Jun 17 12:47:03.306: INFO: Pod logs: 
  I0617 12:46:29.977058       1 log.go:198] OK: Got token
  I0617 12:46:29.977126       1 log.go:198] validating with in-cluster discovery
  I0617 12:46:29.977421       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0617 12:46:29.977450       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8266:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1687006589, NotBefore:1687005989, IssuedAt:1687005989, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8266", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b7e7a0bb-d229-4699-b2e2-e3f7b540932e"}}}
  I0617 12:46:29.986420       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0617 12:46:29.991937       1 log.go:198] OK: Validated signature on JWT
  I0617 12:46:29.992050       1 log.go:198] OK: Got valid claims from token!
  I0617 12:46:29.992132       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8266:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1687006589, NotBefore:1687005989, IssuedAt:1687005989, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8266", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b7e7a0bb-d229-4699-b2e2-e3f7b540932e"}}}

  Jun 17 12:47:03.306: INFO: completed pod
  Jun 17 12:47:03.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8266" for this suite. @ 06/17/23 12:47:03.318
• [34.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 06/17/23 12:47:03.327
  Jun 17 12:47:03.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename disruption @ 06/17/23 12:47:03.328
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:47:03.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:47:03.351
  STEP: Waiting for the pdb to be processed @ 06/17/23 12:47:03.359
  STEP: Updating PodDisruptionBudget status @ 06/17/23 12:47:05.365
  STEP: Waiting for all pods to be running @ 06/17/23 12:47:05.374
  Jun 17 12:47:05.377: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 06/17/23 12:47:07.381
  STEP: Waiting for the pdb to be processed @ 06/17/23 12:47:07.391
  STEP: Patching PodDisruptionBudget status @ 06/17/23 12:47:07.399
  STEP: Waiting for the pdb to be processed @ 06/17/23 12:47:07.407
  Jun 17 12:47:07.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9141" for this suite. @ 06/17/23 12:47:07.414
• [4.092 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 06/17/23 12:47:07.424
  Jun 17 12:47:07.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename configmap @ 06/17/23 12:47:07.424
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:47:07.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:47:07.448
  STEP: Creating configMap configmap-3408/configmap-test-139cd918-f1f7-4896-9dd9-2fd41e726f08 @ 06/17/23 12:47:07.451
  STEP: Creating a pod to test consume configMaps @ 06/17/23 12:47:07.456
  STEP: Saw pod success @ 06/17/23 12:47:11.472
  Jun 17 12:47:11.475: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-configmaps-dcb7c6e2-eac6-4c28-a31a-f8b3a58a7fcb container env-test: <nil>
  STEP: delete the pod @ 06/17/23 12:47:11.482
  Jun 17 12:47:11.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3408" for this suite. @ 06/17/23 12:47:11.501
• [4.084 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 06/17/23 12:47:11.509
  Jun 17 12:47:11.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename dns @ 06/17/23 12:47:11.51
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:47:11.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:47:11.528
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 06/17/23 12:47:11.531
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 06/17/23 12:47:11.531
  STEP: creating a pod to probe DNS @ 06/17/23 12:47:11.531
  STEP: submitting the pod to kubernetes @ 06/17/23 12:47:11.531
  STEP: retrieving the pod @ 06/17/23 12:47:13.548
  STEP: looking for the results for each expected name from probers @ 06/17/23 12:47:13.551
  Jun 17 12:47:13.565: INFO: DNS probes using dns-2855/dns-test-d74d2973-8721-4980-9d8a-063379c3350a succeeded

  Jun 17 12:47:13.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/17/23 12:47:13.568
  STEP: Destroying namespace "dns-2855" for this suite. @ 06/17/23 12:47:13.582
• [2.079 seconds]
------------------------------
S
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 06/17/23 12:47:13.588
  Jun 17 12:47:13.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename endpointslicemirroring @ 06/17/23 12:47:13.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:47:13.609
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:47:13.611
  STEP: mirroring a new custom Endpoint @ 06/17/23 12:47:13.623
  Jun 17 12:47:13.632: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 06/17/23 12:47:15.636
  Jun 17 12:47:15.645: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  STEP: mirroring deletion of a custom Endpoint @ 06/17/23 12:47:17.649
  Jun 17 12:47:17.658: INFO: Waiting for 0 EndpointSlices to exist, got 1
  Jun 17 12:47:19.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-7781" for this suite. @ 06/17/23 12:47:19.665
• [6.085 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 06/17/23 12:47:19.673
  Jun 17 12:47:19.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename watch @ 06/17/23 12:47:19.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:47:19.69
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:47:19.692
  STEP: creating a new configmap @ 06/17/23 12:47:19.695
  STEP: modifying the configmap once @ 06/17/23 12:47:19.701
  STEP: modifying the configmap a second time @ 06/17/23 12:47:19.708
  STEP: deleting the configmap @ 06/17/23 12:47:19.714
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 06/17/23 12:47:19.72
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 06/17/23 12:47:19.721
  Jun 17 12:47:19.721: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-405  f9ceb53c-70cc-4bd2-b628-5b02e6fc91e2 19864 0 2023-06-17 12:47:19 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-17 12:47:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 17 12:47:19.721: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-405  f9ceb53c-70cc-4bd2-b628-5b02e6fc91e2 19865 0 2023-06-17 12:47:19 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-17 12:47:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 17 12:47:19.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-405" for this suite. @ 06/17/23 12:47:19.725
• [0.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 06/17/23 12:47:19.734
  Jun 17 12:47:19.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename services @ 06/17/23 12:47:19.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:47:19.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:47:19.758
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-375 @ 06/17/23 12:47:19.76
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 06/17/23 12:47:19.773
  STEP: creating service externalsvc in namespace services-375 @ 06/17/23 12:47:19.773
  STEP: creating replication controller externalsvc in namespace services-375 @ 06/17/23 12:47:19.786
  I0617 12:47:19.793148      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-375, replica count: 2
  I0617 12:47:22.843762      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 06/17/23 12:47:22.847
  Jun 17 12:47:22.863: INFO: Creating new exec pod
  Jun 17 12:47:24.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-375 exec execpodlvd4h -- /bin/sh -x -c nslookup nodeport-service.services-375.svc.cluster.local'
  Jun 17 12:47:25.033: INFO: stderr: "+ nslookup nodeport-service.services-375.svc.cluster.local\n"
  Jun 17 12:47:25.033: INFO: stdout: "Server:\t\t10.152.183.101\nAddress:\t10.152.183.101#53\n\nnodeport-service.services-375.svc.cluster.local\tcanonical name = externalsvc.services-375.svc.cluster.local.\nName:\texternalsvc.services-375.svc.cluster.local\nAddress: 10.152.183.100\n\n"
  Jun 17 12:47:25.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-375, will wait for the garbage collector to delete the pods @ 06/17/23 12:47:25.037
  Jun 17 12:47:25.096: INFO: Deleting ReplicationController externalsvc took: 5.950393ms
  Jun 17 12:47:25.197: INFO: Terminating ReplicationController externalsvc pods took: 100.295668ms
  Jun 17 12:47:27.011: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-375" for this suite. @ 06/17/23 12:47:27.022
• [7.293 seconds]
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 06/17/23 12:47:27.027
  Jun 17 12:47:27.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename subpath @ 06/17/23 12:47:27.028
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:47:27.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:47:27.047
  STEP: Setting up data @ 06/17/23 12:47:27.051
  STEP: Creating pod pod-subpath-test-projected-dwnz @ 06/17/23 12:47:27.058
  STEP: Creating a pod to test atomic-volume-subpath @ 06/17/23 12:47:27.058
  STEP: Saw pod success @ 06/17/23 12:47:51.12
  Jun 17 12:47:51.124: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-subpath-test-projected-dwnz container test-container-subpath-projected-dwnz: <nil>
  STEP: delete the pod @ 06/17/23 12:47:51.142
  STEP: Deleting pod pod-subpath-test-projected-dwnz @ 06/17/23 12:47:51.158
  Jun 17 12:47:51.158: INFO: Deleting pod "pod-subpath-test-projected-dwnz" in namespace "subpath-1992"
  Jun 17 12:47:51.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1992" for this suite. @ 06/17/23 12:47:51.164
• [24.143 seconds]
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 06/17/23 12:47:51.171
  Jun 17 12:47:51.171: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename sched-preemption @ 06/17/23 12:47:51.172
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:47:51.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:47:51.194
  Jun 17 12:47:51.208: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun 17 12:48:51.224: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 06/17/23 12:48:51.227
  Jun 17 12:48:51.247: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jun 17 12:48:51.255: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jun 17 12:48:51.276: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jun 17 12:48:51.285: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jun 17 12:48:51.301: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jun 17 12:48:51.307: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 06/17/23 12:48:51.307
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 06/17/23 12:48:53.331
  Jun 17 12:48:57.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-1411" for this suite. @ 06/17/23 12:48:57.423
• [66.258 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 06/17/23 12:48:57.432
  Jun 17 12:48:57.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename sched-preemption @ 06/17/23 12:48:57.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:48:57.451
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:48:57.454
  Jun 17 12:48:57.469: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun 17 12:49:57.488: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 06/17/23 12:49:57.492
  Jun 17 12:49:57.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename sched-preemption-path @ 06/17/23 12:49:57.492
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:49:57.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:49:57.525
  Jun 17 12:49:57.545: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Jun 17 12:49:57.548: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Jun 17 12:49:57.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 17 12:49:57.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-1497" for this suite. @ 06/17/23 12:49:57.637
  STEP: Destroying namespace "sched-preemption-7755" for this suite. @ 06/17/23 12:49:57.642
• [60.216 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 06/17/23 12:49:57.65
  Jun 17 12:49:57.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename configmap @ 06/17/23 12:49:57.651
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:49:57.668
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:49:57.672
  STEP: Creating configMap with name configmap-test-volume-6216753f-3ebf-4e4f-8057-9b6e8e0ed381 @ 06/17/23 12:49:57.676
  STEP: Creating a pod to test consume configMaps @ 06/17/23 12:49:57.681
  STEP: Saw pod success @ 06/17/23 12:50:01.706
  Jun 17 12:50:01.710: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-configmaps-bc2803f8-3f06-4ebe-b786-f4f9524e32a7 container agnhost-container: <nil>
  STEP: delete the pod @ 06/17/23 12:50:01.731
  Jun 17 12:50:01.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8891" for this suite. @ 06/17/23 12:50:01.753
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 06/17/23 12:50:01.762
  Jun 17 12:50:01.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubelet-test @ 06/17/23 12:50:01.763
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:50:01.785
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:50:01.788
  STEP: Waiting for pod completion @ 06/17/23 12:50:01.814
  Jun 17 12:50:05.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1903" for this suite. @ 06/17/23 12:50:05.837
• [4.084 seconds]
------------------------------
SS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 06/17/23 12:50:05.846
  Jun 17 12:50:05.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename events @ 06/17/23 12:50:05.847
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:50:05.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:50:05.868
  STEP: creating a test event @ 06/17/23 12:50:05.871
  STEP: listing events in all namespaces @ 06/17/23 12:50:05.883
  STEP: listing events in test namespace @ 06/17/23 12:50:05.888
  STEP: listing events with field selection filtering on source @ 06/17/23 12:50:05.891
  STEP: listing events with field selection filtering on reportingController @ 06/17/23 12:50:05.894
  STEP: getting the test event @ 06/17/23 12:50:05.897
  STEP: patching the test event @ 06/17/23 12:50:05.9
  STEP: getting the test event @ 06/17/23 12:50:05.908
  STEP: updating the test event @ 06/17/23 12:50:05.911
  STEP: getting the test event @ 06/17/23 12:50:05.916
  STEP: deleting the test event @ 06/17/23 12:50:05.919
  STEP: listing events in all namespaces @ 06/17/23 12:50:05.927
  STEP: listing events in test namespace @ 06/17/23 12:50:05.933
  Jun 17 12:50:05.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-958" for this suite. @ 06/17/23 12:50:05.94
• [0.099 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 06/17/23 12:50:05.946
  Jun 17 12:50:05.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 12:50:05.947
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:50:05.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:50:05.972
  STEP: Setting up server cert @ 06/17/23 12:50:06.002
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 12:50:06.219
  STEP: Deploying the webhook pod @ 06/17/23 12:50:06.227
  STEP: Wait for the deployment to be ready @ 06/17/23 12:50:06.24
  Jun 17 12:50:06.248: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/17/23 12:50:08.258
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 12:50:08.266
  Jun 17 12:50:09.267: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun 17 12:50:09.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2496-crds.webhook.example.com via the AdmissionRegistration API @ 06/17/23 12:50:09.792
  STEP: Creating a custom resource that should be mutated by the webhook @ 06/17/23 12:50:09.812
  Jun 17 12:50:11.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1908" for this suite. @ 06/17/23 12:50:12.465
  STEP: Destroying namespace "webhook-markers-3706" for this suite. @ 06/17/23 12:50:12.473
• [6.535 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 06/17/23 12:50:12.485
  Jun 17 12:50:12.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename var-expansion @ 06/17/23 12:50:12.486
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:50:12.506
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:50:12.509
  STEP: Creating a pod to test substitution in container's command @ 06/17/23 12:50:12.513
  STEP: Saw pod success @ 06/17/23 12:50:16.534
  Jun 17 12:50:16.537: INFO: Trying to get logs from node ip-172-31-25-17 pod var-expansion-0fafff0b-0967-45db-9587-967295c1706e container dapi-container: <nil>
  STEP: delete the pod @ 06/17/23 12:50:16.543
  Jun 17 12:50:16.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8469" for this suite. @ 06/17/23 12:50:16.562
• [4.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 06/17/23 12:50:16.569
  Jun 17 12:50:16.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/17/23 12:50:16.57
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:50:16.589
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:50:16.591
  Jun 17 12:50:16.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:50:17.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5549" for this suite. @ 06/17/23 12:50:17.62
• [1.057 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 06/17/23 12:50:17.627
  Jun 17 12:50:17.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl @ 06/17/23 12:50:17.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:50:17.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:50:17.645
  STEP: validating api versions @ 06/17/23 12:50:17.648
  Jun 17 12:50:17.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-7416 api-versions'
  Jun 17 12:50:17.710: INFO: stderr: ""
  Jun 17 12:50:17.710: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Jun 17 12:50:17.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7416" for this suite. @ 06/17/23 12:50:17.714
• [0.093 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 06/17/23 12:50:17.72
  Jun 17 12:50:17.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename downward-api @ 06/17/23 12:50:17.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:50:17.742
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:50:17.745
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 12:50:17.749
  STEP: Saw pod success @ 06/17/23 12:50:21.773
  Jun 17 12:50:21.777: INFO: Trying to get logs from node ip-172-31-25-17 pod downwardapi-volume-9b288c18-27dd-4c23-bcc1-cd8ed09992cb container client-container: <nil>
  STEP: delete the pod @ 06/17/23 12:50:21.782
  Jun 17 12:50:21.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3405" for this suite. @ 06/17/23 12:50:21.802
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 06/17/23 12:50:21.81
  Jun 17 12:50:21.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 12:50:21.811
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:50:21.83
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:50:21.833
  STEP: Setting up server cert @ 06/17/23 12:50:21.859
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 12:50:22.246
  STEP: Deploying the webhook pod @ 06/17/23 12:50:22.251
  STEP: Wait for the deployment to be ready @ 06/17/23 12:50:22.264
  Jun 17 12:50:22.273: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/17/23 12:50:24.282
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 12:50:24.29
  Jun 17 12:50:25.291: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun 17 12:50:25.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2101-crds.webhook.example.com via the AdmissionRegistration API @ 06/17/23 12:50:25.839
  STEP: Creating a custom resource while v1 is storage version @ 06/17/23 12:50:25.874
  STEP: Patching Custom Resource Definition to set v2 as storage @ 06/17/23 12:50:27.949
  STEP: Patching the custom resource while v2 is storage version @ 06/17/23 12:50:27.964
  Jun 17 12:50:28.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2390" for this suite. @ 06/17/23 12:50:28.572
  STEP: Destroying namespace "webhook-markers-9229" for this suite. @ 06/17/23 12:50:28.578
• [6.774 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 06/17/23 12:50:28.585
  Jun 17 12:50:28.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename prestop @ 06/17/23 12:50:28.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:50:28.61
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:50:28.613
  STEP: Creating server pod server in namespace prestop-928 @ 06/17/23 12:50:28.616
  STEP: Waiting for pods to come up. @ 06/17/23 12:50:28.623
  STEP: Creating tester pod tester in namespace prestop-928 @ 06/17/23 12:50:30.633
  STEP: Deleting pre-stop pod @ 06/17/23 12:50:32.647
  Jun 17 12:50:37.659: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Jun 17 12:50:37.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 06/17/23 12:50:37.663
  STEP: Destroying namespace "prestop-928" for this suite. @ 06/17/23 12:50:37.672
• [9.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 06/17/23 12:50:37.685
  Jun 17 12:50:37.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename init-container @ 06/17/23 12:50:37.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:50:37.706
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:50:37.708
  STEP: creating the pod @ 06/17/23 12:50:37.717
  Jun 17 12:50:37.717: INFO: PodSpec: initContainers in spec.initContainers
  Jun 17 12:51:23.380: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-3e820641-f5e0-4a7c-8d3e-c3a08a843e67", GenerateName:"", Namespace:"init-container-5450", SelfLink:"", UID:"f5d18480-01de-4fd4-b874-090348a47ea7", ResourceVersion:"21196", Generation:0, CreationTimestamp:time.Date(2023, time.June, 17, 12, 50, 37, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"717163246"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 17, 12, 50, 37, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e8a618), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 17, 12, 51, 23, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e8a678), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-64h2j", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc006363760), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-64h2j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-64h2j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-64h2j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004c674d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-25-17", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00040c770), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004c67560)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004c67580)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004c67588), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004c6758c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001441330), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 17, 12, 50, 37, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 17, 12, 50, 37, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 17, 12, 50, 37, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 17, 12, 50, 37, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.25.17", PodIP:"192.168.178.139", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.178.139"}}, StartTime:time.Date(2023, time.June, 17, 12, 50, 37, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00040c850)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00040c930)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://25d4ae946668d1849c8bdb37d3a0adbe59c2900abe0e78ff5b43cf59c21d65be", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0063637e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0063637c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004c6760f), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Jun 17 12:51:23.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5450" for this suite. @ 06/17/23 12:51:23.384
• [45.706 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 06/17/23 12:51:23.391
  Jun 17 12:51:23.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-probe @ 06/17/23 12:51:23.392
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:51:23.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:51:23.418
  STEP: Creating pod test-grpc-2dee8cfb-8b80-4239-b35b-192574bc0941 in namespace container-probe-6205 @ 06/17/23 12:51:23.421
  Jun 17 12:51:25.437: INFO: Started pod test-grpc-2dee8cfb-8b80-4239-b35b-192574bc0941 in namespace container-probe-6205
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/17/23 12:51:25.437
  Jun 17 12:51:25.440: INFO: Initial restart count of pod test-grpc-2dee8cfb-8b80-4239-b35b-192574bc0941 is 0
  Jun 17 12:55:25.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/17/23 12:55:25.936
  STEP: Destroying namespace "container-probe-6205" for this suite. @ 06/17/23 12:55:25.952
• [242.575 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 06/17/23 12:55:25.967
  Jun 17 12:55:25.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename csiinlinevolumes @ 06/17/23 12:55:25.968
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:55:25.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:55:25.99
  STEP: creating @ 06/17/23 12:55:25.994
  STEP: getting @ 06/17/23 12:55:26.01
  STEP: listing in namespace @ 06/17/23 12:55:26.013
  STEP: patching @ 06/17/23 12:55:26.017
  STEP: deleting @ 06/17/23 12:55:26.023
  Jun 17 12:55:26.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-8024" for this suite. @ 06/17/23 12:55:26.039
• [0.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 06/17/23 12:55:26.048
  Jun 17 12:55:26.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir-wrapper @ 06/17/23 12:55:26.049
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:55:26.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:55:26.069
  Jun 17 12:55:28.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 06/17/23 12:55:28.102
  STEP: Cleaning up the configmap @ 06/17/23 12:55:28.108
  STEP: Cleaning up the pod @ 06/17/23 12:55:28.113
  STEP: Destroying namespace "emptydir-wrapper-2528" for this suite. @ 06/17/23 12:55:28.125
• [2.083 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 06/17/23 12:55:28.131
  Jun 17 12:55:28.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename init-container @ 06/17/23 12:55:28.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:55:28.152
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:55:28.156
  STEP: creating the pod @ 06/17/23 12:55:28.159
  Jun 17 12:55:28.159: INFO: PodSpec: initContainers in spec.initContainers
  Jun 17 12:55:33.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6318" for this suite. @ 06/17/23 12:55:33.159
• [5.036 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 06/17/23 12:55:33.17
  Jun 17 12:55:33.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename controllerrevisions @ 06/17/23 12:55:33.17
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:55:33.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:55:33.191
  STEP: Creating DaemonSet "e2e-zt7lt-daemon-set" @ 06/17/23 12:55:33.209
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/17/23 12:55:33.216
  Jun 17 12:55:33.219: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:55:33.219: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:55:33.222: INFO: Number of nodes with available pods controlled by daemonset e2e-zt7lt-daemon-set: 0
  Jun 17 12:55:33.222: INFO: Node ip-172-31-25-17 is running 0 daemon pod, expected 1
  Jun 17 12:55:34.226: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:55:34.227: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:55:34.230: INFO: Number of nodes with available pods controlled by daemonset e2e-zt7lt-daemon-set: 1
  Jun 17 12:55:34.230: INFO: Node ip-172-31-25-17 is running 0 daemon pod, expected 1
  Jun 17 12:55:35.226: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:55:35.226: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 12:55:35.229: INFO: Number of nodes with available pods controlled by daemonset e2e-zt7lt-daemon-set: 3
  Jun 17 12:55:35.229: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-zt7lt-daemon-set
  STEP: Confirm DaemonSet "e2e-zt7lt-daemon-set" successfully created with "daemonset-name=e2e-zt7lt-daemon-set" label @ 06/17/23 12:55:35.232
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-zt7lt-daemon-set" @ 06/17/23 12:55:35.238
  Jun 17 12:55:35.241: INFO: Located ControllerRevision: "e2e-zt7lt-daemon-set-8df797cbb"
  STEP: Patching ControllerRevision "e2e-zt7lt-daemon-set-8df797cbb" @ 06/17/23 12:55:35.244
  Jun 17 12:55:35.250: INFO: e2e-zt7lt-daemon-set-8df797cbb has been patched
  STEP: Create a new ControllerRevision @ 06/17/23 12:55:35.25
  Jun 17 12:55:35.254: INFO: Created ControllerRevision: e2e-zt7lt-daemon-set-5b696d8456
  STEP: Confirm that there are two ControllerRevisions @ 06/17/23 12:55:35.254
  Jun 17 12:55:35.255: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun 17 12:55:35.257: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-zt7lt-daemon-set-8df797cbb" @ 06/17/23 12:55:35.257
  STEP: Confirm that there is only one ControllerRevision @ 06/17/23 12:55:35.262
  Jun 17 12:55:35.263: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun 17 12:55:35.265: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-zt7lt-daemon-set-5b696d8456" @ 06/17/23 12:55:35.268
  Jun 17 12:55:35.276: INFO: e2e-zt7lt-daemon-set-5b696d8456 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 06/17/23 12:55:35.276
  W0617 12:55:35.283455      19 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 06/17/23 12:55:35.283
  Jun 17 12:55:35.283: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun 17 12:55:36.287: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun 17 12:55:36.290: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-zt7lt-daemon-set-5b696d8456=updated" @ 06/17/23 12:55:36.29
  STEP: Confirm that there is only one ControllerRevision @ 06/17/23 12:55:36.299
  Jun 17 12:55:36.299: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun 17 12:55:36.301: INFO: Found 1 ControllerRevisions
  Jun 17 12:55:36.304: INFO: ControllerRevision "e2e-zt7lt-daemon-set-5c7f754bbf" has revision 3
  STEP: Deleting DaemonSet "e2e-zt7lt-daemon-set" @ 06/17/23 12:55:36.307
  STEP: deleting DaemonSet.extensions e2e-zt7lt-daemon-set in namespace controllerrevisions-8007, will wait for the garbage collector to delete the pods @ 06/17/23 12:55:36.307
  Jun 17 12:55:36.366: INFO: Deleting DaemonSet.extensions e2e-zt7lt-daemon-set took: 5.563634ms
  Jun 17 12:55:36.466: INFO: Terminating DaemonSet.extensions e2e-zt7lt-daemon-set pods took: 100.34473ms
  Jun 17 12:55:38.269: INFO: Number of nodes with available pods controlled by daemonset e2e-zt7lt-daemon-set: 0
  Jun 17 12:55:38.269: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-zt7lt-daemon-set
  Jun 17 12:55:38.272: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21938"},"items":null}

  Jun 17 12:55:38.275: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21938"},"items":null}

  Jun 17 12:55:38.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-8007" for this suite. @ 06/17/23 12:55:38.29
• [5.127 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 06/17/23 12:55:38.298
  Jun 17 12:55:38.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename gc @ 06/17/23 12:55:38.298
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:55:38.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:55:38.318
  STEP: create the deployment @ 06/17/23 12:55:38.321
  W0617 12:55:38.326354      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 06/17/23 12:55:38.326
  STEP: delete the deployment @ 06/17/23 12:55:38.835
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 06/17/23 12:55:38.842
  STEP: Gathering metrics @ 06/17/23 12:55:39.361
  W0617 12:55:39.364877      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 17 12:55:39.364: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 17 12:55:39.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-734" for this suite. @ 06/17/23 12:55:39.368
• [1.076 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 06/17/23 12:55:39.374
  Jun 17 12:55:39.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename secrets @ 06/17/23 12:55:39.375
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:55:39.394
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:55:39.396
  STEP: Creating secret with name secret-test-03498352-1716-440c-be86-6f349f21c64d @ 06/17/23 12:55:39.4
  STEP: Creating a pod to test consume secrets @ 06/17/23 12:55:39.406
  STEP: Saw pod success @ 06/17/23 12:55:43.425
  Jun 17 12:55:43.428: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-secrets-092491e2-59b5-49c1-8955-e526f5612623 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/17/23 12:55:43.448
  Jun 17 12:55:43.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9914" for this suite. @ 06/17/23 12:55:43.467
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 06/17/23 12:55:43.479
  Jun 17 12:55:43.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 12:55:43.48
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:55:43.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:55:43.499
  STEP: Creating configMap with name projected-configmap-test-volume-7841681a-0f8b-402a-bf53-44e811f4d982 @ 06/17/23 12:55:43.504
  STEP: Creating a pod to test consume configMaps @ 06/17/23 12:55:43.51
  STEP: Saw pod success @ 06/17/23 12:55:47.534
  Jun 17 12:55:47.537: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-projected-configmaps-55105a27-8c6d-4ecf-bf7e-abf3fdabefe0 container agnhost-container: <nil>
  STEP: delete the pod @ 06/17/23 12:55:47.554
  Jun 17 12:55:47.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6733" for this suite. @ 06/17/23 12:55:47.572
• [4.100 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 06/17/23 12:55:47.579
  Jun 17 12:55:47.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir @ 06/17/23 12:55:47.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:55:47.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:55:47.602
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 06/17/23 12:55:47.604
  STEP: Saw pod success @ 06/17/23 12:55:51.624
  Jun 17 12:55:51.627: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-c739655c-15b5-4b86-ad4a-439ed3475f2a container test-container: <nil>
  STEP: delete the pod @ 06/17/23 12:55:51.633
  Jun 17 12:55:51.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7175" for this suite. @ 06/17/23 12:55:51.651
• [4.079 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 06/17/23 12:55:51.659
  Jun 17 12:55:51.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 12:55:51.66
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:55:51.677
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:55:51.68
  STEP: Creating configMap with name projected-configmap-test-volume-map-339be0eb-05c3-4eef-bd86-389d6aee6042 @ 06/17/23 12:55:51.683
  STEP: Creating a pod to test consume configMaps @ 06/17/23 12:55:51.687
  STEP: Saw pod success @ 06/17/23 12:55:55.715
  Jun 17 12:55:55.718: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-projected-configmaps-374cbea7-0ed2-4f6e-b390-f5dbd3c24508 container agnhost-container: <nil>
  STEP: delete the pod @ 06/17/23 12:55:55.724
  Jun 17 12:55:55.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-917" for this suite. @ 06/17/23 12:55:55.741
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 06/17/23 12:55:55.748
  Jun 17 12:55:55.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename downward-api @ 06/17/23 12:55:55.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:55:55.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:55:55.769
  STEP: Creating the pod @ 06/17/23 12:55:55.772
  Jun 17 12:55:58.309: INFO: Successfully updated pod "labelsupdatebcf9b28b-849f-4c27-bc5e-27b40638cf9e"
  Jun 17 12:56:00.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6767" for this suite. @ 06/17/23 12:56:00.327
• [4.585 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 06/17/23 12:56:00.333
  Jun 17 12:56:00.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename disruption @ 06/17/23 12:56:00.334
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:56:00.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:56:00.363
  STEP: creating the pdb @ 06/17/23 12:56:00.366
  STEP: Waiting for the pdb to be processed @ 06/17/23 12:56:00.372
  STEP: updating the pdb @ 06/17/23 12:56:02.379
  STEP: Waiting for the pdb to be processed @ 06/17/23 12:56:02.388
  STEP: patching the pdb @ 06/17/23 12:56:04.394
  STEP: Waiting for the pdb to be processed @ 06/17/23 12:56:04.408
  STEP: Waiting for the pdb to be deleted @ 06/17/23 12:56:06.42
  Jun 17 12:56:06.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-3211" for this suite. @ 06/17/23 12:56:06.427
• [6.099 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 06/17/23 12:56:06.434
  Jun 17 12:56:06.434: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename subpath @ 06/17/23 12:56:06.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:56:06.451
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:56:06.454
  STEP: Setting up data @ 06/17/23 12:56:06.457
  STEP: Creating pod pod-subpath-test-configmap-k4xm @ 06/17/23 12:56:06.464
  STEP: Creating a pod to test atomic-volume-subpath @ 06/17/23 12:56:06.464
  STEP: Saw pod success @ 06/17/23 12:56:30.522
  Jun 17 12:56:30.525: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-subpath-test-configmap-k4xm container test-container-subpath-configmap-k4xm: <nil>
  STEP: delete the pod @ 06/17/23 12:56:30.532
  STEP: Deleting pod pod-subpath-test-configmap-k4xm @ 06/17/23 12:56:30.547
  Jun 17 12:56:30.547: INFO: Deleting pod "pod-subpath-test-configmap-k4xm" in namespace "subpath-4743"
  Jun 17 12:56:30.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4743" for this suite. @ 06/17/23 12:56:30.554
• [24.126 seconds]
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 06/17/23 12:56:30.56
  Jun 17 12:56:30.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename server-version @ 06/17/23 12:56:30.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:56:30.579
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:56:30.582
  STEP: Request ServerVersion @ 06/17/23 12:56:30.584
  STEP: Confirm major version @ 06/17/23 12:56:30.586
  Jun 17 12:56:30.586: INFO: Major version: 1
  STEP: Confirm minor version @ 06/17/23 12:56:30.586
  Jun 17 12:56:30.586: INFO: cleanMinorVersion: 27
  Jun 17 12:56:30.586: INFO: Minor version: 27
  Jun 17 12:56:30.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-4688" for this suite. @ 06/17/23 12:56:30.59
• [0.036 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 06/17/23 12:56:30.597
  Jun 17 12:56:30.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename sched-preemption @ 06/17/23 12:56:30.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:56:30.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:56:30.617
  Jun 17 12:56:30.635: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun 17 12:57:30.654: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 06/17/23 12:57:30.657
  Jun 17 12:57:30.676: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jun 17 12:57:30.683: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jun 17 12:57:30.700: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jun 17 12:57:30.709: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jun 17 12:57:30.734: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jun 17 12:57:30.739: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 06/17/23 12:57:30.74
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 06/17/23 12:57:32.769
  Jun 17 12:57:36.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-1341" for this suite. @ 06/17/23 12:57:36.836
• [66.246 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 06/17/23 12:57:36.844
  Jun 17 12:57:36.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename deployment @ 06/17/23 12:57:36.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:57:36.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:57:36.865
  Jun 17 12:57:36.869: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Jun 17 12:57:36.877: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jun 17 12:57:41.882: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/17/23 12:57:41.882
  Jun 17 12:57:41.882: INFO: Creating deployment "test-rolling-update-deployment"
  Jun 17 12:57:41.887: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Jun 17 12:57:41.892: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  Jun 17 12:57:43.900: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Jun 17 12:57:43.902: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Jun 17 12:57:43.911: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8923  89b8f75b-0bca-4e0e-9907-39ade16037ef 22802 1 2023-06-17 12:57:41 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-06-17 12:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:57:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055d5eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-17 12:57:41 +0000 UTC,LastTransitionTime:2023-06-17 12:57:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-06-17 12:57:43 +0000 UTC,LastTransitionTime:2023-06-17 12:57:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun 17 12:57:43.915: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-8923  704a3172-ac43-46a4-a0a3-0234074ac063 22792 1 2023-06-17 12:57:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 89b8f75b-0bca-4e0e-9907-39ade16037ef 0xc0032ea3c7 0xc0032ea3c8}] [] [{kube-controller-manager Update apps/v1 2023-06-17 12:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89b8f75b-0bca-4e0e-9907-39ade16037ef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:57:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032ea478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun 17 12:57:43.915: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Jun 17 12:57:43.915: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8923  a390b44d-1250-47ad-b5f2-23e7c4454921 22801 2 2023-06-17 12:57:36 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 89b8f75b-0bca-4e0e-9907-39ade16037ef 0xc0032ea277 0xc0032ea278}] [] [{e2e.test Update apps/v1 2023-06-17 12:57:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:57:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89b8f75b-0bca-4e0e-9907-39ade16037ef\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:57:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0032ea358 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 17 12:57:43.918: INFO: Pod "test-rolling-update-deployment-656d657cd8-r9vxm" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-r9vxm test-rolling-update-deployment-656d657cd8- deployment-8923  39654d23-ec57-4294-9f89-f3af9156cf36 22791 0 2023-06-17 12:57:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 704a3172-ac43-46a4-a0a3-0234074ac063 0xc0032ea8c7 0xc0032ea8c8}] [] [{kube-controller-manager Update v1 2023-06-17 12:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"704a3172-ac43-46a4-a0a3-0234074ac063\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 12:57:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qrfvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qrfvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:57:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:57:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:57:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:57:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.18,PodIP:192.168.2.123,StartTime:2023-06-17 12:57:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-17 12:57:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://c38b7ca8fa52afb3564b5df7d94e6177f545247d8efcbfb3a2899d802c2b093c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.123,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 12:57:43.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8923" for this suite. @ 06/17/23 12:57:43.922
• [7.086 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 06/17/23 12:57:43.93
  Jun 17 12:57:43.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename replication-controller @ 06/17/23 12:57:43.931
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:57:43.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:57:43.954
  STEP: creating a ReplicationController @ 06/17/23 12:57:43.96
  STEP: waiting for RC to be added @ 06/17/23 12:57:43.966
  STEP: waiting for available Replicas @ 06/17/23 12:57:43.966
  STEP: patching ReplicationController @ 06/17/23 12:57:45.058
  STEP: waiting for RC to be modified @ 06/17/23 12:57:45.067
  STEP: patching ReplicationController status @ 06/17/23 12:57:45.067
  STEP: waiting for RC to be modified @ 06/17/23 12:57:45.072
  STEP: waiting for available Replicas @ 06/17/23 12:57:45.072
  STEP: fetching ReplicationController status @ 06/17/23 12:57:45.081
  STEP: patching ReplicationController scale @ 06/17/23 12:57:45.084
  STEP: waiting for RC to be modified @ 06/17/23 12:57:45.091
  STEP: waiting for ReplicationController's scale to be the max amount @ 06/17/23 12:57:45.091
  STEP: fetching ReplicationController; ensuring that it's patched @ 06/17/23 12:57:46.144
  STEP: updating ReplicationController status @ 06/17/23 12:57:46.147
  STEP: waiting for RC to be modified @ 06/17/23 12:57:46.155
  STEP: listing all ReplicationControllers @ 06/17/23 12:57:46.155
  STEP: checking that ReplicationController has expected values @ 06/17/23 12:57:46.16
  STEP: deleting ReplicationControllers by collection @ 06/17/23 12:57:46.16
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 06/17/23 12:57:46.167
  Jun 17 12:57:46.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0617 12:57:46.201206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-1399" for this suite. @ 06/17/23 12:57:46.204
• [2.281 seconds]
------------------------------
SS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 06/17/23 12:57:46.211
  Jun 17 12:57:46.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename subjectreview @ 06/17/23 12:57:46.212
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:57:46.228
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:57:46.231
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-9777" @ 06/17/23 12:57:46.234
  Jun 17 12:57:46.251: INFO: saUsername: "system:serviceaccount:subjectreview-9777:e2e"
  Jun 17 12:57:46.251: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-9777"}
  Jun 17 12:57:46.251: INFO: saUID: "fa79be42-25d4-4397-afe5-ed933df71125"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-9777:e2e" @ 06/17/23 12:57:46.251
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-9777:e2e" @ 06/17/23 12:57:46.251
  Jun 17 12:57:46.253: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-9777:e2e" api 'list' configmaps in "subjectreview-9777" namespace @ 06/17/23 12:57:46.253
  Jun 17 12:57:46.254: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-9777:e2e" @ 06/17/23 12:57:46.255
  Jun 17 12:57:46.256: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Jun 17 12:57:46.257: INFO: LocalSubjectAccessReview has been verified
  Jun 17 12:57:46.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-9777" for this suite. @ 06/17/23 12:57:46.261
• [0.056 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 06/17/23 12:57:46.268
  Jun 17 12:57:46.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 12:57:46.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:57:46.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:57:46.289
  STEP: Creating secret with name projected-secret-test-774cf30c-29c8-4c10-a6d6-3d91915fda88 @ 06/17/23 12:57:46.293
  STEP: Creating a pod to test consume secrets @ 06/17/23 12:57:46.299
  E0617 12:57:47.201381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:57:48.201442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:57:49.201678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:57:50.201847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 12:57:50.317
  Jun 17 12:57:50.320: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-projected-secrets-c99fe920-7365-4952-876d-a601c820caf2 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/17/23 12:57:50.334
  Jun 17 12:57:50.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3007" for this suite. @ 06/17/23 12:57:50.353
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 06/17/23 12:57:50.362
  Jun 17 12:57:50.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename deployment @ 06/17/23 12:57:50.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:57:50.377
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:57:50.38
  Jun 17 12:57:50.383: INFO: Creating simple deployment test-new-deployment
  Jun 17 12:57:50.394: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0617 12:57:51.202370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:57:52.202591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 06/17/23 12:57:52.406
  STEP: updating a scale subresource @ 06/17/23 12:57:52.409
  STEP: verifying the deployment Spec.Replicas was modified @ 06/17/23 12:57:52.416
  STEP: Patch a scale subresource @ 06/17/23 12:57:52.419
  Jun 17 12:57:52.434: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-8388  5376e68b-65f5-45e5-a65d-b3a386b0534a 23017 3 2023-06-17 12:57:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-06-17 12:57:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033b32d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-17 12:57:52 +0000 UTC,LastTransitionTime:2023-06-17 12:57:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-06-17 12:57:52 +0000 UTC,LastTransitionTime:2023-06-17 12:57:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun 17 12:57:52.439: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-8388  b0b4bbff-88f1-433e-80c9-e9324003454c 23016 2 2023-06-17 12:57:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 5376e68b-65f5-45e5-a65d-b3a386b0534a 0xc003ef5b77 0xc003ef5b78}] [] [{kube-controller-manager Update apps/v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5376e68b-65f5-45e5-a65d-b3a386b0534a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ef5c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun 17 12:57:52.444: INFO: Pod "test-new-deployment-67bd4bf6dc-nzs2k" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-nzs2k test-new-deployment-67bd4bf6dc- deployment-8388  d355dd0a-d91b-41c0-aae7-1174f80bd11c 23006 0 2023-06-17 12:57:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc b0b4bbff-88f1-433e-80c9-e9324003454c 0xc0061a2027 0xc0061a2028}] [] [{kube-controller-manager Update v1 2023-06-17 12:57:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0b4bbff-88f1-433e-80c9-e9324003454c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.178.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dbmd8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dbmd8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:57:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:57:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:57:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:57:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:192.168.178.175,StartTime:2023-06-17 12:57:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-17 12:57:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c152bd06ddad7d3a67880012192bb87af641f6c17fb0b0f942cdfcb896886a19,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.178.175,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 12:57:52.444: INFO: Pod "test-new-deployment-67bd4bf6dc-shxt2" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-shxt2 test-new-deployment-67bd4bf6dc- deployment-8388  a71cad5b-e891-4e85-86fd-587d47e2201b 23021 0 2023-06-17 12:57:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc b0b4bbff-88f1-433e-80c9-e9324003454c 0xc0061a2237 0xc0061a2238}] [] [{kube-controller-manager Update v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0b4bbff-88f1-433e-80c9-e9324003454c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9q8vp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9q8vp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 12:57:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 12:57:52.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8388" for this suite. @ 06/17/23 12:57:52.448
• [2.095 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 06/17/23 12:57:52.457
  Jun 17 12:57:52.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename watch @ 06/17/23 12:57:52.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:57:52.482
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:57:52.484
  STEP: creating a watch on configmaps with label A @ 06/17/23 12:57:52.487
  STEP: creating a watch on configmaps with label B @ 06/17/23 12:57:52.488
  STEP: creating a watch on configmaps with label A or B @ 06/17/23 12:57:52.49
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 06/17/23 12:57:52.492
  Jun 17 12:57:52.496: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-925  8d987708-b1e2-4b34-8309-408212fb9b88 23037 0 2023-06-17 12:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 17 12:57:52.506: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-925  8d987708-b1e2-4b34-8309-408212fb9b88 23037 0 2023-06-17 12:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 06/17/23 12:57:52.506
  Jun 17 12:57:52.514: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-925  8d987708-b1e2-4b34-8309-408212fb9b88 23038 0 2023-06-17 12:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 17 12:57:52.515: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-925  8d987708-b1e2-4b34-8309-408212fb9b88 23038 0 2023-06-17 12:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 06/17/23 12:57:52.515
  Jun 17 12:57:52.522: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-925  8d987708-b1e2-4b34-8309-408212fb9b88 23039 0 2023-06-17 12:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 17 12:57:52.522: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-925  8d987708-b1e2-4b34-8309-408212fb9b88 23039 0 2023-06-17 12:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 06/17/23 12:57:52.522
  Jun 17 12:57:52.529: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-925  8d987708-b1e2-4b34-8309-408212fb9b88 23040 0 2023-06-17 12:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 17 12:57:52.529: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-925  8d987708-b1e2-4b34-8309-408212fb9b88 23040 0 2023-06-17 12:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 06/17/23 12:57:52.529
  Jun 17 12:57:52.533: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-925  bfe5a049-d40e-4c00-b899-31af15ed01aa 23041 0 2023-06-17 12:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 17 12:57:52.533: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-925  bfe5a049-d40e-4c00-b899-31af15ed01aa 23041 0 2023-06-17 12:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0617 12:57:53.202873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:57:54.203888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:57:55.204006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:57:56.204250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:57:57.204449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:57:58.204544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:57:59.204626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:00.204823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:01.204959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:02.205037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 06/17/23 12:58:02.534
  Jun 17 12:58:02.542: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-925  bfe5a049-d40e-4c00-b899-31af15ed01aa 23100 0 2023-06-17 12:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 17 12:58:02.543: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-925  bfe5a049-d40e-4c00-b899-31af15ed01aa 23100 0 2023-06-17 12:57:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-17 12:57:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0617 12:58:03.205954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:04.206240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:05.206287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:06.206367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:07.206474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:08.206576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:09.206675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:10.206811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:11.207869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:12.207961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 12:58:12.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-925" for this suite. @ 06/17/23 12:58:12.548
• [20.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 06/17/23 12:58:12.558
  Jun 17 12:58:12.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/17/23 12:58:12.559
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:12.577
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:12.58
  Jun 17 12:58:12.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  E0617 12:58:13.208672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 06/17/23 12:58:13.907
  Jun 17 12:58:13.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-3859 --namespace=crd-publish-openapi-3859 create -f -'
  E0617 12:58:14.209498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 12:58:14.536: INFO: stderr: ""
  Jun 17 12:58:14.536: INFO: stdout: "e2e-test-crd-publish-openapi-5081-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jun 17 12:58:14.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-3859 --namespace=crd-publish-openapi-3859 delete e2e-test-crd-publish-openapi-5081-crds test-cr'
  Jun 17 12:58:14.628: INFO: stderr: ""
  Jun 17 12:58:14.628: INFO: stdout: "e2e-test-crd-publish-openapi-5081-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Jun 17 12:58:14.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-3859 --namespace=crd-publish-openapi-3859 apply -f -'
  Jun 17 12:58:14.844: INFO: stderr: ""
  Jun 17 12:58:14.844: INFO: stdout: "e2e-test-crd-publish-openapi-5081-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jun 17 12:58:14.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-3859 --namespace=crd-publish-openapi-3859 delete e2e-test-crd-publish-openapi-5081-crds test-cr'
  Jun 17 12:58:14.920: INFO: stderr: ""
  Jun 17 12:58:14.920: INFO: stdout: "e2e-test-crd-publish-openapi-5081-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 06/17/23 12:58:14.92
  Jun 17 12:58:14.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-3859 explain e2e-test-crd-publish-openapi-5081-crds'
  Jun 17 12:58:15.116: INFO: stderr: ""
  Jun 17 12:58:15.116: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-5081-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0617 12:58:15.210468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:16.210728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 12:58:16.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3859" for this suite. @ 06/17/23 12:58:16.594
• [4.043 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 06/17/23 12:58:16.601
  Jun 17 12:58:16.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename pods @ 06/17/23 12:58:16.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:16.623
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:16.626
  STEP: creating a Pod with a static label @ 06/17/23 12:58:16.636
  STEP: watching for Pod to be ready @ 06/17/23 12:58:16.647
  Jun 17 12:58:16.649: INFO: observed Pod pod-test in namespace pods-2291 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Jun 17 12:58:16.653: INFO: observed Pod pod-test in namespace pods-2291 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 12:58:16 +0000 UTC  }]
  Jun 17 12:58:16.668: INFO: observed Pod pod-test in namespace pods-2291 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 12:58:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-17 12:58:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-17 12:58:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 12:58:16 +0000 UTC  }]
  E0617 12:58:17.210849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 12:58:18.122: INFO: Found Pod pod-test in namespace pods-2291 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 12:58:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 12:58:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 12:58:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 12:58:16 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 06/17/23 12:58:18.126
  STEP: getting the Pod and ensuring that it's patched @ 06/17/23 12:58:18.137
  STEP: replacing the Pod's status Ready condition to False @ 06/17/23 12:58:18.14
  STEP: check the Pod again to ensure its Ready conditions are False @ 06/17/23 12:58:18.152
  STEP: deleting the Pod via a Collection with a LabelSelector @ 06/17/23 12:58:18.152
  STEP: watching for the Pod to be deleted @ 06/17/23 12:58:18.163
  Jun 17 12:58:18.165: INFO: observed event type MODIFIED
  E0617 12:58:18.211264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:19.211626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 12:58:20.090: INFO: observed event type MODIFIED
  Jun 17 12:58:20.123: INFO: observed event type MODIFIED
  E0617 12:58:20.212140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 12:58:20.421: INFO: observed event type MODIFIED
  Jun 17 12:58:21.126: INFO: observed event type MODIFIED
  Jun 17 12:58:21.141: INFO: observed event type MODIFIED
  Jun 17 12:58:21.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2291" for this suite. @ 06/17/23 12:58:21.154
• [4.560 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 06/17/23 12:58:21.161
  Jun 17 12:58:21.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename runtimeclass @ 06/17/23 12:58:21.162
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:21.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:21.186
  E0617 12:58:21.212886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:22.213780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 12:58:23.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0617 12:58:23.213782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "runtimeclass-4181" for this suite. @ 06/17/23 12:58:23.223
• [2.068 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 06/17/23 12:58:23.231
  Jun 17 12:58:23.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename replication-controller @ 06/17/23 12:58:23.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:23.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:23.256
  STEP: Creating replication controller my-hostname-basic-8dd86b11-b71f-424e-946e-5335939545c1 @ 06/17/23 12:58:23.26
  Jun 17 12:58:23.270: INFO: Pod name my-hostname-basic-8dd86b11-b71f-424e-946e-5335939545c1: Found 0 pods out of 1
  E0617 12:58:24.213903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:25.214081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:26.214960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:27.215054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:28.215130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 12:58:28.274: INFO: Pod name my-hostname-basic-8dd86b11-b71f-424e-946e-5335939545c1: Found 1 pods out of 1
  Jun 17 12:58:28.274: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-8dd86b11-b71f-424e-946e-5335939545c1" are running
  Jun 17 12:58:28.278: INFO: Pod "my-hostname-basic-8dd86b11-b71f-424e-946e-5335939545c1-qd7l4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-17 12:58:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-17 12:58:24 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-17 12:58:24 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-17 12:58:23 +0000 UTC Reason: Message:}])
  Jun 17 12:58:28.278: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 06/17/23 12:58:28.278
  Jun 17 12:58:28.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6265" for this suite. @ 06/17/23 12:58:28.297
• [5.074 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 06/17/23 12:58:28.306
  Jun 17 12:58:28.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir @ 06/17/23 12:58:28.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:28.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:28.326
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 06/17/23 12:58:28.332
  E0617 12:58:29.215254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:30.215350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:31.215523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:32.215593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 12:58:32.354
  Jun 17 12:58:32.358: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-78cc3623-68a5-4656-a6bf-a5da8d37ccb7 container test-container: <nil>
  STEP: delete the pod @ 06/17/23 12:58:32.365
  Jun 17 12:58:32.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6281" for this suite. @ 06/17/23 12:58:32.386
• [4.089 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 06/17/23 12:58:32.396
  Jun 17 12:58:32.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename configmap @ 06/17/23 12:58:32.396
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:32.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:32.416
  STEP: Creating configMap with name configmap-test-volume-map-3a4a5017-c3e7-43b2-97fb-1801dee1e0f5 @ 06/17/23 12:58:32.42
  STEP: Creating a pod to test consume configMaps @ 06/17/23 12:58:32.424
  E0617 12:58:33.215707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:34.215805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:35.215899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:36.216052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 12:58:36.446
  Jun 17 12:58:36.449: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-configmaps-32d07e77-ec27-4785-9a15-5aacb93e8722 container agnhost-container: <nil>
  STEP: delete the pod @ 06/17/23 12:58:36.455
  Jun 17 12:58:36.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-939" for this suite. @ 06/17/23 12:58:36.474
• [4.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 06/17/23 12:58:36.481
  Jun 17 12:58:36.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename job @ 06/17/23 12:58:36.482
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:36.503
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:36.506
  STEP: Creating a job @ 06/17/23 12:58:36.509
  STEP: Ensuring active pods == parallelism @ 06/17/23 12:58:36.519
  E0617 12:58:37.216984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:38.217210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 06/17/23 12:58:38.523
  Jun 17 12:58:39.038: INFO: Successfully updated pod "adopt-release-87hbt"
  STEP: Checking that the Job readopts the Pod @ 06/17/23 12:58:39.038
  E0617 12:58:39.218147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:40.218250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 06/17/23 12:58:41.045
  E0617 12:58:41.218876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 12:58:41.556: INFO: Successfully updated pod "adopt-release-87hbt"
  STEP: Checking that the Job releases the Pod @ 06/17/23 12:58:41.556
  E0617 12:58:42.219675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:43.219965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 12:58:43.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1068" for this suite. @ 06/17/23 12:58:43.567
• [7.093 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 06/17/23 12:58:43.574
  Jun 17 12:58:43.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir @ 06/17/23 12:58:43.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:43.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:43.594
  STEP: Creating Pod @ 06/17/23 12:58:43.596
  E0617 12:58:44.222165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:45.222485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 06/17/23 12:58:45.613
  Jun 17 12:58:45.613: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-950 PodName:pod-sharedvolume-dfcdcc98-9efb-4fb7-af22-c248704d6ce5 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 12:58:45.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 12:58:45.614: INFO: ExecWithOptions: Clientset creation
  Jun 17 12:58:45.614: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-950/pods/pod-sharedvolume-dfcdcc98-9efb-4fb7-af22-c248704d6ce5/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Jun 17 12:58:45.679: INFO: Exec stderr: ""
  Jun 17 12:58:45.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-950" for this suite. @ 06/17/23 12:58:45.684
• [2.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 06/17/23 12:58:45.694
  Jun 17 12:58:45.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename pods @ 06/17/23 12:58:45.695
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:45.715
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:45.717
  STEP: Create a pod @ 06/17/23 12:58:45.72
  E0617 12:58:46.223101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:47.223889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 06/17/23 12:58:47.733
  Jun 17 12:58:47.742: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Jun 17 12:58:47.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3217" for this suite. @ 06/17/23 12:58:47.745
• [2.057 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 06/17/23 12:58:47.752
  Jun 17 12:58:47.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename downward-api @ 06/17/23 12:58:47.752
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:47.771
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:47.773
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 12:58:47.776
  E0617 12:58:48.224726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:49.224948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:50.225036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:51.225169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 12:58:51.795
  Jun 17 12:58:51.797: INFO: Trying to get logs from node ip-172-31-25-17 pod downwardapi-volume-9c17b5f1-18b2-4f61-ae4b-194817133c8f container client-container: <nil>
  STEP: delete the pod @ 06/17/23 12:58:51.803
  Jun 17 12:58:51.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5786" for this suite. @ 06/17/23 12:58:51.821
• [4.075 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 06/17/23 12:58:51.827
  Jun 17 12:58:51.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename csistoragecapacity @ 06/17/23 12:58:51.828
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:51.847
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:51.85
  STEP: getting /apis @ 06/17/23 12:58:51.853
  STEP: getting /apis/storage.k8s.io @ 06/17/23 12:58:51.857
  STEP: getting /apis/storage.k8s.io/v1 @ 06/17/23 12:58:51.858
  STEP: creating @ 06/17/23 12:58:51.859
  STEP: watching @ 06/17/23 12:58:51.875
  Jun 17 12:58:51.875: INFO: starting watch
  STEP: getting @ 06/17/23 12:58:51.88
  STEP: listing in namespace @ 06/17/23 12:58:51.888
  STEP: listing across namespaces @ 06/17/23 12:58:51.891
  STEP: patching @ 06/17/23 12:58:51.893
  STEP: updating @ 06/17/23 12:58:51.898
  Jun 17 12:58:51.902: INFO: waiting for watch events with expected annotations in namespace
  Jun 17 12:58:51.902: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 06/17/23 12:58:51.903
  STEP: deleting a collection @ 06/17/23 12:58:51.913
  Jun 17 12:58:51.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-6755" for this suite. @ 06/17/23 12:58:51.931
• [0.111 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 06/17/23 12:58:51.939
  Jun 17 12:58:51.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename downward-api @ 06/17/23 12:58:51.939
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:51.957
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:51.96
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 12:58:51.963
  E0617 12:58:52.226036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:53.226132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:54.226190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:55.226489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 12:58:55.982
  Jun 17 12:58:55.985: INFO: Trying to get logs from node ip-172-31-86-18 pod downwardapi-volume-2f41d929-1574-4e62-9fac-6ce73356eda8 container client-container: <nil>
  STEP: delete the pod @ 06/17/23 12:58:55.999
  Jun 17 12:58:56.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1288" for this suite. @ 06/17/23 12:58:56.017
• [4.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 06/17/23 12:58:56.024
  Jun 17 12:58:56.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename endpointslice @ 06/17/23 12:58:56.025
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:56.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:56.048
  STEP: getting /apis @ 06/17/23 12:58:56.051
  STEP: getting /apis/discovery.k8s.io @ 06/17/23 12:58:56.054
  STEP: getting /apis/discovery.k8s.iov1 @ 06/17/23 12:58:56.056
  STEP: creating @ 06/17/23 12:58:56.057
  STEP: getting @ 06/17/23 12:58:56.071
  STEP: listing @ 06/17/23 12:58:56.073
  STEP: watching @ 06/17/23 12:58:56.076
  Jun 17 12:58:56.076: INFO: starting watch
  STEP: cluster-wide listing @ 06/17/23 12:58:56.078
  STEP: cluster-wide watching @ 06/17/23 12:58:56.081
  Jun 17 12:58:56.081: INFO: starting watch
  STEP: patching @ 06/17/23 12:58:56.082
  STEP: updating @ 06/17/23 12:58:56.086
  Jun 17 12:58:56.093: INFO: waiting for watch events with expected annotations
  Jun 17 12:58:56.093: INFO: saw patched and updated annotations
  STEP: deleting @ 06/17/23 12:58:56.094
  STEP: deleting a collection @ 06/17/23 12:58:56.104
  Jun 17 12:58:56.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4456" for this suite. @ 06/17/23 12:58:56.12
• [0.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 06/17/23 12:58:56.128
  Jun 17 12:58:56.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename events @ 06/17/23 12:58:56.129
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:56.146
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:56.148
  STEP: Create set of events @ 06/17/23 12:58:56.151
  STEP: get a list of Events with a label in the current namespace @ 06/17/23 12:58:56.167
  STEP: delete a list of events @ 06/17/23 12:58:56.17
  Jun 17 12:58:56.170: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 06/17/23 12:58:56.189
  Jun 17 12:58:56.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8324" for this suite. @ 06/17/23 12:58:56.203
• [0.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 06/17/23 12:58:56.215
  Jun 17 12:58:56.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename init-container @ 06/17/23 12:58:56.216
  E0617 12:58:56.227472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:56.232
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:56.235
  STEP: creating the pod @ 06/17/23 12:58:56.238
  Jun 17 12:58:56.238: INFO: PodSpec: initContainers in spec.initContainers
  E0617 12:58:57.228043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:58.228349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:58:59.232069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 12:58:59.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5530" for this suite. @ 06/17/23 12:58:59.79
• [3.580 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 06/17/23 12:58:59.796
  Jun 17 12:58:59.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename secrets @ 06/17/23 12:58:59.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:59.816
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:59.819
  STEP: creating a secret @ 06/17/23 12:58:59.823
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 06/17/23 12:58:59.829
  STEP: patching the secret @ 06/17/23 12:58:59.832
  STEP: deleting the secret using a LabelSelector @ 06/17/23 12:58:59.84
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 06/17/23 12:58:59.846
  Jun 17 12:58:59.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5380" for this suite. @ 06/17/23 12:58:59.854
• [0.066 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 06/17/23 12:58:59.864
  Jun 17 12:58:59.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename sysctl @ 06/17/23 12:58:59.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:59.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:59.888
  STEP: Creating a pod with one valid and two invalid sysctls @ 06/17/23 12:58:59.891
  Jun 17 12:58:59.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-170" for this suite. @ 06/17/23 12:58:59.899
• [0.041 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 06/17/23 12:58:59.905
  Jun 17 12:58:59.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/17/23 12:58:59.907
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:58:59.926
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:58:59.93
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 06/17/23 12:58:59.934
  Jun 17 12:58:59.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  E0617 12:59:00.232261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:01.233091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 12:59:01.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  E0617 12:59:02.233608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:03.234140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:04.234646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:05.234891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:06.235517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 12:59:06.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6797" for this suite. @ 06/17/23 12:59:06.849
• [6.950 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 06/17/23 12:59:06.855
  Jun 17 12:59:06.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 12:59:06.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:59:06.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:59:06.88
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 12:59:06.883
  E0617 12:59:07.235912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:08.235959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:09.236517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:10.236633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 12:59:10.903
  Jun 17 12:59:10.906: INFO: Trying to get logs from node ip-172-31-25-17 pod downwardapi-volume-37f0d38b-41f8-4c85-89fc-959beef1f3a3 container client-container: <nil>
  STEP: delete the pod @ 06/17/23 12:59:10.912
  Jun 17 12:59:10.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5356" for this suite. @ 06/17/23 12:59:10.93
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 06/17/23 12:59:10.938
  Jun 17 12:59:10.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/17/23 12:59:10.939
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:59:10.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:59:10.96
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 06/17/23 12:59:11.002
  Jun 17 12:59:11.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  E0617 12:59:11.237070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:12.238040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:13.238854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:14.239889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:15.240621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:16.240833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 06/17/23 12:59:16.403
  Jun 17 12:59:16.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  E0617 12:59:17.241701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 12:59:17.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  E0617 12:59:18.241749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:19.243598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:20.244286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:21.244850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:22.245395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:23.246195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 12:59:23.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7329" for this suite. @ 06/17/23 12:59:23.318
• [12.386 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 06/17/23 12:59:23.325
  Jun 17 12:59:23.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename subpath @ 06/17/23 12:59:23.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:59:23.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:59:23.348
  STEP: Setting up data @ 06/17/23 12:59:23.351
  STEP: Creating pod pod-subpath-test-downwardapi-tlgp @ 06/17/23 12:59:23.363
  STEP: Creating a pod to test atomic-volume-subpath @ 06/17/23 12:59:23.363
  E0617 12:59:24.246311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:25.246361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:26.247138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:27.247891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:28.247991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:29.248155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:30.248997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:31.249474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:32.249885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:33.250657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:34.250916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:35.251900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:36.252119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:37.252363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:38.252417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:39.252652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:40.252761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:41.253448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:42.253587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:43.253714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:44.253874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:45.254815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:46.255433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:47.255900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 12:59:47.423
  Jun 17 12:59:47.426: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-subpath-test-downwardapi-tlgp container test-container-subpath-downwardapi-tlgp: <nil>
  STEP: delete the pod @ 06/17/23 12:59:47.433
  STEP: Deleting pod pod-subpath-test-downwardapi-tlgp @ 06/17/23 12:59:47.449
  Jun 17 12:59:47.449: INFO: Deleting pod "pod-subpath-test-downwardapi-tlgp" in namespace "subpath-8017"
  Jun 17 12:59:47.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8017" for this suite. @ 06/17/23 12:59:47.455
• [24.135 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 06/17/23 12:59:47.462
  Jun 17 12:59:47.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename dns @ 06/17/23 12:59:47.463
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:59:47.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:59:47.484
  STEP: Creating a test headless service @ 06/17/23 12:59:47.487
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7753.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7753.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7753.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7753.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7753.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7753.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7753.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7753.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7753.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7753.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7753.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7753.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 224.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.224_udp@PTR;check="$$(dig +tcp +noall +answer +search 224.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.224_tcp@PTR;sleep 1; done
   @ 06/17/23 12:59:47.502
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7753.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7753.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7753.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7753.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7753.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7753.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7753.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7753.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7753.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7753.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7753.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7753.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 224.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.224_udp@PTR;check="$$(dig +tcp +noall +answer +search 224.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.224_tcp@PTR;sleep 1; done
   @ 06/17/23 12:59:47.502
  STEP: creating a pod to probe DNS @ 06/17/23 12:59:47.502
  STEP: submitting the pod to kubernetes @ 06/17/23 12:59:47.503
  E0617 12:59:48.256460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:49.256557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/17/23 12:59:49.523
  STEP: looking for the results for each expected name from probers @ 06/17/23 12:59:49.526
  Jun 17 12:59:49.530: INFO: Unable to read wheezy_udp@dns-test-service.dns-7753.svc.cluster.local from pod dns-7753/dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981: the server could not find the requested resource (get pods dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981)
  Jun 17 12:59:49.534: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7753.svc.cluster.local from pod dns-7753/dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981: the server could not find the requested resource (get pods dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981)
  Jun 17 12:59:49.537: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7753.svc.cluster.local from pod dns-7753/dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981: the server could not find the requested resource (get pods dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981)
  Jun 17 12:59:49.541: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7753.svc.cluster.local from pod dns-7753/dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981: the server could not find the requested resource (get pods dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981)
  Jun 17 12:59:49.557: INFO: Unable to read jessie_udp@dns-test-service.dns-7753.svc.cluster.local from pod dns-7753/dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981: the server could not find the requested resource (get pods dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981)
  Jun 17 12:59:49.560: INFO: Unable to read jessie_tcp@dns-test-service.dns-7753.svc.cluster.local from pod dns-7753/dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981: the server could not find the requested resource (get pods dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981)
  Jun 17 12:59:49.564: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7753.svc.cluster.local from pod dns-7753/dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981: the server could not find the requested resource (get pods dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981)
  Jun 17 12:59:49.567: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7753.svc.cluster.local from pod dns-7753/dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981: the server could not find the requested resource (get pods dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981)
  Jun 17 12:59:49.580: INFO: Lookups using dns-7753/dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981 failed for: [wheezy_udp@dns-test-service.dns-7753.svc.cluster.local wheezy_tcp@dns-test-service.dns-7753.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7753.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7753.svc.cluster.local jessie_udp@dns-test-service.dns-7753.svc.cluster.local jessie_tcp@dns-test-service.dns-7753.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7753.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7753.svc.cluster.local]

  E0617 12:59:50.256652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:51.257040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:52.257118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:53.257242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:54.257334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 12:59:54.634: INFO: DNS probes using dns-7753/dns-test-db0f73b3-3556-41e2-b7fc-a8f2ed130981 succeeded

  Jun 17 12:59:54.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/17/23 12:59:54.638
  STEP: deleting the test service @ 06/17/23 12:59:54.652
  STEP: deleting the test headless service @ 06/17/23 12:59:54.674
  STEP: Destroying namespace "dns-7753" for this suite. @ 06/17/23 12:59:54.686
• [7.230 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 06/17/23 12:59:54.693
  Jun 17 12:59:54.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename resourcequota @ 06/17/23 12:59:54.694
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:59:54.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:59:54.714
  STEP: Creating a ResourceQuota @ 06/17/23 12:59:54.717
  STEP: Getting a ResourceQuota @ 06/17/23 12:59:54.721
  STEP: Listing all ResourceQuotas with LabelSelector @ 06/17/23 12:59:54.724
  STEP: Patching the ResourceQuota @ 06/17/23 12:59:54.727
  STEP: Deleting a Collection of ResourceQuotas @ 06/17/23 12:59:54.735
  STEP: Verifying the deleted ResourceQuota @ 06/17/23 12:59:54.743
  Jun 17 12:59:54.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9774" for this suite. @ 06/17/23 12:59:54.749
• [0.062 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 06/17/23 12:59:54.755
  Jun 17 12:59:54.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename svcaccounts @ 06/17/23 12:59:54.756
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:59:54.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:59:54.775
  E0617 12:59:55.258110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:56.259154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 06/17/23 12:59:56.795
  Jun 17 12:59:56.795: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5809 pod-service-account-696a87a3-0306-456a-b9a1-1bd63efe77bc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 06/17/23 12:59:56.927
  Jun 17 12:59:56.927: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5809 pod-service-account-696a87a3-0306-456a-b9a1-1bd63efe77bc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 06/17/23 12:59:57.067
  Jun 17 12:59:57.067: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5809 pod-service-account-696a87a3-0306-456a-b9a1-1bd63efe77bc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Jun 17 12:59:57.190: INFO: Got root ca configmap in namespace "svcaccounts-5809"
  Jun 17 12:59:57.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5809" for this suite. @ 06/17/23 12:59:57.197
• [2.447 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 06/17/23 12:59:57.203
  Jun 17 12:59:57.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename resourcequota @ 06/17/23 12:59:57.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 12:59:57.222
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 12:59:57.225
  STEP: Counting existing ResourceQuota @ 06/17/23 12:59:57.228
  E0617 12:59:57.259423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:58.259821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 12:59:59.260736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:00.261616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:01.262094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 06/17/23 13:00:02.231
  STEP: Ensuring resource quota status is calculated @ 06/17/23 13:00:02.237
  E0617 13:00:02.262641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:03.262961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 06/17/23 13:00:04.241
  STEP: Ensuring ResourceQuota status captures the pod usage @ 06/17/23 13:00:04.256
  E0617 13:00:04.263851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:05.263965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 06/17/23 13:00:06.26
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 06/17/23 13:00:06.263
  E0617 13:00:06.264321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring a pod cannot update its resource requirements @ 06/17/23 13:00:06.265
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 06/17/23 13:00:06.269
  E0617 13:00:07.264449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:08.264719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 06/17/23 13:00:08.273
  STEP: Ensuring resource quota status released the pod usage @ 06/17/23 13:00:08.285
  E0617 13:00:09.264819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:10.264990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:00:10.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8976" for this suite. @ 06/17/23 13:00:10.294
• [13.097 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 06/17/23 13:00:10.301
  Jun 17 13:00:10.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename resourcequota @ 06/17/23 13:00:10.302
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:00:10.319
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:00:10.322
  STEP: Creating a ResourceQuota @ 06/17/23 13:00:10.325
  STEP: Getting a ResourceQuota @ 06/17/23 13:00:10.331
  STEP: Updating a ResourceQuota @ 06/17/23 13:00:10.334
  STEP: Verifying a ResourceQuota was modified @ 06/17/23 13:00:10.338
  STEP: Deleting a ResourceQuota @ 06/17/23 13:00:10.34
  STEP: Verifying the deleted ResourceQuota @ 06/17/23 13:00:10.346
  Jun 17 13:00:10.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5695" for this suite. @ 06/17/23 13:00:10.351
• [0.056 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 06/17/23 13:00:10.358
  Jun 17 13:00:10.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename disruption @ 06/17/23 13:00:10.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:00:10.377
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:00:10.379
  STEP: Waiting for the pdb to be processed @ 06/17/23 13:00:10.386
  E0617 13:00:11.265273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:12.265288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 06/17/23 13:00:12.412
  Jun 17 13:00:12.415: INFO: running pods: 0 < 3
  E0617 13:00:13.265496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:14.265586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:00:14.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-910" for this suite. @ 06/17/23 13:00:14.425
• [4.074 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 06/17/23 13:00:14.433
  Jun 17 13:00:14.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename statefulset @ 06/17/23 13:00:14.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:00:14.451
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:00:14.454
  STEP: Creating service test in namespace statefulset-2867 @ 06/17/23 13:00:14.457
  STEP: Looking for a node to schedule stateful set and pod @ 06/17/23 13:00:14.461
  STEP: Creating pod with conflicting port in namespace statefulset-2867 @ 06/17/23 13:00:14.465
  STEP: Waiting until pod test-pod will start running in namespace statefulset-2867 @ 06/17/23 13:00:14.474
  E0617 13:00:15.269626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:16.270483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-2867 @ 06/17/23 13:00:16.482
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2867 @ 06/17/23 13:00:16.487
  Jun 17 13:00:16.513: INFO: Observed stateful pod in namespace: statefulset-2867, name: ss-0, uid: 50a46f10-19be-492c-8ee4-76469f03a13e, status phase: Pending. Waiting for statefulset controller to delete.
  Jun 17 13:00:16.525: INFO: Observed stateful pod in namespace: statefulset-2867, name: ss-0, uid: 50a46f10-19be-492c-8ee4-76469f03a13e, status phase: Failed. Waiting for statefulset controller to delete.
  Jun 17 13:00:16.547: INFO: Observed stateful pod in namespace: statefulset-2867, name: ss-0, uid: 50a46f10-19be-492c-8ee4-76469f03a13e, status phase: Failed. Waiting for statefulset controller to delete.
  Jun 17 13:00:16.552: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2867
  STEP: Removing pod with conflicting port in namespace statefulset-2867 @ 06/17/23 13:00:16.553
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2867 and will be in running state @ 06/17/23 13:00:16.567
  E0617 13:00:17.271435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:18.271496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:00:18.574: INFO: Deleting all statefulset in ns statefulset-2867
  Jun 17 13:00:18.577: INFO: Scaling statefulset ss to 0
  E0617 13:00:19.271625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:20.271731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:21.272204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:22.272307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:23.272411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:24.272625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:25.272704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:26.273100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:27.273231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:28.273324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:00:28.593: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 17 13:00:28.596: INFO: Deleting statefulset ss
  Jun 17 13:00:28.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2867" for this suite. @ 06/17/23 13:00:28.61
• [14.182 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 06/17/23 13:00:28.617
  Jun 17 13:00:28.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl @ 06/17/23 13:00:28.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:00:28.642
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:00:28.644
  Jun 17 13:00:28.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-531 version'
  Jun 17 13:00:28.704: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Jun 17 13:00:28.704: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.3\", GitCommit:\"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:53:42Z\", GoVersion:\"go1.20.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.3\", GitCommit:\"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", GitTreeState:\"clean\", BuildDate:\"2023-06-15T02:06:40Z\", GoVersion:\"go1.20.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Jun 17 13:00:28.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-531" for this suite. @ 06/17/23 13:00:28.708
• [0.097 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 06/17/23 13:00:28.715
  Jun 17 13:00:28.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename svc-latency @ 06/17/23 13:00:28.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:00:28.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:00:28.738
  Jun 17 13:00:28.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-8334 @ 06/17/23 13:00:28.742
  I0617 13:00:28.748997      19 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8334, replica count: 1
  E0617 13:00:29.273901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0617 13:00:29.800270      19 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0617 13:00:30.274921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0617 13:00:30.801355      19 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 17 13:00:30.911: INFO: Created: latency-svc-w7v2r
  Jun 17 13:00:30.919: INFO: Got endpoints: latency-svc-w7v2r [17.660187ms]
  Jun 17 13:00:30.933: INFO: Created: latency-svc-5m67v
  Jun 17 13:00:30.938: INFO: Created: latency-svc-n5sll
  Jun 17 13:00:30.940: INFO: Got endpoints: latency-svc-5m67v [20.481193ms]
  Jun 17 13:00:30.946: INFO: Got endpoints: latency-svc-n5sll [26.181949ms]
  Jun 17 13:00:30.952: INFO: Created: latency-svc-82nzj
  Jun 17 13:00:30.958: INFO: Created: latency-svc-5mdv5
  Jun 17 13:00:30.965: INFO: Got endpoints: latency-svc-82nzj [45.075382ms]
  Jun 17 13:00:30.966: INFO: Got endpoints: latency-svc-5mdv5 [45.50401ms]
  Jun 17 13:00:30.969: INFO: Created: latency-svc-6kwct
  Jun 17 13:00:30.976: INFO: Created: latency-svc-cvk8p
  Jun 17 13:00:30.978: INFO: Got endpoints: latency-svc-6kwct [57.838978ms]
  Jun 17 13:00:30.983: INFO: Got endpoints: latency-svc-cvk8p [63.070391ms]
  Jun 17 13:00:30.995: INFO: Created: latency-svc-vrbmv
  Jun 17 13:00:31.002: INFO: Got endpoints: latency-svc-vrbmv [82.150031ms]
  Jun 17 13:00:31.011: INFO: Created: latency-svc-2ng7f
  Jun 17 13:00:31.014: INFO: Created: latency-svc-gfxbc
  Jun 17 13:00:31.015: INFO: Got endpoints: latency-svc-2ng7f [94.413117ms]
  Jun 17 13:00:31.019: INFO: Created: latency-svc-gh6dj
  Jun 17 13:00:31.022: INFO: Got endpoints: latency-svc-gfxbc [101.571722ms]
  Jun 17 13:00:31.029: INFO: Got endpoints: latency-svc-gh6dj [108.192207ms]
  Jun 17 13:00:31.035: INFO: Created: latency-svc-lmb2g
  Jun 17 13:00:31.041: INFO: Got endpoints: latency-svc-lmb2g [120.212783ms]
  Jun 17 13:00:31.043: INFO: Created: latency-svc-xdx42
  Jun 17 13:00:31.048: INFO: Got endpoints: latency-svc-xdx42 [126.948404ms]
  Jun 17 13:00:31.061: INFO: Created: latency-svc-tl54c
  Jun 17 13:00:31.078: INFO: Got endpoints: latency-svc-tl54c [156.90179ms]
  Jun 17 13:00:31.081: INFO: Created: latency-svc-4cq6s
  Jun 17 13:00:31.085: INFO: Got endpoints: latency-svc-4cq6s [163.643786ms]
  Jun 17 13:00:31.089: INFO: Created: latency-svc-7jk5s
  Jun 17 13:00:31.093: INFO: Got endpoints: latency-svc-7jk5s [172.15413ms]
  Jun 17 13:00:31.100: INFO: Created: latency-svc-ph2nk
  Jun 17 13:00:31.109: INFO: Got endpoints: latency-svc-ph2nk [168.77324ms]
  Jun 17 13:00:31.176: INFO: Created: latency-svc-chb9l
  Jun 17 13:00:31.176: INFO: Created: latency-svc-dwrhl
  Jun 17 13:00:31.176: INFO: Created: latency-svc-l56xc
  Jun 17 13:00:31.178: INFO: Created: latency-svc-4fc62
  Jun 17 13:00:31.178: INFO: Created: latency-svc-sncv9
  Jun 17 13:00:31.179: INFO: Created: latency-svc-6w8lg
  Jun 17 13:00:31.179: INFO: Created: latency-svc-bn99k
  Jun 17 13:00:31.183: INFO: Created: latency-svc-78h2d
  Jun 17 13:00:31.183: INFO: Created: latency-svc-hcnwq
  Jun 17 13:00:31.183: INFO: Created: latency-svc-9vxnd
  Jun 17 13:00:31.183: INFO: Created: latency-svc-pw4bj
  Jun 17 13:00:31.185: INFO: Created: latency-svc-dfgx7
  Jun 17 13:00:31.192: INFO: Created: latency-svc-6b8wr
  Jun 17 13:00:31.194: INFO: Created: latency-svc-kwmqt
  Jun 17 13:00:31.194: INFO: Got endpoints: latency-svc-dwrhl [179.032175ms]
  Jun 17 13:00:31.195: INFO: Created: latency-svc-hbq7c
  Jun 17 13:00:31.198: INFO: Got endpoints: latency-svc-chb9l [88.672289ms]
  Jun 17 13:00:31.214: INFO: Got endpoints: latency-svc-bn99k [267.83298ms]
  Jun 17 13:00:31.214: INFO: Got endpoints: latency-svc-sncv9 [129.047711ms]
  Jun 17 13:00:31.214: INFO: Got endpoints: latency-svc-6w8lg [165.863011ms]
  Jun 17 13:00:31.216: INFO: Created: latency-svc-547z8
  Jun 17 13:00:31.223: INFO: Created: latency-svc-kfs4z
  Jun 17 13:00:31.229: INFO: Got endpoints: latency-svc-l56xc [251.149311ms]
  Jun 17 13:00:31.230: INFO: Created: latency-svc-2w24d
  Jun 17 13:00:31.233: INFO: Got endpoints: latency-svc-78h2d [250.182327ms]
  Jun 17 13:00:31.233: INFO: Got endpoints: latency-svc-4fc62 [140.05294ms]
  Jun 17 13:00:31.237: INFO: Got endpoints: latency-svc-pw4bj [214.586018ms]
  Jun 17 13:00:31.237: INFO: Got endpoints: latency-svc-9vxnd [195.953113ms]
  Jun 17 13:00:31.238: INFO: Got endpoints: latency-svc-hcnwq [208.441534ms]
  Jun 17 13:00:31.244: INFO: Got endpoints: latency-svc-6b8wr [278.441538ms]
  Jun 17 13:00:31.246: INFO: Created: latency-svc-n5dkj
  Jun 17 13:00:31.252: INFO: Created: latency-svc-sgprt
  Jun 17 13:00:31.256: INFO: Got endpoints: latency-svc-dfgx7 [178.189242ms]
  Jun 17 13:00:31.257: INFO: Got endpoints: latency-svc-hbq7c [291.497972ms]
  Jun 17 13:00:31.261: INFO: Created: latency-svc-f57w4
  Jun 17 13:00:31.261: INFO: Got endpoints: latency-svc-547z8 [67.253289ms]
  Jun 17 13:00:31.262: INFO: Got endpoints: latency-svc-kfs4z [64.157689ms]
  Jun 17 13:00:31.263: INFO: Got endpoints: latency-svc-kwmqt [260.015943ms]
  Jun 17 13:00:31.267: INFO: Got endpoints: latency-svc-2w24d [52.837964ms]
  Jun 17 13:00:31.271: INFO: Created: latency-svc-s6zkx
  Jun 17 13:00:31.272: INFO: Got endpoints: latency-svc-n5dkj [56.681814ms]
  Jun 17 13:00:31.273: INFO: Got endpoints: latency-svc-sgprt [58.18002ms]
  E0617 13:00:31.275824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:00:31.318: INFO: Got endpoints: latency-svc-f57w4 [89.321115ms]
  Jun 17 13:00:31.331: INFO: Created: latency-svc-2s5ql
  Jun 17 13:00:31.335: INFO: Created: latency-svc-q8c5p
  Jun 17 13:00:31.335: INFO: Created: latency-svc-rxwgb
  Jun 17 13:00:31.337: INFO: Created: latency-svc-wkh5l
  Jun 17 13:00:31.337: INFO: Created: latency-svc-prhv4
  Jun 17 13:00:31.337: INFO: Created: latency-svc-g2l7s
  Jun 17 13:00:31.338: INFO: Created: latency-svc-x6vhp
  Jun 17 13:00:31.338: INFO: Created: latency-svc-tvwnb
  Jun 17 13:00:31.339: INFO: Created: latency-svc-2tsfh
  Jun 17 13:00:31.340: INFO: Created: latency-svc-bq46w
  Jun 17 13:00:31.340: INFO: Created: latency-svc-tznw6
  Jun 17 13:00:31.341: INFO: Created: latency-svc-9tzpn
  Jun 17 13:00:31.343: INFO: Created: latency-svc-w4qjs
  Jun 17 13:00:31.344: INFO: Created: latency-svc-s8ltq
  Jun 17 13:00:31.368: INFO: Got endpoints: latency-svc-s6zkx [134.066608ms]
  Jun 17 13:00:31.379: INFO: Created: latency-svc-9dr2p
  Jun 17 13:00:31.418: INFO: Got endpoints: latency-svc-2s5ql [180.78966ms]
  Jun 17 13:00:31.427: INFO: Created: latency-svc-x5zn2
  Jun 17 13:00:31.469: INFO: Got endpoints: latency-svc-rxwgb [151.081292ms]
  Jun 17 13:00:31.481: INFO: Created: latency-svc-dvt9m
  Jun 17 13:00:31.520: INFO: Got endpoints: latency-svc-q8c5p [258.281845ms]
  Jun 17 13:00:31.530: INFO: Created: latency-svc-gkt6m
  Jun 17 13:00:31.567: INFO: Got endpoints: latency-svc-s8ltq [329.49407ms]
  Jun 17 13:00:31.579: INFO: Created: latency-svc-j2qfg
  Jun 17 13:00:31.619: INFO: Got endpoints: latency-svc-bq46w [346.443577ms]
  Jun 17 13:00:31.632: INFO: Created: latency-svc-cmt72
  Jun 17 13:00:31.669: INFO: Got endpoints: latency-svc-9tzpn [396.781238ms]
  Jun 17 13:00:31.680: INFO: Created: latency-svc-xqrr6
  Jun 17 13:00:31.716: INFO: Got endpoints: latency-svc-x6vhp [454.362115ms]
  Jun 17 13:00:31.726: INFO: Created: latency-svc-nlwc9
  Jun 17 13:00:31.767: INFO: Got endpoints: latency-svc-w4qjs [510.634971ms]
  Jun 17 13:00:31.776: INFO: Created: latency-svc-lqv9g
  Jun 17 13:00:31.820: INFO: Got endpoints: latency-svc-tznw6 [576.20528ms]
  Jun 17 13:00:31.830: INFO: Created: latency-svc-rxj8w
  Jun 17 13:00:31.869: INFO: Got endpoints: latency-svc-tvwnb [631.539385ms]
  Jun 17 13:00:31.879: INFO: Created: latency-svc-28q9k
  Jun 17 13:00:31.920: INFO: Got endpoints: latency-svc-2tsfh [652.583201ms]
  Jun 17 13:00:31.931: INFO: Created: latency-svc-s58c5
  Jun 17 13:00:31.968: INFO: Got endpoints: latency-svc-prhv4 [733.639428ms]
  Jun 17 13:00:31.978: INFO: Created: latency-svc-vz9wl
  Jun 17 13:00:32.016: INFO: Got endpoints: latency-svc-wkh5l [758.982376ms]
  Jun 17 13:00:32.027: INFO: Created: latency-svc-6hgsf
  Jun 17 13:00:32.069: INFO: Got endpoints: latency-svc-g2l7s [806.058754ms]
  Jun 17 13:00:32.078: INFO: Created: latency-svc-8mt47
  Jun 17 13:00:32.121: INFO: Got endpoints: latency-svc-9dr2p [753.057443ms]
  Jun 17 13:00:32.131: INFO: Created: latency-svc-n7d7n
  Jun 17 13:00:32.181: INFO: Got endpoints: latency-svc-x5zn2 [762.148657ms]
  Jun 17 13:00:32.192: INFO: Created: latency-svc-8dzq6
  Jun 17 13:00:32.217: INFO: Got endpoints: latency-svc-dvt9m [747.852257ms]
  Jun 17 13:00:32.228: INFO: Created: latency-svc-dz5pz
  Jun 17 13:00:32.267: INFO: Got endpoints: latency-svc-gkt6m [746.591217ms]
  E0617 13:00:32.276177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:00:32.276: INFO: Created: latency-svc-4t5v2
  Jun 17 13:00:32.318: INFO: Got endpoints: latency-svc-j2qfg [750.126489ms]
  Jun 17 13:00:32.327: INFO: Created: latency-svc-5fdgh
  Jun 17 13:00:32.369: INFO: Got endpoints: latency-svc-cmt72 [749.968907ms]
  Jun 17 13:00:32.379: INFO: Created: latency-svc-fjth9
  Jun 17 13:00:32.419: INFO: Got endpoints: latency-svc-xqrr6 [750.110085ms]
  Jun 17 13:00:32.427: INFO: Created: latency-svc-d8hnl
  Jun 17 13:00:32.467: INFO: Got endpoints: latency-svc-nlwc9 [750.655793ms]
  Jun 17 13:00:32.484: INFO: Created: latency-svc-689h6
  Jun 17 13:00:32.534: INFO: Got endpoints: latency-svc-lqv9g [766.673622ms]
  Jun 17 13:00:32.547: INFO: Created: latency-svc-pwfkh
  Jun 17 13:00:32.568: INFO: Got endpoints: latency-svc-rxj8w [748.416311ms]
  Jun 17 13:00:32.577: INFO: Created: latency-svc-2hxfk
  Jun 17 13:00:32.617: INFO: Got endpoints: latency-svc-28q9k [748.254619ms]
  Jun 17 13:00:32.628: INFO: Created: latency-svc-bmzkg
  Jun 17 13:00:32.669: INFO: Got endpoints: latency-svc-s58c5 [748.773284ms]
  Jun 17 13:00:32.677: INFO: Created: latency-svc-tpxjb
  Jun 17 13:00:32.718: INFO: Got endpoints: latency-svc-vz9wl [750.058101ms]
  Jun 17 13:00:32.730: INFO: Created: latency-svc-hvlm2
  Jun 17 13:00:32.768: INFO: Got endpoints: latency-svc-6hgsf [751.830107ms]
  Jun 17 13:00:32.779: INFO: Created: latency-svc-6mw88
  Jun 17 13:00:32.818: INFO: Got endpoints: latency-svc-8mt47 [748.972579ms]
  Jun 17 13:00:32.826: INFO: Created: latency-svc-5vl5m
  Jun 17 13:00:32.869: INFO: Got endpoints: latency-svc-n7d7n [748.104077ms]
  Jun 17 13:00:32.881: INFO: Created: latency-svc-nc8r9
  Jun 17 13:00:32.918: INFO: Got endpoints: latency-svc-8dzq6 [737.424852ms]
  Jun 17 13:00:32.928: INFO: Created: latency-svc-v8rgk
  Jun 17 13:00:32.969: INFO: Got endpoints: latency-svc-dz5pz [751.8887ms]
  Jun 17 13:00:32.979: INFO: Created: latency-svc-4tq28
  Jun 17 13:00:33.018: INFO: Got endpoints: latency-svc-4t5v2 [751.373874ms]
  Jun 17 13:00:33.027: INFO: Created: latency-svc-fxlbt
  Jun 17 13:00:33.067: INFO: Got endpoints: latency-svc-5fdgh [749.260628ms]
  Jun 17 13:00:33.078: INFO: Created: latency-svc-v86kz
  Jun 17 13:00:33.125: INFO: Got endpoints: latency-svc-fjth9 [755.447375ms]
  Jun 17 13:00:33.134: INFO: Created: latency-svc-wl5jp
  Jun 17 13:00:33.170: INFO: Got endpoints: latency-svc-d8hnl [750.894983ms]
  Jun 17 13:00:33.179: INFO: Created: latency-svc-x4f2w
  Jun 17 13:00:33.218: INFO: Got endpoints: latency-svc-689h6 [751.193358ms]
  Jun 17 13:00:33.230: INFO: Created: latency-svc-7wk8q
  Jun 17 13:00:33.269: INFO: Got endpoints: latency-svc-pwfkh [735.010332ms]
  E0617 13:00:33.276459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:00:33.280: INFO: Created: latency-svc-klvpk
  Jun 17 13:00:33.319: INFO: Got endpoints: latency-svc-2hxfk [750.563008ms]
  Jun 17 13:00:33.329: INFO: Created: latency-svc-ffmjs
  Jun 17 13:00:33.369: INFO: Got endpoints: latency-svc-bmzkg [750.923643ms]
  Jun 17 13:00:33.378: INFO: Created: latency-svc-wft87
  Jun 17 13:00:33.419: INFO: Got endpoints: latency-svc-tpxjb [749.908855ms]
  Jun 17 13:00:33.429: INFO: Created: latency-svc-dmq92
  Jun 17 13:00:33.468: INFO: Got endpoints: latency-svc-hvlm2 [750.056005ms]
  Jun 17 13:00:33.477: INFO: Created: latency-svc-5zc27
  Jun 17 13:00:33.520: INFO: Got endpoints: latency-svc-6mw88 [751.473757ms]
  Jun 17 13:00:33.531: INFO: Created: latency-svc-pbdml
  Jun 17 13:00:33.569: INFO: Got endpoints: latency-svc-5vl5m [750.635646ms]
  Jun 17 13:00:33.588: INFO: Created: latency-svc-2fhmc
  Jun 17 13:00:33.618: INFO: Got endpoints: latency-svc-nc8r9 [748.407982ms]
  Jun 17 13:00:33.629: INFO: Created: latency-svc-t4ft2
  Jun 17 13:00:33.669: INFO: Got endpoints: latency-svc-v8rgk [751.150182ms]
  Jun 17 13:00:33.680: INFO: Created: latency-svc-hhdzx
  Jun 17 13:00:33.718: INFO: Got endpoints: latency-svc-4tq28 [749.068312ms]
  Jun 17 13:00:33.729: INFO: Created: latency-svc-7wqwz
  Jun 17 13:00:33.770: INFO: Got endpoints: latency-svc-fxlbt [751.436722ms]
  Jun 17 13:00:33.780: INFO: Created: latency-svc-kpvz7
  Jun 17 13:00:33.819: INFO: Got endpoints: latency-svc-v86kz [750.873209ms]
  Jun 17 13:00:33.830: INFO: Created: latency-svc-tzxh6
  Jun 17 13:00:33.867: INFO: Got endpoints: latency-svc-wl5jp [741.492668ms]
  Jun 17 13:00:33.876: INFO: Created: latency-svc-zkmpf
  Jun 17 13:00:33.918: INFO: Got endpoints: latency-svc-x4f2w [748.09544ms]
  Jun 17 13:00:33.931: INFO: Created: latency-svc-lvkh6
  Jun 17 13:00:33.967: INFO: Got endpoints: latency-svc-7wk8q [748.43364ms]
  Jun 17 13:00:33.976: INFO: Created: latency-svc-7224z
  Jun 17 13:00:34.019: INFO: Got endpoints: latency-svc-klvpk [750.229836ms]
  Jun 17 13:00:34.031: INFO: Created: latency-svc-qk66h
  Jun 17 13:00:34.068: INFO: Got endpoints: latency-svc-ffmjs [748.829989ms]
  Jun 17 13:00:34.078: INFO: Created: latency-svc-lrmkb
  Jun 17 13:00:34.116: INFO: Got endpoints: latency-svc-wft87 [747.171842ms]
  Jun 17 13:00:34.126: INFO: Created: latency-svc-c8b6f
  Jun 17 13:00:34.168: INFO: Got endpoints: latency-svc-dmq92 [749.334343ms]
  Jun 17 13:00:34.177: INFO: Created: latency-svc-pmx5l
  Jun 17 13:00:34.219: INFO: Got endpoints: latency-svc-5zc27 [751.117229ms]
  Jun 17 13:00:34.228: INFO: Created: latency-svc-h96hf
  Jun 17 13:00:34.267: INFO: Got endpoints: latency-svc-pbdml [745.674784ms]
  E0617 13:00:34.277038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:00:34.278: INFO: Created: latency-svc-6twxt
  Jun 17 13:00:34.318: INFO: Got endpoints: latency-svc-2fhmc [749.669317ms]
  Jun 17 13:00:34.330: INFO: Created: latency-svc-sgbgn
  Jun 17 13:00:34.373: INFO: Got endpoints: latency-svc-t4ft2 [754.875331ms]
  Jun 17 13:00:34.385: INFO: Created: latency-svc-slrcm
  Jun 17 13:00:34.469: INFO: Got endpoints: latency-svc-hhdzx [799.282009ms]
  Jun 17 13:00:34.480: INFO: Created: latency-svc-znnvg
  Jun 17 13:00:34.516: INFO: Got endpoints: latency-svc-7wqwz [797.864237ms]
  Jun 17 13:00:34.527: INFO: Created: latency-svc-fjczr
  Jun 17 13:00:34.568: INFO: Got endpoints: latency-svc-kpvz7 [797.925322ms]
  Jun 17 13:00:34.577: INFO: Created: latency-svc-hczjg
  Jun 17 13:00:34.618: INFO: Got endpoints: latency-svc-tzxh6 [798.365658ms]
  Jun 17 13:00:34.630: INFO: Created: latency-svc-tg8hb
  Jun 17 13:00:34.666: INFO: Got endpoints: latency-svc-zkmpf [799.79212ms]
  Jun 17 13:00:34.677: INFO: Created: latency-svc-nmjq6
  Jun 17 13:00:34.716: INFO: Got endpoints: latency-svc-lvkh6 [798.418108ms]
  Jun 17 13:00:34.725: INFO: Created: latency-svc-lxj2j
  Jun 17 13:00:34.771: INFO: Got endpoints: latency-svc-7224z [803.678086ms]
  Jun 17 13:00:34.782: INFO: Created: latency-svc-zgvjz
  Jun 17 13:00:34.820: INFO: Got endpoints: latency-svc-qk66h [799.789414ms]
  Jun 17 13:00:34.828: INFO: Created: latency-svc-9tf4v
  Jun 17 13:00:34.870: INFO: Got endpoints: latency-svc-lrmkb [801.744883ms]
  Jun 17 13:00:34.881: INFO: Created: latency-svc-9rsls
  Jun 17 13:00:34.919: INFO: Got endpoints: latency-svc-c8b6f [802.851121ms]
  Jun 17 13:00:34.934: INFO: Created: latency-svc-c2w8n
  Jun 17 13:00:34.969: INFO: Got endpoints: latency-svc-pmx5l [800.180535ms]
  Jun 17 13:00:34.977: INFO: Created: latency-svc-hdlgd
  Jun 17 13:00:35.016: INFO: Got endpoints: latency-svc-h96hf [796.876974ms]
  Jun 17 13:00:35.027: INFO: Created: latency-svc-xnmd6
  Jun 17 13:00:35.070: INFO: Got endpoints: latency-svc-6twxt [803.045539ms]
  Jun 17 13:00:35.079: INFO: Created: latency-svc-dkkpk
  Jun 17 13:00:35.118: INFO: Got endpoints: latency-svc-sgbgn [799.66017ms]
  Jun 17 13:00:35.129: INFO: Created: latency-svc-4lrcv
  Jun 17 13:00:35.166: INFO: Got endpoints: latency-svc-slrcm [792.399118ms]
  Jun 17 13:00:35.176: INFO: Created: latency-svc-ktf5p
  Jun 17 13:00:35.219: INFO: Got endpoints: latency-svc-znnvg [749.919183ms]
  Jun 17 13:00:35.230: INFO: Created: latency-svc-2gnfq
  Jun 17 13:00:35.270: INFO: Got endpoints: latency-svc-fjczr [753.04252ms]
  E0617 13:00:35.278070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:00:35.281: INFO: Created: latency-svc-s8rfn
  Jun 17 13:00:35.317: INFO: Got endpoints: latency-svc-hczjg [749.163149ms]
  Jun 17 13:00:35.329: INFO: Created: latency-svc-jxdd2
  Jun 17 13:00:35.369: INFO: Got endpoints: latency-svc-tg8hb [751.39586ms]
  Jun 17 13:00:35.379: INFO: Created: latency-svc-fzt98
  Jun 17 13:00:35.416: INFO: Got endpoints: latency-svc-nmjq6 [749.915254ms]
  Jun 17 13:00:35.427: INFO: Created: latency-svc-n7cjn
  Jun 17 13:00:35.467: INFO: Got endpoints: latency-svc-lxj2j [750.306908ms]
  Jun 17 13:00:35.478: INFO: Created: latency-svc-tthfs
  Jun 17 13:00:35.519: INFO: Got endpoints: latency-svc-zgvjz [747.55029ms]
  Jun 17 13:00:35.528: INFO: Created: latency-svc-h4tbx
  Jun 17 13:00:35.567: INFO: Got endpoints: latency-svc-9tf4v [747.754499ms]
  Jun 17 13:00:35.578: INFO: Created: latency-svc-8jtm8
  Jun 17 13:00:35.617: INFO: Got endpoints: latency-svc-9rsls [747.314431ms]
  Jun 17 13:00:35.626: INFO: Created: latency-svc-wcmpt
  Jun 17 13:00:35.670: INFO: Got endpoints: latency-svc-c2w8n [750.488676ms]
  Jun 17 13:00:35.679: INFO: Created: latency-svc-ql7zw
  Jun 17 13:00:35.718: INFO: Got endpoints: latency-svc-hdlgd [749.264912ms]
  Jun 17 13:00:35.727: INFO: Created: latency-svc-s6zxv
  Jun 17 13:00:35.768: INFO: Got endpoints: latency-svc-xnmd6 [751.597493ms]
  Jun 17 13:00:35.776: INFO: Created: latency-svc-hl75q
  Jun 17 13:00:35.818: INFO: Got endpoints: latency-svc-dkkpk [748.155925ms]
  Jun 17 13:00:35.827: INFO: Created: latency-svc-r99gd
  Jun 17 13:00:35.867: INFO: Got endpoints: latency-svc-4lrcv [748.983559ms]
  Jun 17 13:00:35.878: INFO: Created: latency-svc-97446
  Jun 17 13:00:35.918: INFO: Got endpoints: latency-svc-ktf5p [751.473369ms]
  Jun 17 13:00:35.929: INFO: Created: latency-svc-lg5g9
  Jun 17 13:00:35.968: INFO: Got endpoints: latency-svc-2gnfq [748.453505ms]
  Jun 17 13:00:35.982: INFO: Created: latency-svc-7l6ld
  Jun 17 13:00:36.018: INFO: Got endpoints: latency-svc-s8rfn [747.857064ms]
  Jun 17 13:00:36.028: INFO: Created: latency-svc-m56x9
  Jun 17 13:00:36.068: INFO: Got endpoints: latency-svc-jxdd2 [750.414435ms]
  Jun 17 13:00:36.078: INFO: Created: latency-svc-spfvs
  Jun 17 13:00:36.116: INFO: Got endpoints: latency-svc-fzt98 [747.384747ms]
  Jun 17 13:00:36.127: INFO: Created: latency-svc-lmhgc
  Jun 17 13:00:36.168: INFO: Got endpoints: latency-svc-n7cjn [751.220431ms]
  Jun 17 13:00:36.177: INFO: Created: latency-svc-b5grt
  Jun 17 13:00:36.218: INFO: Got endpoints: latency-svc-tthfs [751.112928ms]
  Jun 17 13:00:36.227: INFO: Created: latency-svc-c92pg
  Jun 17 13:00:36.269: INFO: Got endpoints: latency-svc-h4tbx [749.457509ms]
  E0617 13:00:36.278132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:00:36.280: INFO: Created: latency-svc-gqzkb
  Jun 17 13:00:36.318: INFO: Got endpoints: latency-svc-8jtm8 [750.787883ms]
  Jun 17 13:00:36.327: INFO: Created: latency-svc-7txsp
  Jun 17 13:00:36.368: INFO: Got endpoints: latency-svc-wcmpt [750.800824ms]
  Jun 17 13:00:36.380: INFO: Created: latency-svc-vxxvc
  Jun 17 13:00:36.417: INFO: Got endpoints: latency-svc-ql7zw [746.983856ms]
  Jun 17 13:00:36.430: INFO: Created: latency-svc-9qltc
  Jun 17 13:00:36.467: INFO: Got endpoints: latency-svc-s6zxv [748.680794ms]
  Jun 17 13:00:36.475: INFO: Created: latency-svc-8mktr
  Jun 17 13:00:36.519: INFO: Got endpoints: latency-svc-hl75q [750.681718ms]
  Jun 17 13:00:36.528: INFO: Created: latency-svc-hb6xl
  Jun 17 13:00:36.568: INFO: Got endpoints: latency-svc-r99gd [749.597724ms]
  Jun 17 13:00:36.577: INFO: Created: latency-svc-qwv6n
  Jun 17 13:00:36.618: INFO: Got endpoints: latency-svc-97446 [749.993713ms]
  Jun 17 13:00:36.628: INFO: Created: latency-svc-88fzg
  Jun 17 13:00:36.668: INFO: Got endpoints: latency-svc-lg5g9 [749.273342ms]
  Jun 17 13:00:36.678: INFO: Created: latency-svc-hkrj4
  Jun 17 13:00:36.717: INFO: Got endpoints: latency-svc-7l6ld [749.023706ms]
  Jun 17 13:00:36.725: INFO: Created: latency-svc-whshr
  Jun 17 13:00:36.768: INFO: Got endpoints: latency-svc-m56x9 [750.126957ms]
  Jun 17 13:00:36.778: INFO: Created: latency-svc-frq7h
  Jun 17 13:00:36.819: INFO: Got endpoints: latency-svc-spfvs [751.029425ms]
  Jun 17 13:00:36.827: INFO: Created: latency-svc-xrlrh
  Jun 17 13:00:36.867: INFO: Got endpoints: latency-svc-lmhgc [750.679714ms]
  Jun 17 13:00:36.877: INFO: Created: latency-svc-29bv8
  Jun 17 13:00:36.917: INFO: Got endpoints: latency-svc-b5grt [749.688643ms]
  Jun 17 13:00:36.930: INFO: Created: latency-svc-4zw78
  Jun 17 13:00:36.968: INFO: Got endpoints: latency-svc-c92pg [749.625639ms]
  Jun 17 13:00:36.976: INFO: Created: latency-svc-ntvxw
  Jun 17 13:00:37.018: INFO: Got endpoints: latency-svc-gqzkb [748.932114ms]
  Jun 17 13:00:37.026: INFO: Created: latency-svc-2msbf
  Jun 17 13:00:37.066: INFO: Got endpoints: latency-svc-7txsp [747.586019ms]
  Jun 17 13:00:37.076: INFO: Created: latency-svc-69wrr
  Jun 17 13:00:37.118: INFO: Got endpoints: latency-svc-vxxvc [750.099212ms]
  Jun 17 13:00:37.128: INFO: Created: latency-svc-kw4zf
  Jun 17 13:00:37.169: INFO: Got endpoints: latency-svc-9qltc [751.359452ms]
  Jun 17 13:00:37.177: INFO: Created: latency-svc-dd9x2
  Jun 17 13:00:37.218: INFO: Got endpoints: latency-svc-8mktr [750.634413ms]
  Jun 17 13:00:37.228: INFO: Created: latency-svc-lqmxw
  Jun 17 13:00:37.269: INFO: Got endpoints: latency-svc-hb6xl [749.937564ms]
  E0617 13:00:37.278417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:00:37.280: INFO: Created: latency-svc-8lnnm
  Jun 17 13:00:37.318: INFO: Got endpoints: latency-svc-qwv6n [749.769962ms]
  Jun 17 13:00:37.328: INFO: Created: latency-svc-d29kx
  Jun 17 13:00:37.366: INFO: Got endpoints: latency-svc-88fzg [748.716384ms]
  Jun 17 13:00:37.375: INFO: Created: latency-svc-vzkrr
  Jun 17 13:00:37.419: INFO: Got endpoints: latency-svc-hkrj4 [750.719051ms]
  Jun 17 13:00:37.427: INFO: Created: latency-svc-462pt
  Jun 17 13:00:37.468: INFO: Got endpoints: latency-svc-whshr [751.228942ms]
  Jun 17 13:00:37.478: INFO: Created: latency-svc-fff44
  Jun 17 13:00:37.518: INFO: Got endpoints: latency-svc-frq7h [750.386652ms]
  Jun 17 13:00:37.528: INFO: Created: latency-svc-tpkhf
  Jun 17 13:00:37.568: INFO: Got endpoints: latency-svc-xrlrh [748.660976ms]
  Jun 17 13:00:37.578: INFO: Created: latency-svc-fs26k
  Jun 17 13:00:37.617: INFO: Got endpoints: latency-svc-29bv8 [748.937583ms]
  Jun 17 13:00:37.633: INFO: Created: latency-svc-rr292
  Jun 17 13:00:37.668: INFO: Got endpoints: latency-svc-4zw78 [750.931586ms]
  Jun 17 13:00:37.679: INFO: Created: latency-svc-5jd8t
  Jun 17 13:00:37.719: INFO: Got endpoints: latency-svc-ntvxw [750.414344ms]
  Jun 17 13:00:37.728: INFO: Created: latency-svc-fdwtx
  Jun 17 13:00:37.767: INFO: Got endpoints: latency-svc-2msbf [749.127189ms]
  Jun 17 13:00:37.777: INFO: Created: latency-svc-9krll
  Jun 17 13:00:37.818: INFO: Got endpoints: latency-svc-69wrr [751.265238ms]
  Jun 17 13:00:37.828: INFO: Created: latency-svc-m9qr5
  Jun 17 13:00:37.868: INFO: Got endpoints: latency-svc-kw4zf [749.959948ms]
  Jun 17 13:00:37.878: INFO: Created: latency-svc-cqkwt
  Jun 17 13:00:37.917: INFO: Got endpoints: latency-svc-dd9x2 [747.713159ms]
  Jun 17 13:00:37.933: INFO: Created: latency-svc-v9bb2
  Jun 17 13:00:37.970: INFO: Got endpoints: latency-svc-lqmxw [751.839106ms]
  Jun 17 13:00:37.979: INFO: Created: latency-svc-gmr6z
  Jun 17 13:00:38.019: INFO: Got endpoints: latency-svc-8lnnm [750.201949ms]
  Jun 17 13:00:38.028: INFO: Created: latency-svc-58rr4
  Jun 17 13:00:38.068: INFO: Got endpoints: latency-svc-d29kx [749.287833ms]
  Jun 17 13:00:38.077: INFO: Created: latency-svc-vh2qx
  Jun 17 13:00:38.116: INFO: Got endpoints: latency-svc-vzkrr [750.031651ms]
  Jun 17 13:00:38.126: INFO: Created: latency-svc-rjnf2
  Jun 17 13:00:38.168: INFO: Got endpoints: latency-svc-462pt [749.736817ms]
  Jun 17 13:00:38.177: INFO: Created: latency-svc-ztsg7
  Jun 17 13:00:38.217: INFO: Got endpoints: latency-svc-fff44 [749.093366ms]
  Jun 17 13:00:38.227: INFO: Created: latency-svc-45xcd
  Jun 17 13:00:38.268: INFO: Got endpoints: latency-svc-tpkhf [749.876145ms]
  E0617 13:00:38.278665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:00:38.279: INFO: Created: latency-svc-qz6wv
  Jun 17 13:00:38.319: INFO: Got endpoints: latency-svc-fs26k [750.441441ms]
  Jun 17 13:00:38.329: INFO: Created: latency-svc-xh45r
  Jun 17 13:00:38.367: INFO: Got endpoints: latency-svc-rr292 [750.19071ms]
  Jun 17 13:00:38.378: INFO: Created: latency-svc-r4g4k
  Jun 17 13:00:38.417: INFO: Got endpoints: latency-svc-5jd8t [748.365997ms]
  Jun 17 13:00:38.428: INFO: Created: latency-svc-ljlzq
  Jun 17 13:00:38.470: INFO: Got endpoints: latency-svc-fdwtx [750.513104ms]
  Jun 17 13:00:38.479: INFO: Created: latency-svc-rjmwg
  Jun 17 13:00:38.518: INFO: Got endpoints: latency-svc-9krll [750.957281ms]
  Jun 17 13:00:38.527: INFO: Created: latency-svc-ghwd6
  Jun 17 13:00:38.567: INFO: Got endpoints: latency-svc-m9qr5 [749.068185ms]
  Jun 17 13:00:38.581: INFO: Created: latency-svc-bnfhq
  Jun 17 13:00:38.618: INFO: Got endpoints: latency-svc-cqkwt [749.19856ms]
  Jun 17 13:00:38.626: INFO: Created: latency-svc-hb946
  Jun 17 13:00:38.669: INFO: Got endpoints: latency-svc-v9bb2 [751.790208ms]
  Jun 17 13:00:38.678: INFO: Created: latency-svc-rzlc2
  Jun 17 13:00:38.719: INFO: Got endpoints: latency-svc-gmr6z [748.778151ms]
  Jun 17 13:00:38.730: INFO: Created: latency-svc-pgwgs
  Jun 17 13:00:38.767: INFO: Got endpoints: latency-svc-58rr4 [747.566633ms]
  Jun 17 13:00:38.777: INFO: Created: latency-svc-lddz7
  Jun 17 13:00:38.817: INFO: Got endpoints: latency-svc-vh2qx [748.481376ms]
  Jun 17 13:00:38.869: INFO: Got endpoints: latency-svc-rjnf2 [752.275618ms]
  Jun 17 13:00:38.918: INFO: Got endpoints: latency-svc-ztsg7 [748.463612ms]
  Jun 17 13:00:38.971: INFO: Got endpoints: latency-svc-45xcd [753.342462ms]
  Jun 17 13:00:39.017: INFO: Got endpoints: latency-svc-qz6wv [748.194565ms]
  Jun 17 13:00:39.069: INFO: Got endpoints: latency-svc-xh45r [749.93713ms]
  Jun 17 13:00:39.117: INFO: Got endpoints: latency-svc-r4g4k [750.053837ms]
  Jun 17 13:00:39.168: INFO: Got endpoints: latency-svc-ljlzq [750.994188ms]
  Jun 17 13:00:39.219: INFO: Got endpoints: latency-svc-rjmwg [748.905543ms]
  Jun 17 13:00:39.267: INFO: Got endpoints: latency-svc-ghwd6 [748.162316ms]
  E0617 13:00:39.279179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:00:39.318: INFO: Got endpoints: latency-svc-bnfhq [750.965479ms]
  Jun 17 13:00:39.369: INFO: Got endpoints: latency-svc-hb946 [750.536905ms]
  Jun 17 13:00:39.419: INFO: Got endpoints: latency-svc-rzlc2 [749.68172ms]
  Jun 17 13:00:39.467: INFO: Got endpoints: latency-svc-pgwgs [747.747789ms]
  Jun 17 13:00:39.517: INFO: Got endpoints: latency-svc-lddz7 [750.073253ms]
  Jun 17 13:00:39.517: INFO: Latencies: [20.481193ms 26.181949ms 45.075382ms 45.50401ms 52.837964ms 56.681814ms 57.838978ms 58.18002ms 63.070391ms 64.157689ms 67.253289ms 82.150031ms 88.672289ms 89.321115ms 94.413117ms 101.571722ms 108.192207ms 120.212783ms 126.948404ms 129.047711ms 134.066608ms 140.05294ms 151.081292ms 156.90179ms 163.643786ms 165.863011ms 168.77324ms 172.15413ms 178.189242ms 179.032175ms 180.78966ms 195.953113ms 208.441534ms 214.586018ms 250.182327ms 251.149311ms 258.281845ms 260.015943ms 267.83298ms 278.441538ms 291.497972ms 329.49407ms 346.443577ms 396.781238ms 454.362115ms 510.634971ms 576.20528ms 631.539385ms 652.583201ms 733.639428ms 735.010332ms 737.424852ms 741.492668ms 745.674784ms 746.591217ms 746.983856ms 747.171842ms 747.314431ms 747.384747ms 747.55029ms 747.566633ms 747.586019ms 747.713159ms 747.747789ms 747.754499ms 747.852257ms 747.857064ms 748.09544ms 748.104077ms 748.155925ms 748.162316ms 748.194565ms 748.254619ms 748.365997ms 748.407982ms 748.416311ms 748.43364ms 748.453505ms 748.463612ms 748.481376ms 748.660976ms 748.680794ms 748.716384ms 748.773284ms 748.778151ms 748.829989ms 748.905543ms 748.932114ms 748.937583ms 748.972579ms 748.983559ms 749.023706ms 749.068185ms 749.068312ms 749.093366ms 749.127189ms 749.163149ms 749.19856ms 749.260628ms 749.264912ms 749.273342ms 749.287833ms 749.334343ms 749.457509ms 749.597724ms 749.625639ms 749.669317ms 749.68172ms 749.688643ms 749.736817ms 749.769962ms 749.876145ms 749.908855ms 749.915254ms 749.919183ms 749.93713ms 749.937564ms 749.959948ms 749.968907ms 749.993713ms 750.031651ms 750.053837ms 750.056005ms 750.058101ms 750.073253ms 750.099212ms 750.110085ms 750.126489ms 750.126957ms 750.19071ms 750.201949ms 750.229836ms 750.306908ms 750.386652ms 750.414344ms 750.414435ms 750.441441ms 750.488676ms 750.513104ms 750.536905ms 750.563008ms 750.634413ms 750.635646ms 750.655793ms 750.679714ms 750.681718ms 750.719051ms 750.787883ms 750.800824ms 750.873209ms 750.894983ms 750.923643ms 750.931586ms 750.957281ms 750.965479ms 750.994188ms 751.029425ms 751.112928ms 751.117229ms 751.150182ms 751.193358ms 751.220431ms 751.228942ms 751.265238ms 751.359452ms 751.373874ms 751.39586ms 751.436722ms 751.473369ms 751.473757ms 751.597493ms 751.790208ms 751.830107ms 751.839106ms 751.8887ms 752.275618ms 753.04252ms 753.057443ms 753.342462ms 754.875331ms 755.447375ms 758.982376ms 762.148657ms 766.673622ms 792.399118ms 796.876974ms 797.864237ms 797.925322ms 798.365658ms 798.418108ms 799.282009ms 799.66017ms 799.789414ms 799.79212ms 800.180535ms 801.744883ms 802.851121ms 803.045539ms 803.678086ms 806.058754ms]
  Jun 17 13:00:39.517: INFO: 50 %ile: 749.273342ms
  Jun 17 13:00:39.517: INFO: 90 %ile: 755.447375ms
  Jun 17 13:00:39.518: INFO: 99 %ile: 803.678086ms
  Jun 17 13:00:39.518: INFO: Total sample count: 200
  Jun 17 13:00:39.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-8334" for this suite. @ 06/17/23 13:00:39.522
• [10.813 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 06/17/23 13:00:39.535
  Jun 17 13:00:39.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename dns @ 06/17/23 13:00:39.536
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:00:39.554
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:00:39.557
  STEP: Creating a test headless service @ 06/17/23 13:00:39.56
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1493 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1493;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1493 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1493;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1493.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1493.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1493.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1493.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1493.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1493.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1493.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1493.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1493.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1493.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1493.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1493.svc;check="$$(dig +notcp +noall +answer +search 189.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.189_udp@PTR;check="$$(dig +tcp +noall +answer +search 189.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.189_tcp@PTR;sleep 1; done
   @ 06/17/23 13:00:39.586
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1493 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1493;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1493 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1493;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1493.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1493.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1493.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1493.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1493.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1493.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1493.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1493.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1493.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1493.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1493.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1493.svc;check="$$(dig +notcp +noall +answer +search 189.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.189_udp@PTR;check="$$(dig +tcp +noall +answer +search 189.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.189_tcp@PTR;sleep 1; done
   @ 06/17/23 13:00:39.587
  STEP: creating a pod to probe DNS @ 06/17/23 13:00:39.587
  STEP: submitting the pod to kubernetes @ 06/17/23 13:00:39.587
  E0617 13:00:40.279421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:41.279898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/17/23 13:00:41.604
  STEP: looking for the results for each expected name from probers @ 06/17/23 13:00:41.609
  Jun 17 13:00:41.613: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:41.617: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:41.621: INFO: Unable to read wheezy_udp@dns-test-service.dns-1493 from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:41.626: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1493 from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:41.630: INFO: Unable to read wheezy_udp@dns-test-service.dns-1493.svc from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:41.633: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1493.svc from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:41.636: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1493.svc from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:41.640: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1493.svc from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:41.655: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:41.659: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:41.662: INFO: Unable to read jessie_udp@dns-test-service.dns-1493 from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:41.665: INFO: Unable to read jessie_tcp@dns-test-service.dns-1493 from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:41.668: INFO: Unable to read jessie_udp@dns-test-service.dns-1493.svc from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:41.671: INFO: Unable to read jessie_tcp@dns-test-service.dns-1493.svc from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:41.675: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1493.svc from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:41.678: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1493.svc from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:41.691: INFO: Lookups using dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1493 wheezy_tcp@dns-test-service.dns-1493 wheezy_udp@dns-test-service.dns-1493.svc wheezy_tcp@dns-test-service.dns-1493.svc wheezy_udp@_http._tcp.dns-test-service.dns-1493.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1493.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1493 jessie_tcp@dns-test-service.dns-1493 jessie_udp@dns-test-service.dns-1493.svc jessie_tcp@dns-test-service.dns-1493.svc jessie_udp@_http._tcp.dns-test-service.dns-1493.svc jessie_tcp@_http._tcp.dns-test-service.dns-1493.svc]

  E0617 13:00:42.279940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:43.280815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:44.281324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:45.281353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:46.282365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:00:46.724: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1493.svc from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:46.727: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1493.svc from pod dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11: the server could not find the requested resource (get pods dns-test-57d77dd4-b723-4288-93bc-216aefde5c11)
  Jun 17 13:00:46.792: INFO: Lookups using dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-1493.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1493.svc]

  E0617 13:00:47.283243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:48.283585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:49.283680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:50.283748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:51.284453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:00:51.771: INFO: DNS probes using dns-1493/dns-test-57d77dd4-b723-4288-93bc-216aefde5c11 succeeded

  Jun 17 13:00:51.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/17/23 13:00:51.775
  STEP: deleting the test service @ 06/17/23 13:00:51.789
  STEP: deleting the test headless service @ 06/17/23 13:00:51.813
  STEP: Destroying namespace "dns-1493" for this suite. @ 06/17/23 13:00:51.827
• [12.299 seconds]
------------------------------
SS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 06/17/23 13:00:51.835
  Jun 17 13:00:51.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename conformance-tests @ 06/17/23 13:00:51.836
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:00:51.856
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:00:51.867
  STEP: Getting node addresses @ 06/17/23 13:00:51.876
  Jun 17 13:00:51.876: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Jun 17 13:00:51.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-4810" for this suite. @ 06/17/23 13:00:51.888
• [0.061 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 06/17/23 13:00:51.898
  Jun 17 13:00:51.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename downward-api @ 06/17/23 13:00:51.899
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:00:51.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:00:51.922
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 13:00:51.926
  E0617 13:00:52.285082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:53.285271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:54.285377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:55.285442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:00:55.951
  Jun 17 13:00:55.954: INFO: Trying to get logs from node ip-172-31-25-17 pod downwardapi-volume-8de9d433-3c5a-43bf-a7cf-9b407f92ecb9 container client-container: <nil>
  STEP: delete the pod @ 06/17/23 13:00:55.965
  Jun 17 13:00:55.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4895" for this suite. @ 06/17/23 13:00:55.984
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 06/17/23 13:00:55.995
  Jun 17 13:00:55.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename init-container @ 06/17/23 13:00:55.996
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:00:56.016
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:00:56.018
  STEP: creating the pod @ 06/17/23 13:00:56.021
  Jun 17 13:00:56.021: INFO: PodSpec: initContainers in spec.initContainers
  E0617 13:00:56.286066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:57.286288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:58.287365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:00:59.287431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:00:59.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6300" for this suite. @ 06/17/23 13:00:59.522
• [3.534 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 06/17/23 13:00:59.53
  Jun 17 13:00:59.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 13:00:59.531
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:00:59.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:00:59.55
  STEP: Creating configMap with name projected-configmap-test-volume-map-feca1f33-1bb4-416b-84ea-3cb09f4af517 @ 06/17/23 13:00:59.553
  STEP: Creating a pod to test consume configMaps @ 06/17/23 13:00:59.558
  E0617 13:01:00.288517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:01.288535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:02.289457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:03.289690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:01:03.576
  Jun 17 13:01:03.580: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-projected-configmaps-80d88b2c-68f0-48c3-8971-c8d097ff4aa3 container agnhost-container: <nil>
  STEP: delete the pod @ 06/17/23 13:01:03.597
  Jun 17 13:01:03.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-424" for this suite. @ 06/17/23 13:01:03.616
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 06/17/23 13:01:03.625
  Jun 17 13:01:03.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename endpointslice @ 06/17/23 13:01:03.626
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:01:03.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:01:03.652
  Jun 17 13:01:03.664: INFO: Endpoints addresses: [172.31.38.196 172.31.70.122] , ports: [6443]
  Jun 17 13:01:03.664: INFO: EndpointSlices addresses: [172.31.38.196 172.31.70.122] , ports: [6443]
  Jun 17 13:01:03.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-3577" for this suite. @ 06/17/23 13:01:03.667
• [0.048 seconds]
------------------------------
SS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 06/17/23 13:01:03.673
  Jun 17 13:01:03.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename disruption @ 06/17/23 13:01:03.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:01:03.692
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:01:03.695
  STEP: Creating a pdb that targets all three pods in a test replica set @ 06/17/23 13:01:03.698
  STEP: Waiting for the pdb to be processed @ 06/17/23 13:01:03.702
  E0617 13:01:04.289791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:05.289892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 06/17/23 13:01:05.715
  STEP: Waiting for all pods to be running @ 06/17/23 13:01:05.715
  Jun 17 13:01:05.718: INFO: pods: 0 < 3
  E0617 13:01:06.290552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:07.290856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 06/17/23 13:01:07.722
  STEP: Updating the pdb to allow a pod to be evicted @ 06/17/23 13:01:07.731
  STEP: Waiting for the pdb to be processed @ 06/17/23 13:01:07.738
  E0617 13:01:08.290918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:09.291910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 06/17/23 13:01:09.745
  STEP: Waiting for all pods to be running @ 06/17/23 13:01:09.745
  STEP: Waiting for the pdb to observed all healthy pods @ 06/17/23 13:01:09.749
  STEP: Patching the pdb to disallow a pod to be evicted @ 06/17/23 13:01:09.773
  STEP: Waiting for the pdb to be processed @ 06/17/23 13:01:09.798
  E0617 13:01:10.292517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:11.292596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 06/17/23 13:01:11.805
  STEP: locating a running pod @ 06/17/23 13:01:11.808
  STEP: Deleting the pdb to allow a pod to be evicted @ 06/17/23 13:01:11.817
  STEP: Waiting for the pdb to be deleted @ 06/17/23 13:01:11.823
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 06/17/23 13:01:11.825
  STEP: Waiting for all pods to be running @ 06/17/23 13:01:11.825
  Jun 17 13:01:11.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-205" for this suite. @ 06/17/23 13:01:11.847
• [8.182 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 06/17/23 13:01:11.856
  Jun 17 13:01:11.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename daemonsets @ 06/17/23 13:01:11.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:01:11.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:01:11.88
  STEP: Creating a simple DaemonSet "daemon-set" @ 06/17/23 13:01:11.901
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/17/23 13:01:11.906
  Jun 17 13:01:11.909: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:01:11.909: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:01:11.912: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 13:01:11.912: INFO: Node ip-172-31-25-17 is running 0 daemon pod, expected 1
  E0617 13:01:12.293181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:01:12.916: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:01:12.916: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:01:12.920: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 17 13:01:12.920: INFO: Node ip-172-31-25-17 is running 0 daemon pod, expected 1
  E0617 13:01:13.294212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:01:13.917: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:01:13.917: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:01:13.920: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 17 13:01:13.920: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 06/17/23 13:01:13.923
  Jun 17 13:01:13.937: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:01:13.938: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:01:13.941: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 17 13:01:13.941: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 06/17/23 13:01:13.942
  E0617 13:01:14.294974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting DaemonSet "daemon-set" @ 06/17/23 13:01:14.951
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4539, will wait for the garbage collector to delete the pods @ 06/17/23 13:01:14.951
  Jun 17 13:01:15.012: INFO: Deleting DaemonSet.extensions daemon-set took: 6.320026ms
  Jun 17 13:01:15.112: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.177774ms
  E0617 13:01:15.295741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:16.295910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:01:16.915: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 13:01:16.915: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 17 13:01:16.918: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26649"},"items":null}

  Jun 17 13:01:16.921: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26649"},"items":null}

  Jun 17 13:01:16.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4539" for this suite. @ 06/17/23 13:01:16.936
• [5.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 06/17/23 13:01:16.946
  Jun 17 13:01:16.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/17/23 13:01:16.947
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:01:16.969
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:01:16.973
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 06/17/23 13:01:16.98
  Jun 17 13:01:16.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  E0617 13:01:17.296708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:18.297491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:01:18.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  E0617 13:01:19.298620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:20.299597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:21.300588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:22.301084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:23.302051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:01:23.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7920" for this suite. @ 06/17/23 13:01:23.854
• [6.917 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 06/17/23 13:01:23.863
  Jun 17 13:01:23.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 13:01:23.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:01:23.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:01:23.886
  STEP: Setting up server cert @ 06/17/23 13:01:23.909
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 13:01:24.155
  STEP: Deploying the webhook pod @ 06/17/23 13:01:24.164
  STEP: Wait for the deployment to be ready @ 06/17/23 13:01:24.175
  Jun 17 13:01:24.183: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0617 13:01:24.303123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:25.303920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/17/23 13:01:26.193
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 13:01:26.202
  E0617 13:01:26.304909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:01:27.202: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 06/17/23 13:01:27.206
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/17/23 13:01:27.221
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 06/17/23 13:01:27.229
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/17/23 13:01:27.238
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 06/17/23 13:01:27.249
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/17/23 13:01:27.256
  Jun 17 13:01:27.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7231" for this suite. @ 06/17/23 13:01:27.304
  E0617 13:01:27.305069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-markers-5296" for this suite. @ 06/17/23 13:01:27.309
• [3.452 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 06/17/23 13:01:27.318
  Jun 17 13:01:27.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 13:01:27.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:01:27.337
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:01:27.34
  STEP: Creating projection with secret that has name projected-secret-test-254b6a3c-3cea-4923-b873-2e301599dba8 @ 06/17/23 13:01:27.344
  STEP: Creating a pod to test consume secrets @ 06/17/23 13:01:27.348
  E0617 13:01:28.305156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:29.305225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:30.305345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:31.305604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:01:31.372
  Jun 17 13:01:31.375: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-projected-secrets-f1aa376f-a25d-49c5-b061-15b48e20586a container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/17/23 13:01:31.381
  Jun 17 13:01:31.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4719" for this suite. @ 06/17/23 13:01:31.401
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 06/17/23 13:01:31.409
  Jun 17 13:01:31.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename podtemplate @ 06/17/23 13:01:31.41
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:01:31.429
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:01:31.432
  STEP: Create set of pod templates @ 06/17/23 13:01:31.435
  Jun 17 13:01:31.440: INFO: created test-podtemplate-1
  Jun 17 13:01:31.444: INFO: created test-podtemplate-2
  Jun 17 13:01:31.449: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 06/17/23 13:01:31.449
  STEP: delete collection of pod templates @ 06/17/23 13:01:31.452
  Jun 17 13:01:31.452: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 06/17/23 13:01:31.466
  Jun 17 13:01:31.466: INFO: requesting list of pod templates to confirm quantity
  Jun 17 13:01:31.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-6344" for this suite. @ 06/17/23 13:01:31.473
• [0.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 06/17/23 13:01:31.479
  Jun 17 13:01:31.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 06/17/23 13:01:31.48
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:01:31.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:01:31.503
  STEP: create the container to handle the HTTPGet hook request. @ 06/17/23 13:01:31.509
  E0617 13:01:32.305767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:33.306085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 06/17/23 13:01:33.526
  E0617 13:01:34.306506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:35.306601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 06/17/23 13:01:35.545
  STEP: delete the pod with lifecycle hook @ 06/17/23 13:01:35.563
  E0617 13:01:36.306810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:37.306929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:01:37.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8876" for this suite. @ 06/17/23 13:01:37.607
• [6.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 06/17/23 13:01:37.616
  Jun 17 13:01:37.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 13:01:37.617
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:01:37.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:01:37.639
  STEP: Setting up server cert @ 06/17/23 13:01:37.665
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 13:01:38.061
  STEP: Deploying the webhook pod @ 06/17/23 13:01:38.067
  STEP: Wait for the deployment to be ready @ 06/17/23 13:01:38.079
  Jun 17 13:01:38.086: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0617 13:01:38.307839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:39.307905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/17/23 13:01:40.096
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 13:01:40.11
  E0617 13:01:40.308273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:01:41.110: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 06/17/23 13:01:41.113
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 06/17/23 13:01:41.13
  STEP: Creating a dummy validating-webhook-configuration object @ 06/17/23 13:01:41.145
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 06/17/23 13:01:41.153
  STEP: Creating a dummy mutating-webhook-configuration object @ 06/17/23 13:01:41.158
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 06/17/23 13:01:41.166
  Jun 17 13:01:41.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7953" for this suite. @ 06/17/23 13:01:41.216
  STEP: Destroying namespace "webhook-markers-4164" for this suite. @ 06/17/23 13:01:41.223
• [3.615 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 06/17/23 13:01:41.231
  Jun 17 13:01:41.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename watch @ 06/17/23 13:01:41.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:01:41.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:01:41.258
  STEP: creating a watch on configmaps with a certain label @ 06/17/23 13:01:41.261
  STEP: creating a new configmap @ 06/17/23 13:01:41.263
  STEP: modifying the configmap once @ 06/17/23 13:01:41.267
  STEP: changing the label value of the configmap @ 06/17/23 13:01:41.273
  STEP: Expecting to observe a delete notification for the watched object @ 06/17/23 13:01:41.283
  Jun 17 13:01:41.283: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1046  2801827f-adc2-4cd9-81a6-e286e8943145 27036 0 2023-06-17 13:01:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-17 13:01:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 17 13:01:41.283: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1046  2801827f-adc2-4cd9-81a6-e286e8943145 27037 0 2023-06-17 13:01:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-17 13:01:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 17 13:01:41.284: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1046  2801827f-adc2-4cd9-81a6-e286e8943145 27038 0 2023-06-17 13:01:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-17 13:01:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 06/17/23 13:01:41.284
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 06/17/23 13:01:41.291
  E0617 13:01:41.308677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:42.308804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:43.308979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:44.309085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:45.309155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:46.310210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:47.311278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:48.312105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:49.312748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:50.312837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 06/17/23 13:01:51.291
  STEP: modifying the configmap a third time @ 06/17/23 13:01:51.299
  STEP: deleting the configmap @ 06/17/23 13:01:51.307
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 06/17/23 13:01:51.312
  Jun 17 13:01:51.312: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1046  2801827f-adc2-4cd9-81a6-e286e8943145 27103 0 2023-06-17 13:01:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-17 13:01:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  E0617 13:01:51.313031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:01:51.313: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1046  2801827f-adc2-4cd9-81a6-e286e8943145 27104 0 2023-06-17 13:01:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-17 13:01:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 17 13:01:51.313: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1046  2801827f-adc2-4cd9-81a6-e286e8943145 27105 0 2023-06-17 13:01:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-17 13:01:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 17 13:01:51.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1046" for this suite. @ 06/17/23 13:01:51.316
• [10.090 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 06/17/23 13:01:51.323
  Jun 17 13:01:51.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 06/17/23 13:01:51.324
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:01:51.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:01:51.349
  STEP: create the container to handle the HTTPGet hook request. @ 06/17/23 13:01:51.356
  E0617 13:01:52.313152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:53.313245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 06/17/23 13:01:53.374
  E0617 13:01:54.313355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:55.313512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 06/17/23 13:01:55.391
  STEP: delete the pod with lifecycle hook @ 06/17/23 13:01:55.397
  E0617 13:01:56.314013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:01:57.315053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:01:57.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-2682" for this suite. @ 06/17/23 13:01:57.415
• [6.098 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 06/17/23 13:01:57.421
  Jun 17 13:01:57.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename gc @ 06/17/23 13:01:57.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:01:57.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:01:57.442
  STEP: create the deployment @ 06/17/23 13:01:57.444
  W0617 13:01:57.451242      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 06/17/23 13:01:57.451
  STEP: delete the deployment @ 06/17/23 13:01:57.957
  STEP: wait for all rs to be garbage collected @ 06/17/23 13:01:57.963
  STEP: expected 0 rs, got 1 rs @ 06/17/23 13:01:57.969
  STEP: expected 0 pods, got 2 pods @ 06/17/23 13:01:57.972
  E0617 13:01:58.315906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 06/17/23 13:01:58.481
  W0617 13:01:58.485618      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 17 13:01:58.485: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 17 13:01:58.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7811" for this suite. @ 06/17/23 13:01:58.489
• [1.077 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 06/17/23 13:01:58.498
  Jun 17 13:01:58.499: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename field-validation @ 06/17/23 13:01:58.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:01:58.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:01:58.518
  Jun 17 13:01:58.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  W0617 13:01:58.523153      19 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc00545c710 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0617 13:01:59.316699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:00.317010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0617 13:02:01.073769      19 warnings.go:70] unknown field "alpha"
  W0617 13:02:01.073913      19 warnings.go:70] unknown field "beta"
  W0617 13:02:01.073996      19 warnings.go:70] unknown field "delta"
  W0617 13:02:01.074068      19 warnings.go:70] unknown field "epsilon"
  W0617 13:02:01.074131      19 warnings.go:70] unknown field "gamma"
  Jun 17 13:02:01.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7658" for this suite. @ 06/17/23 13:02:01.105
• [2.614 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 06/17/23 13:02:01.113
  Jun 17 13:02:01.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 06/17/23 13:02:01.115
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:02:01.132
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:02:01.135
  STEP: create the container to handle the HTTPGet hook request. @ 06/17/23 13:02:01.142
  E0617 13:02:01.317101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:02.317195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 06/17/23 13:02:03.162
  E0617 13:02:03.318040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:04.318232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 06/17/23 13:02:05.177
  E0617 13:02:05.318602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:06.319581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 06/17/23 13:02:07.19
  Jun 17 13:02:07.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-4596" for this suite. @ 06/17/23 13:02:07.2
• [6.095 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 06/17/23 13:02:07.209
  Jun 17 13:02:07.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename security-context @ 06/17/23 13:02:07.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:02:07.229
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:02:07.232
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 06/17/23 13:02:07.235
  E0617 13:02:07.319990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:08.320053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:09.320789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:10.320857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:02:11.253
  Jun 17 13:02:11.256: INFO: Trying to get logs from node ip-172-31-86-18 pod security-context-4610126d-7028-413c-8231-36aa850d2166 container test-container: <nil>
  STEP: delete the pod @ 06/17/23 13:02:11.262
  Jun 17 13:02:11.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-4419" for this suite. @ 06/17/23 13:02:11.28
• [4.077 seconds]
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 06/17/23 13:02:11.286
  Jun 17 13:02:11.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename deployment @ 06/17/23 13:02:11.287
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:02:11.305
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:02:11.308
  Jun 17 13:02:11.319: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0617 13:02:11.321414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:12.321773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:13.321772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:14.321970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:15.322185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:16.322468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:02:16.323: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/17/23 13:02:16.323
  Jun 17 13:02:16.323: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 06/17/23 13:02:16.331
  Jun 17 13:02:16.342: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7485  3a4429db-0245-4cb9-b1dd-fb4416b3d38d 27418 1 2023-06-17 13:02:16 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-06-17 13:02:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a50a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Jun 17 13:02:16.345: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Jun 17 13:02:16.345: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Jun 17 13:02:16.345: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7485  74e2e01d-2971-4281-a4b6-1a5885339f4c 27420 1 2023-06-17 13:02:11 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 3a4429db-0245-4cb9-b1dd-fb4416b3d38d 0xc005a50dc7 0xc005a50dc8}] [] [{e2e.test Update apps/v1 2023-06-17 13:02:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 13:02:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-17 13:02:16 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"3a4429db-0245-4cb9-b1dd-fb4416b3d38d\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005a50e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun 17 13:02:16.350: INFO: Pod "test-cleanup-controller-5zbxv" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-5zbxv test-cleanup-controller- deployment-7485  777dbee8-0346-4dbc-b6ed-d2ba90c5efb1 27404 0 2023-06-17 13:02:11 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 74e2e01d-2971-4281-a4b6-1a5885339f4c 0xc005a51197 0xc005a51198}] [] [{kube-controller-manager Update v1 2023-06-17 13:02:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"74e2e01d-2971-4281-a4b6-1a5885339f4c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:02:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2k49z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2k49z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:02:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:02:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:02:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:02:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.18,PodIP:192.168.2.73,StartTime:2023-06-17 13:02:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-17 13:02:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://98f5be7cced3bd33cf9ee3bc379b832d0602b47e45bee3b5e757fdc21b265a83,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.73,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:02:16.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7485" for this suite. @ 06/17/23 13:02:16.354
• [5.079 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 06/17/23 13:02:16.367
  Jun 17 13:02:16.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 13:02:16.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:02:16.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:02:16.395
  STEP: Setting up server cert @ 06/17/23 13:02:16.423
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 13:02:16.832
  STEP: Deploying the webhook pod @ 06/17/23 13:02:16.84
  STEP: Wait for the deployment to be ready @ 06/17/23 13:02:16.852
  Jun 17 13:02:16.858: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0617 13:02:17.322829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:18.323888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/17/23 13:02:18.868
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 13:02:18.877
  E0617 13:02:19.324677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:02:19.877: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 06/17/23 13:02:19.881
  STEP: create a pod that should be updated by the webhook @ 06/17/23 13:02:19.896
  Jun 17 13:02:19.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-630" for this suite. @ 06/17/23 13:02:19.964
  STEP: Destroying namespace "webhook-markers-183" for this suite. @ 06/17/23 13:02:19.969
• [3.610 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 06/17/23 13:02:19.977
  Jun 17 13:02:19.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename replicaset @ 06/17/23 13:02:19.978
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:02:20
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:02:20.002
  STEP: Create a Replicaset @ 06/17/23 13:02:20.009
  STEP: Verify that the required pods have come up. @ 06/17/23 13:02:20.013
  Jun 17 13:02:20.016: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0617 13:02:20.324832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:21.325010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:22.325111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:23.325372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:24.325452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:02:25.020: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/17/23 13:02:25.02
  STEP: Getting /status @ 06/17/23 13:02:25.02
  Jun 17 13:02:25.024: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 06/17/23 13:02:25.024
  Jun 17 13:02:25.033: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 06/17/23 13:02:25.033
  Jun 17 13:02:25.035: INFO: Observed &ReplicaSet event: ADDED
  Jun 17 13:02:25.035: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 17 13:02:25.035: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 17 13:02:25.035: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 17 13:02:25.035: INFO: Found replicaset test-rs in namespace replicaset-6589 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jun 17 13:02:25.035: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 06/17/23 13:02:25.035
  Jun 17 13:02:25.036: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jun 17 13:02:25.041: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 06/17/23 13:02:25.041
  Jun 17 13:02:25.042: INFO: Observed &ReplicaSet event: ADDED
  Jun 17 13:02:25.043: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 17 13:02:25.043: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 17 13:02:25.043: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 17 13:02:25.043: INFO: Observed replicaset test-rs in namespace replicaset-6589 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 17 13:02:25.043: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 17 13:02:25.043: INFO: Found replicaset test-rs in namespace replicaset-6589 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Jun 17 13:02:25.043: INFO: Replicaset test-rs has a patched status
  Jun 17 13:02:25.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6589" for this suite. @ 06/17/23 13:02:25.047
• [5.075 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 06/17/23 13:02:25.053
  Jun 17 13:02:25.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename secrets @ 06/17/23 13:02:25.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:02:25.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:02:25.078
  STEP: Creating secret with name secret-test-87b4bdb7-26b1-4ffa-94cd-36c02e1aa1df @ 06/17/23 13:02:25.081
  STEP: Creating a pod to test consume secrets @ 06/17/23 13:02:25.087
  E0617 13:02:25.325548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:26.325643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:27.325749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:28.325838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:02:29.106
  Jun 17 13:02:29.109: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-secrets-4172495b-47e7-4231-913f-b931cce31556 container secret-env-test: <nil>
  STEP: delete the pod @ 06/17/23 13:02:29.116
  Jun 17 13:02:29.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8961" for this suite. @ 06/17/23 13:02:29.137
• [4.089 seconds]
------------------------------
S
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 06/17/23 13:02:29.142
  Jun 17 13:02:29.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename runtimeclass @ 06/17/23 13:02:29.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:02:29.163
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:02:29.165
  Jun 17 13:02:29.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3914" for this suite. @ 06/17/23 13:02:29.178
• [0.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 06/17/23 13:02:29.186
  Jun 17 13:02:29.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename configmap @ 06/17/23 13:02:29.186
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:02:29.203
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:02:29.206
  STEP: Creating configMap with name configmap-test-upd-0d337706-6a44-425b-8a60-a5db6fefaa8f @ 06/17/23 13:02:29.212
  STEP: Creating the pod @ 06/17/23 13:02:29.216
  E0617 13:02:29.326297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:30.326500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 06/17/23 13:02:31.232
  STEP: Waiting for pod with binary data @ 06/17/23 13:02:31.238
  Jun 17 13:02:31.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8033" for this suite. @ 06/17/23 13:02:31.248
• [2.069 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 06/17/23 13:02:31.256
  Jun 17 13:02:31.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubelet-test @ 06/17/23 13:02:31.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:02:31.277
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:02:31.28
  E0617 13:02:31.326861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:32.326963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:33.327676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:34.327756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:02:35.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4459" for this suite. @ 06/17/23 13:02:35.3
• [4.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 06/17/23 13:02:35.309
  Jun 17 13:02:35.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename resourcequota @ 06/17/23 13:02:35.31
  E0617 13:02:35.327920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:02:35.329
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:02:35.332
  STEP: Creating resourceQuota "e2e-rq-status-9fttc" @ 06/17/23 13:02:35.338
  Jun 17 13:02:35.344: INFO: Resource quota "e2e-rq-status-9fttc" reports spec: hard cpu limit of 500m
  Jun 17 13:02:35.344: INFO: Resource quota "e2e-rq-status-9fttc" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-9fttc" /status @ 06/17/23 13:02:35.344
  STEP: Confirm /status for "e2e-rq-status-9fttc" resourceQuota via watch @ 06/17/23 13:02:35.353
  Jun 17 13:02:35.355: INFO: observed resourceQuota "e2e-rq-status-9fttc" in namespace "resourcequota-8458" with hard status: v1.ResourceList(nil)
  Jun 17 13:02:35.355: INFO: Found resourceQuota "e2e-rq-status-9fttc" in namespace "resourcequota-8458" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jun 17 13:02:35.355: INFO: ResourceQuota "e2e-rq-status-9fttc" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 06/17/23 13:02:35.359
  Jun 17 13:02:35.363: INFO: Resource quota "e2e-rq-status-9fttc" reports spec: hard cpu limit of 1
  Jun 17 13:02:35.363: INFO: Resource quota "e2e-rq-status-9fttc" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-9fttc" /status @ 06/17/23 13:02:35.363
  STEP: Confirm /status for "e2e-rq-status-9fttc" resourceQuota via watch @ 06/17/23 13:02:35.369
  Jun 17 13:02:35.371: INFO: observed resourceQuota "e2e-rq-status-9fttc" in namespace "resourcequota-8458" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jun 17 13:02:35.371: INFO: Found resourceQuota "e2e-rq-status-9fttc" in namespace "resourcequota-8458" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jun 17 13:02:35.371: INFO: ResourceQuota "e2e-rq-status-9fttc" /status was patched
  STEP: Get "e2e-rq-status-9fttc" /status @ 06/17/23 13:02:35.371
  Jun 17 13:02:35.374: INFO: Resourcequota "e2e-rq-status-9fttc" reports status: hard cpu of 1
  Jun 17 13:02:35.374: INFO: Resourcequota "e2e-rq-status-9fttc" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-9fttc" /status before checking Spec is unchanged @ 06/17/23 13:02:35.376
  Jun 17 13:02:35.387: INFO: Resourcequota "e2e-rq-status-9fttc" reports status: hard cpu of 2
  Jun 17 13:02:35.387: INFO: Resourcequota "e2e-rq-status-9fttc" reports status: hard memory of 2Gi
  Jun 17 13:02:35.388: INFO: observed resourceQuota "e2e-rq-status-9fttc" in namespace "resourcequota-8458" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jun 17 13:02:35.388: INFO: Found resourceQuota "e2e-rq-status-9fttc" in namespace "resourcequota-8458" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0617 13:02:36.328470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:37.329334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:38.329415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:39.329490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:40.329784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:41.329812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:42.329897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:43.329975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:44.330069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:45.330192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:46.331195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:47.331278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:48.331395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:49.331477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:50.331575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:51.331898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:52.332002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:53.332101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:54.332157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:55.332250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:56.333237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:57.333449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:58.333546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:02:59.333635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:00.333737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:01.334627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:02.334869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:03.335873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:04.335964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:05.336056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:06.336153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:07.336260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:08.336350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:09.336553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:10.336817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:11.337875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:12.338086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:13.338296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:14.338516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:15.338598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:16.338814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:17.339876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:18.340103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:19.340301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:20.340461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:21.340607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:22.340694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:23.340794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:24.341061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:25.341498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:26.341442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:27.341545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:28.342035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:29.342119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:30.342333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:31.342431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:32.342658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:33.342814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:34.343876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:35.344102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:36.344204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:37.344286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:38.344361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:39.344834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:40.344937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:41.345010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:42.345190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:43.345573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:44.345820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:45.345888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:46.346002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:47.346110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:48.346213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:49.346268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:50.346470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:51.346623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:52.346826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:53.346917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:54.347885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:55.348080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:56.348204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:57.348298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:58.348459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:03:59.348621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:00.348825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:01.349380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:02.349613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:03.349860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:04.350818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:05.351893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:06.351951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:07.353052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:08.353270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:09.353373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:10.353448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:11.353661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:12.353727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:13.353945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:14.354152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:15.354253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:16.354337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:17.354426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:18.354503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:19.354599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:20.354733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:21.355787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:22.355850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:23.355951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:24.356026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:25.356124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:26.356177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:27.356271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:28.356379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:29.356577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:30.356761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:31.357533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:32.358059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:33.358132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:34.358323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:35.358341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:36.358452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:37.358561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:38.358637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:39.358800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:40.359870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:41.360720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:42.360779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:43.361619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:44.361712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:45.362405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:46.363368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:47.363870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:48.364050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:49.364230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:50.364532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:51.365597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:52.365703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:53.365858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:54.365954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:55.366292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:56.366509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:57.366597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:58.366785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:04:59.366832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:00.366970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:01.367706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:02.367800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:03.368662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:04.368840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:05.369172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:06.370091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:07.370178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:08.370343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:09.370449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:10.370557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:05:10.395: INFO: ResourceQuota "e2e-rq-status-9fttc" Spec was unchanged and /status reset
  Jun 17 13:05:10.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8458" for this suite. @ 06/17/23 13:05:10.399
• [155.096 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 06/17/23 13:05:10.405
  Jun 17 13:05:10.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename configmap @ 06/17/23 13:05:10.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:05:10.426
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:05:10.429
  STEP: Creating configMap configmap-8613/configmap-test-7ea57150-d631-4091-8d97-cb8213962c9e @ 06/17/23 13:05:10.432
  STEP: Creating a pod to test consume configMaps @ 06/17/23 13:05:10.438
  E0617 13:05:11.370825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:12.370907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:13.371011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:14.371083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:05:14.455
  Jun 17 13:05:14.458: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-configmaps-2038af06-2f95-419e-a35e-5be72150de0b container env-test: <nil>
  STEP: delete the pod @ 06/17/23 13:05:14.472
  Jun 17 13:05:14.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8613" for this suite. @ 06/17/23 13:05:14.49
• [4.090 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 06/17/23 13:05:14.496
  Jun 17 13:05:14.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename pod-network-test @ 06/17/23 13:05:14.497
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:05:14.515
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:05:14.518
  STEP: Performing setup for networking test in namespace pod-network-test-2392 @ 06/17/23 13:05:14.521
  STEP: creating a selector @ 06/17/23 13:05:14.521
  STEP: Creating the service pods in kubernetes @ 06/17/23 13:05:14.521
  Jun 17 13:05:14.522: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0617 13:05:15.371177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:16.371878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:17.372313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:18.372398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:19.372492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:20.372581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:21.373586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:22.373690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:23.373773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:24.374149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:25.373953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:26.374201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:27.374371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:28.374574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:29.375355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:30.375871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:31.375948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:32.377005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:33.377618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:34.377817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:35.378161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:36.378191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 06/17/23 13:05:36.609
  E0617 13:05:37.378292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:38.378393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:05:38.626: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jun 17 13:05:38.626: INFO: Breadth first check of 192.168.178.169 on host 172.31.25.17...
  Jun 17 13:05:38.629: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.178.172:9080/dial?request=hostname&protocol=udp&host=192.168.178.169&port=8081&tries=1'] Namespace:pod-network-test-2392 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 13:05:38.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:05:38.629: INFO: ExecWithOptions: Clientset creation
  Jun 17 13:05:38.629: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2392/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.178.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.178.169%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 17 13:05:38.710: INFO: Waiting for responses: map[]
  Jun 17 13:05:38.710: INFO: reached 192.168.178.169 after 0/1 tries
  Jun 17 13:05:38.710: INFO: Breadth first check of 192.168.91.137 on host 172.31.68.253...
  Jun 17 13:05:38.713: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.178.172:9080/dial?request=hostname&protocol=udp&host=192.168.91.137&port=8081&tries=1'] Namespace:pod-network-test-2392 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 13:05:38.713: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:05:38.713: INFO: ExecWithOptions: Clientset creation
  Jun 17 13:05:38.713: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2392/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.178.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.91.137%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 17 13:05:38.777: INFO: Waiting for responses: map[]
  Jun 17 13:05:38.777: INFO: reached 192.168.91.137 after 0/1 tries
  Jun 17 13:05:38.777: INFO: Breadth first check of 192.168.2.84 on host 172.31.86.18...
  Jun 17 13:05:38.781: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.178.172:9080/dial?request=hostname&protocol=udp&host=192.168.2.84&port=8081&tries=1'] Namespace:pod-network-test-2392 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 13:05:38.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:05:38.781: INFO: ExecWithOptions: Clientset creation
  Jun 17 13:05:38.781: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2392/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.178.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.2.84%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 17 13:05:38.828: INFO: Waiting for responses: map[]
  Jun 17 13:05:38.828: INFO: reached 192.168.2.84 after 0/1 tries
  Jun 17 13:05:38.828: INFO: Going to retry 0 out of 3 pods....
  Jun 17 13:05:38.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-2392" for this suite. @ 06/17/23 13:05:38.832
• [24.342 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 06/17/23 13:05:38.841
  Jun 17 13:05:38.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-probe @ 06/17/23 13:05:38.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:05:38.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:05:38.865
  STEP: Creating pod busybox-97c534cd-bce6-4da7-8cea-96cde8fe6555 in namespace container-probe-6762 @ 06/17/23 13:05:38.868
  E0617 13:05:39.378479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:40.378562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:05:40.882: INFO: Started pod busybox-97c534cd-bce6-4da7-8cea-96cde8fe6555 in namespace container-probe-6762
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/17/23 13:05:40.882
  Jun 17 13:05:40.885: INFO: Initial restart count of pod busybox-97c534cd-bce6-4da7-8cea-96cde8fe6555 is 0
  E0617 13:05:41.379594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:42.379706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:43.380403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:44.380563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:45.380743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:46.381069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:47.381983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:48.382046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:49.382675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:50.382843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:51.382915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:52.383133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:53.383890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:54.384087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:55.384191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:56.384458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:57.385226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:58.385296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:05:59.386058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:00.386378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:01.387201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:02.387869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:03.388728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:04.388909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:05.389001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:06.389223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:07.389315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:08.389388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:09.390032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:10.390450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:11.390535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:12.391516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:13.392196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:14.392383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:15.393226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:16.393341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:17.393438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:18.393528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:19.393640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:20.393708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:21.394418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:22.394636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:23.395255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:24.395889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:25.396658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:26.396730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:27.397249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:28.397347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:29.398113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:30.398355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:31.398822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:32.399872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:33.399988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:34.400321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:35.400849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:36.400936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:37.401636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:38.401742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:39.402742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:40.402813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:41.402917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:42.403016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:43.403625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:44.403700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:45.404481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:46.404547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:47.405198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:48.405277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:49.405826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:50.406002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:51.407072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:52.407170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:53.407699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:54.407780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:55.408615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:56.409622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:57.410326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:58.410418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:06:59.410577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:00.411438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:01.411891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:02.412112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:03.412806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:04.413026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:05.413068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:06.413277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:07.413895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:08.414074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:09.414140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:10.414206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:11.414739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:12.414843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:13.415530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:14.415624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:15.415714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:16.415818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:17.416426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:18.416516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:19.417513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:20.417600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:21.417699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:22.418399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:23.418462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:24.418763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:25.418827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:26.418891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:27.419418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:28.420414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:29.420890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:30.420990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:31.421360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:32.421449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:33.421771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:34.421947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:35.422399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:36.422485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:37.422991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:38.423871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:39.424596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:40.424824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:41.425880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:42.426088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:43.426766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:44.426810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:45.427354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:46.427436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:47.427877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:48.428086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:49.428551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:50.428763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:51.429536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:52.429608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:53.430514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:54.430701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:55.430792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:56.430887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:57.431862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:58.431957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:07:59.432974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:00.433066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:01.434141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:02.434339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:03.434427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:04.434647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:05.434757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:06.434841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:07.435586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:08.436273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:09.437127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:10.437211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:11.438217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:12.438302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:13.438783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:14.438829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:15.439226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:16.440280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:17.441222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:18.441752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:19.441858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:20.441953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:21.442043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:22.442267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:23.443161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:24.443897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:25.444901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:26.444989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:27.445079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:28.445171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:29.445950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:30.446063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:31.446147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:32.446241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:33.446893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:34.447896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:35.448210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:36.448453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:37.448904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:38.448987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:39.449512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:40.449716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:41.450076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:42.451103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:43.452171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:44.452943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:45.453723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:46.454410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:47.454898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:48.454995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:49.455689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:50.455766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:51.456573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:52.456658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:53.457667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:54.457773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:55.458253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:56.458335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:57.458809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:58.458908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:08:59.459826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:00.459901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:01.459901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:02.460001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:03.460385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:04.460578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:05.461330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:06.461422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:07.461831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:08.461908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:09.462597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:10.462806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:11.463186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:12.463893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:13.464204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:14.464415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:15.465331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:16.466318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:17.466424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:18.466708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:19.466826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:20.466905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:21.466970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:22.467066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:23.467363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:24.467602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:25.468617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:26.468929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:27.469074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:28.469240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:29.469619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:30.469722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:31.470153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:32.470259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:33.471037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:34.471874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:35.472255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:36.472491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:37.472819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:38.473082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:39.473303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:40.473507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:09:41.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/17/23 13:09:41.368
  STEP: Destroying namespace "container-probe-6762" for this suite. @ 06/17/23 13:09:41.379
• [242.545 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 06/17/23 13:09:41.387
  Jun 17 13:09:41.387: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename gc @ 06/17/23 13:09:41.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:09:41.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:09:41.41
  Jun 17 13:09:41.443: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"72a3cfa6-e99e-4189-8926-a6d3881a690c", Controller:(*bool)(0xc0040f5d1e), BlockOwnerDeletion:(*bool)(0xc0040f5d1f)}}
  Jun 17 13:09:41.451: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"23d3fa23-1ab6-4eaf-90e1-f7870272c6f4", Controller:(*bool)(0xc0040f5fca), BlockOwnerDeletion:(*bool)(0xc0040f5fcb)}}
  Jun 17 13:09:41.459: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2b602de6-fb61-44e8-89e7-bab36e1493d8", Controller:(*bool)(0xc003e5c336), BlockOwnerDeletion:(*bool)(0xc003e5c337)}}
  E0617 13:09:41.473661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:42.473785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:43.473889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:44.474048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:45.474140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:09:46.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0617 13:09:46.474786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "gc-257" for this suite. @ 06/17/23 13:09:46.476
• [5.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 06/17/23 13:09:46.483
  Jun 17 13:09:46.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename downward-api @ 06/17/23 13:09:46.484
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:09:46.506
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:09:46.509
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 13:09:46.513
  E0617 13:09:47.474904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:48.475013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:49.475888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:50.476121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:09:50.531
  Jun 17 13:09:50.535: INFO: Trying to get logs from node ip-172-31-25-17 pod downwardapi-volume-94e42544-20f3-4a47-8c56-ea95fb8a7ec4 container client-container: <nil>
  STEP: delete the pod @ 06/17/23 13:09:50.553
  Jun 17 13:09:50.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1550" for this suite. @ 06/17/23 13:09:50.571
• [4.094 seconds]
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 06/17/23 13:09:50.578
  Jun 17 13:09:50.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename statefulset @ 06/17/23 13:09:50.579
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:09:50.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:09:50.601
  STEP: Creating service test in namespace statefulset-8902 @ 06/17/23 13:09:50.604
  STEP: Creating statefulset ss in namespace statefulset-8902 @ 06/17/23 13:09:50.608
  Jun 17 13:09:50.619: INFO: Found 0 stateful pods, waiting for 1
  E0617 13:09:51.476531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:52.476608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:53.476747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:54.476790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:55.477196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:56.477458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:57.477712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:58.477798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:09:59.478786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:00.478875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:10:00.624: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 06/17/23 13:10:00.63
  STEP: updating a scale subresource @ 06/17/23 13:10:00.633
  STEP: verifying the statefulset Spec.Replicas was modified @ 06/17/23 13:10:00.639
  STEP: Patch a scale subresource @ 06/17/23 13:10:00.641
  STEP: verifying the statefulset Spec.Replicas was modified @ 06/17/23 13:10:00.65
  Jun 17 13:10:00.655: INFO: Deleting all statefulset in ns statefulset-8902
  Jun 17 13:10:00.658: INFO: Scaling statefulset ss to 0
  E0617 13:10:01.479600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:02.479709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:03.479782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:04.480000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:05.480413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:06.480616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:07.480708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:08.481707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:09.481795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:10.482096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:10:10.676: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 17 13:10:10.678: INFO: Deleting statefulset ss
  Jun 17 13:10:10.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8902" for this suite. @ 06/17/23 13:10:10.693
• [20.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 06/17/23 13:10:10.701
  Jun 17 13:10:10.701: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename configmap @ 06/17/23 13:10:10.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:10:10.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:10:10.722
  STEP: Creating configMap with name cm-test-opt-del-57a7e211-4e87-43af-a597-25b92956848e @ 06/17/23 13:10:10.728
  STEP: Creating configMap with name cm-test-opt-upd-abb918e9-b987-402a-bdcb-803b93b6b767 @ 06/17/23 13:10:10.734
  STEP: Creating the pod @ 06/17/23 13:10:10.738
  E0617 13:10:11.482207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:12.482275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-57a7e211-4e87-43af-a597-25b92956848e @ 06/17/23 13:10:12.773
  STEP: Updating configmap cm-test-opt-upd-abb918e9-b987-402a-bdcb-803b93b6b767 @ 06/17/23 13:10:12.779
  STEP: Creating configMap with name cm-test-opt-create-a7647432-aec1-4469-aa6f-1654b23cacad @ 06/17/23 13:10:12.786
  STEP: waiting to observe update in volume @ 06/17/23 13:10:12.789
  E0617 13:10:13.482637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:14.482830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:15.483893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:16.483972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:10:16.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6136" for this suite. @ 06/17/23 13:10:16.824
• [6.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 06/17/23 13:10:16.832
  Jun 17 13:10:16.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 13:10:16.833
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:10:16.851
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:10:16.853
  STEP: Creating projection with secret that has name projected-secret-test-map-d627fa4a-b6c3-42bb-91b8-aa5c756c3078 @ 06/17/23 13:10:16.856
  STEP: Creating a pod to test consume secrets @ 06/17/23 13:10:16.86
  E0617 13:10:17.486532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:18.486824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:19.486919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:20.487893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:10:20.881
  Jun 17 13:10:20.884: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-projected-secrets-78f5d1f9-d23e-43c8-9b3e-0179f2a51586 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/17/23 13:10:20.901
  Jun 17 13:10:20.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7110" for this suite. @ 06/17/23 13:10:20.921
• [4.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 06/17/23 13:10:20.933
  Jun 17 13:10:20.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename watch @ 06/17/23 13:10:20.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:10:20.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:10:21.002
  STEP: creating a watch on configmaps @ 06/17/23 13:10:21.006
  STEP: creating a new configmap @ 06/17/23 13:10:21.007
  STEP: modifying the configmap once @ 06/17/23 13:10:21.011
  STEP: closing the watch once it receives two notifications @ 06/17/23 13:10:21.019
  Jun 17 13:10:21.019: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5673  a3b9577d-a2ea-4bb8-beaf-4cf17dbf113b 29109 0 2023-06-17 13:10:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-17 13:10:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 17 13:10:21.020: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5673  a3b9577d-a2ea-4bb8-beaf-4cf17dbf113b 29110 0 2023-06-17 13:10:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-17 13:10:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 06/17/23 13:10:21.02
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 06/17/23 13:10:21.027
  STEP: deleting the configmap @ 06/17/23 13:10:21.028
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 06/17/23 13:10:21.033
  Jun 17 13:10:21.033: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5673  a3b9577d-a2ea-4bb8-beaf-4cf17dbf113b 29111 0 2023-06-17 13:10:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-17 13:10:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 17 13:10:21.034: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5673  a3b9577d-a2ea-4bb8-beaf-4cf17dbf113b 29112 0 2023-06-17 13:10:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-17 13:10:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 17 13:10:21.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5673" for this suite. @ 06/17/23 13:10:21.038
• [0.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 06/17/23 13:10:21.046
  Jun 17 13:10:21.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename var-expansion @ 06/17/23 13:10:21.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:10:21.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:10:21.069
  STEP: creating the pod @ 06/17/23 13:10:21.072
  STEP: waiting for pod running @ 06/17/23 13:10:21.081
  E0617 13:10:21.488778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:22.488860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 06/17/23 13:10:23.088
  Jun 17 13:10:23.091: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6520 PodName:var-expansion-de6f35bd-2bb0-4ede-b853-31e8c6d3fa98 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 13:10:23.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:10:23.091: INFO: ExecWithOptions: Clientset creation
  Jun 17 13:10:23.091: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-6520/pods/var-expansion-de6f35bd-2bb0-4ede-b853-31e8c6d3fa98/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 06/17/23 13:10:23.159
  Jun 17 13:10:23.162: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6520 PodName:var-expansion-de6f35bd-2bb0-4ede-b853-31e8c6d3fa98 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 13:10:23.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:10:23.163: INFO: ExecWithOptions: Clientset creation
  Jun 17 13:10:23.163: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-6520/pods/var-expansion-de6f35bd-2bb0-4ede-b853-31e8c6d3fa98/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 06/17/23 13:10:23.231
  E0617 13:10:23.489612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:10:23.743: INFO: Successfully updated pod "var-expansion-de6f35bd-2bb0-4ede-b853-31e8c6d3fa98"
  STEP: waiting for annotated pod running @ 06/17/23 13:10:23.743
  STEP: deleting the pod gracefully @ 06/17/23 13:10:23.746
  Jun 17 13:10:23.746: INFO: Deleting pod "var-expansion-de6f35bd-2bb0-4ede-b853-31e8c6d3fa98" in namespace "var-expansion-6520"
  Jun 17 13:10:23.753: INFO: Wait up to 5m0s for pod "var-expansion-de6f35bd-2bb0-4ede-b853-31e8c6d3fa98" to be fully deleted
  E0617 13:10:24.490237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:25.490455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:26.490450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:27.490559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:28.490825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:29.490906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:30.491100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:31.491561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:32.492310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:33.492407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:34.493238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:35.493465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:36.493901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:37.494751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:38.494845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:39.495896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:40.496744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:41.497086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:42.497191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:43.498055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:44.498137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:45.498313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:46.499162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:47.499926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:48.499995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:49.500724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:50.500816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:51.500878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:52.501766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:53.502285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:54.502825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:55.503145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:10:55.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6520" for this suite. @ 06/17/23 13:10:55.827
• [34.786 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 06/17/23 13:10:55.834
  Jun 17 13:10:55.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename containers @ 06/17/23 13:10:55.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:10:55.855
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:10:55.858
  STEP: Creating a pod to test override arguments @ 06/17/23 13:10:55.86
  E0617 13:10:56.503899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:57.504000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:58.504085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:10:59.504307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:10:59.881
  Jun 17 13:10:59.884: INFO: Trying to get logs from node ip-172-31-25-17 pod client-containers-b70b50dc-f116-4333-a1bf-56090f5d1477 container agnhost-container: <nil>
  STEP: delete the pod @ 06/17/23 13:10:59.89
  Jun 17 13:10:59.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2346" for this suite. @ 06/17/23 13:10:59.908
• [4.081 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 06/17/23 13:10:59.915
  Jun 17 13:10:59.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 13:10:59.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:10:59.936
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:10:59.939
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 13:10:59.942
  E0617 13:11:00.504953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:01.505264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:02.505365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:03.505574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:11:03.96
  Jun 17 13:11:03.963: INFO: Trying to get logs from node ip-172-31-25-17 pod downwardapi-volume-e02b0d13-c808-4222-a094-602df83c0eba container client-container: <nil>
  STEP: delete the pod @ 06/17/23 13:11:03.969
  Jun 17 13:11:03.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9610" for this suite. @ 06/17/23 13:11:03.988
• [4.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 06/17/23 13:11:03.996
  Jun 17 13:11:03.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename pods @ 06/17/23 13:11:03.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:11:04.013
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:11:04.016
  STEP: creating the pod @ 06/17/23 13:11:04.018
  STEP: submitting the pod to kubernetes @ 06/17/23 13:11:04.018
  W0617 13:11:04.026851      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0617 13:11:04.506189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:05.506282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 06/17/23 13:11:06.036
  STEP: updating the pod @ 06/17/23 13:11:06.039
  E0617 13:11:06.507326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:11:06.550: INFO: Successfully updated pod "pod-update-activedeadlineseconds-33b43d98-935e-46e6-812b-e1a6a309198c"
  E0617 13:11:07.507443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:08.507531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:09.507627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:10.507874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:11:10.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8703" for this suite. @ 06/17/23 13:11:10.564
• [6.573 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 06/17/23 13:11:10.569
  Jun 17 13:11:10.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/17/23 13:11:10.57
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:11:10.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:11:10.592
  Jun 17 13:11:10.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:11:11.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1284" for this suite. @ 06/17/23 13:11:11.146
• [0.583 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 06/17/23 13:11:11.155
  Jun 17 13:11:11.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/17/23 13:11:11.156
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:11:11.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:11:11.182
  Jun 17 13:11:11.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  E0617 13:11:11.507984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:12.508421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 06/17/23 13:11:12.594
  Jun 17 13:11:12.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9709 --namespace=crd-publish-openapi-9709 create -f -'
  Jun 17 13:11:13.145: INFO: stderr: ""
  Jun 17 13:11:13.145: INFO: stdout: "e2e-test-crd-publish-openapi-8101-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jun 17 13:11:13.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9709 --namespace=crd-publish-openapi-9709 delete e2e-test-crd-publish-openapi-8101-crds test-cr'
  Jun 17 13:11:13.216: INFO: stderr: ""
  Jun 17 13:11:13.216: INFO: stdout: "e2e-test-crd-publish-openapi-8101-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Jun 17 13:11:13.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9709 --namespace=crd-publish-openapi-9709 apply -f -'
  Jun 17 13:11:13.435: INFO: stderr: ""
  Jun 17 13:11:13.435: INFO: stdout: "e2e-test-crd-publish-openapi-8101-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jun 17 13:11:13.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9709 --namespace=crd-publish-openapi-9709 delete e2e-test-crd-publish-openapi-8101-crds test-cr'
  Jun 17 13:11:13.508: INFO: stderr: ""
  Jun 17 13:11:13.508: INFO: stdout: "e2e-test-crd-publish-openapi-8101-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 06/17/23 13:11:13.508
  Jun 17 13:11:13.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=crd-publish-openapi-9709 explain e2e-test-crd-publish-openapi-8101-crds'
  E0617 13:11:13.508841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:11:13.725: INFO: stderr: ""
  Jun 17 13:11:13.725: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-8101-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0617 13:11:14.508856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:11:15.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9709" for this suite. @ 06/17/23 13:11:15.118
• [3.971 seconds]
------------------------------
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 06/17/23 13:11:15.126
  Jun 17 13:11:15.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename taint-multiple-pods @ 06/17/23 13:11:15.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:11:15.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:11:15.167
  Jun 17 13:11:15.170: INFO: Waiting up to 1m0s for all nodes to be ready
  E0617 13:11:15.508974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:16.508986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:17.509091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:18.509500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:19.510298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:20.510556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:21.510638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:22.510807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:23.511525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:24.511609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:25.511772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:26.512083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:27.513006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:28.513334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:29.513431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:30.513559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:31.514160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:32.514231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:33.515029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:34.515899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:35.515984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:36.516326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:37.516899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:38.516995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:39.517993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:40.518617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:41.519008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:42.519018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:43.519901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:44.519994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:45.520624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:46.520845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:47.521766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:48.521880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:49.522334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:50.523209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:51.523718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:52.523834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:53.524641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:54.524853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:55.525533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:56.525668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:57.526679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:58.526834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:11:59.526928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:00.527029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:01.527115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:02.527885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:03.528371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:04.528479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:05.529379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:06.529460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:07.529587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:08.529797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:09.530097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:10.530158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:11.530246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:12.530308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:13.531192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:14.531298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:12:15.183: INFO: Waiting for terminating namespaces to be deleted...
  Jun 17 13:12:15.186: INFO: Starting informer...
  STEP: Starting pods... @ 06/17/23 13:12:15.186
  Jun 17 13:12:15.401: INFO: Pod1 is running on ip-172-31-25-17. Tainting Node
  E0617 13:12:15.531674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:16.532118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:17.532211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:12:17.620: INFO: Pod2 is running on ip-172-31-25-17. Tainting Node
  STEP: Trying to apply a taint on the Node @ 06/17/23 13:12:17.62
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 06/17/23 13:12:17.63
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 06/17/23 13:12:17.634
  E0617 13:12:18.532305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:19.532500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:20.532688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:21.533278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:22.533609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:23.534298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:12:23.806: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0617 13:12:24.534636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:25.534735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:26.534838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:27.534940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:28.535871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:29.536347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:30.536579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:31.536805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:32.536888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:33.537145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:34.537336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:35.537537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:36.537697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:37.537920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:38.538106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:39.538292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:40.538495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:41.538828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:42.538918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:43.539037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:12:43.851: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Jun 17 13:12:43.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 06/17/23 13:12:43.865
  STEP: Destroying namespace "taint-multiple-pods-2689" for this suite. @ 06/17/23 13:12:43.868
• [88.749 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 06/17/23 13:12:43.876
  Jun 17 13:12:43.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl @ 06/17/23 13:12:43.877
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:12:43.9
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:12:43.903
  STEP: creating Agnhost RC @ 06/17/23 13:12:43.905
  Jun 17 13:12:43.905: INFO: namespace kubectl-8985
  Jun 17 13:12:43.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-8985 create -f -'
  Jun 17 13:12:44.497: INFO: stderr: ""
  Jun 17 13:12:44.497: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 06/17/23 13:12:44.497
  E0617 13:12:44.539274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:12:45.500: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 17 13:12:45.500: INFO: Found 0 / 1
  E0617 13:12:45.540008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:12:46.501: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 17 13:12:46.501: INFO: Found 1 / 1
  Jun 17 13:12:46.501: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jun 17 13:12:46.504: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 17 13:12:46.504: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jun 17 13:12:46.504: INFO: wait on agnhost-primary startup in kubectl-8985 
  Jun 17 13:12:46.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-8985 logs agnhost-primary-hqbvc agnhost-primary'
  E0617 13:12:46.540244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:12:46.583: INFO: stderr: ""
  Jun 17 13:12:46.583: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 06/17/23 13:12:46.583
  Jun 17 13:12:46.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-8985 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Jun 17 13:12:46.662: INFO: stderr: ""
  Jun 17 13:12:46.662: INFO: stdout: "service/rm2 exposed\n"
  Jun 17 13:12:46.666: INFO: Service rm2 in namespace kubectl-8985 found.
  E0617 13:12:47.540323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:48.541306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 06/17/23 13:12:48.672
  Jun 17 13:12:48.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-8985 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Jun 17 13:12:48.757: INFO: stderr: ""
  Jun 17 13:12:48.757: INFO: stdout: "service/rm3 exposed\n"
  Jun 17 13:12:48.761: INFO: Service rm3 in namespace kubectl-8985 found.
  E0617 13:12:49.541650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:50.541728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:12:50.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8985" for this suite. @ 06/17/23 13:12:50.771
• [6.901 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 06/17/23 13:12:50.777
  Jun 17 13:12:50.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename field-validation @ 06/17/23 13:12:50.778
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:12:50.807
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:12:50.81
  Jun 17 13:12:50.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  E0617 13:12:51.542072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:52.542162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0617 13:12:53.360691      19 warnings.go:70] unknown field "alpha"
  W0617 13:12:53.360711      19 warnings.go:70] unknown field "beta"
  W0617 13:12:53.360719      19 warnings.go:70] unknown field "delta"
  W0617 13:12:53.360724      19 warnings.go:70] unknown field "epsilon"
  W0617 13:12:53.360750      19 warnings.go:70] unknown field "gamma"
  Jun 17 13:12:53.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8960" for this suite. @ 06/17/23 13:12:53.39
• [2.619 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 06/17/23 13:12:53.398
  Jun 17 13:12:53.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename namespaces @ 06/17/23 13:12:53.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:12:53.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:12:53.421
  STEP: Creating a test namespace @ 06/17/23 13:12:53.424
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:12:53.441
  STEP: Creating a pod in the namespace @ 06/17/23 13:12:53.444
  STEP: Waiting for the pod to have running status @ 06/17/23 13:12:53.452
  E0617 13:12:53.542787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:54.542826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 06/17/23 13:12:55.459
  STEP: Waiting for the namespace to be removed. @ 06/17/23 13:12:55.465
  E0617 13:12:55.543644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:56.543837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:57.544142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:58.544934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:12:59.545018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:00.545428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:01.546344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:02.546753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:03.547116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:04.548104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:05.548664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 06/17/23 13:13:06.468
  STEP: Verifying there are no pods in the namespace @ 06/17/23 13:13:06.492
  Jun 17 13:13:06.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4479" for this suite. @ 06/17/23 13:13:06.498
  STEP: Destroying namespace "nsdeletetest-1114" for this suite. @ 06/17/23 13:13:06.503
  Jun 17 13:13:06.506: INFO: Namespace nsdeletetest-1114 was already deleted
  STEP: Destroying namespace "nsdeletetest-5466" for this suite. @ 06/17/23 13:13:06.506
• [13.113 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 06/17/23 13:13:06.511
  Jun 17 13:13:06.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename replication-controller @ 06/17/23 13:13:06.512
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:13:06.531
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:13:06.533
  STEP: Creating ReplicationController "e2e-rc-gzwp9" @ 06/17/23 13:13:06.536
  Jun 17 13:13:06.542: INFO: Get Replication Controller "e2e-rc-gzwp9" to confirm replicas
  E0617 13:13:06.549484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:13:07.545: INFO: Get Replication Controller "e2e-rc-gzwp9" to confirm replicas
  Jun 17 13:13:07.548: INFO: Found 1 replicas for "e2e-rc-gzwp9" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-gzwp9" @ 06/17/23 13:13:07.549
  E0617 13:13:07.550053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating a scale subresource @ 06/17/23 13:13:07.551
  STEP: Verifying replicas where modified for replication controller "e2e-rc-gzwp9" @ 06/17/23 13:13:07.557
  Jun 17 13:13:07.557: INFO: Get Replication Controller "e2e-rc-gzwp9" to confirm replicas
  E0617 13:13:08.550450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:13:08.560: INFO: Get Replication Controller "e2e-rc-gzwp9" to confirm replicas
  Jun 17 13:13:08.563: INFO: Found 2 replicas for "e2e-rc-gzwp9" replication controller
  Jun 17 13:13:08.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9298" for this suite. @ 06/17/23 13:13:08.567
• [2.062 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 06/17/23 13:13:08.573
  Jun 17 13:13:08.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename sched-preemption @ 06/17/23 13:13:08.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:13:08.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:13:08.594
  Jun 17 13:13:08.608: INFO: Waiting up to 1m0s for all nodes to be ready
  E0617 13:13:09.551307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:10.551390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:11.552087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:12.552155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:13.552372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:14.552440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:15.552537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:16.552677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:17.552781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:18.552999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:19.553091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:20.553200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:21.553508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:22.553631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:23.554236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:24.554323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:25.554882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:26.555873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:27.556299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:28.556400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:29.556485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:30.556702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:31.556836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:32.556934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:33.557032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:34.557274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:35.557367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:36.557672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:37.558023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:38.558132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:39.559056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:40.559146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:41.560157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:42.560263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:43.561198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:44.561291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:45.561380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:46.561705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:47.561787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:48.562438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:49.562827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:50.563881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:51.564314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:52.564394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:53.564448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:54.565084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:55.565164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:56.565389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:57.565981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:58.566050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:13:59.566167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:00.567155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:01.567246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:02.567877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:03.567987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:04.568331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:05.568434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:06.568681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:07.568788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:08.569067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:14:08.625: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 06/17/23 13:14:08.628
  Jun 17 13:14:08.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename sched-preemption-path @ 06/17/23 13:14:08.629
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:14:08.648
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:14:08.651
  STEP: Finding an available node @ 06/17/23 13:14:08.654
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 06/17/23 13:14:08.654
  E0617 13:14:09.569480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:10.569376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 06/17/23 13:14:10.672
  Jun 17 13:14:10.681: INFO: found a healthy node: ip-172-31-25-17
  E0617 13:14:11.569364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:12.570357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:13.570843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:14.571505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:15.572471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:16.573280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:14:16.748: INFO: pods created so far: [1 1 1]
  Jun 17 13:14:16.748: INFO: length of pods created so far: 3
  E0617 13:14:17.574205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:18.575128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:14:18.759: INFO: pods created so far: [2 2 1]
  E0617 13:14:19.575865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:20.576489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:21.576607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:22.576709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:23.576929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:24.577141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:25.577359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:14:25.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 17 13:14:25.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-8313" for this suite. @ 06/17/23 13:14:25.831
  STEP: Destroying namespace "sched-preemption-7486" for this suite. @ 06/17/23 13:14:25.837
• [77.270 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 06/17/23 13:14:25.845
  Jun 17 13:14:25.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename subpath @ 06/17/23 13:14:25.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:14:25.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:14:25.869
  STEP: Setting up data @ 06/17/23 13:14:25.872
  STEP: Creating pod pod-subpath-test-configmap-qq2v @ 06/17/23 13:14:25.879
  STEP: Creating a pod to test atomic-volume-subpath @ 06/17/23 13:14:25.879
  E0617 13:14:26.577455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:27.577554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:28.578464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:29.579268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:30.579368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:31.579785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:32.579885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:33.580060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:34.580407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:35.581185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:36.582120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:37.582373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:38.582471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:39.582546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:40.583056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:41.583883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:42.584245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:43.585124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:44.586034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:45.586913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:46.587482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:47.587726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:48.587781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:49.587982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:14:49.941
  Jun 17 13:14:49.945: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-subpath-test-configmap-qq2v container test-container-subpath-configmap-qq2v: <nil>
  STEP: delete the pod @ 06/17/23 13:14:49.959
  STEP: Deleting pod pod-subpath-test-configmap-qq2v @ 06/17/23 13:14:49.975
  Jun 17 13:14:49.975: INFO: Deleting pod "pod-subpath-test-configmap-qq2v" in namespace "subpath-6783"
  Jun 17 13:14:49.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6783" for this suite. @ 06/17/23 13:14:49.981
• [24.141 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 06/17/23 13:14:49.994
  Jun 17 13:14:49.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 13:14:49.995
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:14:50.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:14:50.017
  STEP: Creating configMap with name projected-configmap-test-volume-9da61705-5cdd-4eb6-bf92-67a30b944f35 @ 06/17/23 13:14:50.02
  STEP: Creating a pod to test consume configMaps @ 06/17/23 13:14:50.024
  E0617 13:14:50.588846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:51.589253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:52.589630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:53.589723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:14:54.042
  Jun 17 13:14:54.045: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-projected-configmaps-4385c22b-20e0-4153-904b-4a46ecdbe1bf container agnhost-container: <nil>
  STEP: delete the pod @ 06/17/23 13:14:54.062
  Jun 17 13:14:54.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6994" for this suite. @ 06/17/23 13:14:54.081
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 06/17/23 13:14:54.089
  Jun 17 13:14:54.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubelet-test @ 06/17/23 13:14:54.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:14:54.109
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:14:54.111
  Jun 17 13:14:54.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7559" for this suite. @ 06/17/23 13:14:54.144
• [0.060 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 06/17/23 13:14:54.15
  Jun 17 13:14:54.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename dns @ 06/17/23 13:14:54.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:14:54.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:14:54.169
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 06/17/23 13:14:54.172
  Jun 17 13:14:54.179: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7726  a3984809-1e73-4f54-8aaf-05133cd2c8d2 30467 0 2023-06-17 13:14:54 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-06-17 13:14:54 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f2xth,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f2xth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0617 13:14:54.589821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:55.589916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 06/17/23 13:14:56.187
  Jun 17 13:14:56.187: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7726 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 13:14:56.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:14:56.187: INFO: ExecWithOptions: Clientset creation
  Jun 17 13:14:56.187: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-7726/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 06/17/23 13:14:56.306
  Jun 17 13:14:56.306: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7726 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 13:14:56.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:14:56.307: INFO: ExecWithOptions: Clientset creation
  Jun 17 13:14:56.307: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-7726/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun 17 13:14:56.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 17 13:14:56.392: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-7726" for this suite. @ 06/17/23 13:14:56.404
• [2.262 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 06/17/23 13:14:56.412
  Jun 17 13:14:56.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 13:14:56.413
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:14:56.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:14:56.439
  STEP: Creating secret with name s-test-opt-del-6b9fa3d3-3ba6-44b9-b371-a05fa4dbe0d3 @ 06/17/23 13:14:56.451
  STEP: Creating secret with name s-test-opt-upd-c0d7968d-44e9-439d-96a5-76defb6651e0 @ 06/17/23 13:14:56.456
  STEP: Creating the pod @ 06/17/23 13:14:56.46
  E0617 13:14:56.589967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:57.590069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-6b9fa3d3-3ba6-44b9-b371-a05fa4dbe0d3 @ 06/17/23 13:14:58.495
  STEP: Updating secret s-test-opt-upd-c0d7968d-44e9-439d-96a5-76defb6651e0 @ 06/17/23 13:14:58.5
  STEP: Creating secret with name s-test-opt-create-307f9acd-1965-491b-920a-3542d8d589a6 @ 06/17/23 13:14:58.504
  STEP: waiting to observe update in volume @ 06/17/23 13:14:58.508
  E0617 13:14:58.590903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:14:59.591047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:00.591812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:01.592261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:15:02.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5691" for this suite. @ 06/17/23 13:15:02.544
• [6.137 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 06/17/23 13:15:02.55
  Jun 17 13:15:02.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir @ 06/17/23 13:15:02.551
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:15:02.57
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:15:02.573
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 06/17/23 13:15:02.58
  E0617 13:15:02.592277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:03.592412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:04.592439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:05.592570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:06.592744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:15:06.599
  Jun 17 13:15:06.602: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-5d652241-7787-47c7-8764-a9f1461d7fbe container test-container: <nil>
  STEP: delete the pod @ 06/17/23 13:15:06.608
  Jun 17 13:15:06.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3187" for this suite. @ 06/17/23 13:15:06.634
• [4.089 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 06/17/23 13:15:06.64
  Jun 17 13:15:06.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 13:15:06.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:15:06.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:15:06.662
  STEP: Creating the pod @ 06/17/23 13:15:06.664
  E0617 13:15:07.592957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:08.593030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:15:09.202: INFO: Successfully updated pod "annotationupdatede9807af-82e7-4f8e-b5ff-b4c24cc25dd2"
  E0617 13:15:09.593671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:10.593760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:15:11.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7371" for this suite. @ 06/17/23 13:15:11.22
• [4.587 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 06/17/23 13:15:11.228
  Jun 17 13:15:11.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir @ 06/17/23 13:15:11.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:15:11.25
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:15:11.253
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 06/17/23 13:15:11.258
  E0617 13:15:11.594346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:12.594444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:13.595472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:14.595876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:15:15.278
  Jun 17 13:15:15.281: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-85a3e8f1-b065-4173-9f27-a4151d3a28ce container test-container: <nil>
  STEP: delete the pod @ 06/17/23 13:15:15.287
  Jun 17 13:15:15.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4453" for this suite. @ 06/17/23 13:15:15.305
• [4.084 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 06/17/23 13:15:15.312
  Jun 17 13:15:15.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-probe @ 06/17/23 13:15:15.313
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:15:15.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:15:15.335
  STEP: Creating pod test-grpc-524e68b2-4d01-405d-b818-96bdbf863343 in namespace container-probe-8745 @ 06/17/23 13:15:15.338
  E0617 13:15:15.596881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:16.597278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:15:17.354: INFO: Started pod test-grpc-524e68b2-4d01-405d-b818-96bdbf863343 in namespace container-probe-8745
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/17/23 13:15:17.354
  Jun 17 13:15:17.358: INFO: Initial restart count of pod test-grpc-524e68b2-4d01-405d-b818-96bdbf863343 is 0
  E0617 13:15:17.597955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:18.598057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:19.598565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:20.598713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:21.599206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:22.599302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:23.599388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:24.600293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:25.600688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:26.601551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:27.601674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:28.602627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:29.603528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:30.603851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:31.604321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:32.604528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:33.605037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:34.605253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:35.605744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:36.605826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:37.605930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:38.606065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:39.606315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:40.606384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:41.606831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:42.606917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:43.607003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:44.607098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:45.607407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:46.607490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:47.607890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:48.608746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:49.609258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:50.609564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:51.609654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:52.609756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:53.610273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:54.610399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:55.610706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:56.610828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:57.611216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:58.611876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:15:59.611916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:00.612210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:01.612987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:02.613179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:03.613852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:04.614071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:05.614616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:06.614971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:07.615707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:08.615797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:09.616072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:10.616265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:11.616254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:12.616468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:13.617382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:14.617528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:15.618105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:16.618204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:17.618475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:18.618640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:19.619378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:20.619867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:21.620297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:22.620428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:23.620905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:24.621194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:25.621520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:26.622145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:27.622193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:28.622392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:29.623092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:30.623871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:16:31.506: INFO: Restart count of pod container-probe-8745/test-grpc-524e68b2-4d01-405d-b818-96bdbf863343 is now 1 (1m14.148219204s elapsed)
  Jun 17 13:16:31.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/17/23 13:16:31.509
  STEP: Destroying namespace "container-probe-8745" for this suite. @ 06/17/23 13:16:31.522
• [76.217 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 06/17/23 13:16:31.53
  Jun 17 13:16:31.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename var-expansion @ 06/17/23 13:16:31.53
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:16:31.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:16:31.55
  STEP: Creating a pod to test substitution in container's args @ 06/17/23 13:16:31.553
  E0617 13:16:31.624514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:32.624599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:33.625186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:34.625343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:16:35.572
  Jun 17 13:16:35.574: INFO: Trying to get logs from node ip-172-31-25-17 pod var-expansion-84bbb1dc-f1e1-4d7b-a80b-4309e0e2b84a container dapi-container: <nil>
  STEP: delete the pod @ 06/17/23 13:16:35.581
  Jun 17 13:16:35.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7009" for this suite. @ 06/17/23 13:16:35.599
• [4.075 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 06/17/23 13:16:35.605
  Jun 17 13:16:35.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename hostport @ 06/17/23 13:16:35.606
  E0617 13:16:35.625438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:16:35.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:16:35.628
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 06/17/23 13:16:35.634
  E0617 13:16:36.626130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:37.626497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.86.18 on the node which pod1 resides and expect scheduled @ 06/17/23 13:16:37.647
  E0617 13:16:38.627183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:39.627885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:40.627977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:41.628416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:42.628528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:43.628616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:44.628717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:45.628799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:46.629814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:47.629987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:48.630850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:49.630892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.86.18 but use UDP protocol on the node which pod2 resides @ 06/17/23 13:16:49.678
  E0617 13:16:50.630969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:51.631065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:52.631961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:53.632053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 06/17/23 13:16:53.707
  Jun 17 13:16:53.707: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.86.18 http://127.0.0.1:54323/hostname] Namespace:hostport-872 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 13:16:53.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:16:53.708: INFO: ExecWithOptions: Clientset creation
  Jun 17 13:16:53.708: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-872/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.86.18+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.86.18, port: 54323 @ 06/17/23 13:16:53.788
  Jun 17 13:16:53.788: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.86.18:54323/hostname] Namespace:hostport-872 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 13:16:53.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:16:53.788: INFO: ExecWithOptions: Clientset creation
  Jun 17 13:16:53.788: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-872/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.86.18%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.86.18, port: 54323 UDP @ 06/17/23 13:16:53.847
  Jun 17 13:16:53.847: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.86.18 54323] Namespace:hostport-872 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 13:16:53.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:16:53.847: INFO: ExecWithOptions: Clientset creation
  Jun 17 13:16:53.848: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-872/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.86.18+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0617 13:16:54.632746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:55.632979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:56.633310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:57.633436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:16:58.633519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:16:58.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-872" for this suite. @ 06/17/23 13:16:58.928
• [23.329 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 06/17/23 13:16:58.934
  Jun 17 13:16:58.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename replication-controller @ 06/17/23 13:16:58.935
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:16:58.953
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:16:58.955
  STEP: Given a ReplicationController is created @ 06/17/23 13:16:58.959
  STEP: When the matched label of one of its pods change @ 06/17/23 13:16:58.965
  Jun 17 13:16:58.968: INFO: Pod name pod-release: Found 0 pods out of 1
  E0617 13:16:59.633910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:00.634215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:01.634300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:02.634496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:03.635088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:17:03.976: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 06/17/23 13:17:03.987
  E0617 13:17:04.635200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:17:04.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3147" for this suite. @ 06/17/23 13:17:04.997
• [6.069 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 06/17/23 13:17:05.004
  Jun 17 13:17:05.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 13:17:05.005
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:17:05.036
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:17:05.039
  STEP: Setting up server cert @ 06/17/23 13:17:05.084
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 13:17:05.56
  STEP: Deploying the webhook pod @ 06/17/23 13:17:05.588
  STEP: Wait for the deployment to be ready @ 06/17/23 13:17:05.603
  Jun 17 13:17:05.608: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0617 13:17:05.636170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:06.637230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/17/23 13:17:07.618
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 13:17:07.628
  E0617 13:17:07.637738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:17:08.628: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 06/17/23 13:17:08.632
  E0617 13:17:08.638461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource definition that should be denied by the webhook @ 06/17/23 13:17:08.646
  Jun 17 13:17:08.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:17:08.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5016" for this suite. @ 06/17/23 13:17:08.697
  STEP: Destroying namespace "webhook-markers-4264" for this suite. @ 06/17/23 13:17:08.706
• [3.707 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 06/17/23 13:17:08.713
  Jun 17 13:17:08.713: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 13:17:08.714
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:17:08.73
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:17:08.733
  STEP: Setting up server cert @ 06/17/23 13:17:08.764
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 13:17:09.077
  STEP: Deploying the webhook pod @ 06/17/23 13:17:09.082
  STEP: Wait for the deployment to be ready @ 06/17/23 13:17:09.094
  Jun 17 13:17:09.100: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0617 13:17:09.638840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:10.639907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/17/23 13:17:11.11
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 13:17:11.119
  E0617 13:17:11.640817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:17:12.119: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 06/17/23 13:17:12.122
  STEP: create a configmap that should be updated by the webhook @ 06/17/23 13:17:12.136
  Jun 17 13:17:12.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2478" for this suite. @ 06/17/23 13:17:12.194
  STEP: Destroying namespace "webhook-markers-7643" for this suite. @ 06/17/23 13:17:12.202
• [3.495 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 06/17/23 13:17:12.208
  Jun 17 13:17:12.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename gc @ 06/17/23 13:17:12.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:17:12.224
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:17:12.227
  STEP: create the rc @ 06/17/23 13:17:12.23
  W0617 13:17:12.235976      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0617 13:17:12.641548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:13.641708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:14.641776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:15.641954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:16.642062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 06/17/23 13:17:17.241
  STEP: wait for all pods to be garbage collected @ 06/17/23 13:17:17.246
  E0617 13:17:17.642722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:18.642961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:19.642980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:20.643071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:21.643874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 06/17/23 13:17:22.253
  W0617 13:17:22.256874      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 17 13:17:22.256: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 17 13:17:22.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5860" for this suite. @ 06/17/23 13:17:22.26
• [10.059 seconds]
------------------------------
S
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 06/17/23 13:17:22.266
  Jun 17 13:17:22.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename proxy @ 06/17/23 13:17:22.267
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:17:22.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:17:22.29
  STEP: starting an echo server on multiple ports @ 06/17/23 13:17:22.3
  STEP: creating replication controller proxy-service-8qql5 in namespace proxy-6151 @ 06/17/23 13:17:22.301
  I0617 13:17:22.309718      19 runners.go:194] Created replication controller with name: proxy-service-8qql5, namespace: proxy-6151, replica count: 1
  E0617 13:17:22.644464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0617 13:17:23.361091      19 runners.go:194] proxy-service-8qql5 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0617 13:17:23.644935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0617 13:17:24.361637      19 runners.go:194] proxy-service-8qql5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0617 13:17:24.645854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0617 13:17:25.362742      19 runners.go:194] proxy-service-8qql5 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 17 13:17:25.365: INFO: setup took 3.072836086s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 06/17/23 13:17:25.365
  Jun 17 13:17:25.372: INFO: (0) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 6.347597ms)
  Jun 17 13:17:25.380: INFO: (0) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 13.501827ms)
  Jun 17 13:17:25.380: INFO: (0) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 13.618436ms)
  Jun 17 13:17:25.380: INFO: (0) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 13.887637ms)
  Jun 17 13:17:25.381: INFO: (0) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 14.868895ms)
  Jun 17 13:17:25.381: INFO: (0) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 14.856613ms)
  Jun 17 13:17:25.381: INFO: (0) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 15.722704ms)
  Jun 17 13:17:25.381: INFO: (0) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 15.343914ms)
  Jun 17 13:17:25.381: INFO: (0) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 15.50223ms)
  Jun 17 13:17:25.381: INFO: (0) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 15.320644ms)
  Jun 17 13:17:25.381: INFO: (0) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 15.407978ms)
  Jun 17 13:17:25.382: INFO: (0) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 16.138246ms)
  Jun 17 13:17:25.382: INFO: (0) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 16.353117ms)
  Jun 17 13:17:25.382: INFO: (0) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 16.122434ms)
  Jun 17 13:17:25.384: INFO: (0) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 17.699962ms)
  Jun 17 13:17:25.384: INFO: (0) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 17.748149ms)
  Jun 17 13:17:25.388: INFO: (1) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 4.40854ms)
  Jun 17 13:17:25.389: INFO: (1) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 4.741468ms)
  Jun 17 13:17:25.389: INFO: (1) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 4.883266ms)
  Jun 17 13:17:25.390: INFO: (1) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 5.999759ms)
  Jun 17 13:17:25.391: INFO: (1) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 6.959183ms)
  Jun 17 13:17:25.392: INFO: (1) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 7.9664ms)
  Jun 17 13:17:25.392: INFO: (1) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 8.135303ms)
  Jun 17 13:17:25.393: INFO: (1) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 8.29748ms)
  Jun 17 13:17:25.393: INFO: (1) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 8.247551ms)
  Jun 17 13:17:25.393: INFO: (1) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 8.579166ms)
  Jun 17 13:17:25.393: INFO: (1) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 8.648345ms)
  Jun 17 13:17:25.393: INFO: (1) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 8.954434ms)
  Jun 17 13:17:25.394: INFO: (1) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 9.107203ms)
  Jun 17 13:17:25.394: INFO: (1) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 9.680064ms)
  Jun 17 13:17:25.394: INFO: (1) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 9.64017ms)
  Jun 17 13:17:25.395: INFO: (1) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 10.152749ms)
  Jun 17 13:17:25.399: INFO: (2) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 3.83402ms)
  Jun 17 13:17:25.400: INFO: (2) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 4.96054ms)
  Jun 17 13:17:25.400: INFO: (2) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 5.350795ms)
  Jun 17 13:17:25.401: INFO: (2) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 5.551112ms)
  Jun 17 13:17:25.402: INFO: (2) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 7.157624ms)
  Jun 17 13:17:25.403: INFO: (2) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 7.601115ms)
  Jun 17 13:17:25.403: INFO: (2) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 7.575323ms)
  Jun 17 13:17:25.403: INFO: (2) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 8.353978ms)
  Jun 17 13:17:25.403: INFO: (2) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 8.28005ms)
  Jun 17 13:17:25.404: INFO: (2) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 8.461928ms)
  Jun 17 13:17:25.404: INFO: (2) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 9.261649ms)
  Jun 17 13:17:25.404: INFO: (2) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 9.108117ms)
  Jun 17 13:17:25.404: INFO: (2) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 9.415217ms)
  Jun 17 13:17:25.405: INFO: (2) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 9.902119ms)
  Jun 17 13:17:25.405: INFO: (2) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 10.208016ms)
  Jun 17 13:17:25.405: INFO: (2) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 10.297431ms)
  Jun 17 13:17:25.410: INFO: (3) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 4.092163ms)
  Jun 17 13:17:25.410: INFO: (3) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 4.515012ms)
  Jun 17 13:17:25.411: INFO: (3) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 5.099082ms)
  Jun 17 13:17:25.412: INFO: (3) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 5.674453ms)
  Jun 17 13:17:25.413: INFO: (3) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 7.34898ms)
  Jun 17 13:17:25.413: INFO: (3) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 7.701299ms)
  Jun 17 13:17:25.414: INFO: (3) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 8.01662ms)
  Jun 17 13:17:25.414: INFO: (3) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 8.555883ms)
  Jun 17 13:17:25.414: INFO: (3) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 8.341555ms)
  Jun 17 13:17:25.415: INFO: (3) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 8.511896ms)
  Jun 17 13:17:25.415: INFO: (3) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 8.910974ms)
  Jun 17 13:17:25.415: INFO: (3) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 8.685097ms)
  Jun 17 13:17:25.415: INFO: (3) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 9.176211ms)
  Jun 17 13:17:25.416: INFO: (3) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 9.732237ms)
  Jun 17 13:17:25.416: INFO: (3) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 9.796092ms)
  Jun 17 13:17:25.416: INFO: (3) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 9.571347ms)
  Jun 17 13:17:25.420: INFO: (4) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 4.28402ms)
  Jun 17 13:17:25.421: INFO: (4) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 4.566388ms)
  Jun 17 13:17:25.422: INFO: (4) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 5.850642ms)
  Jun 17 13:17:25.422: INFO: (4) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 5.963068ms)
  Jun 17 13:17:25.422: INFO: (4) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 6.207427ms)
  Jun 17 13:17:25.423: INFO: (4) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 6.544882ms)
  Jun 17 13:17:25.423: INFO: (4) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 6.934785ms)
  Jun 17 13:17:25.424: INFO: (4) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 7.843659ms)
  Jun 17 13:17:25.424: INFO: (4) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 7.98869ms)
  Jun 17 13:17:25.425: INFO: (4) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 8.881309ms)
  Jun 17 13:17:25.425: INFO: (4) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 8.882828ms)
  Jun 17 13:17:25.425: INFO: (4) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 8.92955ms)
  Jun 17 13:17:25.426: INFO: (4) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 9.987673ms)
  Jun 17 13:17:25.426: INFO: (4) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 9.766701ms)
  Jun 17 13:17:25.426: INFO: (4) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 10.059282ms)
  Jun 17 13:17:25.426: INFO: (4) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 10.144397ms)
  Jun 17 13:17:25.430: INFO: (5) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 3.755049ms)
  Jun 17 13:17:25.432: INFO: (5) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 5.22824ms)
  Jun 17 13:17:25.432: INFO: (5) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 5.171057ms)
  Jun 17 13:17:25.433: INFO: (5) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 5.917723ms)
  Jun 17 13:17:25.434: INFO: (5) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 6.442771ms)
  Jun 17 13:17:25.434: INFO: (5) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 7.101863ms)
  Jun 17 13:17:25.434: INFO: (5) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 7.608764ms)
  Jun 17 13:17:25.435: INFO: (5) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 7.869335ms)
  Jun 17 13:17:25.436: INFO: (5) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 8.726065ms)
  Jun 17 13:17:25.436: INFO: (5) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 8.906336ms)
  Jun 17 13:17:25.436: INFO: (5) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 9.042316ms)
  Jun 17 13:17:25.436: INFO: (5) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 9.368811ms)
  Jun 17 13:17:25.436: INFO: (5) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 9.644333ms)
  Jun 17 13:17:25.437: INFO: (5) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 9.803382ms)
  Jun 17 13:17:25.437: INFO: (5) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 10.521091ms)
  Jun 17 13:17:25.437: INFO: (5) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 10.378369ms)
  Jun 17 13:17:25.442: INFO: (6) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 4.329603ms)
  Jun 17 13:17:25.443: INFO: (6) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 5.024587ms)
  Jun 17 13:17:25.443: INFO: (6) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 5.488238ms)
  Jun 17 13:17:25.444: INFO: (6) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 5.865038ms)
  Jun 17 13:17:25.444: INFO: (6) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 6.265442ms)
  Jun 17 13:17:25.444: INFO: (6) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 6.579357ms)
  Jun 17 13:17:25.445: INFO: (6) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 7.115553ms)
  Jun 17 13:17:25.445: INFO: (6) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 7.400562ms)
  Jun 17 13:17:25.445: INFO: (6) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 7.580409ms)
  Jun 17 13:17:25.446: INFO: (6) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 8.24534ms)
  Jun 17 13:17:25.446: INFO: (6) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 7.935603ms)
  Jun 17 13:17:25.447: INFO: (6) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 8.769805ms)
  Jun 17 13:17:25.447: INFO: (6) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 8.891776ms)
  Jun 17 13:17:25.448: INFO: (6) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 10.194777ms)
  Jun 17 13:17:25.449: INFO: (6) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 10.534352ms)
  Jun 17 13:17:25.449: INFO: (6) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 10.632371ms)
  Jun 17 13:17:25.453: INFO: (7) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 4.631303ms)
  Jun 17 13:17:25.455: INFO: (7) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 6.07032ms)
  Jun 17 13:17:25.455: INFO: (7) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 6.464038ms)
  Jun 17 13:17:25.455: INFO: (7) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 6.139728ms)
  Jun 17 13:17:25.456: INFO: (7) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 6.553157ms)
  Jun 17 13:17:25.457: INFO: (7) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 8.111768ms)
  Jun 17 13:17:25.458: INFO: (7) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 8.675472ms)
  Jun 17 13:17:25.458: INFO: (7) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 9.018078ms)
  Jun 17 13:17:25.458: INFO: (7) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 8.890594ms)
  Jun 17 13:17:25.459: INFO: (7) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 9.237796ms)
  Jun 17 13:17:25.459: INFO: (7) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 9.575361ms)
  Jun 17 13:17:25.459: INFO: (7) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 9.401063ms)
  Jun 17 13:17:25.459: INFO: (7) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 9.349073ms)
  Jun 17 13:17:25.459: INFO: (7) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 9.588622ms)
  Jun 17 13:17:25.459: INFO: (7) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 10.016323ms)
  Jun 17 13:17:25.459: INFO: (7) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 10.259033ms)
  Jun 17 13:17:25.464: INFO: (8) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 4.501869ms)
  Jun 17 13:17:25.464: INFO: (8) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 5.169679ms)
  Jun 17 13:17:25.466: INFO: (8) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 6.381545ms)
  Jun 17 13:17:25.468: INFO: (8) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 8.363647ms)
  Jun 17 13:17:25.468: INFO: (8) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 7.888159ms)
  Jun 17 13:17:25.468: INFO: (8) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 8.090748ms)
  Jun 17 13:17:25.468: INFO: (8) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 8.029543ms)
  Jun 17 13:17:25.468: INFO: (8) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 8.738481ms)
  Jun 17 13:17:25.468: INFO: (8) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 8.601761ms)
  Jun 17 13:17:25.469: INFO: (8) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 9.712357ms)
  Jun 17 13:17:25.469: INFO: (8) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 9.53577ms)
  Jun 17 13:17:25.470: INFO: (8) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 9.83247ms)
  Jun 17 13:17:25.470: INFO: (8) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 10.603263ms)
  Jun 17 13:17:25.470: INFO: (8) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 10.298156ms)
  Jun 17 13:17:25.470: INFO: (8) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 10.423959ms)
  Jun 17 13:17:25.470: INFO: (8) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 10.762349ms)
  Jun 17 13:17:25.483: INFO: (9) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 11.868221ms)
  Jun 17 13:17:25.483: INFO: (9) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 12.075702ms)
  Jun 17 13:17:25.486: INFO: (9) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 15.215294ms)
  Jun 17 13:17:25.486: INFO: (9) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 15.151575ms)
  Jun 17 13:17:25.490: INFO: (9) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 18.907756ms)
  Jun 17 13:17:25.490: INFO: (9) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 19.168802ms)
  Jun 17 13:17:25.493: INFO: (9) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 22.482491ms)
  Jun 17 13:17:25.493: INFO: (9) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 22.321852ms)
  Jun 17 13:17:25.496: INFO: (9) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 25.657692ms)
  Jun 17 13:17:25.496: INFO: (9) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 25.293621ms)
  Jun 17 13:17:25.498: INFO: (9) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 26.534657ms)
  Jun 17 13:17:25.498: INFO: (9) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 26.571539ms)
  Jun 17 13:17:25.498: INFO: (9) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 27.063666ms)
  Jun 17 13:17:25.498: INFO: (9) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 27.050717ms)
  Jun 17 13:17:25.498: INFO: (9) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 26.873183ms)
  Jun 17 13:17:25.498: INFO: (9) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 26.98713ms)
  Jun 17 13:17:25.510: INFO: (10) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 11.233995ms)
  Jun 17 13:17:25.510: INFO: (10) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 11.234841ms)
  Jun 17 13:17:25.512: INFO: (10) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 13.985758ms)
  Jun 17 13:17:25.512: INFO: (10) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 13.864662ms)
  Jun 17 13:17:25.513: INFO: (10) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 14.517729ms)
  Jun 17 13:17:25.514: INFO: (10) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 15.267792ms)
  Jun 17 13:17:25.516: INFO: (10) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 18.423545ms)
  Jun 17 13:17:25.516: INFO: (10) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 18.39186ms)
  Jun 17 13:17:25.518: INFO: (10) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 20.152815ms)
  Jun 17 13:17:25.518: INFO: (10) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 20.230751ms)
  Jun 17 13:17:25.518: INFO: (10) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 19.999361ms)
  Jun 17 13:17:25.519: INFO: (10) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 21.226833ms)
  Jun 17 13:17:25.519: INFO: (10) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 21.202606ms)
  Jun 17 13:17:25.520: INFO: (10) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 21.542112ms)
  Jun 17 13:17:25.519: INFO: (10) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 21.109073ms)
  Jun 17 13:17:25.520: INFO: (10) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 22.086329ms)
  Jun 17 13:17:25.532: INFO: (11) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 10.86708ms)
  Jun 17 13:17:25.532: INFO: (11) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 10.649757ms)
  Jun 17 13:17:25.532: INFO: (11) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 11.593469ms)
  Jun 17 13:17:25.533: INFO: (11) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 11.857534ms)
  Jun 17 13:17:25.533: INFO: (11) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 12.179447ms)
  Jun 17 13:17:25.534: INFO: (11) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 13.174037ms)
  Jun 17 13:17:25.535: INFO: (11) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 14.479065ms)
  Jun 17 13:17:25.536: INFO: (11) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 15.018751ms)
  Jun 17 13:17:25.537: INFO: (11) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 15.718214ms)
  Jun 17 13:17:25.537: INFO: (11) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 16.376099ms)
  Jun 17 13:17:25.537: INFO: (11) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 16.269295ms)
  Jun 17 13:17:25.537: INFO: (11) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 16.540698ms)
  Jun 17 13:17:25.539: INFO: (11) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 17.99168ms)
  Jun 17 13:17:25.539: INFO: (11) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 18.067633ms)
  Jun 17 13:17:25.539: INFO: (11) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 18.41475ms)
  Jun 17 13:17:25.540: INFO: (11) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 18.498937ms)
  Jun 17 13:17:25.545: INFO: (12) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 5.61735ms)
  Jun 17 13:17:25.546: INFO: (12) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 6.440056ms)
  Jun 17 13:17:25.547: INFO: (12) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 7.150197ms)
  Jun 17 13:17:25.548: INFO: (12) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 7.571683ms)
  Jun 17 13:17:25.549: INFO: (12) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 8.965229ms)
  Jun 17 13:17:25.549: INFO: (12) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 9.554651ms)
  Jun 17 13:17:25.554: INFO: (12) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 14.286641ms)
  Jun 17 13:17:25.556: INFO: (12) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 15.298386ms)
  Jun 17 13:17:25.560: INFO: (12) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 20.438718ms)
  Jun 17 13:17:25.560: INFO: (12) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 20.299821ms)
  Jun 17 13:17:25.560: INFO: (12) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 20.370576ms)
  Jun 17 13:17:25.561: INFO: (12) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 20.990085ms)
  Jun 17 13:17:25.561: INFO: (12) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 20.836007ms)
  Jun 17 13:17:25.561: INFO: (12) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 20.744804ms)
  Jun 17 13:17:25.561: INFO: (12) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 20.793502ms)
  Jun 17 13:17:25.561: INFO: (12) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 21.027888ms)
  Jun 17 13:17:25.569: INFO: (13) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 7.655412ms)
  Jun 17 13:17:25.572: INFO: (13) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 10.337275ms)
  Jun 17 13:17:25.574: INFO: (13) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 12.410805ms)
  Jun 17 13:17:25.579: INFO: (13) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 16.811428ms)
  Jun 17 13:17:25.579: INFO: (13) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 17.019958ms)
  Jun 17 13:17:25.579: INFO: (13) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 17.20367ms)
  Jun 17 13:17:25.579: INFO: (13) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 17.297413ms)
  Jun 17 13:17:25.581: INFO: (13) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 18.892756ms)
  Jun 17 13:17:25.583: INFO: (13) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 21.10385ms)
  Jun 17 13:17:25.583: INFO: (13) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 21.007208ms)
  Jun 17 13:17:25.583: INFO: (13) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 20.880938ms)
  Jun 17 13:17:25.583: INFO: (13) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 21.035762ms)
  Jun 17 13:17:25.583: INFO: (13) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 21.378801ms)
  Jun 17 13:17:25.583: INFO: (13) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 21.573685ms)
  Jun 17 13:17:25.583: INFO: (13) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 21.177389ms)
  Jun 17 13:17:25.583: INFO: (13) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 21.410926ms)
  Jun 17 13:17:25.594: INFO: (14) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 10.441251ms)
  Jun 17 13:17:25.596: INFO: (14) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 11.517088ms)
  Jun 17 13:17:25.596: INFO: (14) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 11.567235ms)
  Jun 17 13:17:25.596: INFO: (14) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 11.829815ms)
  Jun 17 13:17:25.599: INFO: (14) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 15.311129ms)
  Jun 17 13:17:25.599: INFO: (14) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 15.447575ms)
  Jun 17 13:17:25.601: INFO: (14) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 16.400362ms)
  Jun 17 13:17:25.601: INFO: (14) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 16.468332ms)
  Jun 17 13:17:25.601: INFO: (14) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 16.717806ms)
  Jun 17 13:17:25.601: INFO: (14) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 16.937195ms)
  Jun 17 13:17:25.601: INFO: (14) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 17.228297ms)
  Jun 17 13:17:25.601: INFO: (14) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 16.818475ms)
  Jun 17 13:17:25.601: INFO: (14) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 17.067383ms)
  Jun 17 13:17:25.603: INFO: (14) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 18.509211ms)
  Jun 17 13:17:25.603: INFO: (14) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 18.705481ms)
  Jun 17 13:17:25.603: INFO: (14) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 18.936952ms)
  Jun 17 13:17:25.612: INFO: (15) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 8.91236ms)
  Jun 17 13:17:25.612: INFO: (15) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 8.489057ms)
  Jun 17 13:17:25.614: INFO: (15) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 10.994415ms)
  Jun 17 13:17:25.615: INFO: (15) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 11.20419ms)
  Jun 17 13:17:25.616: INFO: (15) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 13.053304ms)
  Jun 17 13:17:25.617: INFO: (15) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 13.206064ms)
  Jun 17 13:17:25.618: INFO: (15) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 13.923497ms)
  Jun 17 13:17:25.618: INFO: (15) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 13.370737ms)
  Jun 17 13:17:25.619: INFO: (15) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 15.251482ms)
  Jun 17 13:17:25.619: INFO: (15) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 15.339573ms)
  Jun 17 13:17:25.620: INFO: (15) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 16.204185ms)
  Jun 17 13:17:25.621: INFO: (15) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 16.638595ms)
  Jun 17 13:17:25.621: INFO: (15) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 17.872219ms)
  Jun 17 13:17:25.622: INFO: (15) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 18.005736ms)
  Jun 17 13:17:25.622: INFO: (15) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 19.121918ms)
  Jun 17 13:17:25.624: INFO: (15) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 20.201484ms)
  Jun 17 13:17:25.633: INFO: (16) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 8.401047ms)
  Jun 17 13:17:25.634: INFO: (16) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 9.832772ms)
  Jun 17 13:17:25.636: INFO: (16) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 11.331216ms)
  Jun 17 13:17:25.637: INFO: (16) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 12.561118ms)
  Jun 17 13:17:25.638: INFO: (16) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 13.510567ms)
  Jun 17 13:17:25.638: INFO: (16) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 13.573903ms)
  Jun 17 13:17:25.639: INFO: (16) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 14.734517ms)
  Jun 17 13:17:25.639: INFO: (16) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 14.993723ms)
  Jun 17 13:17:25.639: INFO: (16) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 15.140228ms)
  Jun 17 13:17:25.641: INFO: (16) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 16.411081ms)
  Jun 17 13:17:25.641: INFO: (16) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 16.889078ms)
  Jun 17 13:17:25.642: INFO: (16) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 17.252002ms)
  Jun 17 13:17:25.642: INFO: (16) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 17.481717ms)
  Jun 17 13:17:25.642: INFO: (16) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 17.532359ms)
  Jun 17 13:17:25.642: INFO: (16) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 17.808618ms)
  Jun 17 13:17:25.642: INFO: (16) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 17.585112ms)
  E0617 13:17:25.645992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:17:25.649: INFO: (17) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 5.121921ms)
  Jun 17 13:17:25.650: INFO: (17) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 5.766144ms)
  Jun 17 13:17:25.660: INFO: (17) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 15.797652ms)
  Jun 17 13:17:25.660: INFO: (17) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 15.153502ms)
  Jun 17 13:17:25.660: INFO: (17) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 15.329745ms)
  Jun 17 13:17:25.660: INFO: (17) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 15.082294ms)
  Jun 17 13:17:25.661: INFO: (17) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 16.010768ms)
  Jun 17 13:17:25.661: INFO: (17) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 16.363075ms)
  Jun 17 13:17:25.661: INFO: (17) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 15.654704ms)
  Jun 17 13:17:25.661: INFO: (17) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 16.26333ms)
  Jun 17 13:17:25.661: INFO: (17) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 16.384261ms)
  Jun 17 13:17:25.661: INFO: (17) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 16.944415ms)
  Jun 17 13:17:25.662: INFO: (17) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 17.069831ms)
  Jun 17 13:17:25.662: INFO: (17) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 17.364849ms)
  Jun 17 13:17:25.663: INFO: (17) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 17.19346ms)
  Jun 17 13:17:25.663: INFO: (17) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 18.010677ms)
  Jun 17 13:17:25.673: INFO: (18) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 9.984357ms)
  Jun 17 13:17:25.674: INFO: (18) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 10.64586ms)
  Jun 17 13:17:25.674: INFO: (18) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 10.908999ms)
  Jun 17 13:17:25.674: INFO: (18) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 11.195421ms)
  Jun 17 13:17:25.674: INFO: (18) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 10.999772ms)
  Jun 17 13:17:25.674: INFO: (18) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 11.501385ms)
  Jun 17 13:17:25.675: INFO: (18) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 11.483916ms)
  Jun 17 13:17:25.675: INFO: (18) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 11.366769ms)
  Jun 17 13:17:25.675: INFO: (18) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 11.371752ms)
  Jun 17 13:17:25.675: INFO: (18) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 12.272607ms)
  Jun 17 13:17:25.676: INFO: (18) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 12.39308ms)
  Jun 17 13:17:25.676: INFO: (18) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 12.441957ms)
  Jun 17 13:17:25.676: INFO: (18) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 12.895538ms)
  Jun 17 13:17:25.676: INFO: (18) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 12.924309ms)
  Jun 17 13:17:25.677: INFO: (18) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 13.346508ms)
  Jun 17 13:17:25.677: INFO: (18) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 13.813665ms)
  Jun 17 13:17:25.681: INFO: (19) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 3.761234ms)
  Jun 17 13:17:25.683: INFO: (19) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:443/proxy/tlsrewritem... (200; 5.33208ms)
  Jun 17 13:17:25.683: INFO: (19) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">... (200; 5.528524ms)
  Jun 17 13:17:25.684: INFO: (19) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:1080/proxy/rewriteme">test<... (200; 6.250289ms)
  Jun 17 13:17:25.684: INFO: (19) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:460/proxy/: tls baz (200; 6.319883ms)
  Jun 17 13:17:25.685: INFO: (19) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname1/proxy/: foo (200; 7.139991ms)
  Jun 17 13:17:25.685: INFO: (19) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs/proxy/rewriteme">test</a> (200; 7.326271ms)
  Jun 17 13:17:25.686: INFO: (19) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname1/proxy/: tls baz (200; 8.563759ms)
  Jun 17 13:17:25.686: INFO: (19) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:162/proxy/: bar (200; 8.406686ms)
  Jun 17 13:17:25.686: INFO: (19) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-8qql5-2q9qs:462/proxy/: tls qux (200; 8.33622ms)
  Jun 17 13:17:25.687: INFO: (19) /api/v1/namespaces/proxy-6151/services/http:proxy-service-8qql5:portname2/proxy/: bar (200; 8.695641ms)
  Jun 17 13:17:25.687: INFO: (19) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname1/proxy/: foo (200; 9.245899ms)
  Jun 17 13:17:25.687: INFO: (19) /api/v1/namespaces/proxy-6151/pods/proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 9.187213ms)
  Jun 17 13:17:25.687: INFO: (19) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-8qql5-2q9qs:160/proxy/: foo (200; 9.599111ms)
  Jun 17 13:17:25.688: INFO: (19) /api/v1/namespaces/proxy-6151/services/proxy-service-8qql5:portname2/proxy/: bar (200; 10.08627ms)
  Jun 17 13:17:25.688: INFO: (19) /api/v1/namespaces/proxy-6151/services/https:proxy-service-8qql5:tlsportname2/proxy/: tls qux (200; 10.247866ms)
  Jun 17 13:17:25.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-8qql5 in namespace proxy-6151, will wait for the garbage collector to delete the pods @ 06/17/23 13:17:25.692
  Jun 17 13:17:25.751: INFO: Deleting ReplicationController proxy-service-8qql5 took: 5.365427ms
  Jun 17 13:17:25.851: INFO: Terminating ReplicationController proxy-service-8qql5 pods took: 100.769884ms
  E0617 13:17:26.646319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-6151" for this suite. @ 06/17/23 13:17:27.552
• [5.293 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 06/17/23 13:17:27.563
  Jun 17 13:17:27.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename containers @ 06/17/23 13:17:27.563
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:17:27.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:17:27.583
  STEP: Creating a pod to test override command @ 06/17/23 13:17:27.586
  E0617 13:17:27.646873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:28.647222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:29.647837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:30.648618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:17:31.607
  Jun 17 13:17:31.610: INFO: Trying to get logs from node ip-172-31-25-17 pod client-containers-a89307a5-2f2e-4c4b-8292-bda4ffef8369 container agnhost-container: <nil>
  STEP: delete the pod @ 06/17/23 13:17:31.616
  Jun 17 13:17:31.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6243" for this suite. @ 06/17/23 13:17:31.635
• [4.078 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 06/17/23 13:17:31.641
  Jun 17 13:17:31.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename gc @ 06/17/23 13:17:31.642
  E0617 13:17:31.649360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:17:31.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:17:31.665
  STEP: create the rc @ 06/17/23 13:17:31.671
  W0617 13:17:31.676729      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0617 13:17:32.649719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:33.649847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:34.650232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:35.654874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:36.655937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:37.656026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 06/17/23 13:17:37.681
  STEP: wait for the rc to be deleted @ 06/17/23 13:17:37.687
  E0617 13:17:38.656625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:39.657940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:40.658038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:41.660230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:42.659186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 06/17/23 13:17:42.692
  E0617 13:17:43.661385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:44.661412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:45.661626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:46.661726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:47.662285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:48.662468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:49.662660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:50.662785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:51.662840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:52.662982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:53.663331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:54.663420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:55.664239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:56.664483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:57.664685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:58.664903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:17:59.665074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:00.665276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:01.665711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:02.665928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:03.666178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:04.666361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:05.666567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:06.667148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:07.667247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:08.667881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:09.668645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:10.668650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:11.669082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:12.669273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 06/17/23 13:18:12.704
  W0617 13:18:12.708074      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 17 13:18:12.708: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 17 13:18:12.708: INFO: Deleting pod "simpletest.rc-2d692" in namespace "gc-1089"
  Jun 17 13:18:12.720: INFO: Deleting pod "simpletest.rc-2mf8p" in namespace "gc-1089"
  Jun 17 13:18:12.732: INFO: Deleting pod "simpletest.rc-4db5m" in namespace "gc-1089"
  Jun 17 13:18:12.745: INFO: Deleting pod "simpletest.rc-4j952" in namespace "gc-1089"
  Jun 17 13:18:12.755: INFO: Deleting pod "simpletest.rc-4kk8d" in namespace "gc-1089"
  Jun 17 13:18:12.768: INFO: Deleting pod "simpletest.rc-4qc72" in namespace "gc-1089"
  Jun 17 13:18:12.781: INFO: Deleting pod "simpletest.rc-5ncfk" in namespace "gc-1089"
  Jun 17 13:18:12.793: INFO: Deleting pod "simpletest.rc-6d4rn" in namespace "gc-1089"
  Jun 17 13:18:12.806: INFO: Deleting pod "simpletest.rc-6f2cm" in namespace "gc-1089"
  Jun 17 13:18:12.817: INFO: Deleting pod "simpletest.rc-6j9hq" in namespace "gc-1089"
  Jun 17 13:18:12.834: INFO: Deleting pod "simpletest.rc-6mg4t" in namespace "gc-1089"
  Jun 17 13:18:12.851: INFO: Deleting pod "simpletest.rc-6qzcg" in namespace "gc-1089"
  Jun 17 13:18:12.862: INFO: Deleting pod "simpletest.rc-7j6gc" in namespace "gc-1089"
  Jun 17 13:18:12.879: INFO: Deleting pod "simpletest.rc-7pkc7" in namespace "gc-1089"
  Jun 17 13:18:12.892: INFO: Deleting pod "simpletest.rc-7z9r6" in namespace "gc-1089"
  Jun 17 13:18:12.906: INFO: Deleting pod "simpletest.rc-8rb7b" in namespace "gc-1089"
  Jun 17 13:18:12.921: INFO: Deleting pod "simpletest.rc-8sl66" in namespace "gc-1089"
  Jun 17 13:18:12.936: INFO: Deleting pod "simpletest.rc-9drl9" in namespace "gc-1089"
  Jun 17 13:18:12.946: INFO: Deleting pod "simpletest.rc-9ksc8" in namespace "gc-1089"
  Jun 17 13:18:12.958: INFO: Deleting pod "simpletest.rc-9mkph" in namespace "gc-1089"
  Jun 17 13:18:12.969: INFO: Deleting pod "simpletest.rc-9sx4d" in namespace "gc-1089"
  Jun 17 13:18:12.981: INFO: Deleting pod "simpletest.rc-bdrkl" in namespace "gc-1089"
  Jun 17 13:18:12.995: INFO: Deleting pod "simpletest.rc-bfrpx" in namespace "gc-1089"
  Jun 17 13:18:13.007: INFO: Deleting pod "simpletest.rc-bhz54" in namespace "gc-1089"
  Jun 17 13:18:13.018: INFO: Deleting pod "simpletest.rc-btc8s" in namespace "gc-1089"
  Jun 17 13:18:13.031: INFO: Deleting pod "simpletest.rc-bwt5x" in namespace "gc-1089"
  Jun 17 13:18:13.051: INFO: Deleting pod "simpletest.rc-c727s" in namespace "gc-1089"
  Jun 17 13:18:13.064: INFO: Deleting pod "simpletest.rc-c97cn" in namespace "gc-1089"
  Jun 17 13:18:13.082: INFO: Deleting pod "simpletest.rc-cpx8n" in namespace "gc-1089"
  Jun 17 13:18:13.095: INFO: Deleting pod "simpletest.rc-dnwjw" in namespace "gc-1089"
  Jun 17 13:18:13.106: INFO: Deleting pod "simpletest.rc-dpqcd" in namespace "gc-1089"
  Jun 17 13:18:13.119: INFO: Deleting pod "simpletest.rc-dqxbr" in namespace "gc-1089"
  Jun 17 13:18:13.133: INFO: Deleting pod "simpletest.rc-f57th" in namespace "gc-1089"
  Jun 17 13:18:13.144: INFO: Deleting pod "simpletest.rc-fj5cc" in namespace "gc-1089"
  Jun 17 13:18:13.161: INFO: Deleting pod "simpletest.rc-fp5wd" in namespace "gc-1089"
  Jun 17 13:18:13.174: INFO: Deleting pod "simpletest.rc-fr7nq" in namespace "gc-1089"
  Jun 17 13:18:13.185: INFO: Deleting pod "simpletest.rc-g7zq7" in namespace "gc-1089"
  Jun 17 13:18:13.198: INFO: Deleting pod "simpletest.rc-gnnh8" in namespace "gc-1089"
  Jun 17 13:18:13.208: INFO: Deleting pod "simpletest.rc-gqdgl" in namespace "gc-1089"
  Jun 17 13:18:13.220: INFO: Deleting pod "simpletest.rc-gwsjf" in namespace "gc-1089"
  Jun 17 13:18:13.233: INFO: Deleting pod "simpletest.rc-hdxcz" in namespace "gc-1089"
  Jun 17 13:18:13.244: INFO: Deleting pod "simpletest.rc-hkhlf" in namespace "gc-1089"
  Jun 17 13:18:13.260: INFO: Deleting pod "simpletest.rc-hm4qb" in namespace "gc-1089"
  Jun 17 13:18:13.272: INFO: Deleting pod "simpletest.rc-hqhfl" in namespace "gc-1089"
  Jun 17 13:18:13.287: INFO: Deleting pod "simpletest.rc-hr2x5" in namespace "gc-1089"
  Jun 17 13:18:13.301: INFO: Deleting pod "simpletest.rc-hvcwq" in namespace "gc-1089"
  Jun 17 13:18:13.312: INFO: Deleting pod "simpletest.rc-hxjq5" in namespace "gc-1089"
  Jun 17 13:18:13.327: INFO: Deleting pod "simpletest.rc-k67tp" in namespace "gc-1089"
  Jun 17 13:18:13.338: INFO: Deleting pod "simpletest.rc-k8729" in namespace "gc-1089"
  Jun 17 13:18:13.353: INFO: Deleting pod "simpletest.rc-kkvhz" in namespace "gc-1089"
  Jun 17 13:18:13.367: INFO: Deleting pod "simpletest.rc-krh2c" in namespace "gc-1089"
  Jun 17 13:18:13.378: INFO: Deleting pod "simpletest.rc-l2nmk" in namespace "gc-1089"
  Jun 17 13:18:13.392: INFO: Deleting pod "simpletest.rc-lv2hd" in namespace "gc-1089"
  Jun 17 13:18:13.403: INFO: Deleting pod "simpletest.rc-mh5gl" in namespace "gc-1089"
  Jun 17 13:18:13.413: INFO: Deleting pod "simpletest.rc-ntjrb" in namespace "gc-1089"
  Jun 17 13:18:13.425: INFO: Deleting pod "simpletest.rc-pj4p2" in namespace "gc-1089"
  Jun 17 13:18:13.435: INFO: Deleting pod "simpletest.rc-pjgh5" in namespace "gc-1089"
  Jun 17 13:18:13.449: INFO: Deleting pod "simpletest.rc-pnpzw" in namespace "gc-1089"
  Jun 17 13:18:13.460: INFO: Deleting pod "simpletest.rc-pnzlx" in namespace "gc-1089"
  Jun 17 13:18:13.472: INFO: Deleting pod "simpletest.rc-pr2n5" in namespace "gc-1089"
  Jun 17 13:18:13.485: INFO: Deleting pod "simpletest.rc-pvkgg" in namespace "gc-1089"
  Jun 17 13:18:13.496: INFO: Deleting pod "simpletest.rc-qmfsv" in namespace "gc-1089"
  Jun 17 13:18:13.509: INFO: Deleting pod "simpletest.rc-qmtnr" in namespace "gc-1089"
  Jun 17 13:18:13.522: INFO: Deleting pod "simpletest.rc-qp98t" in namespace "gc-1089"
  Jun 17 13:18:13.552: INFO: Deleting pod "simpletest.rc-qpg6n" in namespace "gc-1089"
  Jun 17 13:18:13.562: INFO: Deleting pod "simpletest.rc-qrrs4" in namespace "gc-1089"
  Jun 17 13:18:13.605: INFO: Deleting pod "simpletest.rc-r985b" in namespace "gc-1089"
  Jun 17 13:18:13.657: INFO: Deleting pod "simpletest.rc-rbj69" in namespace "gc-1089"
  E0617 13:18:13.669599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:18:13.707: INFO: Deleting pod "simpletest.rc-rc2b5" in namespace "gc-1089"
  Jun 17 13:18:13.757: INFO: Deleting pod "simpletest.rc-rcmkr" in namespace "gc-1089"
  Jun 17 13:18:13.807: INFO: Deleting pod "simpletest.rc-rhb89" in namespace "gc-1089"
  Jun 17 13:18:13.856: INFO: Deleting pod "simpletest.rc-rr4hn" in namespace "gc-1089"
  Jun 17 13:18:13.908: INFO: Deleting pod "simpletest.rc-rvgww" in namespace "gc-1089"
  Jun 17 13:18:13.954: INFO: Deleting pod "simpletest.rc-s2r8l" in namespace "gc-1089"
  Jun 17 13:18:14.002: INFO: Deleting pod "simpletest.rc-s9bmg" in namespace "gc-1089"
  Jun 17 13:18:14.055: INFO: Deleting pod "simpletest.rc-sj99b" in namespace "gc-1089"
  Jun 17 13:18:14.103: INFO: Deleting pod "simpletest.rc-sscb5" in namespace "gc-1089"
  Jun 17 13:18:14.155: INFO: Deleting pod "simpletest.rc-ssvqk" in namespace "gc-1089"
  Jun 17 13:18:14.206: INFO: Deleting pod "simpletest.rc-t92fs" in namespace "gc-1089"
  Jun 17 13:18:14.256: INFO: Deleting pod "simpletest.rc-t95bz" in namespace "gc-1089"
  Jun 17 13:18:14.308: INFO: Deleting pod "simpletest.rc-tpz9r" in namespace "gc-1089"
  Jun 17 13:18:14.365: INFO: Deleting pod "simpletest.rc-tzrcc" in namespace "gc-1089"
  Jun 17 13:18:14.408: INFO: Deleting pod "simpletest.rc-v454g" in namespace "gc-1089"
  Jun 17 13:18:14.456: INFO: Deleting pod "simpletest.rc-w5hl2" in namespace "gc-1089"
  Jun 17 13:18:14.508: INFO: Deleting pod "simpletest.rc-wjzkv" in namespace "gc-1089"
  Jun 17 13:18:14.555: INFO: Deleting pod "simpletest.rc-wvjq4" in namespace "gc-1089"
  Jun 17 13:18:14.607: INFO: Deleting pod "simpletest.rc-wx8mc" in namespace "gc-1089"
  Jun 17 13:18:14.658: INFO: Deleting pod "simpletest.rc-wxr59" in namespace "gc-1089"
  E0617 13:18:14.670618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:18:14.709: INFO: Deleting pod "simpletest.rc-wz9kq" in namespace "gc-1089"
  Jun 17 13:18:14.753: INFO: Deleting pod "simpletest.rc-wzbg4" in namespace "gc-1089"
  Jun 17 13:18:14.806: INFO: Deleting pod "simpletest.rc-x9vx6" in namespace "gc-1089"
  Jun 17 13:18:14.864: INFO: Deleting pod "simpletest.rc-xmkv6" in namespace "gc-1089"
  Jun 17 13:18:14.906: INFO: Deleting pod "simpletest.rc-xnchv" in namespace "gc-1089"
  Jun 17 13:18:14.959: INFO: Deleting pod "simpletest.rc-xnmr6" in namespace "gc-1089"
  Jun 17 13:18:15.004: INFO: Deleting pod "simpletest.rc-z2lqp" in namespace "gc-1089"
  Jun 17 13:18:15.054: INFO: Deleting pod "simpletest.rc-z7fp5" in namespace "gc-1089"
  Jun 17 13:18:15.104: INFO: Deleting pod "simpletest.rc-zkbnx" in namespace "gc-1089"
  Jun 17 13:18:15.156: INFO: Deleting pod "simpletest.rc-zp5mn" in namespace "gc-1089"
  Jun 17 13:18:15.202: INFO: Deleting pod "simpletest.rc-zq4jg" in namespace "gc-1089"
  Jun 17 13:18:15.256: INFO: Deleting pod "simpletest.rc-zwl8s" in namespace "gc-1089"
  Jun 17 13:18:15.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1089" for this suite. @ 06/17/23 13:18:15.348
• [43.759 seconds]
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 06/17/23 13:18:15.401
  Jun 17 13:18:15.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename pod-network-test @ 06/17/23 13:18:15.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:18:15.42
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:18:15.423
  STEP: Performing setup for networking test in namespace pod-network-test-7119 @ 06/17/23 13:18:15.427
  STEP: creating a selector @ 06/17/23 13:18:15.427
  STEP: Creating the service pods in kubernetes @ 06/17/23 13:18:15.427
  Jun 17 13:18:15.427: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0617 13:18:15.671215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:16.671603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:17.672343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:18.672462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:19.672945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:20.673086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:21.673723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:22.675219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:23.675725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:24.675767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:25.676381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:26.677137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:27.677709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:28.677987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:29.678304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:30.678395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:31.678806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:32.678838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:33.679893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:34.679995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:35.680762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:36.680804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 06/17/23 13:18:37.518
  E0617 13:18:37.681076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:38.681188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:18:39.549: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jun 17 13:18:39.549: INFO: Going to poll 192.168.178.171 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jun 17 13:18:39.552: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.178.171:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7119 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 13:18:39.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:18:39.552: INFO: ExecWithOptions: Clientset creation
  Jun 17 13:18:39.552: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-7119/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.178.171%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun 17 13:18:39.617: INFO: Found all 1 expected endpoints: [netserver-0]
  Jun 17 13:18:39.617: INFO: Going to poll 192.168.91.173 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jun 17 13:18:39.620: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.91.173:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7119 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 13:18:39.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:18:39.621: INFO: ExecWithOptions: Clientset creation
  Jun 17 13:18:39.621: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-7119/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.91.173%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0617 13:18:39.681867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:18:39.690: INFO: Found all 1 expected endpoints: [netserver-1]
  Jun 17 13:18:39.690: INFO: Going to poll 192.168.2.68 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jun 17 13:18:39.693: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.2.68:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7119 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 13:18:39.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:18:39.694: INFO: ExecWithOptions: Clientset creation
  Jun 17 13:18:39.694: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-7119/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.2.68%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun 17 13:18:39.748: INFO: Found all 1 expected endpoints: [netserver-2]
  Jun 17 13:18:39.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7119" for this suite. @ 06/17/23 13:18:39.752
• [24.357 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 06/17/23 13:18:39.758
  Jun 17 13:18:39.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-probe @ 06/17/23 13:18:39.759
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:18:39.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:18:39.781
  STEP: Creating pod liveness-0f6f77c9-f64a-4985-b761-11d49dee92e3 in namespace container-probe-2757 @ 06/17/23 13:18:39.784
  E0617 13:18:40.681974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:41.682045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:18:41.799: INFO: Started pod liveness-0f6f77c9-f64a-4985-b761-11d49dee92e3 in namespace container-probe-2757
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/17/23 13:18:41.799
  Jun 17 13:18:41.803: INFO: Initial restart count of pod liveness-0f6f77c9-f64a-4985-b761-11d49dee92e3 is 0
  E0617 13:18:42.682152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:43.682244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:44.682347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:45.682439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:46.682842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:47.682923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:48.683020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:49.683905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:50.684754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:51.685182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:52.685479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:53.685515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:54.686460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:55.686579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:56.686882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:57.687915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:58.688634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:18:59.688993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:00.689833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:01.690242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:19:01.847: INFO: Restart count of pod container-probe-2757/liveness-0f6f77c9-f64a-4985-b761-11d49dee92e3 is now 1 (20.044051517s elapsed)
  E0617 13:19:02.691142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:03.691242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:04.691332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:05.691411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:06.691523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:07.691561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:08.691970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:09.692797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:10.693416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:11.693827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:12.693914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:13.694141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:14.694383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:15.694593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:16.694803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:17.694911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:18.695875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:19.696936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:20.697044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:21.697115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:19:21.887: INFO: Restart count of pod container-probe-2757/liveness-0f6f77c9-f64a-4985-b761-11d49dee92e3 is now 2 (40.084508208s elapsed)
  E0617 13:19:22.697534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:23.697748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:24.697846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:25.698051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:26.698721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:27.698818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:28.699648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:29.699843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:30.700556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:31.701445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:32.701538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:33.701878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:34.702039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:35.702313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:36.702355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:37.702547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:38.703024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:39.703874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:40.704531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:41.704936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:19:41.927: INFO: Restart count of pod container-probe-2757/liveness-0f6f77c9-f64a-4985-b761-11d49dee92e3 is now 3 (1m0.12437223s elapsed)
  E0617 13:19:42.705243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:43.705341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:44.706393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:45.706605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:46.706807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:47.706894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:48.707866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:49.708047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:50.708131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:51.708474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:52.708573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:53.708768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:54.709074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:55.709282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:56.709780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:57.710712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:58.710811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:19:59.710918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:00.711672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:01.711760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:20:01.965: INFO: Restart count of pod container-probe-2757/liveness-0f6f77c9-f64a-4985-b761-11d49dee92e3 is now 4 (1m20.162016664s elapsed)
  E0617 13:20:02.712086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:03.712286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:04.712710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:05.712799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:06.713537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:07.713742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:08.714655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:09.715007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:10.715948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:11.716598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:12.717635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:13.717747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:14.717832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:15.718107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:16.718198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:17.718351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:18.718738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:19.718812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:20.718909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:21.719868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:22.720269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:23.720375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:24.720428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:25.720575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:26.720671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:27.721437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:28.721543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:29.722421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:30.722835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:31.723707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:32.724772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:33.724870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:34.725725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:35.726799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:36.726830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:37.727873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:38.728573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:39.729421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:40.729998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:41.730237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:42.731279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:43.731870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:44.732415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:45.732512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:46.733033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:47.733219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:48.733320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:49.733494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:50.733590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:51.733684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:52.734158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:53.734362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:54.734637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:55.734810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:56.735736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:57.735962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:58.736336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:20:59.736525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:00.737312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:01.738114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:02.738356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:03.738447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:04.739337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:05.739429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:06.739870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:07.740854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:08.740939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:09.741599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:10.742454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:11.742798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:12.742892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:13.742997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:14.743875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:15.744067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:21:16.120: INFO: Restart count of pod container-probe-2757/liveness-0f6f77c9-f64a-4985-b761-11d49dee92e3 is now 5 (2m34.31769818s elapsed)
  Jun 17 13:21:16.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/17/23 13:21:16.124
  STEP: Destroying namespace "container-probe-2757" for this suite. @ 06/17/23 13:21:16.138
• [156.387 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 06/17/23 13:21:16.148
  Jun 17 13:21:16.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-runtime @ 06/17/23 13:21:16.149
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:21:16.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:21:16.172
  STEP: create the container @ 06/17/23 13:21:16.175
  W0617 13:21:16.183244      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 06/17/23 13:21:16.183
  E0617 13:21:16.746890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:17.747281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:18.747388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 06/17/23 13:21:19.201
  STEP: the container should be terminated @ 06/17/23 13:21:19.204
  STEP: the termination message should be set @ 06/17/23 13:21:19.204
  Jun 17 13:21:19.204: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 06/17/23 13:21:19.204
  Jun 17 13:21:19.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9958" for this suite. @ 06/17/23 13:21:19.222
• [3.081 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 06/17/23 13:21:19.229
  Jun 17 13:21:19.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl-logs @ 06/17/23 13:21:19.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:21:19.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:21:19.25
  STEP: creating an pod @ 06/17/23 13:21:19.252
  Jun 17 13:21:19.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-logs-4564 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Jun 17 13:21:19.324: INFO: stderr: ""
  Jun 17 13:21:19.324: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 06/17/23 13:21:19.324
  Jun 17 13:21:19.324: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0617 13:21:19.747889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:20.747997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:21:21.331: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 06/17/23 13:21:21.331
  Jun 17 13:21:21.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-logs-4564 logs logs-generator logs-generator'
  Jun 17 13:21:21.412: INFO: stderr: ""
  Jun 17 13:21:21.412: INFO: stdout: "I0617 13:21:20.033546       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/n5tp 213\nI0617 13:21:20.233618       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/88vg 384\nI0617 13:21:20.434188       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/6cd 524\nI0617 13:21:20.634490       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/ndm5 566\nI0617 13:21:20.833635       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/r8k 293\nI0617 13:21:21.033931       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/cvg 251\nI0617 13:21:21.234217       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/6q4 542\n"
  STEP: limiting log lines @ 06/17/23 13:21:21.412
  Jun 17 13:21:21.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-logs-4564 logs logs-generator logs-generator --tail=1'
  Jun 17 13:21:21.483: INFO: stderr: ""
  Jun 17 13:21:21.483: INFO: stdout: "I0617 13:21:21.434508       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/nqbv 231\n"
  Jun 17 13:21:21.483: INFO: got output "I0617 13:21:21.434508       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/nqbv 231\n"
  STEP: limiting log bytes @ 06/17/23 13:21:21.483
  Jun 17 13:21:21.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-logs-4564 logs logs-generator logs-generator --limit-bytes=1'
  Jun 17 13:21:21.553: INFO: stderr: ""
  Jun 17 13:21:21.553: INFO: stdout: "I"
  Jun 17 13:21:21.553: INFO: got output "I"
  STEP: exposing timestamps @ 06/17/23 13:21:21.553
  Jun 17 13:21:21.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-logs-4564 logs logs-generator logs-generator --tail=1 --timestamps'
  Jun 17 13:21:21.636: INFO: stderr: ""
  Jun 17 13:21:21.636: INFO: stdout: "2023-06-17T13:21:21.434590397Z I0617 13:21:21.434508       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/nqbv 231\n"
  Jun 17 13:21:21.636: INFO: got output "2023-06-17T13:21:21.434590397Z I0617 13:21:21.434508       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/nqbv 231\n"
  STEP: restricting to a time range @ 06/17/23 13:21:21.636
  E0617 13:21:21.748821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:22.749753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:23.749930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:21:24.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-logs-4564 logs logs-generator logs-generator --since=1s'
  Jun 17 13:21:24.208: INFO: stderr: ""
  Jun 17 13:21:24.208: INFO: stdout: "I0617 13:21:23.233627       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/wjsh 272\nI0617 13:21:23.433915       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/d2x 576\nI0617 13:21:23.634205       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/5bp 272\nI0617 13:21:23.834495       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/8mf 323\nI0617 13:21:24.033722       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/6x4d 422\n"
  Jun 17 13:21:24.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-logs-4564 logs logs-generator logs-generator --since=24h'
  Jun 17 13:21:24.280: INFO: stderr: ""
  Jun 17 13:21:24.280: INFO: stdout: "I0617 13:21:20.033546       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/n5tp 213\nI0617 13:21:20.233618       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/88vg 384\nI0617 13:21:20.434188       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/6cd 524\nI0617 13:21:20.634490       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/ndm5 566\nI0617 13:21:20.833635       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/r8k 293\nI0617 13:21:21.033931       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/cvg 251\nI0617 13:21:21.234217       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/6q4 542\nI0617 13:21:21.434508       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/nqbv 231\nI0617 13:21:21.634621       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/skn 477\nI0617 13:21:21.833908       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/cmjb 240\nI0617 13:21:22.034199       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/7ztd 207\nI0617 13:21:22.234494       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/6pgh 496\nI0617 13:21:22.433638       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/zzh 416\nI0617 13:21:22.633930       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/lmhg 477\nI0617 13:21:22.834112       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/n7mk 488\nI0617 13:21:23.034397       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/z2kw 556\nI0617 13:21:23.233627       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/wjsh 272\nI0617 13:21:23.433915       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/d2x 576\nI0617 13:21:23.634205       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/5bp 272\nI0617 13:21:23.834495       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/8mf 323\nI0617 13:21:24.033722       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/6x4d 422\nI0617 13:21:24.234013       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/8l5 293\n"
  Jun 17 13:21:24.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-logs-4564 delete pod logs-generator'
  E0617 13:21:24.750038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:21:24.865: INFO: stderr: ""
  Jun 17 13:21:24.865: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Jun 17 13:21:24.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-4564" for this suite. @ 06/17/23 13:21:24.869
• [5.645 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 06/17/23 13:21:24.875
  Jun 17 13:21:24.875: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename emptydir @ 06/17/23 13:21:24.876
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:21:24.894
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:21:24.897
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 06/17/23 13:21:24.899
  E0617 13:21:25.750136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:26.750208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:27.751189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:28.751962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:21:28.921
  Jun 17 13:21:28.925: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-1ac80d35-e805-42f7-a888-ad99f4c9823d container test-container: <nil>
  STEP: delete the pod @ 06/17/23 13:21:28.931
  Jun 17 13:21:28.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7589" for this suite. @ 06/17/23 13:21:28.949
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 06/17/23 13:21:28.956
  Jun 17 13:21:28.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename downward-api @ 06/17/23 13:21:28.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:21:28.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:21:28.978
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 13:21:28.981
  E0617 13:21:29.752896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:30.753109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:31.753616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:32.753819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:21:33
  Jun 17 13:21:33.004: INFO: Trying to get logs from node ip-172-31-25-17 pod downwardapi-volume-a8fac1c3-211d-447a-91e0-0dee1ddfd383 container client-container: <nil>
  STEP: delete the pod @ 06/17/23 13:21:33.009
  Jun 17 13:21:33.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7598" for this suite. @ 06/17/23 13:21:33.028
• [4.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 06/17/23 13:21:33.035
  Jun 17 13:21:33.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 13:21:33.036
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:21:33.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:21:33.056
  STEP: Creating projection with secret that has name projected-secret-test-map-e9c5dd2f-5302-4065-8f40-730dd32d6ab5 @ 06/17/23 13:21:33.06
  STEP: Creating a pod to test consume secrets @ 06/17/23 13:21:33.064
  E0617 13:21:33.753912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:34.754153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:35.754779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:36.754835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:21:37.087
  Jun 17 13:21:37.090: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-projected-secrets-c889fa81-ccc5-42c6-919e-d5ac1a1f32d9 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/17/23 13:21:37.096
  Jun 17 13:21:37.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7642" for this suite. @ 06/17/23 13:21:37.115
• [4.086 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 06/17/23 13:21:37.121
  Jun 17 13:21:37.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename cronjob @ 06/17/23 13:21:37.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:21:37.143
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:21:37.145
  STEP: Creating a cronjob @ 06/17/23 13:21:37.149
  STEP: Ensuring more than one job is running at a time @ 06/17/23 13:21:37.153
  E0617 13:21:37.755318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:38.755420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:39.755764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:40.755969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:41.756043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:42.756083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:43.756264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:44.757184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:45.757276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:46.757544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:47.757661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:48.757897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:49.758377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:50.758955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:51.760040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:52.760422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:53.760504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:54.760685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:55.760774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:56.761383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:57.761786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:58.762021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:21:59.762120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:00.762327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:01.763380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:02.763483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:03.763862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:04.764069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:05.764153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:06.764498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:07.764605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:08.764816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:09.765012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:10.765488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:11.765937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:12.766044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:13.766848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:14.766941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:15.767685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:16.768017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:17.768483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:18.768680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:19.769455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:20.769675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:21.770430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:22.771486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:23.772460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:24.772560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:25.773467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:26.773553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:27.774118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:28.774306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:29.774926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:30.774965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:31.775833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:32.776001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:33.776578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:34.776671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:35.776930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:36.777129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:37.777459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:38.778217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:39.778633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:40.779125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:41.779871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:42.779993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:43.780963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:44.781796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:45.782796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:46.783877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:47.784890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:48.785192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:49.786087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:50.786560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:51.786716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:52.786803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:53.787874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:54.787965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:55.788743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:56.788837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:57.788940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:58.789312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:22:59.789427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:00.790158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 06/17/23 13:23:01.157
  STEP: Removing cronjob @ 06/17/23 13:23:01.161
  Jun 17 13:23:01.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2861" for this suite. @ 06/17/23 13:23:01.17
• [84.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 06/17/23 13:23:01.178
  Jun 17 13:23:01.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-probe @ 06/17/23 13:23:01.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:23:01.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:23:01.204
  STEP: Creating pod liveness-efb922c9-c335-4385-844b-b64cae2cd5c1 in namespace container-probe-2986 @ 06/17/23 13:23:01.206
  E0617 13:23:01.794666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:02.794829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:23:03.224: INFO: Started pod liveness-efb922c9-c335-4385-844b-b64cae2cd5c1 in namespace container-probe-2986
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/17/23 13:23:03.224
  Jun 17 13:23:03.227: INFO: Initial restart count of pod liveness-efb922c9-c335-4385-844b-b64cae2cd5c1 is 0
  E0617 13:23:03.794920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:04.795887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:05.795945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:06.796189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:07.796890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:08.796986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:09.797068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:10.798015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:11.798829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:12.799898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:13.800977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:14.801160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:15.801766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:16.801963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:17.802948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:18.803934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:19.804009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:20.804444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:21.805145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:22.805251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:23:23.269: INFO: Restart count of pod container-probe-2986/liveness-efb922c9-c335-4385-844b-b64cae2cd5c1 is now 1 (20.042147901s elapsed)
  Jun 17 13:23:23.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/17/23 13:23:23.273
  STEP: Destroying namespace "container-probe-2986" for this suite. @ 06/17/23 13:23:23.286
• [22.113 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 06/17/23 13:23:23.291
  Jun 17 13:23:23.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename dns @ 06/17/23 13:23:23.292
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:23:23.311
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:23:23.313
  STEP: Creating a test headless service @ 06/17/23 13:23:23.316
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7258.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7258.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 06/17/23 13:23:23.32
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7258.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7258.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 06/17/23 13:23:23.32
  STEP: creating a pod to probe DNS @ 06/17/23 13:23:23.32
  STEP: submitting the pod to kubernetes @ 06/17/23 13:23:23.321
  E0617 13:23:23.805407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:24.805713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/17/23 13:23:25.338
  STEP: looking for the results for each expected name from probers @ 06/17/23 13:23:25.342
  Jun 17 13:23:25.357: INFO: DNS probes using dns-7258/dns-test-0c3f29a1-2df2-48a2-8473-7b197f8576c7 succeeded

  Jun 17 13:23:25.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/17/23 13:23:25.36
  STEP: deleting the test headless service @ 06/17/23 13:23:25.372
  STEP: Destroying namespace "dns-7258" for this suite. @ 06/17/23 13:23:25.383
• [2.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 06/17/23 13:23:25.39
  Jun 17 13:23:25.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename replication-controller @ 06/17/23 13:23:25.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:23:25.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:23:25.409
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 06/17/23 13:23:25.413
  E0617 13:23:25.806473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:26.806920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replication controller with a matching selector is created @ 06/17/23 13:23:27.431
  STEP: Then the orphan pod is adopted @ 06/17/23 13:23:27.437
  E0617 13:23:27.806954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:23:28.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3572" for this suite. @ 06/17/23 13:23:28.448
• [3.064 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 06/17/23 13:23:28.455
  Jun 17 13:23:28.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 13:23:28.455
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:23:28.473
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:23:28.475
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 13:23:28.478
  E0617 13:23:28.807650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:29.807722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:30.808787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:31.808879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:23:32.498
  Jun 17 13:23:32.501: INFO: Trying to get logs from node ip-172-31-86-18 pod downwardapi-volume-e47c1913-1e07-4ead-a086-cb2c1d2d23ff container client-container: <nil>
  STEP: delete the pod @ 06/17/23 13:23:32.517
  Jun 17 13:23:32.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2447" for this suite. @ 06/17/23 13:23:32.535
• [4.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 06/17/23 13:23:32.542
  Jun 17 13:23:32.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename configmap @ 06/17/23 13:23:32.543
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:23:32.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:23:32.564
  STEP: Creating configMap with name configmap-test-volume-d9b7ea7e-fcb8-4993-8d40-84dbe19ad413 @ 06/17/23 13:23:32.567
  STEP: Creating a pod to test consume configMaps @ 06/17/23 13:23:32.572
  E0617 13:23:32.808953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:33.809137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:34.809605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:35.809694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:23:36.59
  Jun 17 13:23:36.593: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-configmaps-bfeeb9c9-1e5c-4670-b34b-21553c14bf47 container agnhost-container: <nil>
  STEP: delete the pod @ 06/17/23 13:23:36.607
  Jun 17 13:23:36.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7556" for this suite. @ 06/17/23 13:23:36.626
• [4.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 06/17/23 13:23:36.636
  Jun 17 13:23:36.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename daemonsets @ 06/17/23 13:23:36.636
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:23:36.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:23:36.658
  Jun 17 13:23:36.687: INFO: Create a RollingUpdate DaemonSet
  Jun 17 13:23:36.691: INFO: Check that daemon pods launch on every node of the cluster
  Jun 17 13:23:36.695: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:23:36.695: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:23:36.699: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 13:23:36.699: INFO: Node ip-172-31-25-17 is running 0 daemon pod, expected 1
  E0617 13:23:36.810478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:23:37.703: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:23:37.703: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:23:37.706: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 17 13:23:37.706: INFO: Node ip-172-31-25-17 is running 0 daemon pod, expected 1
  E0617 13:23:37.811363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:23:38.703: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:23:38.703: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:23:38.706: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 17 13:23:38.706: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Jun 17 13:23:38.706: INFO: Update the DaemonSet to trigger a rollout
  Jun 17 13:23:38.713: INFO: Updating DaemonSet daemon-set
  E0617 13:23:38.811680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:39.812235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:23:40.727: INFO: Roll back the DaemonSet before rollout is complete
  Jun 17 13:23:40.737: INFO: Updating DaemonSet daemon-set
  Jun 17 13:23:40.737: INFO: Make sure DaemonSet rollback is complete
  Jun 17 13:23:40.740: INFO: Wrong image for pod: daemon-set-4ktz2. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Jun 17 13:23:40.740: INFO: Pod daemon-set-4ktz2 is not available
  Jun 17 13:23:40.743: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:23:40.743: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0617 13:23:40.812759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:23:41.751: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:23:41.751: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0617 13:23:41.813157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:23:42.747: INFO: Pod daemon-set-w9q6z is not available
  Jun 17 13:23:42.750: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:23:42.750: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 06/17/23 13:23:42.756
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-342, will wait for the garbage collector to delete the pods @ 06/17/23 13:23:42.756
  E0617 13:23:42.813404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:23:42.815: INFO: Deleting DaemonSet.extensions daemon-set took: 5.293192ms
  Jun 17 13:23:42.916: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.860865ms
  E0617 13:23:43.814415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:23:44.219: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 13:23:44.219: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 17 13:23:44.222: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35417"},"items":null}

  Jun 17 13:23:44.225: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35417"},"items":null}

  Jun 17 13:23:44.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-342" for this suite. @ 06/17/23 13:23:44.239
• [7.610 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 06/17/23 13:23:44.249
  Jun 17 13:23:44.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename sched-pred @ 06/17/23 13:23:44.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:23:44.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:23:44.273
  Jun 17 13:23:44.276: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jun 17 13:23:44.283: INFO: Waiting for terminating namespaces to be deleted...
  Jun 17 13:23:44.285: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-25-17 before test
  Jun 17 13:23:44.289: INFO: nginx-ingress-controller-kubernetes-worker-dlnl8 from ingress-nginx-kubernetes-worker started at 2023-06-17 13:12:43 +0000 UTC (1 container statuses recorded)
  Jun 17 13:23:44.289: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 17 13:23:44.289: INFO: sonobuoy from sonobuoy started at 2023-06-17 12:02:27 +0000 UTC (1 container statuses recorded)
  Jun 17 13:23:44.289: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jun 17 13:23:44.289: INFO: sonobuoy-systemd-logs-daemon-set-1a2f55ba8cf84413-46gwp from sonobuoy started at 2023-06-17 12:02:29 +0000 UTC (2 container statuses recorded)
  Jun 17 13:23:44.289: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 17 13:23:44.289: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 17 13:23:44.289: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-68-253 before test
  Jun 17 13:23:44.294: INFO: default-http-backend-kubernetes-worker-65fc475d49-7hp8p from ingress-nginx-kubernetes-worker started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 13:23:44.294: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jun 17 13:23:44.294: INFO: nginx-ingress-controller-kubernetes-worker-hfc8s from ingress-nginx-kubernetes-worker started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 13:23:44.294: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 17 13:23:44.294: INFO: calico-kube-controllers-867f6b8548-wrvps from kube-system started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 13:23:44.294: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jun 17 13:23:44.294: INFO: coredns-5c7f76ccb8-gc7gk from kube-system started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 13:23:44.294: INFO: 	Container coredns ready: true, restart count 0
  Jun 17 13:23:44.294: INFO: kube-state-metrics-5b95b4459c-dgvxs from kube-system started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 13:23:44.294: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jun 17 13:23:44.294: INFO: metrics-server-v0.5.2-6cf8c8b69c-z9drm from kube-system started at 2023-06-17 11:49:37 +0000 UTC (2 container statuses recorded)
  Jun 17 13:23:44.294: INFO: 	Container metrics-server ready: true, restart count 0
  Jun 17 13:23:44.294: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jun 17 13:23:44.294: INFO: dashboard-metrics-scraper-6b8586b5c9-bpx9l from kubernetes-dashboard started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 13:23:44.294: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jun 17 13:23:44.294: INFO: kubernetes-dashboard-6869f4cd5f-blwp9 from kubernetes-dashboard started at 2023-06-17 11:49:37 +0000 UTC (1 container statuses recorded)
  Jun 17 13:23:44.294: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jun 17 13:23:44.294: INFO: sonobuoy-systemd-logs-daemon-set-1a2f55ba8cf84413-dk2pv from sonobuoy started at 2023-06-17 12:02:29 +0000 UTC (2 container statuses recorded)
  Jun 17 13:23:44.294: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 17 13:23:44.294: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 17 13:23:44.294: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-86-18 before test
  Jun 17 13:23:44.298: INFO: nginx-ingress-controller-kubernetes-worker-jqhpt from ingress-nginx-kubernetes-worker started at 2023-06-17 11:58:10 +0000 UTC (1 container statuses recorded)
  Jun 17 13:23:44.298: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 17 13:23:44.298: INFO: sonobuoy-e2e-job-c63a99f81b61472e from sonobuoy started at 2023-06-17 12:02:29 +0000 UTC (2 container statuses recorded)
  Jun 17 13:23:44.298: INFO: 	Container e2e ready: true, restart count 0
  Jun 17 13:23:44.299: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 17 13:23:44.299: INFO: sonobuoy-systemd-logs-daemon-set-1a2f55ba8cf84413-9ngvl from sonobuoy started at 2023-06-17 12:02:29 +0000 UTC (2 container statuses recorded)
  Jun 17 13:23:44.299: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 17 13:23:44.299: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 06/17/23 13:23:44.299
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.1769750ab78bd79f], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] @ 06/17/23 13:23:44.325
  E0617 13:23:44.815143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:23:45.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9322" for this suite. @ 06/17/23 13:23:45.324
• [1.082 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 06/17/23 13:23:45.332
  Jun 17 13:23:45.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-probe @ 06/17/23 13:23:45.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:23:45.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:23:45.352
  E0617 13:23:45.815454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:46.815631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:47.815702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:48.815902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:49.815947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:50.816394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:51.816495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:52.816700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:53.817313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:54.817402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:55.818375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:56.818467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:57.818912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:58.818987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:23:59.819888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:00.820325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:01.820436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:02.820511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:03.820963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:04.821174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:05.821586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:06.821897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:24:07.410: INFO: Container started at 2023-06-17 13:23:46 +0000 UTC, pod became ready at 2023-06-17 13:24:05 +0000 UTC
  Jun 17 13:24:07.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-6631" for this suite. @ 06/17/23 13:24:07.414
• [22.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 06/17/23 13:24:07.423
  Jun 17 13:24:07.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename services @ 06/17/23 13:24:07.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:24:07.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:24:07.446
  STEP: creating service endpoint-test2 in namespace services-2874 @ 06/17/23 13:24:07.45
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2874 to expose endpoints map[] @ 06/17/23 13:24:07.46
  Jun 17 13:24:07.463: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E0617 13:24:07.821966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:24:08.472: INFO: successfully validated that service endpoint-test2 in namespace services-2874 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-2874 @ 06/17/23 13:24:08.472
  E0617 13:24:08.822599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:09.823391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2874 to expose endpoints map[pod1:[80]] @ 06/17/23 13:24:10.489
  Jun 17 13:24:10.499: INFO: successfully validated that service endpoint-test2 in namespace services-2874 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 06/17/23 13:24:10.499
  Jun 17 13:24:10.499: INFO: Creating new exec pod
  E0617 13:24:10.824024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:11.824438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:12.825133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:24:13.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-2874 exec execpoddm26g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jun 17 13:24:13.643: INFO: stderr: "+ + ncecho -v hostName\n -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jun 17 13:24:13.643: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 13:24:13.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-2874 exec execpoddm26g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.195 80'
  Jun 17 13:24:13.782: INFO: stderr: "+ nc -v -t -w 2 10.152.183.195 80\n+ echo hostName\nConnection to 10.152.183.195 80 port [tcp/http] succeeded!\n"
  Jun 17 13:24:13.782: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-2874 @ 06/17/23 13:24:13.782
  E0617 13:24:13.825375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:14.825480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2874 to expose endpoints map[pod1:[80] pod2:[80]] @ 06/17/23 13:24:15.798
  Jun 17 13:24:15.809: INFO: successfully validated that service endpoint-test2 in namespace services-2874 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 06/17/23 13:24:15.809
  E0617 13:24:15.826371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:24:16.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-2874 exec execpoddm26g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0617 13:24:16.826990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:24:16.935: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jun 17 13:24:16.935: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 13:24:16.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-2874 exec execpoddm26g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.195 80'
  Jun 17 13:24:17.066: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.195 80\nConnection to 10.152.183.195 80 port [tcp/http] succeeded!\n"
  Jun 17 13:24:17.066: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-2874 @ 06/17/23 13:24:17.066
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2874 to expose endpoints map[pod2:[80]] @ 06/17/23 13:24:17.09
  Jun 17 13:24:17.101: INFO: successfully validated that service endpoint-test2 in namespace services-2874 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 06/17/23 13:24:17.101
  E0617 13:24:17.827884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:24:18.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-2874 exec execpoddm26g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jun 17 13:24:18.234: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jun 17 13:24:18.234: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 17 13:24:18.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-2874 exec execpoddm26g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.195 80'
  Jun 17 13:24:18.374: INFO: stderr: "+ nc -v -t -w 2 10.152.183.195 80\n+ echo hostName\nConnection to 10.152.183.195 80 port [tcp/http] succeeded!\n"
  Jun 17 13:24:18.374: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-2874 @ 06/17/23 13:24:18.374
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2874 to expose endpoints map[] @ 06/17/23 13:24:18.387
  Jun 17 13:24:18.399: INFO: successfully validated that service endpoint-test2 in namespace services-2874 exposes endpoints map[]
  Jun 17 13:24:18.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2874" for this suite. @ 06/17/23 13:24:18.415
• [10.998 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 06/17/23 13:24:18.421
  Jun 17 13:24:18.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename endpointslice @ 06/17/23 13:24:18.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:24:18.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:24:18.444
  E0617 13:24:18.828629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:19.828848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:20.829256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:21.829349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:22.829425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 06/17/23 13:24:23.515
  E0617 13:24:23.830276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:24.830417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:25.830727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:26.830858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:27.830972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 06/17/23 13:24:28.523
  E0617 13:24:28.831063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:29.831251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:30.832170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:31.832237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:32.833257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 06/17/23 13:24:33.53
  E0617 13:24:33.833969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:34.834238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:35.835136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:36.835170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:37.835902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 06/17/23 13:24:38.537
  Jun 17 13:24:38.558: INFO: EndpointSlice for Service endpointslice-8452/example-named-port not found
  E0617 13:24:38.836489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:39.836746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:40.837457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:41.837512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:42.837739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:43.837841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:44.837924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:45.838935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:46.839022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:47.839873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:24:48.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8452" for this suite. @ 06/17/23 13:24:48.568
• [30.152 seconds]
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 06/17/23 13:24:48.574
  Jun 17 13:24:48.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 13:24:48.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:24:48.596
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:24:48.6
  STEP: Creating projection with secret that has name projected-secret-test-beaff0fd-3087-432a-b26b-1dcc6ada98dc @ 06/17/23 13:24:48.604
  STEP: Creating a pod to test consume secrets @ 06/17/23 13:24:48.608
  E0617 13:24:48.840744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:49.840841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:50.841473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:51.841552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:24:52.625
  Jun 17 13:24:52.628: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-projected-secrets-b9431b64-9094-4b33-ab9f-568cb44ea21b container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/17/23 13:24:52.635
  Jun 17 13:24:52.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4979" for this suite. @ 06/17/23 13:24:52.653
• [4.085 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 06/17/23 13:24:52.659
  Jun 17 13:24:52.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename resourcequota @ 06/17/23 13:24:52.66
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:24:52.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:24:52.678
  STEP: Counting existing ResourceQuota @ 06/17/23 13:24:52.681
  E0617 13:24:52.842031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:53.842956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:54.843492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:55.843951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:56.844050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 06/17/23 13:24:57.685
  STEP: Ensuring resource quota status is calculated @ 06/17/23 13:24:57.689
  E0617 13:24:57.844495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:24:58.844703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 06/17/23 13:24:59.694
  STEP: Ensuring resource quota status captures replicaset creation @ 06/17/23 13:24:59.707
  E0617 13:24:59.845271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:00.846220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 06/17/23 13:25:01.71
  STEP: Ensuring resource quota status released usage @ 06/17/23 13:25:01.717
  E0617 13:25:01.846501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:02.846611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:25:03.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6984" for this suite. @ 06/17/23 13:25:03.724
• [11.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 06/17/23 13:25:03.731
  Jun 17 13:25:03.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename var-expansion @ 06/17/23 13:25:03.732
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:25:03.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:25:03.754
  STEP: creating the pod with failed condition @ 06/17/23 13:25:03.757
  E0617 13:25:03.847190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:04.847294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:05.847707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:06.847805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:07.848176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:08.848424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:09.849389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:10.849839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:11.850820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:12.850920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:13.851686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:14.852278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:15.853365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:16.853444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:17.853474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:18.853571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:19.854624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:20.855096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:21.855398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:22.855898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:23.856352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:24.856550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:25.857485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:26.857919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:27.858479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:28.858538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:29.858587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:30.859056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:31.859886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:32.859987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:33.860712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:34.860806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:35.861798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:36.861911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:37.862286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:38.862382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:39.862614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:40.863119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:41.863977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:42.864117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:43.864152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:44.864385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:45.864593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:46.864847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:47.865178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:48.865397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:49.866127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:50.866329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:51.866487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:52.866718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:53.866725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:54.866845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:55.867742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:56.868349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:57.868473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:58.868566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:25:59.868663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:00.869241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:01.869477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:02.870356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:03.871276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:04.871378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:05.872062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:06.872155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:07.872252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:08.872361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:09.872752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:10.873512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:11.874303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:12.874802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:13.874895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:14.875867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:15.876172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:16.877060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:17.877137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:18.877233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:19.877513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:20.878252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:21.878338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:22.879396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:23.879432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:24.879506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:25.879779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:26.880545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:27.880914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:28.881008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:29.881606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:30.882375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:31.882628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:32.883359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:33.883629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:34.883726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:35.884765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:36.884873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:37.885034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:38.885134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:39.885327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:40.886148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:41.886238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:42.886801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:43.887882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:44.887963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:45.888253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:46.888688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:47.888743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:48.888797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:49.889671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:50.890233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:51.890440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:52.890821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:53.890978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:54.891069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:55.891382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:56.892003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:57.892097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:58.892985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:26:59.893072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:00.893493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:01.893824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:02.893912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 06/17/23 13:27:03.766
  E0617 13:27:03.894715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:27:04.278: INFO: Successfully updated pod "var-expansion-033a1bf4-4614-465d-be28-d702dba644de"
  STEP: waiting for pod running @ 06/17/23 13:27:04.278
  E0617 13:27:04.895632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:05.895746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 06/17/23 13:27:06.285
  Jun 17 13:27:06.285: INFO: Deleting pod "var-expansion-033a1bf4-4614-465d-be28-d702dba644de" in namespace "var-expansion-7984"
  Jun 17 13:27:06.292: INFO: Wait up to 5m0s for pod "var-expansion-033a1bf4-4614-465d-be28-d702dba644de" to be fully deleted
  E0617 13:27:06.896222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:07.896314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:08.896569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:09.896675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:10.896982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:11.897212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:12.897987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:13.898092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:14.898189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:15.898471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:16.899070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:17.899893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:18.900754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:19.900811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:20.901264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:21.901387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:22.901412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:23.901673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:24.902615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:25.902941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:26.903652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:27.903752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:28.904038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:29.904115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:30.904947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:31.905152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:32.905247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:33.905337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:34.906340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:35.907071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:36.907159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:37.907254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:27:38.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7984" for this suite. @ 06/17/23 13:27:38.369
• [154.644 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 06/17/23 13:27:38.376
  Jun 17 13:27:38.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename services @ 06/17/23 13:27:38.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:27:38.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:27:38.398
  STEP: creating service nodeport-test with type=NodePort in namespace services-3382 @ 06/17/23 13:27:38.401
  STEP: creating replication controller nodeport-test in namespace services-3382 @ 06/17/23 13:27:38.415
  I0617 13:27:38.420964      19 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-3382, replica count: 2
  E0617 13:27:38.907360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:39.908396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:40.908602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0617 13:27:41.473380      19 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 17 13:27:41.473: INFO: Creating new exec pod
  E0617 13:27:41.909488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:42.909816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:43.910339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:27:44.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-3382 exec execpodqjlcq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jun 17 13:27:44.634: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jun 17 13:27:44.634: INFO: stdout: ""
  E0617 13:27:44.910803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:27:45.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-3382 exec execpodqjlcq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jun 17 13:27:45.773: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jun 17 13:27:45.773: INFO: stdout: ""
  E0617 13:27:45.911448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:27:46.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-3382 exec execpodqjlcq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jun 17 13:27:46.765: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jun 17 13:27:46.765: INFO: stdout: ""
  E0617 13:27:46.911826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:27:47.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-3382 exec execpodqjlcq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jun 17 13:27:47.760: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jun 17 13:27:47.760: INFO: stdout: ""
  E0617 13:27:47.912310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:27:48.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-3382 exec execpodqjlcq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jun 17 13:27:48.761: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jun 17 13:27:48.761: INFO: stdout: ""
  E0617 13:27:48.912381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:27:49.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-3382 exec execpodqjlcq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jun 17 13:27:49.761: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jun 17 13:27:49.761: INFO: stdout: ""
  E0617 13:27:49.912759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:27:50.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-3382 exec execpodqjlcq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jun 17 13:27:50.757: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jun 17 13:27:50.757: INFO: stdout: "nodeport-test-sv9s5"
  Jun 17 13:27:50.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-3382 exec execpodqjlcq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.95 80'
  Jun 17 13:27:50.897: INFO: stderr: "+ nc -v -t -w 2 10.152.183.95 80\n+ echo hostName\nConnection to 10.152.183.95 80 port [tcp/http] succeeded!\n"
  Jun 17 13:27:50.897: INFO: stdout: "nodeport-test-sv9s5"
  Jun 17 13:27:50.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-3382 exec execpodqjlcq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.25.17 31004'
  E0617 13:27:50.913402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:27:51.024: INFO: stderr: "+ nc -v -t -w 2 172.31.25.17 31004\n+ echo hostName\nConnection to 172.31.25.17 31004 port [tcp/*] succeeded!\n"
  Jun 17 13:27:51.024: INFO: stdout: "nodeport-test-sv9s5"
  Jun 17 13:27:51.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-3382 exec execpodqjlcq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.86.18 31004'
  Jun 17 13:27:51.158: INFO: stderr: "+ nc -v -t -w 2 172.31.86.18 31004\n+ echo hostName\nConnection to 172.31.86.18 31004 port [tcp/*] succeeded!\n"
  Jun 17 13:27:51.158: INFO: stdout: ""
  E0617 13:27:51.913479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:27:52.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-3382 exec execpodqjlcq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.86.18 31004'
  Jun 17 13:27:52.281: INFO: stderr: "+ nc -v -t -w 2 172.31.86.18 31004\n+ echo hostName\nConnection to 172.31.86.18 31004 port [tcp/*] succeeded!\n"
  Jun 17 13:27:52.281: INFO: stdout: ""
  E0617 13:27:52.914023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:27:53.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=services-3382 exec execpodqjlcq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.86.18 31004'
  Jun 17 13:27:53.289: INFO: stderr: "+ nc -v -t -w 2 172.31.86.18 31004\n+ echo hostName\nConnection to 172.31.86.18 31004 port [tcp/*] succeeded!\n"
  Jun 17 13:27:53.289: INFO: stdout: "nodeport-test-dvdgg"
  Jun 17 13:27:53.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3382" for this suite. @ 06/17/23 13:27:53.293
• [14.923 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 06/17/23 13:27:53.299
  Jun 17 13:27:53.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-runtime @ 06/17/23 13:27:53.3
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:27:53.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:27:53.324
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 06/17/23 13:27:53.338
  E0617 13:27:53.915131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:54.915269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:55.915580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:56.915681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:57.916629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:58.916734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:27:59.916813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:00.917872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:01.917959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:02.918722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:03.919479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:04.919878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:05.920657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:06.920726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 06/17/23 13:28:07.397
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 06/17/23 13:28:07.4
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 06/17/23 13:28:07.406
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 06/17/23 13:28:07.406
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 06/17/23 13:28:07.424
  E0617 13:28:07.920842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:08.921811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:09.922209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 06/17/23 13:28:10.439
  E0617 13:28:10.922591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 06/17/23 13:28:11.446
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 06/17/23 13:28:11.452
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 06/17/23 13:28:11.452
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 06/17/23 13:28:11.472
  E0617 13:28:11.923633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 06/17/23 13:28:12.479
  E0617 13:28:12.923942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:13.924733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 06/17/23 13:28:14.49
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 06/17/23 13:28:14.496
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 06/17/23 13:28:14.496
  Jun 17 13:28:14.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4548" for this suite. @ 06/17/23 13:28:14.523
• [21.229 seconds]
------------------------------
SSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 06/17/23 13:28:14.529
  Jun 17 13:28:14.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename limitrange @ 06/17/23 13:28:14.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:28:14.551
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:28:14.553
  STEP: Creating a LimitRange @ 06/17/23 13:28:14.556
  STEP: Setting up watch @ 06/17/23 13:28:14.556
  STEP: Submitting a LimitRange @ 06/17/23 13:28:14.662
  STEP: Verifying LimitRange creation was observed @ 06/17/23 13:28:14.667
  STEP: Fetching the LimitRange to ensure it has proper values @ 06/17/23 13:28:14.668
  Jun 17 13:28:14.671: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jun 17 13:28:14.671: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 06/17/23 13:28:14.671
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 06/17/23 13:28:14.676
  Jun 17 13:28:14.679: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jun 17 13:28:14.679: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 06/17/23 13:28:14.68
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 06/17/23 13:28:14.686
  Jun 17 13:28:14.689: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Jun 17 13:28:14.689: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 06/17/23 13:28:14.689
  STEP: Failing to create a Pod with more than max resources @ 06/17/23 13:28:14.691
  STEP: Updating a LimitRange @ 06/17/23 13:28:14.693
  STEP: Verifying LimitRange updating is effective @ 06/17/23 13:28:14.698
  E0617 13:28:14.925347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:15.926105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 06/17/23 13:28:16.703
  STEP: Failing to create a Pod with more than max resources @ 06/17/23 13:28:16.71
  STEP: Deleting a LimitRange @ 06/17/23 13:28:16.712
  STEP: Verifying the LimitRange was deleted @ 06/17/23 13:28:16.72
  E0617 13:28:16.927050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:17.927147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:18.927224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:19.927319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:20.927982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:28:21.724: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 06/17/23 13:28:21.724
  Jun 17 13:28:21.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-3983" for this suite. @ 06/17/23 13:28:21.735
• [7.213 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 06/17/23 13:28:21.743
  Jun 17 13:28:21.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 13:28:21.744
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:28:21.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:28:21.764
  STEP: Setting up server cert @ 06/17/23 13:28:21.787
  E0617 13:28:21.928515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 13:28:21.998
  STEP: Deploying the webhook pod @ 06/17/23 13:28:22.008
  STEP: Wait for the deployment to be ready @ 06/17/23 13:28:22.019
  Jun 17 13:28:22.027: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0617 13:28:22.929186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:23.929300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/17/23 13:28:24.038
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 13:28:24.049
  E0617 13:28:24.929459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:28:25.050: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 06/17/23 13:28:25.054
  STEP: create a pod that should be denied by the webhook @ 06/17/23 13:28:25.069
  STEP: create a pod that causes the webhook to hang @ 06/17/23 13:28:25.081
  E0617 13:28:25.929877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:26.930391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:27.930478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:28.931198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:29.931290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:30.932211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:31.932381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:32.932553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:33.932641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:34.932817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 06/17/23 13:28:35.089
  STEP: create a configmap that should be admitted by the webhook @ 06/17/23 13:28:35.128
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 06/17/23 13:28:35.137
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 06/17/23 13:28:35.144
  STEP: create a namespace that bypass the webhook @ 06/17/23 13:28:35.149
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 06/17/23 13:28:35.17
  Jun 17 13:28:35.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6698" for this suite. @ 06/17/23 13:28:35.223
  STEP: Destroying namespace "webhook-markers-7525" for this suite. @ 06/17/23 13:28:35.229
  STEP: Destroying namespace "exempted-namespace-434" for this suite. @ 06/17/23 13:28:35.239
• [13.502 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 06/17/23 13:28:35.248
  Jun 17 13:28:35.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename tables @ 06/17/23 13:28:35.248
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:28:35.264
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:28:35.269
  Jun 17 13:28:35.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-4459" for this suite. @ 06/17/23 13:28:35.278
• [0.040 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 06/17/23 13:28:35.288
  Jun 17 13:28:35.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename pods @ 06/17/23 13:28:35.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:28:35.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:28:35.313
  STEP: creating pod @ 06/17/23 13:28:35.317
  E0617 13:28:35.933110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:36.933240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:28:37.338: INFO: Pod pod-hostip-b049415d-9952-43c4-b02e-bf7b49c084bd has hostIP: 172.31.25.17
  Jun 17 13:28:37.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9805" for this suite. @ 06/17/23 13:28:37.342
• [2.059 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 06/17/23 13:28:37.348
  Jun 17 13:28:37.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 13:28:37.349
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:28:37.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:28:37.372
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 13:28:37.375
  E0617 13:28:37.934216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:38.934302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:39.934416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:40.934777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:28:41.392
  Jun 17 13:28:41.396: INFO: Trying to get logs from node ip-172-31-25-17 pod downwardapi-volume-7a30789f-33e8-4faa-9ed6-cd1b47fa9a1e container client-container: <nil>
  STEP: delete the pod @ 06/17/23 13:28:41.414
  Jun 17 13:28:41.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3848" for this suite. @ 06/17/23 13:28:41.431
• [4.089 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 06/17/23 13:28:41.438
  Jun 17 13:28:41.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename statefulset @ 06/17/23 13:28:41.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:28:41.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:28:41.458
  STEP: Creating service test in namespace statefulset-8186 @ 06/17/23 13:28:41.46
  STEP: Creating stateful set ss in namespace statefulset-8186 @ 06/17/23 13:28:41.466
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8186 @ 06/17/23 13:28:41.472
  Jun 17 13:28:41.475: INFO: Found 0 stateful pods, waiting for 1
  E0617 13:28:41.935650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:42.935770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:43.935869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:44.935977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:45.936099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:46.936234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:47.936318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:48.936490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:49.937017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:50.937276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:28:51.481: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 06/17/23 13:28:51.481
  Jun 17 13:28:51.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-8186 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 17 13:28:51.628: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 17 13:28:51.628: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 17 13:28:51.628: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 17 13:28:51.632: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0617 13:28:51.937827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:52.938217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:53.938335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:54.938525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:55.938672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:56.938829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:57.939022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:58.939082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:28:59.939183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:00.939260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:01.636: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jun 17 13:29:01.636: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 17 13:29:01.654: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
  Jun 17 13:29:01.654: INFO: ss-0  ip-172-31-86-18  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:28:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:28:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:28:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:28:41 +0000 UTC  }]
  Jun 17 13:29:01.654: INFO: 
  Jun 17 13:29:01.654: INFO: StatefulSet ss has not reached scale 3, at 1
  E0617 13:29:01.940062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:02.659: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996454227s
  E0617 13:29:02.940287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:03.662: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993068693s
  E0617 13:29:03.941019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:04.667: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988416896s
  E0617 13:29:04.941551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:05.671: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98472854s
  E0617 13:29:05.941685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:06.675: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980664039s
  E0617 13:29:06.942523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:07.679: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976415301s
  E0617 13:29:07.942703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:08.683: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972676807s
  E0617 13:29:08.943663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:09.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.968742012s
  E0617 13:29:09.944398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:10.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 965.081119ms
  E0617 13:29:10.944448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8186 @ 06/17/23 13:29:11.691
  Jun 17 13:29:11.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-8186 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 17 13:29:11.833: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 17 13:29:11.833: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 17 13:29:11.833: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 17 13:29:11.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-8186 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0617 13:29:11.945187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:11.960: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jun 17 13:29:11.960: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 17 13:29:11.960: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 17 13:29:11.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-8186 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 17 13:29:12.087: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jun 17 13:29:12.087: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 17 13:29:12.087: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 17 13:29:12.091: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  E0617 13:29:12.945275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:13.945608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:14.945834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:15.946116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:16.946892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:17.946987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:18.947066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:19.947875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:20.948289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:21.948519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:22.096: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 17 13:29:22.096: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 17 13:29:22.096: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 06/17/23 13:29:22.096
  Jun 17 13:29:22.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-8186 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 17 13:29:22.229: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 17 13:29:22.229: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 17 13:29:22.229: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 17 13:29:22.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-8186 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 17 13:29:22.360: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 17 13:29:22.360: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 17 13:29:22.360: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 17 13:29:22.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=statefulset-8186 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 17 13:29:22.485: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 17 13:29:22.485: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 17 13:29:22.485: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 17 13:29:22.485: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 17 13:29:22.488: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0617 13:29:22.949526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:23.949638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:24.949821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:25.950866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:26.951031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:27.951130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:28.951882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:29.951986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:30.952717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:31.952811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:32.496: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jun 17 13:29:32.496: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jun 17 13:29:32.496: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jun 17 13:29:32.509: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Jun 17 13:29:32.509: INFO: ss-0  ip-172-31-86-18   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:28:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:28:41 +0000 UTC  }]
  Jun 17 13:29:32.509: INFO: ss-1  ip-172-31-25-17   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:01 +0000 UTC  }]
  Jun 17 13:29:32.509: INFO: ss-2  ip-172-31-68-253  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:01 +0000 UTC  }]
  Jun 17 13:29:32.509: INFO: 
  Jun 17 13:29:32.509: INFO: StatefulSet ss has not reached scale 0, at 3
  E0617 13:29:32.953670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:33.512: INFO: POD   NODE              PHASE      GRACE  CONDITIONS
  Jun 17 13:29:33.512: INFO: ss-1  ip-172-31-25-17   Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:01 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:22 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:22 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:01 +0000 UTC  }]
  Jun 17 13:29:33.512: INFO: ss-2  ip-172-31-68-253  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:01 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:23 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:23 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-17 13:29:01 +0000 UTC  }]
  Jun 17 13:29:33.512: INFO: 
  Jun 17 13:29:33.512: INFO: StatefulSet ss has not reached scale 0, at 2
  E0617 13:29:33.954252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:34.516: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993026651s
  E0617 13:29:34.954804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:35.519: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989476439s
  E0617 13:29:35.955055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:36.523: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.986250051s
  E0617 13:29:36.955686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:37.526: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.982754712s
  E0617 13:29:37.955794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:38.530: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.978397771s
  E0617 13:29:38.956175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:39.534: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.975228016s
  E0617 13:29:39.956272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:40.538: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.970927116s
  E0617 13:29:40.956903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:41.541: INFO: Verifying statefulset ss doesn't scale past 0 for another 967.712427ms
  E0617 13:29:41.957867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8186 @ 06/17/23 13:29:42.541
  Jun 17 13:29:42.545: INFO: Scaling statefulset ss to 0
  Jun 17 13:29:42.555: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 17 13:29:42.558: INFO: Deleting all statefulset in ns statefulset-8186
  Jun 17 13:29:42.560: INFO: Scaling statefulset ss to 0
  Jun 17 13:29:42.569: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 17 13:29:42.572: INFO: Deleting statefulset ss
  Jun 17 13:29:42.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8186" for this suite. @ 06/17/23 13:29:42.588
• [61.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 06/17/23 13:29:42.599
  Jun 17 13:29:42.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 13:29:42.6
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:29:42.623
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:29:42.626
  STEP: Setting up server cert @ 06/17/23 13:29:42.654
  E0617 13:29:42.959726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:43.960041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 13:29:44.18
  STEP: Deploying the webhook pod @ 06/17/23 13:29:44.188
  STEP: Wait for the deployment to be ready @ 06/17/23 13:29:44.2
  Jun 17 13:29:44.209: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0617 13:29:44.960111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:45.960827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/17/23 13:29:46.219
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 13:29:46.228
  E0617 13:29:46.961691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:47.229: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 06/17/23 13:29:47.232
  STEP: Registering slow webhook via the AdmissionRegistration API @ 06/17/23 13:29:47.232
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 06/17/23 13:29:47.246
  E0617 13:29:47.961787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 06/17/23 13:29:48.256
  STEP: Registering slow webhook via the AdmissionRegistration API @ 06/17/23 13:29:48.256
  E0617 13:29:48.961862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 06/17/23 13:29:49.286
  STEP: Registering slow webhook via the AdmissionRegistration API @ 06/17/23 13:29:49.286
  E0617 13:29:49.961952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:50.962360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:51.962420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:52.962525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:53.962758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 06/17/23 13:29:54.32
  STEP: Registering slow webhook via the AdmissionRegistration API @ 06/17/23 13:29:54.32
  E0617 13:29:54.962837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:55.963843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:56.963942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:57.964119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:29:58.964321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:29:59.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3112" for this suite. @ 06/17/23 13:29:59.402
  STEP: Destroying namespace "webhook-markers-3824" for this suite. @ 06/17/23 13:29:59.408
• [16.818 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 06/17/23 13:29:59.417
  Jun 17 13:29:59.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename job @ 06/17/23 13:29:59.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:29:59.447
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:29:59.45
  STEP: Creating Indexed job @ 06/17/23 13:29:59.453
  STEP: Ensuring job reaches completions @ 06/17/23 13:29:59.458
  E0617 13:29:59.964408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:00.965334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:01.965427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:02.965668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:03.966363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:04.966455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:05.967141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:06.967218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 06/17/23 13:30:07.462
  Jun 17 13:30:07.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4784" for this suite. @ 06/17/23 13:30:07.469
• [8.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 06/17/23 13:30:07.478
  Jun 17 13:30:07.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-probe @ 06/17/23 13:30:07.479
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:30:07.501
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:30:07.504
  STEP: Creating pod busybox-d7bf2eaf-0dd8-4857-bc2f-f6f5224af701 in namespace container-probe-1523 @ 06/17/23 13:30:07.507
  E0617 13:30:07.967875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:08.968056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:30:09.522: INFO: Started pod busybox-d7bf2eaf-0dd8-4857-bc2f-f6f5224af701 in namespace container-probe-1523
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/17/23 13:30:09.522
  Jun 17 13:30:09.525: INFO: Initial restart count of pod busybox-d7bf2eaf-0dd8-4857-bc2f-f6f5224af701 is 0
  E0617 13:30:09.968213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:10.968421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:11.968988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:12.969110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:13.969259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:14.970169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:15.970581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:16.970678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:17.971580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:18.971875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:19.972191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:20.972609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:21.973397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:22.973711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:23.973794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:24.973993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:25.974115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:26.974311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:27.974415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:28.974585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:29.975422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:30.976322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:31.976475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:32.976573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:33.976855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:34.977040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:35.977357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:36.977451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:37.977548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:38.977763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:39.977861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:40.978241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:41.978820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:42.978886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:43.978974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:44.979889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:45.980406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:46.980480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:47.980730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:48.980834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:49.981092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:50.981201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:51.981873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:52.981976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:53.982460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:54.982660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:55.982741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:56.982824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:57.982913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:30:58.983876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:30:59.628: INFO: Restart count of pod container-probe-1523/busybox-d7bf2eaf-0dd8-4857-bc2f-f6f5224af701 is now 1 (50.102894287s elapsed)
  Jun 17 13:30:59.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/17/23 13:30:59.631
  STEP: Destroying namespace "container-probe-1523" for this suite. @ 06/17/23 13:30:59.643
• [52.171 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 06/17/23 13:30:59.649
  Jun 17 13:30:59.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl @ 06/17/23 13:30:59.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:30:59.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:30:59.674
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 06/17/23 13:30:59.677
  Jun 17 13:30:59.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-8097 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jun 17 13:30:59.746: INFO: stderr: ""
  Jun 17 13:30:59.746: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 06/17/23 13:30:59.746
  E0617 13:30:59.984644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:00.985335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:01.985434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:02.986474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:03.986568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 06/17/23 13:31:04.797
  Jun 17 13:31:04.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-8097 get pod e2e-test-httpd-pod -o json'
  Jun 17 13:31:04.860: INFO: stderr: ""
  Jun 17 13:31:04.860: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-06-17T13:30:59Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8097\",\n        \"resourceVersion\": \"37516\",\n        \"uid\": \"45fc8eb7-a6de-4139-b06a-46a9dd985777\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-vzcmd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-25-17\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-vzcmd\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-17T13:30:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-17T13:31:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-17T13:31:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-17T13:30:59Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://056ad0b4f0192d8df837a6523bcc5c2b5941385debbbaeb310fddaf8b310cdef\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-06-17T13:31:00Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.25.17\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.178.191\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.178.191\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-06-17T13:30:59Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 06/17/23 13:31:04.86
  Jun 17 13:31:04.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-8097 replace -f -'
  E0617 13:31:04.986725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:31:05.156: INFO: stderr: ""
  Jun 17 13:31:05.156: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 06/17/23 13:31:05.156
  Jun 17 13:31:05.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-8097 delete pods e2e-test-httpd-pod'
  E0617 13:31:05.986813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:06.986930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:31:07.032: INFO: stderr: ""
  Jun 17 13:31:07.032: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jun 17 13:31:07.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8097" for this suite. @ 06/17/23 13:31:07.037
• [7.393 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 06/17/23 13:31:07.043
  Jun 17 13:31:07.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename csiinlinevolumes @ 06/17/23 13:31:07.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:31:07.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:31:07.068
  STEP: creating @ 06/17/23 13:31:07.071
  STEP: getting @ 06/17/23 13:31:07.086
  STEP: listing @ 06/17/23 13:31:07.091
  STEP: deleting @ 06/17/23 13:31:07.094
  Jun 17 13:31:07.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-2419" for this suite. @ 06/17/23 13:31:07.112
• [0.074 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 06/17/23 13:31:07.118
  Jun 17 13:31:07.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename certificates @ 06/17/23 13:31:07.118
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:31:07.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:31:07.136
  STEP: getting /apis @ 06/17/23 13:31:07.662
  STEP: getting /apis/certificates.k8s.io @ 06/17/23 13:31:07.666
  STEP: getting /apis/certificates.k8s.io/v1 @ 06/17/23 13:31:07.668
  STEP: creating @ 06/17/23 13:31:07.669
  STEP: getting @ 06/17/23 13:31:07.685
  STEP: listing @ 06/17/23 13:31:07.688
  STEP: watching @ 06/17/23 13:31:07.691
  Jun 17 13:31:07.691: INFO: starting watch
  STEP: patching @ 06/17/23 13:31:07.692
  STEP: updating @ 06/17/23 13:31:07.698
  Jun 17 13:31:07.704: INFO: waiting for watch events with expected annotations
  Jun 17 13:31:07.704: INFO: saw patched and updated annotations
  STEP: getting /approval @ 06/17/23 13:31:07.704
  STEP: patching /approval @ 06/17/23 13:31:07.707
  STEP: updating /approval @ 06/17/23 13:31:07.713
  STEP: getting /status @ 06/17/23 13:31:07.72
  STEP: patching /status @ 06/17/23 13:31:07.723
  STEP: updating /status @ 06/17/23 13:31:07.729
  STEP: deleting @ 06/17/23 13:31:07.736
  STEP: deleting a collection @ 06/17/23 13:31:07.746
  Jun 17 13:31:07.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-6455" for this suite. @ 06/17/23 13:31:07.762
• [0.650 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 06/17/23 13:31:07.768
  Jun 17 13:31:07.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename services @ 06/17/23 13:31:07.769
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:31:07.786
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:31:07.789
  STEP: creating a collection of services @ 06/17/23 13:31:07.792
  Jun 17 13:31:07.792: INFO: Creating e2e-svc-a-k22h6
  Jun 17 13:31:07.800: INFO: Creating e2e-svc-b-v5fzt
  Jun 17 13:31:07.811: INFO: Creating e2e-svc-c-f6mrb
  STEP: deleting service collection @ 06/17/23 13:31:07.822
  Jun 17 13:31:07.847: INFO: Collection of services has been deleted
  Jun 17 13:31:07.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2555" for this suite. @ 06/17/23 13:31:07.853
• [0.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 06/17/23 13:31:07.861
  Jun 17 13:31:07.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename downward-api @ 06/17/23 13:31:07.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:31:07.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:31:07.878
  STEP: Creating a pod to test downward API volume plugin @ 06/17/23 13:31:07.881
  E0617 13:31:07.987267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:08.987420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:09.988442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:10.988836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:31:11.902
  Jun 17 13:31:11.905: INFO: Trying to get logs from node ip-172-31-25-17 pod downwardapi-volume-59c774db-9537-4baf-8462-23986f83864b container client-container: <nil>
  STEP: delete the pod @ 06/17/23 13:31:11.919
  Jun 17 13:31:11.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6966" for this suite. @ 06/17/23 13:31:11.936
• [4.083 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 06/17/23 13:31:11.944
  Jun 17 13:31:11.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename pod-network-test @ 06/17/23 13:31:11.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:31:11.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:31:11.965
  STEP: Performing setup for networking test in namespace pod-network-test-1338 @ 06/17/23 13:31:11.967
  STEP: creating a selector @ 06/17/23 13:31:11.967
  STEP: Creating the service pods in kubernetes @ 06/17/23 13:31:11.968
  Jun 17 13:31:11.968: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0617 13:31:11.989539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:12.989693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:13.989877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:14.990248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:15.991221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:16.991321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:17.991890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:18.991987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:19.992234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:20.992327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:21.992437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:22.992525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:23.992737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 06/17/23 13:31:24.041
  E0617 13:31:24.992852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:25.993880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:31:26.068: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jun 17 13:31:26.068: INFO: Going to poll 192.168.178.166 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jun 17 13:31:26.071: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.178.166 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1338 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 13:31:26.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:31:26.071: INFO: ExecWithOptions: Clientset creation
  Jun 17 13:31:26.071: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1338/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.178.166+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0617 13:31:26.993995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:31:27.130: INFO: Found all 1 expected endpoints: [netserver-0]
  Jun 17 13:31:27.130: INFO: Going to poll 192.168.91.177 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jun 17 13:31:27.133: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.91.177 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1338 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 13:31:27.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:31:27.134: INFO: ExecWithOptions: Clientset creation
  Jun 17 13:31:27.134: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1338/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.91.177+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0617 13:31:27.994101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:31:28.196: INFO: Found all 1 expected endpoints: [netserver-1]
  Jun 17 13:31:28.196: INFO: Going to poll 192.168.2.82 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jun 17 13:31:28.200: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.2.82 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1338 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 17 13:31:28.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  Jun 17 13:31:28.201: INFO: ExecWithOptions: Clientset creation
  Jun 17 13:31:28.201: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1338/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.2.82+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0617 13:31:28.994253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:31:29.265: INFO: Found all 1 expected endpoints: [netserver-2]
  Jun 17 13:31:29.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1338" for this suite. @ 06/17/23 13:31:29.269
• [17.332 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 06/17/23 13:31:29.278
  Jun 17 13:31:29.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename var-expansion @ 06/17/23 13:31:29.279
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:31:29.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:31:29.309
  E0617 13:31:29.994548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:30.995117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:31:31.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 17 13:31:31.331: INFO: Deleting pod "var-expansion-48167c9f-e3dd-4551-a502-15de496747eb" in namespace "var-expansion-8385"
  Jun 17 13:31:31.338: INFO: Wait up to 5m0s for pod "var-expansion-48167c9f-e3dd-4551-a502-15de496747eb" to be fully deleted
  E0617 13:31:31.995223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:32.995896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-8385" for this suite. @ 06/17/23 13:31:33.345
• [4.073 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 06/17/23 13:31:33.351
  Jun 17 13:31:33.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename projected @ 06/17/23 13:31:33.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:31:33.372
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:31:33.375
  STEP: Creating the pod @ 06/17/23 13:31:33.378
  E0617 13:31:33.995994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:34.996071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:31:35.913: INFO: Successfully updated pod "labelsupdate38516ea8-0329-4805-9cb4-c65e40b691c8"
  E0617 13:31:35.996656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:36.997668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:37.998416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:38.998618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:31:39.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2005" for this suite. @ 06/17/23 13:31:39.938
• [6.593 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 06/17/23 13:31:39.945
  Jun 17 13:31:39.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename kubectl @ 06/17/23 13:31:39.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:31:39.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:31:39.965
  STEP: starting the proxy server @ 06/17/23 13:31:39.967
  Jun 17 13:31:39.968: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=kubectl-8835 proxy -p 0 --disable-filter'
  E0617 13:31:39.999478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: curling proxy /api/ output @ 06/17/23 13:31:40.017
  Jun 17 13:31:40.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8835" for this suite. @ 06/17/23 13:31:40.029
• [0.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 06/17/23 13:31:40.035
  Jun 17 13:31:40.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename services @ 06/17/23 13:31:40.036
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:31:40.053
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:31:40.056
  Jun 17 13:31:40.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7007" for this suite. @ 06/17/23 13:31:40.065
• [0.035 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 06/17/23 13:31:40.072
  Jun 17 13:31:40.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename configmap @ 06/17/23 13:31:40.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:31:40.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:31:40.092
  STEP: Creating configMap with name configmap-test-volume-a2871a81-8407-4272-9cdb-f23bfcb0e61f @ 06/17/23 13:31:40.095
  STEP: Creating a pod to test consume configMaps @ 06/17/23 13:31:40.099
  E0617 13:31:40.999792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:42.000137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:43.000649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:44.000715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:31:44.119
  Jun 17 13:31:44.122: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-configmaps-a271f1fb-0264-41b2-a510-2791c869641f container configmap-volume-test: <nil>
  STEP: delete the pod @ 06/17/23 13:31:44.14
  Jun 17 13:31:44.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1541" for this suite. @ 06/17/23 13:31:44.159
• [4.094 seconds]
------------------------------
S
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 06/17/23 13:31:44.165
  Jun 17 13:31:44.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename endpointslice @ 06/17/23 13:31:44.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:31:44.182
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:31:44.184
  E0617 13:31:45.000790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:46.001032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:47.001123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:48.001985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:31:48.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4896" for this suite. @ 06/17/23 13:31:48.234
• [4.074 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 06/17/23 13:31:48.24
  Jun 17 13:31:48.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename daemonsets @ 06/17/23 13:31:48.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:31:48.262
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:31:48.265
  STEP: Creating simple DaemonSet "daemon-set" @ 06/17/23 13:31:48.283
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/17/23 13:31:48.287
  Jun 17 13:31:48.291: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:31:48.291: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:31:48.295: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 17 13:31:48.295: INFO: Node ip-172-31-25-17 is running 0 daemon pod, expected 1
  E0617 13:31:49.002256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:31:49.299: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:31:49.299: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:31:49.302: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 17 13:31:49.302: INFO: Node ip-172-31-68-253 is running 0 daemon pod, expected 1
  E0617 13:31:50.003160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:31:50.300: INFO: DaemonSet pods can't tolerate node ip-172-31-38-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:31:50.301: INFO: DaemonSet pods can't tolerate node ip-172-31-70-122 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 17 13:31:50.305: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 17 13:31:50.305: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 06/17/23 13:31:50.308
  STEP: DeleteCollection of the DaemonSets @ 06/17/23 13:31:50.313
  STEP: Verify that ReplicaSets have been deleted @ 06/17/23 13:31:50.323
  Jun 17 13:31:50.332: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38066"},"items":null}

  Jun 17 13:31:50.335: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38066"},"items":[{"metadata":{"name":"daemon-set-546w4","generateName":"daemon-set-","namespace":"daemonsets-3116","uid":"44c8dc04-f13d-492e-875f-a82d401fb4b6","resourceVersion":"38047","creationTimestamp":"2023-06-17T13:31:48Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"24b33813-a885-4819-b7cd-fe5accc2fdb2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-17T13:31:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24b33813-a885-4819-b7cd-fe5accc2fdb2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-17T13:31:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.178.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-cxk2w","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-cxk2w","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-25-17","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-25-17"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-17T13:31:48Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-17T13:31:49Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-17T13:31:49Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-17T13:31:48Z"}],"hostIP":"172.31.25.17","podIP":"192.168.178.175","podIPs":[{"ip":"192.168.178.175"}],"startTime":"2023-06-17T13:31:48Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-17T13:31:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a2fd1227f11daf7fe55383d83091ed886912c7756cc917fd4c30b56288916e9a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-d29bt","generateName":"daemon-set-","namespace":"daemonsets-3116","uid":"09b5054b-3990-4880-938c-800ca440366f","resourceVersion":"38062","creationTimestamp":"2023-06-17T13:31:48Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"24b33813-a885-4819-b7cd-fe5accc2fdb2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-17T13:31:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24b33813-a885-4819-b7cd-fe5accc2fdb2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-17T13:31:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wmqt8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wmqt8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-86-18","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-86-18"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-17T13:31:48Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-17T13:31:49Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-17T13:31:49Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-17T13:31:48Z"}],"hostIP":"172.31.86.18","podIP":"192.168.2.86","podIPs":[{"ip":"192.168.2.86"}],"startTime":"2023-06-17T13:31:48Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-17T13:31:49Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://b9fb402f9c1276b91882c4017231c0ba17506e220f7fba9bf773faad7e68729e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-q9bpz","generateName":"daemon-set-","namespace":"daemonsets-3116","uid":"e5eeb028-4f1e-4158-8fcb-510e20daf58d","resourceVersion":"38064","creationTimestamp":"2023-06-17T13:31:48Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"24b33813-a885-4819-b7cd-fe5accc2fdb2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-17T13:31:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24b33813-a885-4819-b7cd-fe5accc2fdb2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-17T13:31:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.91.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4fbln","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4fbln","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-68-253","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-68-253"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-17T13:31:48Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-17T13:31:49Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-17T13:31:49Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-17T13:31:48Z"}],"hostIP":"172.31.68.253","podIP":"192.168.91.175","podIPs":[{"ip":"192.168.91.175"}],"startTime":"2023-06-17T13:31:48Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-17T13:31:49Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://552b95ded6cb23feaa4911c3cbbc3fa74a2bc9d33bf1a0a17916817733afb196","started":true}],"qosClass":"BestEffort"}}]}

  Jun 17 13:31:50.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3116" for this suite. @ 06/17/23 13:31:50.357
• [2.122 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 06/17/23 13:31:50.364
  Jun 17 13:31:50.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename deployment @ 06/17/23 13:31:50.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:31:50.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:31:50.389
  Jun 17 13:31:50.392: INFO: Creating deployment "webserver-deployment"
  Jun 17 13:31:50.396: INFO: Waiting for observed generation 1
  E0617 13:31:51.003399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:52.003426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:31:52.402: INFO: Waiting for all required pods to come up
  Jun 17 13:31:52.406: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 06/17/23 13:31:52.406
  E0617 13:31:53.003856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:54.003948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:31:54.414: INFO: Waiting for deployment "webserver-deployment" to complete
  Jun 17 13:31:54.420: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Jun 17 13:31:54.430: INFO: Updating deployment webserver-deployment
  Jun 17 13:31:54.430: INFO: Waiting for observed generation 2
  E0617 13:31:55.004136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:56.004498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:31:56.436: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Jun 17 13:31:56.439: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Jun 17 13:31:56.442: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jun 17 13:31:56.450: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Jun 17 13:31:56.450: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Jun 17 13:31:56.452: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jun 17 13:31:56.458: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Jun 17 13:31:56.458: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Jun 17 13:31:56.466: INFO: Updating deployment webserver-deployment
  Jun 17 13:31:56.466: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Jun 17 13:31:56.472: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Jun 17 13:31:56.475: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  E0617 13:31:57.005565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:31:58.006072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:31:58.495: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-7306  281f60a5-19a9-4ab0-b8cd-cb25d462960b 38487 3 2023-06-17 13:31:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ffee78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-17 13:31:56 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-06-17 13:31:56 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Jun 17 13:31:58.500: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-7306  dc0fd5a4-e8f0-450c-afac-7050374e1006 38476 3 2023-06-17 13:31:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 281f60a5-19a9-4ab0-b8cd-cb25d462960b 0xc005fff407 0xc005fff408}] [] [{kube-controller-manager Update apps/v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"281f60a5-19a9-4ab0-b8cd-cb25d462960b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005fff4a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 17 13:31:58.500: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Jun 17 13:31:58.504: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-7306  c203b764-c77f-46f7-b9e1-73659ff55b6e 38470 3 2023-06-17 13:31:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 281f60a5-19a9-4ab0-b8cd-cb25d462960b 0xc005fff317 0xc005fff318}] [] [{kube-controller-manager Update apps/v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"281f60a5-19a9-4ab0-b8cd-cb25d462960b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005fff3a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Jun 17 13:31:58.512: INFO: Pod "webserver-deployment-67bd4bf6dc-454gc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-454gc webserver-deployment-67bd4bf6dc- deployment-7306  a5616caa-b488-4671-b551-f3d1b9968526 38453 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc005d78507 0xc005d78508}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9wqwj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9wqwj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.253,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.512: INFO: Pod "webserver-deployment-67bd4bf6dc-56kq5" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-56kq5 webserver-deployment-67bd4bf6dc- deployment-7306  8ccb6fee-911f-4f8c-a82d-0503e6a29baf 38247 0 2023-06-17 13:31:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc005d786d7 0xc005d786d8}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-brqkk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-brqkk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.18,PodIP:192.168.2.83,StartTime:2023-06-17 13:31:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-17 13:31:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://848b7013ed81a35bbe5a88aaf10ca61db2624e85d46097fae027c7db8d450510,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.83,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.512: INFO: Pod "webserver-deployment-67bd4bf6dc-66bzl" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-66bzl webserver-deployment-67bd4bf6dc- deployment-7306  bcf9414f-f817-4e50-ac78-586c136cf996 38235 0 2023-06-17 13:31:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc005d788c7 0xc005d788c8}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.178.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g75fx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g75fx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:192.168.178.141,StartTime:2023-06-17 13:31:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-17 13:31:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6ba37182cad8c946b3b4d229d66c1c00d6a2bcf4d6b395bc5edd872e81008580,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.178.141,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.515: INFO: Pod "webserver-deployment-67bd4bf6dc-66kkd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-66kkd webserver-deployment-67bd4bf6dc- deployment-7306  67afaf55-699a-4479-9e53-4f22010d053e 38427 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc005d78ab7 0xc005d78ab8}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wkjvv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wkjvv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.18,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.515: INFO: Pod "webserver-deployment-67bd4bf6dc-8jhzn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8jhzn webserver-deployment-67bd4bf6dc- deployment-7306  a282aed6-7d1f-4e7f-87ef-fc4acc6ea141 38481 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc005d78c87 0xc005d78c88}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9cfmx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9cfmx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.18,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.515: INFO: Pod "webserver-deployment-67bd4bf6dc-9xknx" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9xknx webserver-deployment-67bd4bf6dc- deployment-7306  adb47447-2583-45ac-a28f-66416d93d9b1 38241 0 2023-06-17 13:31:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc005d78e57 0xc005d78e58}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.178.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8bnd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8bnd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:192.168.178.154,StartTime:2023-06-17 13:31:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-17 13:31:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://dce220459709c31c0bf21f396bad4c6c191436ee1f267f4c363fb9d89c1f6dc8,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.178.154,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.516: INFO: Pod "webserver-deployment-67bd4bf6dc-bkzwb" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-bkzwb webserver-deployment-67bd4bf6dc- deployment-7306  ae93ff9a-cd54-47cb-95e4-e7acb379ab14 38229 0 2023-06-17 13:31:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc005d79047 0xc005d79048}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.91.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kq5rh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kq5rh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.253,PodIP:192.168.91.180,StartTime:2023-06-17 13:31:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-17 13:31:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ab7b3c807bdb8127f0fd76d1b132c9a5e548eb4797cc78c0306b02210003dfbe,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.91.180,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.516: INFO: Pod "webserver-deployment-67bd4bf6dc-bwbkb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-bwbkb webserver-deployment-67bd4bf6dc- deployment-7306  03e262c9-989e-4577-8539-23cca6083f14 38485 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc005d79237 0xc005d79238}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vbgcb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vbgcb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.516: INFO: Pod "webserver-deployment-67bd4bf6dc-fdk6h" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-fdk6h webserver-deployment-67bd4bf6dc- deployment-7306  11335dd3-1e27-4f26-b53a-d37ef0a8fe6c 38250 0 2023-06-17 13:31:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc005d79407 0xc005d79408}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-69l5b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-69l5b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.18,PodIP:192.168.2.88,StartTime:2023-06-17 13:31:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-17 13:31:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5cee99b6c7700b52810dfe248aaef0a3a3e89606bad7fcb2af3ea90a228f4ad7,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.88,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.516: INFO: Pod "webserver-deployment-67bd4bf6dc-gdn6m" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gdn6m webserver-deployment-67bd4bf6dc- deployment-7306  16cb3fa6-72ea-437e-a5c3-f6ffc7613c90 38419 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc005d795f7 0xc005d795f8}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72fsx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72fsx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.517: INFO: Pod "webserver-deployment-67bd4bf6dc-hlnw7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hlnw7 webserver-deployment-67bd4bf6dc- deployment-7306  22549951-c56a-4ff9-9ea2-9d66b75b118f 38491 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc005d797c7 0xc005d797c8}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kghkm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kghkm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.523: INFO: Pod "webserver-deployment-67bd4bf6dc-jk6ln" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jk6ln webserver-deployment-67bd4bf6dc- deployment-7306  ae1174a7-75b8-4d1f-aad9-ee5023742b9e 38484 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc005d79997 0xc005d79998}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6trln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6trln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.253,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.524: INFO: Pod "webserver-deployment-67bd4bf6dc-l7msv" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-l7msv webserver-deployment-67bd4bf6dc- deployment-7306  539027f8-dd37-49b9-a288-4a847d12683e 38226 0 2023-06-17 13:31:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc005d79da7 0xc005d79da8}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.91.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gfblv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gfblv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.253,PodIP:192.168.91.178,StartTime:2023-06-17 13:31:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-17 13:31:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ff5b913ef335f66d98ea0a2bf83a152d28b79ead7ee92907558a0e02dd87a3a5,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.91.178,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.524: INFO: Pod "webserver-deployment-67bd4bf6dc-lc4px" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-lc4px webserver-deployment-67bd4bf6dc- deployment-7306  7f103b81-01d9-460a-b76f-056e5054eb15 38495 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc005d79f97 0xc005d79f98}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-66nqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-66nqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.524: INFO: Pod "webserver-deployment-67bd4bf6dc-ls55g" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-ls55g webserver-deployment-67bd4bf6dc- deployment-7306  c34e5301-cd79-4bac-9fb3-7ae088304162 38223 0 2023-06-17 13:31:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc00561e167 0xc00561e168}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.91.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5n6lb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5n6lb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.253,PodIP:192.168.91.179,StartTime:2023-06-17 13:31:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-17 13:31:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d37c118a9e170e5a77e1bec1f29859e0ad7de13a5e1256509165658c7488a8b2,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.91.179,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.524: INFO: Pod "webserver-deployment-67bd4bf6dc-nkcs5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nkcs5 webserver-deployment-67bd4bf6dc- deployment-7306  48f8c1f8-8ff1-4a3a-ab7e-ac99895a30a3 38490 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc00561e357 0xc00561e358}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9bpmz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9bpmz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.18,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.525: INFO: Pod "webserver-deployment-67bd4bf6dc-qdbpp" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-qdbpp webserver-deployment-67bd4bf6dc- deployment-7306  3edabbd0-887a-4bbe-a3f3-a960d0f44367 38462 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc00561e527 0xc00561e528}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7vqxv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7vqxv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.253,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.525: INFO: Pod "webserver-deployment-67bd4bf6dc-rcfd9" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rcfd9 webserver-deployment-67bd4bf6dc- deployment-7306  48680da2-9472-4b1a-bb9d-404544d47fa0 38445 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc00561e6f7 0xc00561e6f8}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-52r2s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52r2s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.527: INFO: Pod "webserver-deployment-67bd4bf6dc-v7wqc" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-v7wqc webserver-deployment-67bd4bf6dc- deployment-7306  18036e5e-aff9-4077-adcb-e825019988b1 38253 0 2023-06-17 13:31:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc00561e8c7 0xc00561e8c8}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vp7pz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vp7pz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.18,PodIP:192.168.2.89,StartTime:2023-06-17 13:31:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-17 13:31:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cdb499ffed424351a5bbaf374ff10bd78f507eacf71e7cb6ee5a6277db56b85a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.89,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.528: INFO: Pod "webserver-deployment-67bd4bf6dc-vvms8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vvms8 webserver-deployment-67bd4bf6dc- deployment-7306  7a039d53-b808-4d92-82d4-3e75319f850e 38472 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c203b764-c77f-46f7-b9e1-73659ff55b6e 0xc00561eac7 0xc00561eac8}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c203b764-c77f-46f7-b9e1-73659ff55b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cdw7p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cdw7p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.18,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.528: INFO: Pod "webserver-deployment-7b75d79cf5-2445v" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-2445v webserver-deployment-7b75d79cf5- deployment-7306  569a3f6c-6a59-4670-a11f-ffbac19dfeaa 38438 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 dc0fd5a4-e8f0-450c-afac-7050374e1006 0xc00561ec97 0xc00561ec98}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc0fd5a4-e8f0-450c-afac-7050374e1006\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v8846,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v8846,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.18,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.528: INFO: Pod "webserver-deployment-7b75d79cf5-6x8tv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-6x8tv webserver-deployment-7b75d79cf5- deployment-7306  c042b2a5-0766-40b4-8d9a-2ab9555e604e 38377 0 2023-06-17 13:31:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 dc0fd5a4-e8f0-450c-afac-7050374e1006 0xc00561ee87 0xc00561ee88}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc0fd5a4-e8f0-450c-afac-7050374e1006\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5mvb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5mvb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.18,PodIP:192.168.2.92,StartTime:2023-06-17 13:31:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.92,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.528: INFO: Pod "webserver-deployment-7b75d79cf5-7nrp5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-7nrp5 webserver-deployment-7b75d79cf5- deployment-7306  03225654-9489-471d-bcfb-8702f7c21976 38380 0 2023-06-17 13:31:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 dc0fd5a4-e8f0-450c-afac-7050374e1006 0xc00561f0b7 0xc00561f0b8}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc0fd5a4-e8f0-450c-afac-7050374e1006\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7mfg5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7mfg5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.18,PodIP:192.168.2.91,StartTime:2023-06-17 13:31:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.91,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.530: INFO: Pod "webserver-deployment-7b75d79cf5-8xzj4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-8xzj4 webserver-deployment-7b75d79cf5- deployment-7306  b9776a32-98cf-4c30-956c-f02b346a8f62 38391 0 2023-06-17 13:31:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 dc0fd5a4-e8f0-450c-afac-7050374e1006 0xc00561f2d7 0xc00561f2d8}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc0fd5a4-e8f0-450c-afac-7050374e1006\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.178.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cgzm4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cgzm4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:192.168.178.145,StartTime:2023-06-17 13:31:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.178.145,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.535: INFO: Pod "webserver-deployment-7b75d79cf5-9fjfg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-9fjfg webserver-deployment-7b75d79cf5- deployment-7306  b41006ab-6daf-4d69-aa0f-c4ddaa300d7a 38482 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 dc0fd5a4-e8f0-450c-afac-7050374e1006 0xc00561f4f7 0xc00561f4f8}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc0fd5a4-e8f0-450c-afac-7050374e1006\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z7bsv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z7bsv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.253,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.535: INFO: Pod "webserver-deployment-7b75d79cf5-9l8vf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-9l8vf webserver-deployment-7b75d79cf5- deployment-7306  13f50821-311e-466a-b43c-8dd2864e2d49 38393 0 2023-06-17 13:31:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 dc0fd5a4-e8f0-450c-afac-7050374e1006 0xc00561f6e7 0xc00561f6e8}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc0fd5a4-e8f0-450c-afac-7050374e1006\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.178.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2gnjn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2gnjn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:192.168.178.163,StartTime:2023-06-17 13:31:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.178.163,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.536: INFO: Pod "webserver-deployment-7b75d79cf5-jjvnr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-jjvnr webserver-deployment-7b75d79cf5- deployment-7306  52e5554a-0a42-4860-be8a-fc1e2df60b2f 38488 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 dc0fd5a4-e8f0-450c-afac-7050374e1006 0xc00561f907 0xc00561f908}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc0fd5a4-e8f0-450c-afac-7050374e1006\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rc8dv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rc8dv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-18,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.18,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.536: INFO: Pod "webserver-deployment-7b75d79cf5-lkxn9" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lkxn9 webserver-deployment-7b75d79cf5- deployment-7306  6e83d1e1-6554-47d8-91a4-ab7ef79c4e66 38477 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 dc0fd5a4-e8f0-450c-afac-7050374e1006 0xc00561faf7 0xc00561faf8}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc0fd5a4-e8f0-450c-afac-7050374e1006\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-54x5l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-54x5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.536: INFO: Pod "webserver-deployment-7b75d79cf5-mkkbq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-mkkbq webserver-deployment-7b75d79cf5- deployment-7306  ca5a0fef-57c9-4f9c-b188-a34c1a5ca463 38497 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 dc0fd5a4-e8f0-450c-afac-7050374e1006 0xc00561fce7 0xc00561fce8}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc0fd5a4-e8f0-450c-afac-7050374e1006\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2fn5j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2fn5j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.536: INFO: Pod "webserver-deployment-7b75d79cf5-r7ll9" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-r7ll9 webserver-deployment-7b75d79cf5- deployment-7306  605843fb-7e5b-4606-9bfb-d50b735cbbf4 38494 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 dc0fd5a4-e8f0-450c-afac-7050374e1006 0xc00561fed7 0xc00561fed8}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc0fd5a4-e8f0-450c-afac-7050374e1006\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fvzz7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fvzz7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-25-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.25.17,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.537: INFO: Pod "webserver-deployment-7b75d79cf5-rt2r2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-rt2r2 webserver-deployment-7b75d79cf5- deployment-7306  85b7d9eb-f7ef-47bc-a38f-e215c7b36749 38386 0 2023-06-17 13:31:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 dc0fd5a4-e8f0-450c-afac-7050374e1006 0xc0047c2267 0xc0047c2268}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc0fd5a4-e8f0-450c-afac-7050374e1006\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.91.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v9qll,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v9qll,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.253,PodIP:192.168.91.181,StartTime:2023-06-17 13:31:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.91.181,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.537: INFO: Pod "webserver-deployment-7b75d79cf5-t9zfn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-t9zfn webserver-deployment-7b75d79cf5- deployment-7306  39dac734-54ff-4719-aba3-0d52ab017370 38474 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 dc0fd5a4-e8f0-450c-afac-7050374e1006 0xc0047c2487 0xc0047c2488}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc0fd5a4-e8f0-450c-afac-7050374e1006\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hb6jr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hb6jr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.253,PodIP:,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.538: INFO: Pod "webserver-deployment-7b75d79cf5-z4vgs" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-z4vgs webserver-deployment-7b75d79cf5- deployment-7306  9560aae8-f6f1-41ce-85d3-1dbc6f88394b 38560 0 2023-06-17 13:31:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 dc0fd5a4-e8f0-450c-afac-7050374e1006 0xc0047c2677 0xc0047c2678}] [] [{kube-controller-manager Update v1 2023-06-17 13:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc0fd5a4-e8f0-450c-afac-7050374e1006\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-17 13:31:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.91.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-szdzw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-szdzw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-17 13:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.253,PodIP:192.168.91.176,StartTime:2023-06-17 13:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.91.176,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 17 13:31:58.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7306" for this suite. @ 06/17/23 13:31:58.543
• [8.186 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 06/17/23 13:31:58.556
  Jun 17 13:31:58.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename taint-single-pod @ 06/17/23 13:31:58.563
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:31:58.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:31:58.593
  Jun 17 13:31:58.599: INFO: Waiting up to 1m0s for all nodes to be ready
  E0617 13:31:59.007076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:00.007178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:01.007614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:02.007827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:03.007932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:04.008162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:05.008286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:06.009293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:07.010264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:08.010386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:09.010940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:10.011217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:11.011315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:12.011366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:13.011885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:14.012004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:15.012130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:16.012158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:17.012909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:18.013080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:19.013659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:20.013853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:21.014187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:22.014362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:23.014900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:24.015868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:25.016796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:26.017032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:27.017808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:28.017996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:29.018090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:30.018281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:31.018339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:32.018529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:33.018780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:34.018819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:35.019862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:36.020242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:37.021124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:38.021216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:39.022236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:40.022421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:41.022563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:42.022810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:43.023871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:44.024058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:45.024846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:46.025257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:47.025954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:48.026142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:49.026417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:50.026508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:51.027372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:52.028432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:53.029356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:54.029583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:55.029696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:56.030680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:57.031360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:32:58.031869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:32:58.620: INFO: Waiting for terminating namespaces to be deleted...
  Jun 17 13:32:58.623: INFO: Starting informer...
  STEP: Starting pod... @ 06/17/23 13:32:58.623
  Jun 17 13:32:58.836: INFO: Pod is running on ip-172-31-25-17. Tainting Node
  STEP: Trying to apply a taint on the Node @ 06/17/23 13:32:58.836
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 06/17/23 13:32:58.853
  STEP: Waiting short time to make sure Pod is queued for deletion @ 06/17/23 13:32:58.855
  Jun 17 13:32:58.855: INFO: Pod wasn't evicted. Proceeding
  Jun 17 13:32:58.855: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 06/17/23 13:32:58.866
  STEP: Waiting some time to make sure that toleration time passed. @ 06/17/23 13:32:58.876
  E0617 13:32:59.032869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:00.033042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:01.033312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:02.033396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:03.033514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:04.033895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:05.034126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:06.034936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:07.035868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:08.036005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:09.036151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:10.036210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:11.036780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:12.037797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:13.037935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:14.038133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:15.038364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:16.039221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:17.039867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:18.040583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:19.040725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:20.040821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:21.040946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:22.041631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:23.041736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:24.041798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:25.041863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:26.042080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:27.042283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:28.042458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:29.042956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:30.043044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:31.043136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:32.043279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:33.043376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:34.043876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:35.044073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:36.044329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:37.044855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:38.045197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:39.045366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:40.045585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:41.045837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:42.046063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:43.046255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:44.046359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:45.046529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:46.047248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:47.047351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:48.047873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:49.047972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:50.048182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:51.048544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:52.048659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:53.048775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:54.048979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:55.049149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:56.049366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:57.049558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:58.049714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:33:59.049859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:00.049959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:01.050427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:02.050554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:03.050648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:04.050836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:05.051878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:06.052930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:07.053099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:08.053288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:09.053484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:10.053785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:11.054800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:12.055301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:13.055410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:34:13.876: INFO: Pod wasn't evicted. Test successful
  Jun 17 13:34:13.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-4598" for this suite. @ 06/17/23 13:34:13.88
• [135.331 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 06/17/23 13:34:13.888
  Jun 17 13:34:13.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename proxy @ 06/17/23 13:34:13.889
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:34:13.906
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:34:13.908
  Jun 17 13:34:13.911: INFO: Creating pod...
  E0617 13:34:14.055886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:15.055952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:34:15.926: INFO: Creating service...
  Jun 17 13:34:15.934: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2150/pods/agnhost/proxy?method=DELETE
  Jun 17 13:34:15.941: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jun 17 13:34:15.941: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2150/pods/agnhost/proxy?method=OPTIONS
  Jun 17 13:34:15.945: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jun 17 13:34:15.945: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2150/pods/agnhost/proxy?method=PATCH
  Jun 17 13:34:15.948: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jun 17 13:34:15.948: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2150/pods/agnhost/proxy?method=POST
  Jun 17 13:34:15.952: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jun 17 13:34:15.952: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2150/pods/agnhost/proxy?method=PUT
  Jun 17 13:34:15.956: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jun 17 13:34:15.956: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2150/services/e2e-proxy-test-service/proxy?method=DELETE
  Jun 17 13:34:15.960: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jun 17 13:34:15.960: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2150/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Jun 17 13:34:15.965: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jun 17 13:34:15.965: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2150/services/e2e-proxy-test-service/proxy?method=PATCH
  Jun 17 13:34:15.970: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jun 17 13:34:15.970: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2150/services/e2e-proxy-test-service/proxy?method=POST
  Jun 17 13:34:15.975: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jun 17 13:34:15.975: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2150/services/e2e-proxy-test-service/proxy?method=PUT
  Jun 17 13:34:15.980: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jun 17 13:34:15.981: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2150/pods/agnhost/proxy?method=GET
  Jun 17 13:34:15.984: INFO: http.Client request:GET StatusCode:301
  Jun 17 13:34:15.984: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2150/services/e2e-proxy-test-service/proxy?method=GET
  Jun 17 13:34:15.987: INFO: http.Client request:GET StatusCode:301
  Jun 17 13:34:15.987: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2150/pods/agnhost/proxy?method=HEAD
  Jun 17 13:34:15.990: INFO: http.Client request:HEAD StatusCode:301
  Jun 17 13:34:15.990: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2150/services/e2e-proxy-test-service/proxy?method=HEAD
  Jun 17 13:34:15.994: INFO: http.Client request:HEAD StatusCode:301
  Jun 17 13:34:15.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-2150" for this suite. @ 06/17/23 13:34:15.998
• [2.117 seconds]
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 06/17/23 13:34:16.005
  Jun 17 13:34:16.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename container-runtime @ 06/17/23 13:34:16.006
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:34:16.025
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:34:16.028
  STEP: create the container @ 06/17/23 13:34:16.031
  W0617 13:34:16.040013      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 06/17/23 13:34:16.04
  E0617 13:34:16.056260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:17.056636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:18.057584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 06/17/23 13:34:19.054
  STEP: the container should be terminated @ 06/17/23 13:34:19.057
  STEP: the termination message should be set @ 06/17/23 13:34:19.057
  Jun 17 13:34:19.057: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 06/17/23 13:34:19.057
  E0617 13:34:19.057649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:34:19.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1668" for this suite. @ 06/17/23 13:34:19.075
• [3.079 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 06/17/23 13:34:19.084
  Jun 17 13:34:19.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename gc @ 06/17/23 13:34:19.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:34:19.107
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:34:19.109
  STEP: create the rc1 @ 06/17/23 13:34:19.116
  STEP: create the rc2 @ 06/17/23 13:34:19.121
  E0617 13:34:20.058626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:21.059346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:22.059426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:23.059584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:24.059709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:25.061106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 06/17/23 13:34:25.135
  STEP: delete the rc simpletest-rc-to-be-deleted @ 06/17/23 13:34:25.514
  STEP: wait for the rc to be deleted @ 06/17/23 13:34:25.52
  E0617 13:34:26.061744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:27.062594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:28.063729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:29.067023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:30.067918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:34:30.612: INFO: 69 pods remaining
  Jun 17 13:34:30.612: INFO: 69 pods has nil DeletionTimestamp
  Jun 17 13:34:30.612: INFO: 
  E0617 13:34:31.067964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:32.068385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:33.068484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:34.068783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:35.068907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 06/17/23 13:34:35.533
  W0617 13:34:35.536976      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 17 13:34:35.537: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 17 13:34:35.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-26zrc" in namespace "gc-5527"
  Jun 17 13:34:35.550: INFO: Deleting pod "simpletest-rc-to-be-deleted-2c54p" in namespace "gc-5527"
  Jun 17 13:34:35.562: INFO: Deleting pod "simpletest-rc-to-be-deleted-2dr8k" in namespace "gc-5527"
  Jun 17 13:34:35.574: INFO: Deleting pod "simpletest-rc-to-be-deleted-2ngmb" in namespace "gc-5527"
  Jun 17 13:34:35.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-2nxsl" in namespace "gc-5527"
  Jun 17 13:34:35.599: INFO: Deleting pod "simpletest-rc-to-be-deleted-49wf9" in namespace "gc-5527"
  Jun 17 13:34:35.610: INFO: Deleting pod "simpletest-rc-to-be-deleted-4cd7q" in namespace "gc-5527"
  Jun 17 13:34:35.633: INFO: Deleting pod "simpletest-rc-to-be-deleted-4p7pr" in namespace "gc-5527"
  Jun 17 13:34:35.647: INFO: Deleting pod "simpletest-rc-to-be-deleted-5kvd5" in namespace "gc-5527"
  Jun 17 13:34:35.662: INFO: Deleting pod "simpletest-rc-to-be-deleted-5rf2z" in namespace "gc-5527"
  Jun 17 13:34:35.672: INFO: Deleting pod "simpletest-rc-to-be-deleted-6hsnm" in namespace "gc-5527"
  Jun 17 13:34:35.686: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nsjr" in namespace "gc-5527"
  Jun 17 13:34:35.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vf8g" in namespace "gc-5527"
  Jun 17 13:34:35.711: INFO: Deleting pod "simpletest-rc-to-be-deleted-72j7x" in namespace "gc-5527"
  Jun 17 13:34:35.723: INFO: Deleting pod "simpletest-rc-to-be-deleted-7brsr" in namespace "gc-5527"
  Jun 17 13:34:35.735: INFO: Deleting pod "simpletest-rc-to-be-deleted-86vtn" in namespace "gc-5527"
  Jun 17 13:34:35.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-8btjf" in namespace "gc-5527"
  Jun 17 13:34:35.761: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bwqh" in namespace "gc-5527"
  Jun 17 13:34:35.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jmxl" in namespace "gc-5527"
  Jun 17 13:34:35.784: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mq95" in namespace "gc-5527"
  Jun 17 13:34:35.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-8q7f2" in namespace "gc-5527"
  Jun 17 13:34:35.808: INFO: Deleting pod "simpletest-rc-to-be-deleted-962nj" in namespace "gc-5527"
  Jun 17 13:34:35.821: INFO: Deleting pod "simpletest-rc-to-be-deleted-98xm8" in namespace "gc-5527"
  Jun 17 13:34:35.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qncx" in namespace "gc-5527"
  Jun 17 13:34:35.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vs4p" in namespace "gc-5527"
  Jun 17 13:34:35.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-9x2q5" in namespace "gc-5527"
  Jun 17 13:34:35.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-b949l" in namespace "gc-5527"
  Jun 17 13:34:35.889: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgz2f" in namespace "gc-5527"
  Jun 17 13:34:35.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxchv" in namespace "gc-5527"
  Jun 17 13:34:35.916: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7xpm" in namespace "gc-5527"
  Jun 17 13:34:35.930: INFO: Deleting pod "simpletest-rc-to-be-deleted-cghsl" in namespace "gc-5527"
  Jun 17 13:34:35.945: INFO: Deleting pod "simpletest-rc-to-be-deleted-cs2jb" in namespace "gc-5527"
  Jun 17 13:34:35.958: INFO: Deleting pod "simpletest-rc-to-be-deleted-csrrz" in namespace "gc-5527"
  Jun 17 13:34:35.971: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctpfg" in namespace "gc-5527"
  Jun 17 13:34:35.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7l5t" in namespace "gc-5527"
  Jun 17 13:34:36.000: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7zqv" in namespace "gc-5527"
  Jun 17 13:34:36.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4rxx" in namespace "gc-5527"
  Jun 17 13:34:36.030: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7864" in namespace "gc-5527"
  Jun 17 13:34:36.042: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgh9f" in namespace "gc-5527"
  Jun 17 13:34:36.058: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhz66" in namespace "gc-5527"
  E0617 13:34:36.069597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:34:36.070: INFO: Deleting pod "simpletest-rc-to-be-deleted-fp6k7" in namespace "gc-5527"
  Jun 17 13:34:36.083: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftbm5" in namespace "gc-5527"
  Jun 17 13:34:36.102: INFO: Deleting pod "simpletest-rc-to-be-deleted-gc5g2" in namespace "gc-5527"
  Jun 17 13:34:36.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-glh7p" in namespace "gc-5527"
  Jun 17 13:34:36.134: INFO: Deleting pod "simpletest-rc-to-be-deleted-gqs68" in namespace "gc-5527"
  Jun 17 13:34:36.147: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsblm" in namespace "gc-5527"
  Jun 17 13:34:36.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-gx5n5" in namespace "gc-5527"
  Jun 17 13:34:36.171: INFO: Deleting pod "simpletest-rc-to-be-deleted-h26bv" in namespace "gc-5527"
  Jun 17 13:34:36.181: INFO: Deleting pod "simpletest-rc-to-be-deleted-h5sj8" in namespace "gc-5527"
  Jun 17 13:34:36.195: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6nhp" in namespace "gc-5527"
  Jun 17 13:34:36.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5527" for this suite. @ 06/17/23 13:34:36.209
• [17.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 06/17/23 13:34:36.217
  Jun 17 13:34:36.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename security-context-test @ 06/17/23 13:34:36.218
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:34:36.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:34:36.253
  E0617 13:34:37.069974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:38.070727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:39.071446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:40.072193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:41.072918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:42.073234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:43.073708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:44.074311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:34:44.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-943" for this suite. @ 06/17/23 13:34:44.319
• [8.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 06/17/23 13:34:44.328
  Jun 17 13:34:44.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename pods @ 06/17/23 13:34:44.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:34:44.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:34:44.352
  STEP: Create set of pods @ 06/17/23 13:34:44.358
  Jun 17 13:34:44.367: INFO: created test-pod-1
  Jun 17 13:34:44.375: INFO: created test-pod-2
  Jun 17 13:34:44.383: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 06/17/23 13:34:44.384
  E0617 13:34:45.075092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:46.075580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 06/17/23 13:34:46.43
  Jun 17 13:34:46.433: INFO: Pod quantity 3 is different from expected quantity 0
  E0617 13:34:47.075871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:34:47.437: INFO: Pod quantity 2 is different from expected quantity 0
  E0617 13:34:48.076372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:34:48.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7270" for this suite. @ 06/17/23 13:34:48.44
• [4.118 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 06/17/23 13:34:48.447
  Jun 17 13:34:48.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename namespaces @ 06/17/23 13:34:48.448
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:34:48.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:34:48.471
  STEP: Creating a test namespace @ 06/17/23 13:34:48.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:34:48.491
  STEP: Creating a service in the namespace @ 06/17/23 13:34:48.494
  STEP: Deleting the namespace @ 06/17/23 13:34:48.503
  STEP: Waiting for the namespace to be removed. @ 06/17/23 13:34:48.511
  E0617 13:34:49.077404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:50.077480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:51.078297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:52.078388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:53.079451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:54.079537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 06/17/23 13:34:54.514
  STEP: Verifying there is no service in the namespace @ 06/17/23 13:34:54.531
  Jun 17 13:34:54.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5227" for this suite. @ 06/17/23 13:34:54.537
  STEP: Destroying namespace "nsdeletetest-6554" for this suite. @ 06/17/23 13:34:54.544
  Jun 17 13:34:54.546: INFO: Namespace nsdeletetest-6554 was already deleted
  STEP: Destroying namespace "nsdeletetest-7596" for this suite. @ 06/17/23 13:34:54.546
• [6.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 06/17/23 13:34:54.555
  Jun 17 13:34:54.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename dns @ 06/17/23 13:34:54.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:34:54.574
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:34:54.577
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3484.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3484.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 06/17/23 13:34:54.58
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3484.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3484.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 06/17/23 13:34:54.58
  STEP: creating a pod to probe /etc/hosts @ 06/17/23 13:34:54.58
  STEP: submitting the pod to kubernetes @ 06/17/23 13:34:54.58
  E0617 13:34:55.080568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:56.080803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/17/23 13:34:56.596
  STEP: looking for the results for each expected name from probers @ 06/17/23 13:34:56.599
  Jun 17 13:34:56.613: INFO: DNS probes using dns-3484/dns-test-1e95ddf9-72bd-44fc-8a68-7dede6241ad1 succeeded

  Jun 17 13:34:56.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/17/23 13:34:56.617
  STEP: Destroying namespace "dns-3484" for this suite. @ 06/17/23 13:34:56.629
• [2.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 06/17/23 13:34:56.636
  Jun 17 13:34:56.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename statefulset @ 06/17/23 13:34:56.637
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:34:56.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:34:56.662
  STEP: Creating service test in namespace statefulset-9642 @ 06/17/23 13:34:56.665
  STEP: Creating statefulset ss in namespace statefulset-9642 @ 06/17/23 13:34:56.671
  Jun 17 13:34:56.681: INFO: Found 0 stateful pods, waiting for 1
  E0617 13:34:57.081686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:58.081710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:34:59.081795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:00.081875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:01.082429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:02.082514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:03.083013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:04.083102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:05.083175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:06.083404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:35:06.685: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 06/17/23 13:35:06.691
  STEP: Getting /status @ 06/17/23 13:35:06.701
  Jun 17 13:35:06.705: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 06/17/23 13:35:06.705
  Jun 17 13:35:06.714: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 06/17/23 13:35:06.714
  Jun 17 13:35:06.715: INFO: Observed &StatefulSet event: ADDED
  Jun 17 13:35:06.715: INFO: Found Statefulset ss in namespace statefulset-9642 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 17 13:35:06.715: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 06/17/23 13:35:06.715
  Jun 17 13:35:06.715: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jun 17 13:35:06.723: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 06/17/23 13:35:06.723
  Jun 17 13:35:06.725: INFO: Observed &StatefulSet event: ADDED
  Jun 17 13:35:06.725: INFO: Deleting all statefulset in ns statefulset-9642
  Jun 17 13:35:06.729: INFO: Scaling statefulset ss to 0
  E0617 13:35:07.084306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:08.084647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:09.084742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:10.084940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:11.085421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:12.085620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:13.085718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:14.085791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:15.085889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:16.085969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:35:16.746: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 17 13:35:16.749: INFO: Deleting statefulset ss
  Jun 17 13:35:16.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9642" for this suite. @ 06/17/23 13:35:16.767
• [20.139 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 06/17/23 13:35:16.775
  Jun 17 13:35:16.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 13:35:16.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:35:16.797
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:35:16.801
  STEP: Setting up server cert @ 06/17/23 13:35:16.833
  E0617 13:35:17.087075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 13:35:17.109
  STEP: Deploying the webhook pod @ 06/17/23 13:35:17.119
  STEP: Wait for the deployment to be ready @ 06/17/23 13:35:17.135
  Jun 17 13:35:17.143: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0617 13:35:18.087948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:19.088183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/17/23 13:35:19.154
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 13:35:19.164
  E0617 13:35:20.089124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:35:20.164: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 06/17/23 13:35:20.168
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 06/17/23 13:35:20.187
  STEP: Creating a configMap that should not be mutated @ 06/17/23 13:35:20.194
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 06/17/23 13:35:20.203
  STEP: Creating a configMap that should be mutated @ 06/17/23 13:35:20.21
  Jun 17 13:35:20.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-761" for this suite. @ 06/17/23 13:35:20.272
  STEP: Destroying namespace "webhook-markers-7079" for this suite. @ 06/17/23 13:35:20.278
• [3.510 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 06/17/23 13:35:20.285
  Jun 17 13:35:20.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename sysctl @ 06/17/23 13:35:20.286
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:35:20.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:35:20.309
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 06/17/23 13:35:20.315
  STEP: Watching for error events or started pod @ 06/17/23 13:35:20.322
  E0617 13:35:21.089229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:22.089439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 06/17/23 13:35:22.327
  E0617 13:35:23.089494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:24.089717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 06/17/23 13:35:24.339
  STEP: Getting logs from the pod @ 06/17/23 13:35:24.339
  STEP: Checking that the sysctl is actually updated @ 06/17/23 13:35:24.346
  Jun 17 13:35:24.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-7392" for this suite. @ 06/17/23 13:35:24.35
• [4.070 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 06/17/23 13:35:24.356
  Jun 17 13:35:24.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename runtimeclass @ 06/17/23 13:35:24.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:35:24.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:35:24.379
  STEP: getting /apis @ 06/17/23 13:35:24.382
  STEP: getting /apis/node.k8s.io @ 06/17/23 13:35:24.387
  STEP: getting /apis/node.k8s.io/v1 @ 06/17/23 13:35:24.389
  STEP: creating @ 06/17/23 13:35:24.39
  STEP: watching @ 06/17/23 13:35:24.404
  Jun 17 13:35:24.404: INFO: starting watch
  STEP: getting @ 06/17/23 13:35:24.408
  STEP: listing @ 06/17/23 13:35:24.411
  STEP: patching @ 06/17/23 13:35:24.414
  STEP: updating @ 06/17/23 13:35:24.419
  Jun 17 13:35:24.423: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 06/17/23 13:35:24.424
  STEP: deleting a collection @ 06/17/23 13:35:24.435
  Jun 17 13:35:24.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6138" for this suite. @ 06/17/23 13:35:24.455
• [0.104 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 06/17/23 13:35:24.462
  Jun 17 13:35:24.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/17/23 13:35:24.463
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:35:24.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:35:24.487
  Jun 17 13:35:24.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  E0617 13:35:25.090454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:26.091004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:27.091173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:35:27.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-3581" for this suite. @ 06/17/23 13:35:27.608
• [3.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 06/17/23 13:35:27.616
  Jun 17 13:35:27.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 13:35:27.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:35:27.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:35:27.641
  STEP: Setting up server cert @ 06/17/23 13:35:27.676
  E0617 13:35:28.091903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 13:35:28.119
  STEP: Deploying the webhook pod @ 06/17/23 13:35:28.125
  STEP: Wait for the deployment to be ready @ 06/17/23 13:35:28.139
  Jun 17 13:35:28.146: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0617 13:35:29.092157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:30.092269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/17/23 13:35:30.155
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 13:35:30.165
  E0617 13:35:31.093105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:35:31.166: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 06/17/23 13:35:31.169
  STEP: create a pod @ 06/17/23 13:35:31.185
  E0617 13:35:32.093192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:33.093409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 06/17/23 13:35:33.2
  Jun 17 13:35:33.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2190159689 --namespace=webhook-6834 attach --namespace=webhook-6834 to-be-attached-pod -i -c=container1'
  Jun 17 13:35:33.284: INFO: rc: 1
  Jun 17 13:35:33.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6834" for this suite. @ 06/17/23 13:35:33.327
  STEP: Destroying namespace "webhook-markers-9380" for this suite. @ 06/17/23 13:35:33.332
• [5.724 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 06/17/23 13:35:33.341
  Jun 17 13:35:33.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename services @ 06/17/23 13:35:33.342
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:35:33.361
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:35:33.364
  STEP: creating an Endpoint @ 06/17/23 13:35:33.37
  STEP: waiting for available Endpoint @ 06/17/23 13:35:33.374
  STEP: listing all Endpoints @ 06/17/23 13:35:33.376
  STEP: updating the Endpoint @ 06/17/23 13:35:33.379
  STEP: fetching the Endpoint @ 06/17/23 13:35:33.387
  STEP: patching the Endpoint @ 06/17/23 13:35:33.389
  STEP: fetching the Endpoint @ 06/17/23 13:35:33.397
  STEP: deleting the Endpoint by Collection @ 06/17/23 13:35:33.4
  STEP: waiting for Endpoint deletion @ 06/17/23 13:35:33.407
  STEP: fetching the Endpoint @ 06/17/23 13:35:33.408
  Jun 17 13:35:33.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-956" for this suite. @ 06/17/23 13:35:33.414
• [0.080 seconds]
------------------------------
SSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 06/17/23 13:35:33.422
  Jun 17 13:35:33.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename cronjob @ 06/17/23 13:35:33.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:35:33.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:35:33.445
  STEP: Creating a suspended cronjob @ 06/17/23 13:35:33.447
  STEP: Ensuring no jobs are scheduled @ 06/17/23 13:35:33.452
  E0617 13:35:34.093480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:35.093581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:36.094018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:37.094119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:38.094157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:39.094413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:40.094507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:41.094828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:42.095834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:43.095933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:44.096013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:45.096639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:46.096918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:47.097108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:48.097859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:49.097957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:50.098028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:51.098381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:52.098476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:53.098727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:54.099509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:55.099795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:56.100262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:57.100402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:58.100725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:35:59.101397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:00.101699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:01.102713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:02.102843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:03.102926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:04.103010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:05.103877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:06.104895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:07.104985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:08.105486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:09.105789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:10.106221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:11.105944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:12.105947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:13.106114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:14.106206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:15.106405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:16.106946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:17.107869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:18.107968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:19.108151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:20.108203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:21.108276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:22.108463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:23.108659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:24.109743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:25.109994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:26.110793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:27.110820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:28.111322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:29.111414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:30.111854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:31.112351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:32.112518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:33.112655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:34.113585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:35.113689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:36.114320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:37.114501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:38.115412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:39.116282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:40.117342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:41.118385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:42.118807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:43.119873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:44.120745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:45.120829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:46.121169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:47.121391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:48.121649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:49.122270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:50.122371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:51.122748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:52.122826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:53.123878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:54.124521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:55.125077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:56.125784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:57.125874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:58.126372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:36:59.126574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:00.126819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:01.127173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:02.127869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:03.128645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:04.129189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:05.129398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:06.130002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:07.130104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:08.130190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:09.130390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:10.131089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:11.131441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:12.131911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:13.132090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:14.133139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:15.133312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:16.133988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:17.134091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:18.134216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:19.134295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:20.134669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:21.135235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:22.135939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:23.136034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:24.136680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:25.136782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:26.137429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:27.137640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:28.137734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:29.137911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:30.138373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:31.138763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:32.139088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:33.139187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:34.139866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:35.140079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:36.140865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:37.140936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:38.141630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:39.142527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:40.143340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:41.143420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:42.143516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:43.143876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:44.144164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:45.144363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:46.145290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:47.145378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:48.145477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:49.145701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:50.146591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:51.146805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:52.147412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:53.147881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:54.148368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:55.149348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:56.150366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:57.151381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:58.152281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:37:59.152373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:00.152469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:01.152838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:02.153726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:03.154779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:04.154824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:05.155878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:06.156098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:07.156305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:08.156977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:09.158021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:10.158434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:11.158534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:12.158802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:13.159862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:14.160274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:15.160430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:16.160989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:17.161673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:18.162537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:19.162753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:20.163251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:21.164140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:22.164672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:23.164950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:24.165040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:25.165245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:26.165299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:27.165837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:28.166412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:29.166566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:30.166658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:31.167154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:32.167250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:33.167874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:34.167969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:35.168066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:36.168115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:37.168315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:38.169196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:39.169380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:40.169949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:41.170365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:42.170534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:43.170648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:44.170817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:45.171868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:46.172288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:47.172457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:48.172485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:49.172679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:50.172768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:51.173259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:52.173589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:53.173770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:54.174105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:55.174855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:56.175382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:57.175867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:58.176021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:38:59.176736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:00.176928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:01.177072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:02.177143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:03.177343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:04.177639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:05.177914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:06.178152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:07.178728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:08.179580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:09.179677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:10.179771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:11.180188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:12.180276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:13.180790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:14.181716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:15.181819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:16.182462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:17.182662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:18.182797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:19.182885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:20.183533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:21.184521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:22.184612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:23.184783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:24.185012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:25.185220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:26.185326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:27.185521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:28.185632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:29.186678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:30.186808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:31.187216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:32.187877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:33.188100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:34.188193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:35.188362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:36.188457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:37.188671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:38.189735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:39.189884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:40.189983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:41.190086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:42.190170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:43.190817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:44.191717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:45.192002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:46.192093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:47.192362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:48.192454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:49.192871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:50.193718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:51.194163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:52.194258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:53.194425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:54.194608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:55.194812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:56.195400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:57.195495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:58.195972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:39:59.196175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:00.196418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:01.196561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:02.196857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:03.197051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:04.197722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:05.197815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:06.198733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:07.198820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:08.199869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:09.200018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:10.200295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:11.200223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:12.200880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:13.201863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:14.201978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:15.202064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:16.202159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:17.203105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:18.203799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:19.204005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:20.204417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:21.204508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:22.204526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:23.204722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:24.204866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:25.205297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:26.205932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:27.206023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:28.206220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:29.206418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:30.206804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:31.207080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:32.207157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:33.207884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 06/17/23 13:40:33.459
  STEP: Removing cronjob @ 06/17/23 13:40:33.462
  Jun 17 13:40:33.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5844" for this suite. @ 06/17/23 13:40:33.47
• [300.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 06/17/23 13:40:33.477
  Jun 17 13:40:33.477: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename aggregator @ 06/17/23 13:40:33.478
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:40:33.506
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:40:33.509
  Jun 17 13:40:33.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Registering the sample API server. @ 06/17/23 13:40:33.512
  Jun 17 13:40:33.861: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Jun 17 13:40:33.900: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  E0617 13:40:34.208511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:35.208706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:40:35.942: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0617 13:40:36.209716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:37.210595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:40:37.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0617 13:40:38.211038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:39.211880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:40:39.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0617 13:40:40.212580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:41.213536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:40:41.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0617 13:40:42.214202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:43.214595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:40:43.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0617 13:40:44.215453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:45.215557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:40:45.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0617 13:40:46.215743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:47.215779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:40:47.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0617 13:40:48.216692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:49.216803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:40:49.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0617 13:40:50.217390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:51.217388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:40:51.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0617 13:40:52.218132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:53.219130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:40:53.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0617 13:40:54.219733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:55.219782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:40:56.068: INFO: Waited 113.694108ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 06/17/23 13:40:56.111
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 06/17/23 13:40:56.115
  STEP: List APIServices @ 06/17/23 13:40:56.121
  Jun 17 13:40:56.128: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 06/17/23 13:40:56.128
  Jun 17 13:40:56.142: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 06/17/23 13:40:56.142
  Jun 17 13:40:56.153: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.June, 17, 13, 40, 55, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 06/17/23 13:40:56.153
  Jun 17 13:40:56.156: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-06-17 13:40:55 +0000 UTC Passed all checks passed}
  Jun 17 13:40:56.156: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 17 13:40:56.156: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 06/17/23 13:40:56.156
  Jun 17 13:40:56.168: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1691628226" @ 06/17/23 13:40:56.168
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 06/17/23 13:40:56.186
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 06/17/23 13:40:56.194
  STEP: Patch APIService Status @ 06/17/23 13:40:56.197
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 06/17/23 13:40:56.207
  Jun 17 13:40:56.211: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-06-17 13:40:55 +0000 UTC Passed all checks passed}
  Jun 17 13:40:56.211: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 17 13:40:56.211: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Jun 17 13:40:56.211: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 06/17/23 13:40:56.212
  STEP: Confirm that the generated APIService has been deleted @ 06/17/23 13:40:56.217
  Jun 17 13:40:56.217: INFO: Requesting list of APIServices to confirm quantity
  E0617 13:40:56.220566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:40:56.221: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Jun 17 13:40:56.222: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Jun 17 13:40:56.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-8289" for this suite. @ 06/17/23 13:40:56.343
• [22.874 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 06/17/23 13:40:56.351
  Jun 17 13:40:56.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename crd-webhook @ 06/17/23 13:40:56.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:40:56.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:40:56.374
  STEP: Setting up server cert @ 06/17/23 13:40:56.379
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 06/17/23 13:40:56.612
  STEP: Deploying the custom resource conversion webhook pod @ 06/17/23 13:40:56.617
  STEP: Wait for the deployment to be ready @ 06/17/23 13:40:56.629
  Jun 17 13:40:56.636: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0617 13:40:57.221551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:40:58.221678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/17/23 13:40:58.646
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 13:40:58.657
  E0617 13:40:59.222203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:40:59.658: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jun 17 13:40:59.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  E0617 13:41:00.222898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:41:01.223897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:41:02.223963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 06/17/23 13:41:02.255
  STEP: v2 custom resource should be converted @ 06/17/23 13:41:02.261
  Jun 17 13:41:02.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-1475" for this suite. @ 06/17/23 13:41:02.838
• [6.495 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 06/17/23 13:41:02.847
  Jun 17 13:41:02.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename replicaset @ 06/17/23 13:41:02.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:41:02.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:41:02.876
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 06/17/23 13:41:02.88
  E0617 13:41:03.224793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:41:04.224900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 06/17/23 13:41:04.899
  STEP: Then the orphan pod is adopted @ 06/17/23 13:41:04.904
  E0617 13:41:05.225731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 06/17/23 13:41:05.913
  Jun 17 13:41:05.916: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 06/17/23 13:41:05.928
  E0617 13:41:06.226620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:41:06.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9329" for this suite. @ 06/17/23 13:41:06.94
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 06/17/23 13:41:06.949
  Jun 17 13:41:06.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename pods @ 06/17/23 13:41:06.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:41:06.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:41:06.973
  Jun 17 13:41:06.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: creating the pod @ 06/17/23 13:41:06.981
  STEP: submitting the pod to kubernetes @ 06/17/23 13:41:06.982
  E0617 13:41:07.226863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:41:08.226923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:41:09.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1864" for this suite. @ 06/17/23 13:41:09.031
• [2.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 06/17/23 13:41:09.039
  Jun 17 13:41:09.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename configmap @ 06/17/23 13:41:09.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:41:09.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:41:09.059
  STEP: Creating configMap with name configmap-test-volume-caadcddb-343f-46f9-9ba0-2524a9fcd16a @ 06/17/23 13:41:09.062
  STEP: Creating a pod to test consume configMaps @ 06/17/23 13:41:09.068
  E0617 13:41:09.227023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:41:10.227121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:41:11.227916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:41:12.228033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:41:13.087
  Jun 17 13:41:13.090: INFO: Trying to get logs from node ip-172-31-86-18 pod pod-configmaps-d7000926-952d-4434-acd9-df9ae4cfcf1e container agnhost-container: <nil>
  STEP: delete the pod @ 06/17/23 13:41:13.104
  Jun 17 13:41:13.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6202" for this suite. @ 06/17/23 13:41:13.122
• [4.089 seconds]
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 06/17/23 13:41:13.128
  Jun 17 13:41:13.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename svcaccounts @ 06/17/23 13:41:13.129
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:41:13.149
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:41:13.151
  STEP: Creating ServiceAccount "e2e-sa-rxr4w"  @ 06/17/23 13:41:13.154
  Jun 17 13:41:13.159: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-rxr4w"  @ 06/17/23 13:41:13.159
  Jun 17 13:41:13.166: INFO: AutomountServiceAccountToken: true
  Jun 17 13:41:13.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2466" for this suite. @ 06/17/23 13:41:13.17
• [0.049 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 06/17/23 13:41:13.177
  Jun 17 13:41:13.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 13:41:13.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:41:13.195
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:41:13.198
  E0617 13:41:13.228759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 06/17/23 13:41:13.233
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 13:41:13.724
  STEP: Deploying the webhook pod @ 06/17/23 13:41:13.733
  STEP: Wait for the deployment to be ready @ 06/17/23 13:41:13.745
  Jun 17 13:41:13.752: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0617 13:41:14.228994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:41:15.229096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/17/23 13:41:15.762
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 13:41:15.77
  E0617 13:41:16.229883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:41:16.770: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 06/17/23 13:41:16.842
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/17/23 13:41:16.884
  STEP: Deleting the collection of validation webhooks @ 06/17/23 13:41:16.919
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/17/23 13:41:16.96
  Jun 17 13:41:16.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7316" for this suite. @ 06/17/23 13:41:17.004
  STEP: Destroying namespace "webhook-markers-3980" for this suite. @ 06/17/23 13:41:17.009
• [3.840 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 06/17/23 13:41:17.018
  Jun 17 13:41:17.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename var-expansion @ 06/17/23 13:41:17.019
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:41:17.038
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:41:17.041
  STEP: Creating a pod to test env composition @ 06/17/23 13:41:17.044
  E0617 13:41:17.230036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:41:18.230169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:41:19.230811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:41:20.230829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:41:21.063
  Jun 17 13:41:21.067: INFO: Trying to get logs from node ip-172-31-25-17 pod var-expansion-36284f01-f113-401b-a1d8-e6db11420b53 container dapi-container: <nil>
  STEP: delete the pod @ 06/17/23 13:41:21.087
  Jun 17 13:41:21.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7729" for this suite. @ 06/17/23 13:41:21.109
• [4.097 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 06/17/23 13:41:21.116
  Jun 17 13:41:21.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename webhook @ 06/17/23 13:41:21.117
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:41:21.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:41:21.144
  STEP: Setting up server cert @ 06/17/23 13:41:21.178
  E0617 13:41:21.231766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/17/23 13:41:21.358
  STEP: Deploying the webhook pod @ 06/17/23 13:41:21.365
  STEP: Wait for the deployment to be ready @ 06/17/23 13:41:21.381
  Jun 17 13:41:21.389: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0617 13:41:22.232825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:41:23.232938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/17/23 13:41:23.399
  STEP: Verifying the service has paired with the endpoint @ 06/17/23 13:41:23.407
  E0617 13:41:24.233924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 17 13:41:24.408: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 06/17/23 13:41:24.411
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 06/17/23 13:41:24.412
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 06/17/23 13:41:24.412
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 06/17/23 13:41:24.412
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 06/17/23 13:41:24.414
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 06/17/23 13:41:24.414
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 06/17/23 13:41:24.415
  Jun 17 13:41:24.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-573" for this suite. @ 06/17/23 13:41:24.452
  STEP: Destroying namespace "webhook-markers-1300" for this suite. @ 06/17/23 13:41:24.457
• [3.348 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 06/17/23 13:41:24.465
  Jun 17 13:41:24.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename pods @ 06/17/23 13:41:24.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:41:24.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:41:24.484
  STEP: creating the pod @ 06/17/23 13:41:24.487
  STEP: submitting the pod to kubernetes @ 06/17/23 13:41:24.487
  E0617 13:41:25.234008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:41:26.234267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 06/17/23 13:41:26.506
  STEP: updating the pod @ 06/17/23 13:41:26.509
  Jun 17 13:41:27.021: INFO: Successfully updated pod "pod-update-f3770a06-4e77-48e5-9824-47ae3d47cad0"
  STEP: verifying the updated pod is in kubernetes @ 06/17/23 13:41:27.025
  Jun 17 13:41:27.030: INFO: Pod update OK
  Jun 17 13:41:27.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3659" for this suite. @ 06/17/23 13:41:27.034
• [2.575 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 06/17/23 13:41:27.04
  Jun 17 13:41:27.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2190159689
  STEP: Building a namespace api object, basename secrets @ 06/17/23 13:41:27.041
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/17/23 13:41:27.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/17/23 13:41:27.062
  STEP: Creating secret with name secret-test-06445a46-3ebd-47da-9bb2-e2b640f516ce @ 06/17/23 13:41:27.065
  STEP: Creating a pod to test consume secrets @ 06/17/23 13:41:27.069
  E0617 13:41:27.234677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:41:28.234837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:41:29.235619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0617 13:41:30.235700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/17/23 13:41:31.088
  Jun 17 13:41:31.091: INFO: Trying to get logs from node ip-172-31-25-17 pod pod-secrets-e25b2473-6594-48ed-85fc-2797d50e4d0c container secret-volume-test: <nil>
  STEP: delete the pod @ 06/17/23 13:41:31.097
  Jun 17 13:41:31.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1438" for this suite. @ 06/17/23 13:41:31.12
• [4.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Jun 17 13:41:31.128: INFO: Running AfterSuite actions on node 1
  Jun 17 13:41:31.128: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.050 seconds]
------------------------------

Ran 378 of 7207 Specs in 5929.986 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h38m50.509224372s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

