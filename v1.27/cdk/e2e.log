  I0610 12:07:12.719367      18 e2e.go:117] Starting e2e run "78244405-d6bd-408b-af19-702c6f7dfe06" on Ginkgo node 1
  Jun 10 12:07:12.763: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1686398832 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Jun 10 12:07:13.027: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:07:13.028: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Jun 10 12:07:13.083: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Jun 10 12:07:13.091: INFO: e2e test version: v1.27.2
  Jun 10 12:07:13.093: INFO: kube-apiserver version: v1.27.2
  Jun 10 12:07:13.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:07:13.099: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.072 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 06/10/23 12:07:13.511
  Jun 10 12:07:13.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/10/23 12:07:13.512
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:07:13.548
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:07:13.561
  Jun 10 12:07:13.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:07:16.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5335" for this suite. @ 06/10/23 12:07:16.737
• [3.234 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 06/10/23 12:07:16.747
  Jun 10 12:07:16.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename services @ 06/10/23 12:07:16.748
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:07:16.771
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:07:16.776
  STEP: creating service endpoint-test2 in namespace services-5012 @ 06/10/23 12:07:16.78
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5012 to expose endpoints map[] @ 06/10/23 12:07:16.792
  Jun 10 12:07:16.806: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  Jun 10 12:07:17.816: INFO: successfully validated that service endpoint-test2 in namespace services-5012 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-5012 @ 06/10/23 12:07:17.817
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5012 to expose endpoints map[pod1:[80]] @ 06/10/23 12:07:21.848
  Jun 10 12:07:21.860: INFO: successfully validated that service endpoint-test2 in namespace services-5012 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 06/10/23 12:07:21.86
  Jun 10 12:07:21.860: INFO: Creating new exec pod
  Jun 10 12:07:24.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5012 exec execpodvq87d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jun 10 12:07:25.058: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jun 10 12:07:25.058: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 12:07:25.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5012 exec execpodvq87d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.58 80'
  Jun 10 12:07:25.221: INFO: stderr: "+ nc -v -t -w 2 10.152.183.58 80\nConnection to 10.152.183.58 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun 10 12:07:25.221: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-5012 @ 06/10/23 12:07:25.221
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5012 to expose endpoints map[pod1:[80] pod2:[80]] @ 06/10/23 12:07:31.254
  Jun 10 12:07:31.271: INFO: successfully validated that service endpoint-test2 in namespace services-5012 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 06/10/23 12:07:31.271
  Jun 10 12:07:32.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5012 exec execpodvq87d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jun 10 12:07:32.493: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jun 10 12:07:32.493: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 12:07:32.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5012 exec execpodvq87d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.58 80'
  Jun 10 12:07:32.708: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.58 80\nConnection to 10.152.183.58 80 port [tcp/http] succeeded!\n"
  Jun 10 12:07:32.708: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-5012 @ 06/10/23 12:07:32.708
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5012 to expose endpoints map[pod2:[80]] @ 06/10/23 12:07:32.728
  Jun 10 12:07:33.777: INFO: successfully validated that service endpoint-test2 in namespace services-5012 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 06/10/23 12:07:33.777
  Jun 10 12:07:34.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5012 exec execpodvq87d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jun 10 12:07:34.953: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jun 10 12:07:34.953: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 12:07:34.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5012 exec execpodvq87d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.58 80'
  Jun 10 12:07:35.134: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.58 80\nConnection to 10.152.183.58 80 port [tcp/http] succeeded!\n"
  Jun 10 12:07:35.134: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-5012 @ 06/10/23 12:07:35.135
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5012 to expose endpoints map[] @ 06/10/23 12:07:35.155
  Jun 10 12:07:35.172: INFO: successfully validated that service endpoint-test2 in namespace services-5012 exposes endpoints map[]
  Jun 10 12:07:35.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5012" for this suite. @ 06/10/23 12:07:35.201
• [18.462 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 06/10/23 12:07:35.21
  Jun 10 12:07:35.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 12:07:35.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:07:35.231
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:07:35.236
  STEP: Setting up server cert @ 06/10/23 12:07:35.265
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 12:07:35.985
  STEP: Deploying the webhook pod @ 06/10/23 12:07:35.995
  STEP: Wait for the deployment to be ready @ 06/10/23 12:07:36.009
  Jun 10 12:07:36.018: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/10/23 12:07:38.031
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 12:07:38.043
  Jun 10 12:07:39.044: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 06/10/23 12:07:39.144
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/10/23 12:07:39.197
  STEP: Deleting the collection of validation webhooks @ 06/10/23 12:07:39.258
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/10/23 12:07:39.313
  Jun 10 12:07:39.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3661" for this suite. @ 06/10/23 12:07:39.386
  STEP: Destroying namespace "webhook-markers-4319" for this suite. @ 06/10/23 12:07:39.395
• [4.197 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 06/10/23 12:07:39.408
  Jun 10 12:07:39.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename watch @ 06/10/23 12:07:39.409
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:07:39.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:07:39.434
  STEP: getting a starting resourceVersion @ 06/10/23 12:07:39.438
  STEP: starting a background goroutine to produce watch events @ 06/10/23 12:07:39.441
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 06/10/23 12:07:39.441
  Jun 10 12:07:42.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-933" for this suite. @ 06/10/23 12:07:42.264
• [2.909 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 06/10/23 12:07:42.318
  Jun 10 12:07:42.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 12:07:42.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:07:42.335
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:07:42.341
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 12:07:42.345
  STEP: Saw pod success @ 06/10/23 12:07:46.37
  Jun 10 12:07:46.373: INFO: Trying to get logs from node ip-172-31-27-177 pod downwardapi-volume-77ae2eeb-ab10-425e-ac42-e2fbdf58217c container client-container: <nil>
  STEP: delete the pod @ 06/10/23 12:07:46.399
  Jun 10 12:07:46.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1284" for this suite. @ 06/10/23 12:07:46.426
• [4.117 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 06/10/23 12:07:46.435
  Jun 10 12:07:46.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename secrets @ 06/10/23 12:07:46.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:07:46.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:07:46.472
  Jun 10 12:07:46.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7485" for this suite. @ 06/10/23 12:07:46.534
• [0.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 06/10/23 12:07:46.546
  Jun 10 12:07:46.546: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename configmap @ 06/10/23 12:07:46.547
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:07:46.569
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:07:46.576
  Jun 10 12:07:46.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8317" for this suite. @ 06/10/23 12:07:46.639
• [0.103 seconds]
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 06/10/23 12:07:46.649
  Jun 10 12:07:46.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename deployment @ 06/10/23 12:07:46.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:07:46.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:07:46.67
  Jun 10 12:07:46.674: INFO: Creating deployment "test-recreate-deployment"
  Jun 10 12:07:46.681: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Jun 10 12:07:46.697: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  Jun 10 12:07:48.705: INFO: Waiting deployment "test-recreate-deployment" to complete
  Jun 10 12:07:48.709: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Jun 10 12:07:48.720: INFO: Updating deployment test-recreate-deployment
  Jun 10 12:07:48.720: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Jun 10 12:07:48.827: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9546  a2fdd515-b663-4efb-8aea-e29d38543bb7 3281 2 2023-06-10 12:07:46 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-10 12:07:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 12:07:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039e3ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-10 12:07:48 +0000 UTC,LastTransitionTime:2023-06-10 12:07:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-06-10 12:07:48 +0000 UTC,LastTransitionTime:2023-06-10 12:07:46 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Jun 10 12:07:48.832: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-9546  8f2c2fcc-34e0-4dc3-8073-41dbf457972c 3280 1 2023-06-10 12:07:48 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment a2fdd515-b663-4efb-8aea-e29d38543bb7 0xc0039e3e97 0xc0039e3e98}] [] [{kube-controller-manager Update apps/v1 2023-06-10 12:07:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2fdd515-b663-4efb-8aea-e29d38543bb7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 12:07:48 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039e3f38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 10 12:07:48.832: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Jun 10 12:07:48.832: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-9546  ba6eea36-fcaa-4c0e-906b-0dce64df128c 3270 2 2023-06-10 12:07:46 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment a2fdd515-b663-4efb-8aea-e29d38543bb7 0xc0039e3f97 0xc0039e3f98}] [] [{kube-controller-manager Update apps/v1 2023-06-10 12:07:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2fdd515-b663-4efb-8aea-e29d38543bb7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 12:07:48 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fd4048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 10 12:07:48.837: INFO: Pod "test-recreate-deployment-54757ffd6c-wpxpq" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-wpxpq test-recreate-deployment-54757ffd6c- deployment-9546  869f7e0c-7a25-4c10-8158-a90b7aa2fbc1 3282 0 2023-06-10 12:07:48 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 8f2c2fcc-34e0-4dc3-8073-41dbf457972c 0xc003fd44c7 0xc003fd44c8}] [] [{kube-controller-manager Update v1 2023-06-10 12:07:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f2c2fcc-34e0-4dc3-8073-41dbf457972c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 12:07:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g65lq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g65lq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 12:07:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 12:07:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 12:07:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 12:07:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.177,PodIP:,StartTime:2023-06-10 12:07:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 12:07:48.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9546" for this suite. @ 06/10/23 12:07:48.842
• [2.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 06/10/23 12:07:48.853
  Jun 10 12:07:48.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename cronjob @ 06/10/23 12:07:48.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:07:48.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:07:48.871
  STEP: Creating a suspended cronjob @ 06/10/23 12:07:48.874
  STEP: Ensuring no jobs are scheduled @ 06/10/23 12:07:48.881
  STEP: Ensuring no job exists by listing jobs explicitly @ 06/10/23 12:12:48.89
  STEP: Removing cronjob @ 06/10/23 12:12:48.893
  Jun 10 12:12:48.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2205" for this suite. @ 06/10/23 12:12:48.908
• [300.064 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 06/10/23 12:12:48.918
  Jun 10 12:12:48.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename namespaces @ 06/10/23 12:12:48.919
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:12:48.941
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:12:48.945
  STEP: Read namespace status @ 06/10/23 12:12:48.949
  Jun 10 12:12:48.954: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 06/10/23 12:12:48.954
  Jun 10 12:12:48.962: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 06/10/23 12:12:48.962
  Jun 10 12:12:48.973: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Jun 10 12:12:48.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5659" for this suite. @ 06/10/23 12:12:48.978
• [0.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 06/10/23 12:12:48.989
  Jun 10 12:12:48.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename hostport @ 06/10/23 12:12:48.99
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:12:49.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:12:49.012
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 06/10/23 12:12:49.02
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.89.0 on the node which pod1 resides and expect scheduled @ 06/10/23 12:12:51.042
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.89.0 but use UDP protocol on the node which pod2 resides @ 06/10/23 12:12:53.056
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 06/10/23 12:13:07.119
  Jun 10 12:13:07.119: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.89.0 http://127.0.0.1:54323/hostname] Namespace:hostport-1452 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 12:13:07.119: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:13:07.119: INFO: ExecWithOptions: Clientset creation
  Jun 10 12:13:07.120: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-1452/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.89.0+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.89.0, port: 54323 @ 06/10/23 12:13:07.197
  Jun 10 12:13:07.197: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.89.0:54323/hostname] Namespace:hostport-1452 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 12:13:07.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:13:07.198: INFO: ExecWithOptions: Clientset creation
  Jun 10 12:13:07.198: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-1452/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.89.0%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.89.0, port: 54323 UDP @ 06/10/23 12:13:07.273
  Jun 10 12:13:07.273: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.89.0 54323] Namespace:hostport-1452 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 12:13:07.273: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:13:07.274: INFO: ExecWithOptions: Clientset creation
  Jun 10 12:13:07.274: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-1452/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.89.0+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  Jun 10 12:13:12.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-1452" for this suite. @ 06/10/23 12:13:12.355
• [23.375 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 06/10/23 12:13:12.364
  Jun 10 12:13:12.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 12:13:12.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:13:12.389
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:13:12.394
  STEP: Creating projection with secret that has name projected-secret-test-map-6d523d5d-e032-4411-8fa3-5273785ae7c8 @ 06/10/23 12:13:12.399
  STEP: Creating a pod to test consume secrets @ 06/10/23 12:13:12.404
  STEP: Saw pod success @ 06/10/23 12:13:16.436
  Jun 10 12:13:16.440: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-projected-secrets-46c1ce3f-577b-4c24-8700-26cbf7ce6eac container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/10/23 12:13:16.485
  Jun 10 12:13:16.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7070" for this suite. @ 06/10/23 12:13:16.558
• [4.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 06/10/23 12:13:16.567
  Jun 10 12:13:16.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename sched-preemption @ 06/10/23 12:13:16.568
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:13:16.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:13:16.595
  Jun 10 12:13:16.622: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun 10 12:14:16.647: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 06/10/23 12:14:16.651
  Jun 10 12:14:16.674: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jun 10 12:14:16.682: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jun 10 12:14:16.708: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jun 10 12:14:16.714: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jun 10 12:14:16.740: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jun 10 12:14:16.747: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 06/10/23 12:14:16.747
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 06/10/23 12:14:18.779
  Jun 10 12:14:22.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-384" for this suite. @ 06/10/23 12:14:22.869
• [66.310 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 06/10/23 12:14:22.882
  Jun 10 12:14:22.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename resourcequota @ 06/10/23 12:14:22.883
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:14:22.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:14:22.905
  STEP: Counting existing ResourceQuota @ 06/10/23 12:14:22.91
  STEP: Creating a ResourceQuota @ 06/10/23 12:14:27.917
  STEP: Ensuring resource quota status is calculated @ 06/10/23 12:14:27.932
  Jun 10 12:14:29.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8600" for this suite. @ 06/10/23 12:14:29.941
• [7.067 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 06/10/23 12:14:29.951
  Jun 10 12:14:29.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename statefulset @ 06/10/23 12:14:29.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:14:29.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:14:29.974
  STEP: Creating service test in namespace statefulset-2062 @ 06/10/23 12:14:29.979
  STEP: Creating a new StatefulSet @ 06/10/23 12:14:29.985
  Jun 10 12:14:30.001: INFO: Found 0 stateful pods, waiting for 3
  Jun 10 12:14:40.008: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 10 12:14:40.008: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 10 12:14:40.008: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
  Jun 10 12:14:50.007: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 10 12:14:50.007: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 10 12:14:50.007: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 06/10/23 12:14:50.02
  Jun 10 12:14:50.043: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 06/10/23 12:14:50.043
  STEP: Not applying an update when the partition is greater than the number of replicas @ 06/10/23 12:15:00.064
  STEP: Performing a canary update @ 06/10/23 12:15:00.064
  Jun 10 12:15:00.084: INFO: Updating stateful set ss2
  Jun 10 12:15:00.096: INFO: Waiting for Pod statefulset-2062/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 06/10/23 12:15:10.106
  Jun 10 12:15:10.170: INFO: Found 2 stateful pods, waiting for 3
  Jun 10 12:15:20.176: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 10 12:15:20.176: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 10 12:15:20.176: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 06/10/23 12:15:20.184
  Jun 10 12:15:20.207: INFO: Updating stateful set ss2
  Jun 10 12:15:20.215: INFO: Waiting for Pod statefulset-2062/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jun 10 12:15:30.248: INFO: Updating stateful set ss2
  Jun 10 12:15:30.256: INFO: Waiting for StatefulSet statefulset-2062/ss2 to complete update
  Jun 10 12:15:30.257: INFO: Waiting for Pod statefulset-2062/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jun 10 12:15:40.271: INFO: Deleting all statefulset in ns statefulset-2062
  Jun 10 12:15:40.274: INFO: Scaling statefulset ss2 to 0
  Jun 10 12:15:50.299: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 10 12:15:50.303: INFO: Deleting statefulset ss2
  Jun 10 12:15:50.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2062" for this suite. @ 06/10/23 12:15:50.325
• [80.381 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 06/10/23 12:15:50.333
  Jun 10 12:15:50.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename proxy @ 06/10/23 12:15:50.334
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:15:50.35
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:15:50.354
  Jun 10 12:15:50.359: INFO: Creating pod...
  Jun 10 12:15:52.380: INFO: Creating service...
  Jun 10 12:15:52.390: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-625/pods/agnhost/proxy?method=DELETE
  Jun 10 12:15:52.408: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jun 10 12:15:52.408: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-625/pods/agnhost/proxy?method=OPTIONS
  Jun 10 12:15:52.413: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jun 10 12:15:52.413: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-625/pods/agnhost/proxy?method=PATCH
  Jun 10 12:15:52.419: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jun 10 12:15:52.419: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-625/pods/agnhost/proxy?method=POST
  Jun 10 12:15:52.423: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jun 10 12:15:52.423: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-625/pods/agnhost/proxy?method=PUT
  Jun 10 12:15:52.427: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jun 10 12:15:52.427: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-625/services/e2e-proxy-test-service/proxy?method=DELETE
  Jun 10 12:15:52.434: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jun 10 12:15:52.435: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-625/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Jun 10 12:15:52.442: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jun 10 12:15:52.442: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-625/services/e2e-proxy-test-service/proxy?method=PATCH
  Jun 10 12:15:52.449: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jun 10 12:15:52.450: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-625/services/e2e-proxy-test-service/proxy?method=POST
  Jun 10 12:15:52.456: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jun 10 12:15:52.457: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-625/services/e2e-proxy-test-service/proxy?method=PUT
  Jun 10 12:15:52.462: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jun 10 12:15:52.462: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-625/pods/agnhost/proxy?method=GET
  Jun 10 12:15:52.466: INFO: http.Client request:GET StatusCode:301
  Jun 10 12:15:52.466: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-625/services/e2e-proxy-test-service/proxy?method=GET
  Jun 10 12:15:52.471: INFO: http.Client request:GET StatusCode:301
  Jun 10 12:15:52.472: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-625/pods/agnhost/proxy?method=HEAD
  Jun 10 12:15:52.475: INFO: http.Client request:HEAD StatusCode:301
  Jun 10 12:15:52.476: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-625/services/e2e-proxy-test-service/proxy?method=HEAD
  Jun 10 12:15:52.480: INFO: http.Client request:HEAD StatusCode:301
  Jun 10 12:15:52.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-625" for this suite. @ 06/10/23 12:15:52.485
• [2.160 seconds]
------------------------------
S
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 06/10/23 12:15:52.494
  Jun 10 12:15:52.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename services @ 06/10/23 12:15:52.495
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:15:52.515
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:15:52.519
  STEP: creating service multi-endpoint-test in namespace services-649 @ 06/10/23 12:15:52.523
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-649 to expose endpoints map[] @ 06/10/23 12:15:52.536
  Jun 10 12:15:52.550: INFO: successfully validated that service multi-endpoint-test in namespace services-649 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-649 @ 06/10/23 12:15:52.55
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-649 to expose endpoints map[pod1:[100]] @ 06/10/23 12:15:54.579
  Jun 10 12:15:54.592: INFO: successfully validated that service multi-endpoint-test in namespace services-649 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-649 @ 06/10/23 12:15:54.592
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-649 to expose endpoints map[pod1:[100] pod2:[101]] @ 06/10/23 12:15:56.614
  Jun 10 12:15:56.630: INFO: successfully validated that service multi-endpoint-test in namespace services-649 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 06/10/23 12:15:56.63
  Jun 10 12:15:56.630: INFO: Creating new exec pod
  Jun 10 12:15:59.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-649 exec execpodhkh5h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Jun 10 12:15:59.810: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Jun 10 12:15:59.810: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 12:15:59.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-649 exec execpodhkh5h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.183 80'
  Jun 10 12:15:59.956: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.183 80\nConnection to 10.152.183.183 80 port [tcp/http] succeeded!\n"
  Jun 10 12:15:59.956: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 12:15:59.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-649 exec execpodhkh5h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Jun 10 12:16:00.115: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Jun 10 12:16:00.115: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 12:16:00.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-649 exec execpodhkh5h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.183 81'
  Jun 10 12:16:00.270: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.183 81\nConnection to 10.152.183.183 81 port [tcp/*] succeeded!\n"
  Jun 10 12:16:00.270: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-649 @ 06/10/23 12:16:00.27
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-649 to expose endpoints map[pod2:[101]] @ 06/10/23 12:16:00.293
  Jun 10 12:16:01.321: INFO: successfully validated that service multi-endpoint-test in namespace services-649 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-649 @ 06/10/23 12:16:01.321
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-649 to expose endpoints map[] @ 06/10/23 12:16:01.341
  Jun 10 12:16:01.350: INFO: successfully validated that service multi-endpoint-test in namespace services-649 exposes endpoints map[]
  Jun 10 12:16:01.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-649" for this suite. @ 06/10/23 12:16:01.372
• [8.887 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 06/10/23 12:16:01.383
  Jun 10 12:16:01.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename pod-network-test @ 06/10/23 12:16:01.385
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:16:01.402
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:16:01.407
  STEP: Performing setup for networking test in namespace pod-network-test-2661 @ 06/10/23 12:16:01.411
  STEP: creating a selector @ 06/10/23 12:16:01.411
  STEP: Creating the service pods in kubernetes @ 06/10/23 12:16:01.411
  Jun 10 12:16:01.411: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 06/10/23 12:16:23.539
  Jun 10 12:16:25.559: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jun 10 12:16:25.559: INFO: Breadth first check of 192.168.109.17 on host 172.31.27.177...
  Jun 10 12:16:25.562: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.109.18:9080/dial?request=hostname&protocol=http&host=192.168.109.17&port=8083&tries=1'] Namespace:pod-network-test-2661 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 12:16:25.562: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:16:25.563: INFO: ExecWithOptions: Clientset creation
  Jun 10 12:16:25.563: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2661/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.109.18%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.109.17%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 10 12:16:25.642: INFO: Waiting for responses: map[]
  Jun 10 12:16:25.642: INFO: reached 192.168.109.17 after 0/1 tries
  Jun 10 12:16:25.642: INFO: Breadth first check of 192.168.92.12 on host 172.31.46.40...
  Jun 10 12:16:25.646: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.109.18:9080/dial?request=hostname&protocol=http&host=192.168.92.12&port=8083&tries=1'] Namespace:pod-network-test-2661 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 12:16:25.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:16:25.647: INFO: ExecWithOptions: Clientset creation
  Jun 10 12:16:25.648: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2661/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.109.18%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.92.12%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 10 12:16:25.723: INFO: Waiting for responses: map[]
  Jun 10 12:16:25.723: INFO: reached 192.168.92.12 after 0/1 tries
  Jun 10 12:16:25.723: INFO: Breadth first check of 192.168.149.77 on host 172.31.89.0...
  Jun 10 12:16:25.728: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.109.18:9080/dial?request=hostname&protocol=http&host=192.168.149.77&port=8083&tries=1'] Namespace:pod-network-test-2661 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 12:16:25.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:16:25.729: INFO: ExecWithOptions: Clientset creation
  Jun 10 12:16:25.729: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2661/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.109.18%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.149.77%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 10 12:16:25.809: INFO: Waiting for responses: map[]
  Jun 10 12:16:25.809: INFO: reached 192.168.149.77 after 0/1 tries
  Jun 10 12:16:25.809: INFO: Going to retry 0 out of 3 pods....
  Jun 10 12:16:25.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-2661" for this suite. @ 06/10/23 12:16:25.814
• [24.438 seconds]
------------------------------
SSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 06/10/23 12:16:25.822
  Jun 10 12:16:25.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename dns @ 06/10/23 12:16:25.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:16:25.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:16:25.85
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 06/10/23 12:16:25.854
  Jun 10 12:16:25.861: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5678  2e6e09bd-8bf1-422c-8622-bb703a96d628 5271 0 2023-06-10 12:16:25 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-06-10 12:16:25 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4j9h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4j9h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 06/10/23 12:16:27.872
  Jun 10 12:16:27.872: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5678 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 12:16:27.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:16:27.873: INFO: ExecWithOptions: Clientset creation
  Jun 10 12:16:27.873: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-5678/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 06/10/23 12:16:27.967
  Jun 10 12:16:27.967: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5678 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 12:16:27.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:16:27.968: INFO: ExecWithOptions: Clientset creation
  Jun 10 12:16:27.968: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-5678/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun 10 12:16:28.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 10 12:16:28.064: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-5678" for this suite. @ 06/10/23 12:16:28.078
• [2.265 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 06/10/23 12:16:28.089
  Jun 10 12:16:28.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-probe @ 06/10/23 12:16:28.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:16:28.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:16:28.109
  STEP: Creating pod busybox-d6887870-6d69-411e-9983-d25c5c381ae2 in namespace container-probe-2380 @ 06/10/23 12:16:28.114
  Jun 10 12:16:30.134: INFO: Started pod busybox-d6887870-6d69-411e-9983-d25c5c381ae2 in namespace container-probe-2380
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/10/23 12:16:30.134
  Jun 10 12:16:30.138: INFO: Initial restart count of pod busybox-d6887870-6d69-411e-9983-d25c5c381ae2 is 0
  Jun 10 12:17:20.269: INFO: Restart count of pod container-probe-2380/busybox-d6887870-6d69-411e-9983-d25c5c381ae2 is now 1 (50.130879167s elapsed)
  Jun 10 12:17:20.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/10/23 12:17:20.276
  STEP: Destroying namespace "container-probe-2380" for this suite. @ 06/10/23 12:17:20.29
• [52.209 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 06/10/23 12:17:20.299
  Jun 10 12:17:20.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename statefulset @ 06/10/23 12:17:20.3
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:17:20.319
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:17:20.323
  STEP: Creating service test in namespace statefulset-9264 @ 06/10/23 12:17:20.328
  STEP: Creating statefulset ss in namespace statefulset-9264 @ 06/10/23 12:17:20.334
  Jun 10 12:17:20.350: INFO: Found 0 stateful pods, waiting for 1
  Jun 10 12:17:30.356: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 06/10/23 12:17:30.364
  STEP: updating a scale subresource @ 06/10/23 12:17:30.368
  STEP: verifying the statefulset Spec.Replicas was modified @ 06/10/23 12:17:30.375
  STEP: Patch a scale subresource @ 06/10/23 12:17:30.38
  STEP: verifying the statefulset Spec.Replicas was modified @ 06/10/23 12:17:30.393
  Jun 10 12:17:30.401: INFO: Deleting all statefulset in ns statefulset-9264
  Jun 10 12:17:30.405: INFO: Scaling statefulset ss to 0
  Jun 10 12:17:40.456: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 10 12:17:40.461: INFO: Deleting statefulset ss
  Jun 10 12:17:40.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9264" for this suite. @ 06/10/23 12:17:40.485
• [20.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 06/10/23 12:17:40.504
  Jun 10 12:17:40.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename namespaces @ 06/10/23 12:17:40.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:17:40.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:17:40.534
  STEP: Creating a test namespace @ 06/10/23 12:17:40.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:17:40.556
  STEP: Creating a pod in the namespace @ 06/10/23 12:17:40.561
  STEP: Waiting for the pod to have running status @ 06/10/23 12:17:40.57
  STEP: Deleting the namespace @ 06/10/23 12:17:42.581
  STEP: Waiting for the namespace to be removed. @ 06/10/23 12:17:42.588
  STEP: Recreating the namespace @ 06/10/23 12:17:53.593
  STEP: Verifying there are no pods in the namespace @ 06/10/23 12:17:53.617
  Jun 10 12:17:53.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4776" for this suite. @ 06/10/23 12:17:53.627
  STEP: Destroying namespace "nsdeletetest-8801" for this suite. @ 06/10/23 12:17:53.635
  Jun 10 12:17:53.639: INFO: Namespace nsdeletetest-8801 was already deleted
  STEP: Destroying namespace "nsdeletetest-1307" for this suite. @ 06/10/23 12:17:53.639
• [13.143 seconds]
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 06/10/23 12:17:53.647
  Jun 10 12:17:53.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/10/23 12:17:53.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:17:53.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:17:53.671
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 06/10/23 12:17:53.674
  Jun 10 12:17:53.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:17:55.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:18:00.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3280" for this suite. @ 06/10/23 12:18:00.983
• [7.344 seconds]
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 06/10/23 12:18:00.991
  Jun 10 12:18:00.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-probe @ 06/10/23 12:18:00.992
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:18:01.009
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:18:01.014
  Jun 10 12:18:23.096: INFO: Container started at 2023-06-10 12:18:01 +0000 UTC, pod became ready at 2023-06-10 12:18:21 +0000 UTC
  Jun 10 12:18:23.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-3500" for this suite. @ 06/10/23 12:18:23.1
• [22.117 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 06/10/23 12:18:23.109
  Jun 10 12:18:23.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/10/23 12:18:23.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:18:23.131
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:18:23.135
  Jun 10 12:18:23.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:18:24.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6815" for this suite. @ 06/10/23 12:18:24.169
• [1.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 06/10/23 12:18:24.182
  Jun 10 12:18:24.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename dns @ 06/10/23 12:18:24.183
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:18:24.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:18:24.206
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 06/10/23 12:18:24.21
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 06/10/23 12:18:24.21
  STEP: creating a pod to probe DNS @ 06/10/23 12:18:24.21
  STEP: submitting the pod to kubernetes @ 06/10/23 12:18:24.21
  STEP: retrieving the pod @ 06/10/23 12:18:32.247
  STEP: looking for the results for each expected name from probers @ 06/10/23 12:18:32.25
  Jun 10 12:18:32.270: INFO: DNS probes using dns-9490/dns-test-2db495cb-5432-4109-a0c9-05ccdd4338ba succeeded

  Jun 10 12:18:32.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/10/23 12:18:32.275
  STEP: Destroying namespace "dns-9490" for this suite. @ 06/10/23 12:18:32.29
• [8.115 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 06/10/23 12:18:32.298
  Jun 10 12:18:32.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename watch @ 06/10/23 12:18:32.299
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:18:32.32
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:18:32.324
  STEP: creating a watch on configmaps with label A @ 06/10/23 12:18:32.328
  STEP: creating a watch on configmaps with label B @ 06/10/23 12:18:32.33
  STEP: creating a watch on configmaps with label A or B @ 06/10/23 12:18:32.331
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 06/10/23 12:18:32.333
  Jun 10 12:18:32.340: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7366  c7f8a626-ca5a-417b-8f7d-06d24dcd9510 5917 0 2023-06-10 12:18:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-10 12:18:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 12:18:32.340: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7366  c7f8a626-ca5a-417b-8f7d-06d24dcd9510 5917 0 2023-06-10 12:18:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-10 12:18:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 06/10/23 12:18:32.34
  Jun 10 12:18:32.351: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7366  c7f8a626-ca5a-417b-8f7d-06d24dcd9510 5918 0 2023-06-10 12:18:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-10 12:18:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 12:18:32.351: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7366  c7f8a626-ca5a-417b-8f7d-06d24dcd9510 5918 0 2023-06-10 12:18:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-10 12:18:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 06/10/23 12:18:32.351
  Jun 10 12:18:32.361: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7366  c7f8a626-ca5a-417b-8f7d-06d24dcd9510 5919 0 2023-06-10 12:18:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-10 12:18:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 12:18:32.361: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7366  c7f8a626-ca5a-417b-8f7d-06d24dcd9510 5919 0 2023-06-10 12:18:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-10 12:18:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 06/10/23 12:18:32.361
  Jun 10 12:18:32.369: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7366  c7f8a626-ca5a-417b-8f7d-06d24dcd9510 5920 0 2023-06-10 12:18:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-10 12:18:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 12:18:32.369: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7366  c7f8a626-ca5a-417b-8f7d-06d24dcd9510 5920 0 2023-06-10 12:18:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-10 12:18:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 06/10/23 12:18:32.369
  Jun 10 12:18:32.375: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7366  0fd08341-e26b-4504-a8d2-8753134a66c5 5921 0 2023-06-10 12:18:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-10 12:18:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 12:18:32.375: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7366  0fd08341-e26b-4504-a8d2-8753134a66c5 5921 0 2023-06-10 12:18:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-10 12:18:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 06/10/23 12:18:42.379
  Jun 10 12:18:42.388: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7366  0fd08341-e26b-4504-a8d2-8753134a66c5 5964 0 2023-06-10 12:18:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-10 12:18:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 12:18:42.388: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7366  0fd08341-e26b-4504-a8d2-8753134a66c5 5964 0 2023-06-10 12:18:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-10 12:18:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 12:18:52.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7366" for this suite. @ 06/10/23 12:18:52.399
• [20.108 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 06/10/23 12:18:52.407
  Jun 10 12:18:52.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-runtime @ 06/10/23 12:18:52.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:18:52.428
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:18:52.432
  STEP: create the container @ 06/10/23 12:18:52.436
  W0610 12:18:52.445286      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 06/10/23 12:18:52.445
  STEP: get the container status @ 06/10/23 12:18:55.465
  STEP: the container should be terminated @ 06/10/23 12:18:55.469
  STEP: the termination message should be set @ 06/10/23 12:18:55.469
  Jun 10 12:18:55.469: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 06/10/23 12:18:55.469
  Jun 10 12:18:55.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7286" for this suite. @ 06/10/23 12:18:55.492
• [3.092 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 06/10/23 12:18:55.5
  Jun 10 12:18:55.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename resourcequota @ 06/10/23 12:18:55.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:18:55.519
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:18:55.522
  STEP: Creating a ResourceQuota @ 06/10/23 12:18:55.527
  STEP: Getting a ResourceQuota @ 06/10/23 12:18:55.533
  STEP: Listing all ResourceQuotas with LabelSelector @ 06/10/23 12:18:55.538
  STEP: Patching the ResourceQuota @ 06/10/23 12:18:55.543
  STEP: Deleting a Collection of ResourceQuotas @ 06/10/23 12:18:55.552
  STEP: Verifying the deleted ResourceQuota @ 06/10/23 12:18:55.564
  Jun 10 12:18:55.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9124" for this suite. @ 06/10/23 12:18:55.573
• [0.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 06/10/23 12:18:55.586
  Jun 10 12:18:55.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename secrets @ 06/10/23 12:18:55.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:18:55.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:18:55.61
  STEP: Creating secret with name secret-test-bb802fae-16be-451a-a85f-12ffe4903843 @ 06/10/23 12:18:55.614
  STEP: Creating a pod to test consume secrets @ 06/10/23 12:18:55.62
  STEP: Saw pod success @ 06/10/23 12:18:59.648
  Jun 10 12:18:59.653: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-secrets-c8fa2469-d844-4361-ac41-5f73333a9742 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/10/23 12:18:59.679
  Jun 10 12:18:59.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9742" for this suite. @ 06/10/23 12:18:59.705
• [4.125 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 06/10/23 12:18:59.712
  Jun 10 12:18:59.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename subpath @ 06/10/23 12:18:59.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:18:59.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:18:59.737
  STEP: Setting up data @ 06/10/23 12:18:59.743
  STEP: Creating pod pod-subpath-test-downwardapi-bj4p @ 06/10/23 12:18:59.754
  STEP: Creating a pod to test atomic-volume-subpath @ 06/10/23 12:18:59.755
  STEP: Saw pod success @ 06/10/23 12:19:23.84
  Jun 10 12:19:23.844: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-subpath-test-downwardapi-bj4p container test-container-subpath-downwardapi-bj4p: <nil>
  STEP: delete the pod @ 06/10/23 12:19:23.852
  STEP: Deleting pod pod-subpath-test-downwardapi-bj4p @ 06/10/23 12:19:23.872
  Jun 10 12:19:23.872: INFO: Deleting pod "pod-subpath-test-downwardapi-bj4p" in namespace "subpath-298"
  Jun 10 12:19:23.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-298" for this suite. @ 06/10/23 12:19:23.88
• [24.176 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 06/10/23 12:19:23.891
  Jun 10 12:19:23.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename services @ 06/10/23 12:19:23.892
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:19:23.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:19:23.919
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-920 @ 06/10/23 12:19:23.923
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 06/10/23 12:19:23.944
  STEP: creating service externalsvc in namespace services-920 @ 06/10/23 12:19:23.944
  STEP: creating replication controller externalsvc in namespace services-920 @ 06/10/23 12:19:23.967
  I0610 12:19:23.976784      18 runners.go:194] Created replication controller with name: externalsvc, namespace: services-920, replica count: 2
  I0610 12:19:27.029799      18 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 06/10/23 12:19:27.034
  Jun 10 12:19:27.058: INFO: Creating new exec pod
  Jun 10 12:19:29.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-920 exec execpodb7w5h -- /bin/sh -x -c nslookup nodeport-service.services-920.svc.cluster.local'
  Jun 10 12:19:29.265: INFO: stderr: "+ nslookup nodeport-service.services-920.svc.cluster.local\n"
  Jun 10 12:19:29.265: INFO: stdout: "Server:\t\t10.152.183.119\nAddress:\t10.152.183.119#53\n\nnodeport-service.services-920.svc.cluster.local\tcanonical name = externalsvc.services-920.svc.cluster.local.\nName:\texternalsvc.services-920.svc.cluster.local\nAddress: 10.152.183.170\n\n"
  Jun 10 12:19:29.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-920, will wait for the garbage collector to delete the pods @ 06/10/23 12:19:29.27
  Jun 10 12:19:29.332: INFO: Deleting ReplicationController externalsvc took: 6.933671ms
  Jun 10 12:19:29.433: INFO: Terminating ReplicationController externalsvc pods took: 100.962614ms
  Jun 10 12:19:31.154: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-920" for this suite. @ 06/10/23 12:19:31.167
• [7.285 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 06/10/23 12:19:31.178
  Jun 10 12:19:31.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/10/23 12:19:31.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:19:31.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:19:31.201
  STEP: fetching the /apis discovery document @ 06/10/23 12:19:31.254
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 06/10/23 12:19:31.256
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 06/10/23 12:19:31.256
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 06/10/23 12:19:31.256
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 06/10/23 12:19:31.258
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 06/10/23 12:19:31.258
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 06/10/23 12:19:31.26
  Jun 10 12:19:31.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4032" for this suite. @ 06/10/23 12:19:31.265
• [0.095 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 06/10/23 12:19:31.274
  Jun 10 12:19:31.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename deployment @ 06/10/23 12:19:31.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:19:31.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:19:31.303
  Jun 10 12:19:31.318: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  Jun 10 12:19:36.334: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/10/23 12:19:36.334
  Jun 10 12:19:36.334: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 06/10/23 12:19:36.358
  Jun 10 12:19:36.388: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1569  99167832-1b96-4988-ad8b-34c1e4eb7110 6324 1 2023-06-10 12:19:36 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-06-10 12:19:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004551b48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Jun 10 12:19:36.405: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Jun 10 12:19:36.405: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Jun 10 12:19:36.405: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1569  e28c8689-ae45-461b-a397-3d6c161d9a03 6325 1 2023-06-10 12:19:31 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 99167832-1b96-4988-ad8b-34c1e4eb7110 0xc0045c7b6f 0xc0045c7b80}] [] [{e2e.test Update apps/v1 2023-06-10 12:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 12:19:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-10 12:19:36 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"99167832-1b96-4988-ad8b-34c1e4eb7110\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0045c7c38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun 10 12:19:36.412: INFO: Pod "test-cleanup-controller-7tj5n" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-7tj5n test-cleanup-controller- deployment-1569  03f1016b-53b9-46d2-80f2-051ba59fe486 6314 0 2023-06-10 12:19:31 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller e28c8689-ae45-461b-a397-3d6c161d9a03 0xc004551e3f 0xc004551e50}] [] [{kube-controller-manager Update v1 2023-06-10 12:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e28c8689-ae45-461b-a397-3d6c161d9a03\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 12:19:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.109.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hzc4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hzc4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 12:19:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 12:19:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 12:19:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 12:19:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.177,PodIP:192.168.109.30,StartTime:2023-06-10 12:19:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-10 12:19:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://96b585d3a2986ded7ba42546b20d57b5eb510a5dd6ad9f3571fd36ef57cc5878,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.109.30,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 12:19:36.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1569" for this suite. @ 06/10/23 12:19:36.431
• [5.182 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 06/10/23 12:19:36.46
  Jun 10 12:19:36.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename services @ 06/10/23 12:19:36.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:19:36.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:19:36.552
  STEP: creating a Service @ 06/10/23 12:19:36.578
  STEP: watching for the Service to be added @ 06/10/23 12:19:36.619
  Jun 10 12:19:36.625: INFO: Found Service test-service-mkmjf in namespace services-9344 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Jun 10 12:19:36.625: INFO: Service test-service-mkmjf created
  STEP: Getting /status @ 06/10/23 12:19:36.625
  Jun 10 12:19:36.641: INFO: Service test-service-mkmjf has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 06/10/23 12:19:36.641
  STEP: watching for the Service to be patched @ 06/10/23 12:19:36.653
  Jun 10 12:19:36.659: INFO: observed Service test-service-mkmjf in namespace services-9344 with annotations: map[] & LoadBalancer: {[]}
  Jun 10 12:19:36.659: INFO: Found Service test-service-mkmjf in namespace services-9344 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Jun 10 12:19:36.661: INFO: Service test-service-mkmjf has service status patched
  STEP: updating the ServiceStatus @ 06/10/23 12:19:36.661
  Jun 10 12:19:36.680: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 06/10/23 12:19:36.68
  Jun 10 12:19:36.682: INFO: Observed Service test-service-mkmjf in namespace services-9344 with annotations: map[] & Conditions: {[]}
  Jun 10 12:19:36.682: INFO: Observed event: &Service{ObjectMeta:{test-service-mkmjf  services-9344  ccad4f29-c6ce-429e-8363-668d0d2e0021 6355 0 2023-06-10 12:19:36 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-06-10 12:19:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-06-10 12:19:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.192,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.192],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Jun 10 12:19:36.683: INFO: Found Service test-service-mkmjf in namespace services-9344 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jun 10 12:19:36.683: INFO: Service test-service-mkmjf has service status updated
  STEP: patching the service @ 06/10/23 12:19:36.683
  STEP: watching for the Service to be patched @ 06/10/23 12:19:36.697
  Jun 10 12:19:36.699: INFO: observed Service test-service-mkmjf in namespace services-9344 with labels: map[test-service-static:true]
  Jun 10 12:19:36.699: INFO: observed Service test-service-mkmjf in namespace services-9344 with labels: map[test-service-static:true]
  Jun 10 12:19:36.699: INFO: observed Service test-service-mkmjf in namespace services-9344 with labels: map[test-service-static:true]
  Jun 10 12:19:36.700: INFO: Found Service test-service-mkmjf in namespace services-9344 with labels: map[test-service:patched test-service-static:true]
  Jun 10 12:19:36.700: INFO: Service test-service-mkmjf patched
  STEP: deleting the service @ 06/10/23 12:19:36.7
  STEP: watching for the Service to be deleted @ 06/10/23 12:19:36.721
  Jun 10 12:19:36.724: INFO: Observed event: ADDED
  Jun 10 12:19:36.724: INFO: Observed event: MODIFIED
  Jun 10 12:19:36.724: INFO: Observed event: MODIFIED
  Jun 10 12:19:36.724: INFO: Observed event: MODIFIED
  Jun 10 12:19:36.725: INFO: Found Service test-service-mkmjf in namespace services-9344 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Jun 10 12:19:36.725: INFO: Service test-service-mkmjf deleted
  Jun 10 12:19:36.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9344" for this suite. @ 06/10/23 12:19:36.731
• [0.281 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 06/10/23 12:19:36.743
  Jun 10 12:19:36.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 12:19:36.744
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:19:36.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:19:36.771
  STEP: Setting up server cert @ 06/10/23 12:19:36.811
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 12:19:37.127
  STEP: Deploying the webhook pod @ 06/10/23 12:19:37.138
  STEP: Wait for the deployment to be ready @ 06/10/23 12:19:37.155
  Jun 10 12:19:37.176: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/10/23 12:19:39.189
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 12:19:39.201
  Jun 10 12:19:40.201: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 06/10/23 12:19:40.206
  STEP: create a pod @ 06/10/23 12:19:40.225
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 06/10/23 12:19:42.249
  Jun 10 12:19:42.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=webhook-2558 attach --namespace=webhook-2558 to-be-attached-pod -i -c=container1'
  Jun 10 12:19:42.357: INFO: rc: 1
  Jun 10 12:19:42.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2558" for this suite. @ 06/10/23 12:19:42.428
  STEP: Destroying namespace "webhook-markers-9736" for this suite. @ 06/10/23 12:19:42.438
• [5.704 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 06/10/23 12:19:42.448
  Jun 10 12:19:42.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-runtime @ 06/10/23 12:19:42.449
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:19:42.471
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:19:42.5
  STEP: create the container @ 06/10/23 12:19:42.507
  W0610 12:19:42.525769      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 06/10/23 12:19:42.526
  STEP: get the container status @ 06/10/23 12:19:46.557
  STEP: the container should be terminated @ 06/10/23 12:19:46.561
  STEP: the termination message should be set @ 06/10/23 12:19:46.561
  Jun 10 12:19:46.561: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 06/10/23 12:19:46.561
  Jun 10 12:19:46.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7591" for this suite. @ 06/10/23 12:19:46.585
• [4.145 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 06/10/23 12:19:46.596
  Jun 10 12:19:46.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 12:19:46.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:19:46.618
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:19:46.622
  STEP: Creating configMap with name projected-configmap-test-volume-map-8d16ca39-e725-4284-b0e2-e80bf0aed21b @ 06/10/23 12:19:46.625
  STEP: Creating a pod to test consume configMaps @ 06/10/23 12:19:46.631
  STEP: Saw pod success @ 06/10/23 12:19:50.659
  Jun 10 12:19:50.662: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-projected-configmaps-5a3b9765-a017-4ab9-837e-3f3ac1af3c48 container agnhost-container: <nil>
  STEP: delete the pod @ 06/10/23 12:19:50.671
  Jun 10 12:19:50.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3416" for this suite. @ 06/10/23 12:19:50.694
• [4.106 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 06/10/23 12:19:50.703
  Jun 10 12:19:50.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubelet-test @ 06/10/23 12:19:50.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:19:50.721
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:19:50.731
  STEP: Waiting for pod completion @ 06/10/23 12:19:50.748
  Jun 10 12:19:54.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4172" for this suite. @ 06/10/23 12:19:54.778
• [4.081 seconds]
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 06/10/23 12:19:54.784
  Jun 10 12:19:54.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename pods @ 06/10/23 12:19:54.785
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:19:54.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:19:54.807
  Jun 10 12:19:54.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: creating the pod @ 06/10/23 12:19:54.811
  STEP: submitting the pod to kubernetes @ 06/10/23 12:19:54.812
  Jun 10 12:19:56.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1288" for this suite. @ 06/10/23 12:19:56.858
• [2.083 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 06/10/23 12:19:56.87
  Jun 10 12:19:56.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename subpath @ 06/10/23 12:19:56.871
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:19:56.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:19:56.899
  STEP: Setting up data @ 06/10/23 12:19:56.903
  STEP: Creating pod pod-subpath-test-secret-mt5h @ 06/10/23 12:19:56.914
  STEP: Creating a pod to test atomic-volume-subpath @ 06/10/23 12:19:56.914
  STEP: Saw pod success @ 06/10/23 12:20:21.005
  Jun 10 12:20:21.008: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-subpath-test-secret-mt5h container test-container-subpath-secret-mt5h: <nil>
  STEP: delete the pod @ 06/10/23 12:20:21.018
  STEP: Deleting pod pod-subpath-test-secret-mt5h @ 06/10/23 12:20:21.038
  Jun 10 12:20:21.038: INFO: Deleting pod "pod-subpath-test-secret-mt5h" in namespace "subpath-1913"
  Jun 10 12:20:21.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1913" for this suite. @ 06/10/23 12:20:21.047
• [24.183 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 06/10/23 12:20:21.056
  Jun 10 12:20:21.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename pods @ 06/10/23 12:20:21.057
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:20:21.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:20:21.081
  STEP: Saw pod success @ 06/10/23 12:20:27.16
  Jun 10 12:20:27.165: INFO: Trying to get logs from node ip-172-31-89-0 pod client-envvars-ee528cee-e6df-43c4-847e-d0f15e8e8ff4 container env3cont: <nil>
  STEP: delete the pod @ 06/10/23 12:20:27.189
  Jun 10 12:20:27.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4262" for this suite. @ 06/10/23 12:20:27.212
• [6.163 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 06/10/23 12:20:27.221
  Jun 10 12:20:27.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename statefulset @ 06/10/23 12:20:27.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:20:27.242
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:20:27.247
  STEP: Creating service test in namespace statefulset-4389 @ 06/10/23 12:20:27.251
  STEP: Creating a new StatefulSet @ 06/10/23 12:20:27.257
  Jun 10 12:20:27.273: INFO: Found 0 stateful pods, waiting for 3
  Jun 10 12:20:37.281: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 10 12:20:37.281: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 10 12:20:37.281: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Jun 10 12:20:37.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4389 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 10 12:20:37.450: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 10 12:20:37.450: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 10 12:20:37.450: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 06/10/23 12:20:47.472
  Jun 10 12:20:47.495: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 06/10/23 12:20:47.495
  STEP: Updating Pods in reverse ordinal order @ 06/10/23 12:20:57.514
  Jun 10 12:20:57.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4389 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 10 12:20:57.680: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 10 12:20:57.680: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 10 12:20:57.680: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  STEP: Rolling back to a previous revision @ 06/10/23 12:21:07.71
  Jun 10 12:21:07.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4389 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 10 12:21:07.883: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 10 12:21:07.883: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 10 12:21:07.883: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 10 12:21:17.928: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 06/10/23 12:21:27.947
  Jun 10 12:21:27.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4389 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 10 12:21:28.107: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 10 12:21:28.107: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 10 12:21:28.107: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 10 12:21:38.133: INFO: Deleting all statefulset in ns statefulset-4389
  Jun 10 12:21:38.137: INFO: Scaling statefulset ss2 to 0
  Jun 10 12:21:48.158: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 10 12:21:48.162: INFO: Deleting statefulset ss2
  Jun 10 12:21:48.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4389" for this suite. @ 06/10/23 12:21:48.186
• [80.973 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 06/10/23 12:21:48.198
  Jun 10 12:21:48.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename init-container @ 06/10/23 12:21:48.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:21:48.218
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:21:48.222
  STEP: creating the pod @ 06/10/23 12:21:48.226
  Jun 10 12:21:48.226: INFO: PodSpec: initContainers in spec.initContainers
  Jun 10 12:22:27.589: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-d113bc2a-b6b7-4855-a6a5-f17e4c5d446c", GenerateName:"", Namespace:"init-container-1909", SelfLink:"", UID:"35c596af-a688-408d-bb88-16ac5d8891b3", ResourceVersion:"7578", Generation:0, CreationTimestamp:time.Date(2023, time.June, 10, 12, 21, 48, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"226789148"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 10, 12, 21, 48, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000f962e8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 10, 12, 22, 27, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000f96348), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-42bvs", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000934220), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-42bvs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-42bvs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-42bvs", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0043085b0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-27-177", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0004e2c40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004308640)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004308660)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004308668), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00430866c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001002150), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 10, 12, 21, 48, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 10, 12, 21, 48, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 10, 12, 21, 48, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 10, 12, 21, 48, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.27.177", PodIP:"192.168.109.42", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.109.42"}}, StartTime:time.Date(2023, time.June, 10, 12, 21, 48, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0004e2f50)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0004e2fc0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://62831f51964c1f0addd5e75c1a282085ff2fc472d61eeb9f2143e53459dfa84f", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0009342c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0009342a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0043086e4), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Jun 10 12:22:27.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1909" for this suite. @ 06/10/23 12:22:27.595
• [39.406 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 06/10/23 12:22:27.607
  Jun 10 12:22:27.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename podtemplate @ 06/10/23 12:22:27.608
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:22:27.629
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:22:27.633
  STEP: Create a pod template @ 06/10/23 12:22:27.638
  STEP: Replace a pod template @ 06/10/23 12:22:27.644
  Jun 10 12:22:27.657: INFO: Found updated podtemplate annotation: "true"

  Jun 10 12:22:27.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-7031" for this suite. @ 06/10/23 12:22:27.662
• [0.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 06/10/23 12:22:27.678
  Jun 10 12:22:27.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl @ 06/10/23 12:22:27.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:22:27.746
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:22:27.75
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 06/10/23 12:22:27.754
  Jun 10 12:22:27.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-3468 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Jun 10 12:22:27.841: INFO: stderr: ""
  Jun 10 12:22:27.841: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 06/10/23 12:22:27.841
  Jun 10 12:22:27.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-3468 delete pods e2e-test-httpd-pod'
  Jun 10 12:22:30.296: INFO: stderr: ""
  Jun 10 12:22:30.296: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jun 10 12:22:30.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3468" for this suite. @ 06/10/23 12:22:30.301
• [2.629 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 06/10/23 12:22:30.309
  Jun 10 12:22:30.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename cronjob @ 06/10/23 12:22:30.311
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:22:30.327
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:22:30.334
  STEP: Creating a cronjob @ 06/10/23 12:22:30.338
  STEP: creating @ 06/10/23 12:22:30.338
  STEP: getting @ 06/10/23 12:22:30.346
  STEP: listing @ 06/10/23 12:22:30.349
  STEP: watching @ 06/10/23 12:22:30.353
  Jun 10 12:22:30.354: INFO: starting watch
  STEP: cluster-wide listing @ 06/10/23 12:22:30.355
  STEP: cluster-wide watching @ 06/10/23 12:22:30.359
  Jun 10 12:22:30.359: INFO: starting watch
  STEP: patching @ 06/10/23 12:22:30.361
  STEP: updating @ 06/10/23 12:22:30.368
  Jun 10 12:22:30.379: INFO: waiting for watch events with expected annotations
  Jun 10 12:22:30.379: INFO: saw patched and updated annotations
  STEP: patching /status @ 06/10/23 12:22:30.38
  STEP: updating /status @ 06/10/23 12:22:30.388
  STEP: get /status @ 06/10/23 12:22:30.4
  STEP: deleting @ 06/10/23 12:22:30.404
  STEP: deleting a collection @ 06/10/23 12:22:30.421
  Jun 10 12:22:30.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6963" for this suite. @ 06/10/23 12:22:30.439
• [0.138 seconds]
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 06/10/23 12:22:30.447
  Jun 10 12:22:30.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename var-expansion @ 06/10/23 12:22:30.449
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:22:30.465
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:22:30.47
  STEP: Creating a pod to test env composition @ 06/10/23 12:22:30.474
  STEP: Saw pod success @ 06/10/23 12:22:34.498
  Jun 10 12:22:34.502: INFO: Trying to get logs from node ip-172-31-89-0 pod var-expansion-b9cc7685-e634-4a3c-963c-04712aafd232 container dapi-container: <nil>
  STEP: delete the pod @ 06/10/23 12:22:34.523
  Jun 10 12:22:34.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2666" for this suite. @ 06/10/23 12:22:34.546
• [4.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 06/10/23 12:22:34.563
  Jun 10 12:22:34.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 12:22:34.564
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:22:34.588
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:22:34.593
  STEP: Creating the pod @ 06/10/23 12:22:34.597
  Jun 10 12:22:37.158: INFO: Successfully updated pod "annotationupdate9b758831-c41e-467d-a988-8310b7393f3a"
  Jun 10 12:22:39.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7004" for this suite. @ 06/10/23 12:22:39.189
• [4.634 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 06/10/23 12:22:39.199
  Jun 10 12:22:39.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename services @ 06/10/23 12:22:39.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:22:39.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:22:39.226
  STEP: creating service in namespace services-5387 @ 06/10/23 12:22:39.23
  STEP: creating service affinity-nodeport in namespace services-5387 @ 06/10/23 12:22:39.23
  STEP: creating replication controller affinity-nodeport in namespace services-5387 @ 06/10/23 12:22:39.254
  I0610 12:22:39.271597      18 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-5387, replica count: 3
  I0610 12:22:42.322583      18 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 10 12:22:42.335: INFO: Creating new exec pod
  Jun 10 12:22:45.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5387 exec execpod-affinitymtsq2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Jun 10 12:22:45.518: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Jun 10 12:22:45.518: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 12:22:45.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5387 exec execpod-affinitymtsq2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.120 80'
  Jun 10 12:22:45.664: INFO: stderr: "+ nc -v -t -w 2 10.152.183.120 80\nConnection to 10.152.183.120 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun 10 12:22:45.664: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 12:22:45.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5387 exec execpod-affinitymtsq2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.46.40 30889'
  Jun 10 12:22:45.814: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.46.40 30889\nConnection to 172.31.46.40 30889 port [tcp/*] succeeded!\n"
  Jun 10 12:22:45.814: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 12:22:45.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5387 exec execpod-affinitymtsq2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.27.177 30889'
  Jun 10 12:22:45.964: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.27.177 30889\nConnection to 172.31.27.177 30889 port [tcp/*] succeeded!\n"
  Jun 10 12:22:45.964: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 12:22:45.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5387 exec execpod-affinitymtsq2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.27.177:30889/ ; done'
  Jun 10 12:22:46.201: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30889/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30889/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30889/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30889/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30889/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30889/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30889/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30889/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30889/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30889/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30889/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30889/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30889/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30889/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30889/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30889/\n"
  Jun 10 12:22:46.201: INFO: stdout: "\naffinity-nodeport-f4zf2\naffinity-nodeport-f4zf2\naffinity-nodeport-f4zf2\naffinity-nodeport-f4zf2\naffinity-nodeport-f4zf2\naffinity-nodeport-f4zf2\naffinity-nodeport-f4zf2\naffinity-nodeport-f4zf2\naffinity-nodeport-f4zf2\naffinity-nodeport-f4zf2\naffinity-nodeport-f4zf2\naffinity-nodeport-f4zf2\naffinity-nodeport-f4zf2\naffinity-nodeport-f4zf2\naffinity-nodeport-f4zf2\naffinity-nodeport-f4zf2"
  Jun 10 12:22:46.201: INFO: Received response from host: affinity-nodeport-f4zf2
  Jun 10 12:22:46.201: INFO: Received response from host: affinity-nodeport-f4zf2
  Jun 10 12:22:46.201: INFO: Received response from host: affinity-nodeport-f4zf2
  Jun 10 12:22:46.201: INFO: Received response from host: affinity-nodeport-f4zf2
  Jun 10 12:22:46.201: INFO: Received response from host: affinity-nodeport-f4zf2
  Jun 10 12:22:46.201: INFO: Received response from host: affinity-nodeport-f4zf2
  Jun 10 12:22:46.201: INFO: Received response from host: affinity-nodeport-f4zf2
  Jun 10 12:22:46.201: INFO: Received response from host: affinity-nodeport-f4zf2
  Jun 10 12:22:46.201: INFO: Received response from host: affinity-nodeport-f4zf2
  Jun 10 12:22:46.201: INFO: Received response from host: affinity-nodeport-f4zf2
  Jun 10 12:22:46.201: INFO: Received response from host: affinity-nodeport-f4zf2
  Jun 10 12:22:46.201: INFO: Received response from host: affinity-nodeport-f4zf2
  Jun 10 12:22:46.201: INFO: Received response from host: affinity-nodeport-f4zf2
  Jun 10 12:22:46.201: INFO: Received response from host: affinity-nodeport-f4zf2
  Jun 10 12:22:46.201: INFO: Received response from host: affinity-nodeport-f4zf2
  Jun 10 12:22:46.201: INFO: Received response from host: affinity-nodeport-f4zf2
  Jun 10 12:22:46.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 10 12:22:46.206: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-5387, will wait for the garbage collector to delete the pods @ 06/10/23 12:22:46.222
  Jun 10 12:22:46.289: INFO: Deleting ReplicationController affinity-nodeport took: 8.337828ms
  Jun 10 12:22:46.390: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.789034ms
  STEP: Destroying namespace "services-5387" for this suite. @ 06/10/23 12:22:48.722
• [9.532 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 06/10/23 12:22:48.732
  Jun 10 12:22:48.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename field-validation @ 06/10/23 12:22:48.733
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:22:48.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:22:48.758
  Jun 10 12:22:48.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  W0610 12:22:51.331153      18 warnings.go:70] unknown field "alpha"
  W0610 12:22:51.331295      18 warnings.go:70] unknown field "beta"
  W0610 12:22:51.331423      18 warnings.go:70] unknown field "delta"
  W0610 12:22:51.331521      18 warnings.go:70] unknown field "epsilon"
  W0610 12:22:51.331625      18 warnings.go:70] unknown field "gamma"
  Jun 10 12:22:51.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9827" for this suite. @ 06/10/23 12:22:51.371
• [2.646 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 06/10/23 12:22:51.38
  Jun 10 12:22:51.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 12:22:51.381
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:22:51.402
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:22:51.406
  STEP: Setting up server cert @ 06/10/23 12:22:51.435
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 12:22:51.861
  STEP: Deploying the webhook pod @ 06/10/23 12:22:51.869
  STEP: Wait for the deployment to be ready @ 06/10/23 12:22:51.887
  Jun 10 12:22:51.911: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/10/23 12:22:53.927
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 12:22:53.939
  Jun 10 12:22:54.940: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun 10 12:22:54.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 06/10/23 12:22:55.458
  STEP: Creating a custom resource that should be denied by the webhook @ 06/10/23 12:22:55.479
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 06/10/23 12:22:57.514
  STEP: Updating the custom resource with disallowed data should be denied @ 06/10/23 12:22:57.521
  STEP: Deleting the custom resource should be denied @ 06/10/23 12:22:57.534
  STEP: Remove the offending key and value from the custom resource data @ 06/10/23 12:22:57.542
  STEP: Deleting the updated custom resource should be successful @ 06/10/23 12:22:57.555
  Jun 10 12:22:57.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2781" for this suite. @ 06/10/23 12:22:58.151
  STEP: Destroying namespace "webhook-markers-1213" for this suite. @ 06/10/23 12:22:58.16
• [6.787 seconds]
------------------------------
S
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 06/10/23 12:22:58.167
  Jun 10 12:22:58.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename job @ 06/10/23 12:22:58.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:22:58.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:22:58.191
  STEP: Creating a job @ 06/10/23 12:22:58.196
  STEP: Ensuring active pods == parallelism @ 06/10/23 12:22:58.204
  STEP: delete a job @ 06/10/23 12:23:00.209
  STEP: deleting Job.batch foo in namespace job-7096, will wait for the garbage collector to delete the pods @ 06/10/23 12:23:00.209
  Jun 10 12:23:00.272: INFO: Deleting Job.batch foo took: 7.790741ms
  Jun 10 12:23:00.373: INFO: Terminating Job.batch foo pods took: 101.115348ms
  STEP: Ensuring job was deleted @ 06/10/23 12:23:31.874
  Jun 10 12:23:31.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7096" for this suite. @ 06/10/23 12:23:31.884
• [33.724 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 06/10/23 12:23:31.893
  Jun 10 12:23:31.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename daemonsets @ 06/10/23 12:23:31.894
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:23:31.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:23:31.922
  STEP: Creating simple DaemonSet "daemon-set" @ 06/10/23 12:23:31.95
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/10/23 12:23:31.956
  Jun 10 12:23:31.961: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 12:23:31.961: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 12:23:31.966: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 12:23:31.966: INFO: Node ip-172-31-27-177 is running 0 daemon pod, expected 1
  Jun 10 12:23:32.971: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 12:23:32.971: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 12:23:32.976: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 12:23:32.976: INFO: Node ip-172-31-27-177 is running 0 daemon pod, expected 1
  Jun 10 12:23:33.971: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 12:23:33.971: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 12:23:33.976: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 10 12:23:33.976: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 06/10/23 12:23:33.981
  STEP: DeleteCollection of the DaemonSets @ 06/10/23 12:23:33.99
  STEP: Verify that ReplicaSets have been deleted @ 06/10/23 12:23:34.008
  Jun 10 12:23:34.039: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8230"},"items":null}

  Jun 10 12:23:34.044: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8230"},"items":[{"metadata":{"name":"daemon-set-9gndk","generateName":"daemon-set-","namespace":"daemonsets-9175","uid":"694c409e-9eef-4bdd-844f-a24a93f7ba5f","resourceVersion":"8227","creationTimestamp":"2023-06-10T12:23:31Z","deletionTimestamp":"2023-06-10T12:24:04Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"08c70831-9edc-46a6-8f28-5a52e2dd4114","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-10T12:23:31Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08c70831-9edc-46a6-8f28-5a52e2dd4114\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-10T12:23:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.109.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wj9xm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wj9xm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-27-177","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-27-177"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-10T12:23:31Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-10T12:23:33Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-10T12:23:33Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-10T12:23:31Z"}],"hostIP":"172.31.27.177","podIP":"192.168.109.49","podIPs":[{"ip":"192.168.109.49"}],"startTime":"2023-06-10T12:23:31Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-10T12:23:32Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://d56ce71345b2463038dbd884d23e07be7d07da1eb70d507f967dc50e0444ef9d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-p2jm8","generateName":"daemon-set-","namespace":"daemonsets-9175","uid":"c288b322-6fc6-46b9-bed6-2434641a298e","resourceVersion":"8230","creationTimestamp":"2023-06-10T12:23:31Z","deletionTimestamp":"2023-06-10T12:24:04Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"08c70831-9edc-46a6-8f28-5a52e2dd4114","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-10T12:23:31Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08c70831-9edc-46a6-8f28-5a52e2dd4114\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-10T12:23:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.149.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2fhfc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2fhfc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-89-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-89-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-10T12:23:31Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-10T12:23:33Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-10T12:23:33Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-10T12:23:31Z"}],"hostIP":"172.31.89.0","podIP":"192.168.149.87","podIPs":[{"ip":"192.168.149.87"}],"startTime":"2023-06-10T12:23:31Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-10T12:23:32Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://7a4fa69bfe6ed0f70e41e0f157e0d24982c584afa0d39cd964aa4a42301c1c13","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-xlb5f","generateName":"daemon-set-","namespace":"daemonsets-9175","uid":"732285f4-9bf5-42b5-be3b-e15c4a2306b9","resourceVersion":"8229","creationTimestamp":"2023-06-10T12:23:31Z","deletionTimestamp":"2023-06-10T12:24:04Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"08c70831-9edc-46a6-8f28-5a52e2dd4114","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-10T12:23:31Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08c70831-9edc-46a6-8f28-5a52e2dd4114\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-10T12:23:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.92.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-c6dx8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-c6dx8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-46-40","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-46-40"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-10T12:23:32Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-10T12:23:33Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-10T12:23:33Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-10T12:23:31Z"}],"hostIP":"172.31.46.40","podIP":"192.168.92.17","podIPs":[{"ip":"192.168.92.17"}],"startTime":"2023-06-10T12:23:32Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-10T12:23:32Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://738813f091a60d8cfd3731af85ac3f3bc53f1aa127e5a57f863460b69a429273","started":true}],"qosClass":"BestEffort"}}]}

  Jun 10 12:23:34.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9175" for this suite. @ 06/10/23 12:23:34.074
• [2.188 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 06/10/23 12:23:34.084
  Jun 10 12:23:34.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl @ 06/10/23 12:23:34.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:23:34.107
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:23:34.114
  STEP: creating Agnhost RC @ 06/10/23 12:23:34.119
  Jun 10 12:23:34.119: INFO: namespace kubectl-1073
  Jun 10 12:23:34.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-1073 create -f -'
  Jun 10 12:23:35.702: INFO: stderr: ""
  Jun 10 12:23:35.702: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 06/10/23 12:23:35.702
  Jun 10 12:23:36.707: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 10 12:23:36.707: INFO: Found 0 / 1
  Jun 10 12:23:37.708: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 10 12:23:37.708: INFO: Found 0 / 1
  Jun 10 12:23:38.707: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 10 12:23:38.707: INFO: Found 1 / 1
  Jun 10 12:23:38.707: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jun 10 12:23:38.711: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 10 12:23:38.711: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jun 10 12:23:38.711: INFO: wait on agnhost-primary startup in kubectl-1073 
  Jun 10 12:23:38.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-1073 logs agnhost-primary-bzglr agnhost-primary'
  Jun 10 12:23:38.809: INFO: stderr: ""
  Jun 10 12:23:38.809: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 06/10/23 12:23:38.809
  Jun 10 12:23:38.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-1073 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Jun 10 12:23:38.920: INFO: stderr: ""
  Jun 10 12:23:38.920: INFO: stdout: "service/rm2 exposed\n"
  Jun 10 12:23:38.927: INFO: Service rm2 in namespace kubectl-1073 found.
  STEP: exposing service @ 06/10/23 12:23:40.934
  Jun 10 12:23:40.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-1073 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Jun 10 12:23:41.040: INFO: stderr: ""
  Jun 10 12:23:41.040: INFO: stdout: "service/rm3 exposed\n"
  Jun 10 12:23:41.051: INFO: Service rm3 in namespace kubectl-1073 found.
  Jun 10 12:23:43.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1073" for this suite. @ 06/10/23 12:23:43.065
• [8.988 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 06/10/23 12:23:43.073
  Jun 10 12:23:43.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename dns @ 06/10/23 12:23:43.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:23:43.093
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:23:43.097
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4507.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4507.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 06/10/23 12:23:43.101
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4507.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4507.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 06/10/23 12:23:43.101
  STEP: creating a pod to probe /etc/hosts @ 06/10/23 12:23:43.101
  STEP: submitting the pod to kubernetes @ 06/10/23 12:23:43.101
  STEP: retrieving the pod @ 06/10/23 12:23:45.126
  STEP: looking for the results for each expected name from probers @ 06/10/23 12:23:45.13
  Jun 10 12:23:45.151: INFO: DNS probes using dns-4507/dns-test-c6792ed7-cea4-45ef-a5c0-8814bd5b9bbc succeeded

  Jun 10 12:23:45.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/10/23 12:23:45.155
  STEP: Destroying namespace "dns-4507" for this suite. @ 06/10/23 12:23:45.17
• [2.104 seconds]
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 06/10/23 12:23:45.178
  Jun 10 12:23:45.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir @ 06/10/23 12:23:45.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:23:45.203
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:23:45.207
  STEP: Creating Pod @ 06/10/23 12:23:45.211
  STEP: Reading file content from the nginx-container @ 06/10/23 12:23:47.238
  Jun 10 12:23:47.238: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8362 PodName:pod-sharedvolume-6e1513d5-4af0-4793-951b-99eb0aa88c2f ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 12:23:47.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:23:47.239: INFO: ExecWithOptions: Clientset creation
  Jun 10 12:23:47.239: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-8362/pods/pod-sharedvolume-6e1513d5-4af0-4793-951b-99eb0aa88c2f/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Jun 10 12:23:47.323: INFO: Exec stderr: ""
  Jun 10 12:23:47.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8362" for this suite. @ 06/10/23 12:23:47.328
• [2.159 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 06/10/23 12:23:47.337
  Jun 10 12:23:47.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename daemonsets @ 06/10/23 12:23:47.338
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:23:47.358
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:23:47.363
  STEP: Creating simple DaemonSet "daemon-set" @ 06/10/23 12:23:47.397
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/10/23 12:23:47.403
  Jun 10 12:23:47.409: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 12:23:47.409: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 12:23:47.413: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 12:23:47.413: INFO: Node ip-172-31-27-177 is running 0 daemon pod, expected 1
  Jun 10 12:23:48.418: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 12:23:48.418: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 12:23:48.424: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 12:23:48.424: INFO: Node ip-172-31-27-177 is running 0 daemon pod, expected 1
  Jun 10 12:23:49.418: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 12:23:49.418: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 12:23:49.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 10 12:23:49.424: INFO: Node ip-172-31-27-177 is running 0 daemon pod, expected 1
  Jun 10 12:23:50.418: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 12:23:50.419: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 12:23:50.425: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 10 12:23:50.425: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 06/10/23 12:23:50.429
  Jun 10 12:23:50.435: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 06/10/23 12:23:50.435
  Jun 10 12:23:50.445: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 06/10/23 12:23:50.446
  Jun 10 12:23:50.449: INFO: Observed &DaemonSet event: ADDED
  Jun 10 12:23:50.449: INFO: Observed &DaemonSet event: MODIFIED
  Jun 10 12:23:50.450: INFO: Observed &DaemonSet event: MODIFIED
  Jun 10 12:23:50.450: INFO: Observed &DaemonSet event: MODIFIED
  Jun 10 12:23:50.451: INFO: Observed &DaemonSet event: MODIFIED
  Jun 10 12:23:50.451: INFO: Observed &DaemonSet event: MODIFIED
  Jun 10 12:23:50.452: INFO: Found daemon set daemon-set in namespace daemonsets-4170 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jun 10 12:23:50.452: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 06/10/23 12:23:50.452
  STEP: watching for the daemon set status to be patched @ 06/10/23 12:23:50.462
  Jun 10 12:23:50.466: INFO: Observed &DaemonSet event: ADDED
  Jun 10 12:23:50.466: INFO: Observed &DaemonSet event: MODIFIED
  Jun 10 12:23:50.467: INFO: Observed &DaemonSet event: MODIFIED
  Jun 10 12:23:50.467: INFO: Observed &DaemonSet event: MODIFIED
  Jun 10 12:23:50.468: INFO: Observed &DaemonSet event: MODIFIED
  Jun 10 12:23:50.468: INFO: Observed &DaemonSet event: MODIFIED
  Jun 10 12:23:50.468: INFO: Observed daemon set daemon-set in namespace daemonsets-4170 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jun 10 12:23:50.468: INFO: Observed &DaemonSet event: MODIFIED
  Jun 10 12:23:50.468: INFO: Found daemon set daemon-set in namespace daemonsets-4170 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Jun 10 12:23:50.468: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 06/10/23 12:23:50.474
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4170, will wait for the garbage collector to delete the pods @ 06/10/23 12:23:50.474
  Jun 10 12:23:50.540: INFO: Deleting DaemonSet.extensions daemon-set took: 7.549057ms
  Jun 10 12:23:50.641: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.874865ms
  Jun 10 12:23:51.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 12:23:51.945: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 10 12:23:51.949: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8527"},"items":null}

  Jun 10 12:23:51.953: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8527"},"items":null}

  Jun 10 12:23:51.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4170" for this suite. @ 06/10/23 12:23:51.977
• [4.648 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 06/10/23 12:23:51.99
  Jun 10 12:23:51.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename gc @ 06/10/23 12:23:51.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:23:52.005
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:23:52.011
  STEP: create the rc @ 06/10/23 12:23:52.02
  W0610 12:23:52.027256      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 06/10/23 12:23:58.044
  STEP: wait for the rc to be deleted @ 06/10/23 12:23:58.061
  Jun 10 12:23:59.097: INFO: 80 pods remaining
  Jun 10 12:23:59.097: INFO: 80 pods has nil DeletionTimestamp
  Jun 10 12:23:59.097: INFO: 
  Jun 10 12:24:00.080: INFO: 70 pods remaining
  Jun 10 12:24:00.080: INFO: 70 pods has nil DeletionTimestamp
  Jun 10 12:24:00.080: INFO: 
  Jun 10 12:24:01.074: INFO: 60 pods remaining
  Jun 10 12:24:01.074: INFO: 60 pods has nil DeletionTimestamp
  Jun 10 12:24:01.074: INFO: 
  Jun 10 12:24:02.078: INFO: 40 pods remaining
  Jun 10 12:24:02.078: INFO: 40 pods has nil DeletionTimestamp
  Jun 10 12:24:02.078: INFO: 
  Jun 10 12:24:03.096: INFO: 30 pods remaining
  Jun 10 12:24:03.096: INFO: 30 pods has nil DeletionTimestamp
  Jun 10 12:24:03.096: INFO: 
  Jun 10 12:24:04.077: INFO: 20 pods remaining
  Jun 10 12:24:04.077: INFO: 20 pods has nil DeletionTimestamp
  Jun 10 12:24:04.077: INFO: 
  STEP: Gathering metrics @ 06/10/23 12:24:05.076
  W0610 12:24:05.083426      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 10 12:24:05.083: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 10 12:24:05.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3543" for this suite. @ 06/10/23 12:24:05.097
• [13.114 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 06/10/23 12:24:05.118
  Jun 10 12:24:05.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename replicaset @ 06/10/23 12:24:05.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:24:05.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:24:05.144
  Jun 10 12:24:05.148: INFO: Creating ReplicaSet my-hostname-basic-d0327b76-b0b6-45d1-b099-404d788c2fba
  Jun 10 12:24:05.160: INFO: Pod name my-hostname-basic-d0327b76-b0b6-45d1-b099-404d788c2fba: Found 0 pods out of 1
  Jun 10 12:24:10.166: INFO: Pod name my-hostname-basic-d0327b76-b0b6-45d1-b099-404d788c2fba: Found 1 pods out of 1
  Jun 10 12:24:10.166: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d0327b76-b0b6-45d1-b099-404d788c2fba" is running
  Jun 10 12:24:10.171: INFO: Pod "my-hostname-basic-d0327b76-b0b6-45d1-b099-404d788c2fba-n7gdh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-10 12:24:05 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-10 12:24:07 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-10 12:24:07 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-10 12:24:05 +0000 UTC Reason: Message:}])
  Jun 10 12:24:10.172: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 06/10/23 12:24:10.172
  Jun 10 12:24:10.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-105" for this suite. @ 06/10/23 12:24:10.194
• [5.083 seconds]
------------------------------
S
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 06/10/23 12:24:10.202
  Jun 10 12:24:10.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename security-context-test @ 06/10/23 12:24:10.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:24:10.228
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:24:10.233
  Jun 10 12:24:22.328: INFO: Got logs for pod "busybox-privileged-false-569ad235-1cbd-40ea-9ab9-4bac202847a4": "ip: RTNETLINK answers: Operation not permitted\n"
  Jun 10 12:24:22.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6926" for this suite. @ 06/10/23 12:24:22.333
• [12.141 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 06/10/23 12:24:22.343
  Jun 10 12:24:22.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename dns @ 06/10/23 12:24:22.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:24:22.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:24:22.374
  STEP: Creating a test externalName service @ 06/10/23 12:24:22.379
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-443.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-443.svc.cluster.local; sleep 1; done
   @ 06/10/23 12:24:22.39
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-443.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-443.svc.cluster.local; sleep 1; done
   @ 06/10/23 12:24:22.39
  STEP: creating a pod to probe DNS @ 06/10/23 12:24:22.39
  STEP: submitting the pod to kubernetes @ 06/10/23 12:24:22.39
  STEP: retrieving the pod @ 06/10/23 12:24:24.423
  STEP: looking for the results for each expected name from probers @ 06/10/23 12:24:24.427
  Jun 10 12:24:24.437: INFO: DNS probes using dns-test-3be71545-7661-43a9-a1fc-3fdb5e918f31 succeeded

  STEP: changing the externalName to bar.example.com @ 06/10/23 12:24:24.438
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-443.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-443.svc.cluster.local; sleep 1; done
   @ 06/10/23 12:24:24.45
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-443.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-443.svc.cluster.local; sleep 1; done
   @ 06/10/23 12:24:24.45
  STEP: creating a second pod to probe DNS @ 06/10/23 12:24:24.451
  STEP: submitting the pod to kubernetes @ 06/10/23 12:24:24.451
  STEP: retrieving the pod @ 06/10/23 12:24:32.491
  STEP: looking for the results for each expected name from probers @ 06/10/23 12:24:32.495
  Jun 10 12:24:32.507: INFO: DNS probes using dns-test-6d90c6e3-2547-4229-aad5-ddd9725af2ae succeeded

  STEP: changing the service to type=ClusterIP @ 06/10/23 12:24:32.507
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-443.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-443.svc.cluster.local; sleep 1; done
   @ 06/10/23 12:24:32.535
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-443.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-443.svc.cluster.local; sleep 1; done
   @ 06/10/23 12:24:32.535
  STEP: creating a third pod to probe DNS @ 06/10/23 12:24:32.535
  STEP: submitting the pod to kubernetes @ 06/10/23 12:24:32.545
  STEP: retrieving the pod @ 06/10/23 12:24:34.569
  STEP: looking for the results for each expected name from probers @ 06/10/23 12:24:34.573
  Jun 10 12:24:34.586: INFO: DNS probes using dns-test-d3371b46-93f5-4847-978c-2545e32a7efd succeeded

  Jun 10 12:24:34.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/10/23 12:24:34.591
  STEP: deleting the pod @ 06/10/23 12:24:34.603
  STEP: deleting the pod @ 06/10/23 12:24:34.632
  STEP: deleting the test externalName service @ 06/10/23 12:24:34.659
  STEP: Destroying namespace "dns-443" for this suite. @ 06/10/23 12:24:34.692
• [12.358 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 06/10/23 12:24:34.702
  Jun 10 12:24:34.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename svcaccounts @ 06/10/23 12:24:34.703
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:24:34.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:24:34.738
  Jun 10 12:24:34.776: INFO: created pod pod-service-account-defaultsa
  Jun 10 12:24:34.776: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Jun 10 12:24:34.786: INFO: created pod pod-service-account-mountsa
  Jun 10 12:24:34.786: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Jun 10 12:24:34.798: INFO: created pod pod-service-account-nomountsa
  Jun 10 12:24:34.798: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Jun 10 12:24:34.804: INFO: created pod pod-service-account-defaultsa-mountspec
  Jun 10 12:24:34.805: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Jun 10 12:24:34.816: INFO: created pod pod-service-account-mountsa-mountspec
  Jun 10 12:24:34.817: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Jun 10 12:24:34.829: INFO: created pod pod-service-account-nomountsa-mountspec
  Jun 10 12:24:34.832: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Jun 10 12:24:34.839: INFO: created pod pod-service-account-defaultsa-nomountspec
  Jun 10 12:24:34.840: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Jun 10 12:24:34.854: INFO: created pod pod-service-account-mountsa-nomountspec
  Jun 10 12:24:34.854: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Jun 10 12:24:34.862: INFO: created pod pod-service-account-nomountsa-nomountspec
  Jun 10 12:24:34.862: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Jun 10 12:24:34.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3796" for this suite. @ 06/10/23 12:24:34.87
• [0.183 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 06/10/23 12:24:34.887
  Jun 10 12:24:34.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename var-expansion @ 06/10/23 12:24:34.889
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:24:34.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:24:34.922
  STEP: creating the pod @ 06/10/23 12:24:34.927
  STEP: waiting for pod running @ 06/10/23 12:24:34.942
  STEP: creating a file in subpath @ 06/10/23 12:24:38.961
  Jun 10 12:24:38.967: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8423 PodName:var-expansion-b992fa78-0f66-46db-9b21-45a19a68db52 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 12:24:38.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:24:38.968: INFO: ExecWithOptions: Clientset creation
  Jun 10 12:24:38.968: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-8423/pods/var-expansion-b992fa78-0f66-46db-9b21-45a19a68db52/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 06/10/23 12:24:39.046
  Jun 10 12:24:39.051: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8423 PodName:var-expansion-b992fa78-0f66-46db-9b21-45a19a68db52 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 12:24:39.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:24:39.051: INFO: ExecWithOptions: Clientset creation
  Jun 10 12:24:39.051: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-8423/pods/var-expansion-b992fa78-0f66-46db-9b21-45a19a68db52/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 06/10/23 12:24:39.12
  Jun 10 12:24:39.637: INFO: Successfully updated pod "var-expansion-b992fa78-0f66-46db-9b21-45a19a68db52"
  STEP: waiting for annotated pod running @ 06/10/23 12:24:39.638
  STEP: deleting the pod gracefully @ 06/10/23 12:24:39.642
  Jun 10 12:24:39.642: INFO: Deleting pod "var-expansion-b992fa78-0f66-46db-9b21-45a19a68db52" in namespace "var-expansion-8423"
  Jun 10 12:24:39.650: INFO: Wait up to 5m0s for pod "var-expansion-b992fa78-0f66-46db-9b21-45a19a68db52" to be fully deleted
  Jun 10 12:25:11.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8423" for this suite. @ 06/10/23 12:25:11.74
• [36.861 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 06/10/23 12:25:11.749
  Jun 10 12:25:11.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename configmap @ 06/10/23 12:25:11.75
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:25:11.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:25:11.776
  STEP: Creating configMap with name configmap-test-volume-map-cdaa87fd-6caa-44e8-b807-c46dc8b651b2 @ 06/10/23 12:25:11.784
  STEP: Creating a pod to test consume configMaps @ 06/10/23 12:25:11.79
  STEP: Saw pod success @ 06/10/23 12:25:15.819
  Jun 10 12:25:15.823: INFO: Trying to get logs from node ip-172-31-89-0 pod pod-configmaps-dff5d079-d6b6-48d1-9654-c6c9f7ebb221 container agnhost-container: <nil>
  STEP: delete the pod @ 06/10/23 12:25:15.843
  Jun 10 12:25:15.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8455" for this suite. @ 06/10/23 12:25:15.868
• [4.127 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 06/10/23 12:25:15.878
  Jun 10 12:25:15.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename endpointslice @ 06/10/23 12:25:15.879
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:25:15.895
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:25:15.901
  Jun 10 12:25:15.918: INFO: Endpoints addresses: [172.31.13.130 172.31.22.107] , ports: [6443]
  Jun 10 12:25:15.918: INFO: EndpointSlices addresses: [172.31.13.130 172.31.22.107] , ports: [6443]
  Jun 10 12:25:15.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7170" for this suite. @ 06/10/23 12:25:15.927
• [0.058 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 06/10/23 12:25:15.937
  Jun 10 12:25:15.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 06/10/23 12:25:15.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:25:15.953
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:25:15.96
  STEP: creating a target pod @ 06/10/23 12:25:15.964
  STEP: adding an ephemeral container @ 06/10/23 12:25:17.988
  STEP: checking pod container endpoints @ 06/10/23 12:25:20.01
  Jun 10 12:25:20.010: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2678 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 12:25:20.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:25:20.011: INFO: ExecWithOptions: Clientset creation
  Jun 10 12:25:20.011: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-2678/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Jun 10 12:25:20.093: INFO: Exec stderr: ""
  Jun 10 12:25:20.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-2678" for this suite. @ 06/10/23 12:25:20.107
• [4.178 seconds]
------------------------------
SS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 06/10/23 12:25:20.116
  Jun 10 12:25:20.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename endpointslice @ 06/10/23 12:25:20.117
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:25:20.141
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:25:20.146
  STEP: getting /apis @ 06/10/23 12:25:20.151
  STEP: getting /apis/discovery.k8s.io @ 06/10/23 12:25:20.156
  STEP: getting /apis/discovery.k8s.iov1 @ 06/10/23 12:25:20.158
  STEP: creating @ 06/10/23 12:25:20.16
  STEP: getting @ 06/10/23 12:25:20.187
  STEP: listing @ 06/10/23 12:25:20.197
  STEP: watching @ 06/10/23 12:25:20.204
  Jun 10 12:25:20.205: INFO: starting watch
  STEP: cluster-wide listing @ 06/10/23 12:25:20.207
  STEP: cluster-wide watching @ 06/10/23 12:25:20.218
  Jun 10 12:25:20.218: INFO: starting watch
  STEP: patching @ 06/10/23 12:25:20.224
  STEP: updating @ 06/10/23 12:25:20.244
  Jun 10 12:25:20.257: INFO: waiting for watch events with expected annotations
  Jun 10 12:25:20.258: INFO: saw patched and updated annotations
  STEP: deleting @ 06/10/23 12:25:20.258
  STEP: deleting a collection @ 06/10/23 12:25:20.289
  Jun 10 12:25:20.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-9776" for this suite. @ 06/10/23 12:25:20.314
• [0.205 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 06/10/23 12:25:20.322
  Jun 10 12:25:20.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl @ 06/10/23 12:25:20.323
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:25:20.337
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:25:20.342
  Jun 10 12:25:20.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-3656 create -f -'
  Jun 10 12:25:20.672: INFO: stderr: ""
  Jun 10 12:25:20.672: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Jun 10 12:25:20.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-3656 create -f -'
  Jun 10 12:25:21.010: INFO: stderr: ""
  Jun 10 12:25:21.010: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 06/10/23 12:25:21.01
  Jun 10 12:25:22.016: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 10 12:25:22.016: INFO: Found 0 / 1
  Jun 10 12:25:23.015: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 10 12:25:23.015: INFO: Found 1 / 1
  Jun 10 12:25:23.015: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jun 10 12:25:23.020: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 10 12:25:23.020: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jun 10 12:25:23.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-3656 describe pod agnhost-primary-d996l'
  Jun 10 12:25:23.117: INFO: stderr: ""
  Jun 10 12:25:23.117: INFO: stdout: "Name:             agnhost-primary-d996l\nNamespace:        kubectl-3656\nPriority:         0\nService Account:  default\nNode:             ip-172-31-89-0/172.31.89.0\nStart Time:       Sat, 10 Jun 2023 12:25:20 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.149.126\nIPs:\n  IP:           192.168.149.126\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://68d77b86c8ba019e28e8d338d3d81c44d76760dca442e4df26aef02f6e5b1956\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 10 Jun 2023 12:25:21 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bvfp9 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-bvfp9:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-3656/agnhost-primary-d996l to ip-172-31-89-0\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
  Jun 10 12:25:23.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-3656 describe rc agnhost-primary'
  Jun 10 12:25:23.215: INFO: stderr: ""
  Jun 10 12:25:23.215: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3656\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-d996l\n"
  Jun 10 12:25:23.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-3656 describe service agnhost-primary'
  Jun 10 12:25:23.314: INFO: stderr: ""
  Jun 10 12:25:23.314: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3656\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.169\nIPs:               10.152.183.169\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.149.126:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Jun 10 12:25:23.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-3656 describe node ip-172-31-13-130'
  Jun 10 12:25:23.452: INFO: stderr: ""
  Jun 10 12:25:23.452: INFO: stdout: "Name:               ip-172-31-13-130\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-control-plane\n                    juju-charm=kubernetes-control-plane\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-13-130\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 10 Jun 2023 11:52:57 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-13-130\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 10 Jun 2023 12:25:16 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 10 Jun 2023 12:22:56 +0000   Sat, 10 Jun 2023 11:52:57 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 10 Jun 2023 12:22:56 +0000   Sat, 10 Jun 2023 11:52:57 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 10 Jun 2023 12:22:56 +0000   Sat, 10 Jun 2023 11:52:57 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 10 Jun 2023 12:22:56 +0000   Sat, 10 Jun 2023 11:52:58 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.13.130\n  Hostname:    ip-172-31-13-130\nCapacity:\n  cpu:                2\n  ephemeral-storage:  16069568Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16127544Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  14809713845\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16025144Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 ec2edd7856413ae0ebe5e3a597473272\n  System UUID:                ec2edd78-5641-3ae0-ebe5-e3a597473272\n  Boot ID:                    3fc8cae7-3e1e-49c1-82d6-84965831da14\n  Kernel Version:             5.19.0-1026-aws\n  OS Image:                   Ubuntu 22.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.8\n  Kubelet Version:            v1.27.2\n  Kube-Proxy Version:         v1.27.2\nNon-terminated Pods:          (1 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-31eb0473969f40ee-vmxbw    0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests  Limits\n  --------           --------  ------\n  cpu                0 (0%)    0 (0%)\n  memory             0 (0%)    0 (0%)\n  ephemeral-storage  0 (0%)    0 (0%)\n  hugepages-1Gi      0 (0%)    0 (0%)\n  hugepages-2Mi      0 (0%)    0 (0%)\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 32m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      32m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  32m (x2 over 32m)  kubelet          Node ip-172-31-13-130 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    32m (x2 over 32m)  kubelet          Node ip-172-31-13-130 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     32m (x2 over 32m)  kubelet          Node ip-172-31-13-130 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  32m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeReady                32m                kubelet          Node ip-172-31-13-130 status is now: NodeReady\n  Normal   RegisteredNode           32m                node-controller  Node ip-172-31-13-130 event: Registered Node ip-172-31-13-130 in Controller\n"
  Jun 10 12:25:23.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-3656 describe namespace kubectl-3656'
  Jun 10 12:25:23.547: INFO: stderr: ""
  Jun 10 12:25:23.547: INFO: stdout: "Name:         kubectl-3656\nLabels:       e2e-framework=kubectl\n              e2e-run=78244405-d6bd-408b-af19-702c6f7dfe06\n              kubernetes.io/metadata.name=kubectl-3656\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Jun 10 12:25:23.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3656" for this suite. @ 06/10/23 12:25:23.553
• [3.238 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 06/10/23 12:25:23.561
  Jun 10 12:25:23.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename events @ 06/10/23 12:25:23.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:25:23.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:25:23.586
  STEP: creating a test event @ 06/10/23 12:25:23.59
  STEP: listing all events in all namespaces @ 06/10/23 12:25:23.596
  STEP: patching the test event @ 06/10/23 12:25:23.605
  STEP: fetching the test event @ 06/10/23 12:25:23.613
  STEP: updating the test event @ 06/10/23 12:25:23.616
  STEP: getting the test event @ 06/10/23 12:25:23.628
  STEP: deleting the test event @ 06/10/23 12:25:23.632
  STEP: listing all events in all namespaces @ 06/10/23 12:25:23.641
  Jun 10 12:25:23.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7632" for this suite. @ 06/10/23 12:25:23.655
• [0.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 06/10/23 12:25:23.664
  Jun 10 12:25:23.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename replicaset @ 06/10/23 12:25:23.665
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:25:23.683
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:25:23.687
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 06/10/23 12:25:23.691
  STEP: When a replicaset with a matching selector is created @ 06/10/23 12:25:25.718
  STEP: Then the orphan pod is adopted @ 06/10/23 12:25:25.723
  STEP: When the matched label of one of its pods change @ 06/10/23 12:25:26.734
  Jun 10 12:25:26.738: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 06/10/23 12:25:26.752
  Jun 10 12:25:27.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8185" for this suite. @ 06/10/23 12:25:27.766
• [4.109 seconds]
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 06/10/23 12:25:27.773
  Jun 10 12:25:27.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl @ 06/10/23 12:25:27.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:25:27.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:25:27.795
  STEP: starting the proxy server @ 06/10/23 12:25:27.799
  Jun 10 12:25:27.799: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-2047 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 06/10/23 12:25:27.857
  Jun 10 12:25:27.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2047" for this suite. @ 06/10/23 12:25:27.872
• [0.106 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 06/10/23 12:25:27.879
  Jun 10 12:25:27.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename resourcequota @ 06/10/23 12:25:27.88
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:25:27.905
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:25:27.909
  STEP: Counting existing ResourceQuota @ 06/10/23 12:25:27.917
  STEP: Creating a ResourceQuota @ 06/10/23 12:25:32.922
  STEP: Ensuring resource quota status is calculated @ 06/10/23 12:25:32.934
  STEP: Creating a ReplicaSet @ 06/10/23 12:25:34.94
  STEP: Ensuring resource quota status captures replicaset creation @ 06/10/23 12:25:34.957
  STEP: Deleting a ReplicaSet @ 06/10/23 12:25:36.965
  STEP: Ensuring resource quota status released usage @ 06/10/23 12:25:36.976
  Jun 10 12:25:38.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5818" for this suite. @ 06/10/23 12:25:38.986
• [11.116 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 06/10/23 12:25:38.997
  Jun 10 12:25:38.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 12:25:38.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:25:39.022
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:25:39.028
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 12:25:39.032
  STEP: Saw pod success @ 06/10/23 12:25:43.062
  Jun 10 12:25:43.065: INFO: Trying to get logs from node ip-172-31-89-0 pod downwardapi-volume-eb4a5ebb-21b4-46b1-a516-27ecde788c17 container client-container: <nil>
  STEP: delete the pod @ 06/10/23 12:25:43.077
  Jun 10 12:25:43.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9380" for this suite. @ 06/10/23 12:25:43.102
• [4.113 seconds]
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 06/10/23 12:25:43.11
  Jun 10 12:25:43.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename endpointslicemirroring @ 06/10/23 12:25:43.111
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:25:43.131
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:25:43.135
  STEP: mirroring a new custom Endpoint @ 06/10/23 12:25:43.154
  Jun 10 12:25:43.169: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 06/10/23 12:25:45.173
  Jun 10 12:25:45.184: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  STEP: mirroring deletion of a custom Endpoint @ 06/10/23 12:25:47.189
  Jun 10 12:25:47.200: INFO: Waiting for 0 EndpointSlices to exist, got 1
  Jun 10 12:25:49.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-8111" for this suite. @ 06/10/23 12:25:49.21
• [6.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 06/10/23 12:25:49.221
  Jun 10 12:25:49.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename pods @ 06/10/23 12:25:49.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:25:49.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:25:49.243
  STEP: creating the pod @ 06/10/23 12:25:49.247
  STEP: setting up watch @ 06/10/23 12:25:49.248
  STEP: submitting the pod to kubernetes @ 06/10/23 12:25:49.352
  STEP: verifying the pod is in kubernetes @ 06/10/23 12:25:49.362
  STEP: verifying pod creation was observed @ 06/10/23 12:25:49.37
  STEP: deleting the pod gracefully @ 06/10/23 12:25:51.388
  STEP: verifying pod deletion was observed @ 06/10/23 12:25:51.398
  Jun 10 12:25:52.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2207" for this suite. @ 06/10/23 12:25:52.57
• [3.356 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 06/10/23 12:25:52.578
  Jun 10 12:25:52.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename pods @ 06/10/23 12:25:52.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:25:52.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:25:52.601
  STEP: creating the pod @ 06/10/23 12:25:52.608
  STEP: submitting the pod to kubernetes @ 06/10/23 12:25:52.608
  STEP: verifying the pod is in kubernetes @ 06/10/23 12:25:54.64
  STEP: updating the pod @ 06/10/23 12:25:54.644
  Jun 10 12:25:55.156: INFO: Successfully updated pod "pod-update-84aa2ec7-1334-493e-851e-bf1a7802608d"
  STEP: verifying the updated pod is in kubernetes @ 06/10/23 12:25:55.161
  Jun 10 12:25:55.252: INFO: Pod update OK
  Jun 10 12:25:55.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2115" for this suite. @ 06/10/23 12:25:55.257
• [2.686 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 06/10/23 12:25:55.269
  Jun 10 12:25:55.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename events @ 06/10/23 12:25:55.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:25:55.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:25:55.291
  STEP: creating a test event @ 06/10/23 12:25:55.296
  STEP: listing events in all namespaces @ 06/10/23 12:25:55.305
  STEP: listing events in test namespace @ 06/10/23 12:25:55.314
  STEP: listing events with field selection filtering on source @ 06/10/23 12:25:55.323
  STEP: listing events with field selection filtering on reportingController @ 06/10/23 12:25:55.328
  STEP: getting the test event @ 06/10/23 12:25:55.332
  STEP: patching the test event @ 06/10/23 12:25:55.336
  STEP: getting the test event @ 06/10/23 12:25:55.35
  STEP: updating the test event @ 06/10/23 12:25:55.354
  STEP: getting the test event @ 06/10/23 12:25:55.364
  STEP: deleting the test event @ 06/10/23 12:25:55.368
  STEP: listing events in all namespaces @ 06/10/23 12:25:55.376
  STEP: listing events in test namespace @ 06/10/23 12:25:55.385
  Jun 10 12:25:55.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-3717" for this suite. @ 06/10/23 12:25:55.394
• [0.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 06/10/23 12:25:55.407
  Jun 10 12:25:55.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/10/23 12:25:55.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:25:55.432
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:25:55.436
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 06/10/23 12:25:55.44
  Jun 10 12:25:55.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 06/10/23 12:26:01.772
  Jun 10 12:26:01.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:26:03.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:26:09.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2125" for this suite. @ 06/10/23 12:26:09.266
• [13.868 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 06/10/23 12:26:09.275
  Jun 10 12:26:09.275: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename sched-pred @ 06/10/23 12:26:09.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:26:09.293
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:26:09.296
  Jun 10 12:26:09.299: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jun 10 12:26:09.308: INFO: Waiting for terminating namespaces to be deleted...
  Jun 10 12:26:09.312: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-27-177 before test
  Jun 10 12:26:09.319: INFO: default-http-backend-kubernetes-worker-65fc475d49-7vmxm from ingress-nginx-kubernetes-worker started at 2023-06-10 11:54:56 +0000 UTC (1 container statuses recorded)
  Jun 10 12:26:09.319: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jun 10 12:26:09.319: INFO: nginx-ingress-controller-kubernetes-worker-vn24d from ingress-nginx-kubernetes-worker started at 2023-06-10 11:54:55 +0000 UTC (1 container statuses recorded)
  Jun 10 12:26:09.319: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 10 12:26:09.319: INFO: calico-kube-controllers-86c9c69795-8gfrp from kube-system started at 2023-06-10 11:55:04 +0000 UTC (1 container statuses recorded)
  Jun 10 12:26:09.319: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jun 10 12:26:09.319: INFO: sonobuoy-systemd-logs-daemon-set-31eb0473969f40ee-l8bg7 from sonobuoy started at 2023-06-10 12:06:59 +0000 UTC (2 container statuses recorded)
  Jun 10 12:26:09.319: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 10 12:26:09.319: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 10 12:26:09.319: INFO: pod-service-account-defaultsa from svcaccounts-3796 started at 2023-06-10 12:24:34 +0000 UTC (1 container statuses recorded)
  Jun 10 12:26:09.319: INFO: 	Container token-test ready: false, restart count 0
  Jun 10 12:26:09.319: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-3796 started at 2023-06-10 12:24:34 +0000 UTC (1 container statuses recorded)
  Jun 10 12:26:09.319: INFO: 	Container token-test ready: false, restart count 0
  Jun 10 12:26:09.319: INFO: pod-service-account-mountsa from svcaccounts-3796 started at 2023-06-10 12:24:34 +0000 UTC (1 container statuses recorded)
  Jun 10 12:26:09.319: INFO: 	Container token-test ready: false, restart count 0
  Jun 10 12:26:09.319: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-3796 started at 2023-06-10 12:24:34 +0000 UTC (1 container statuses recorded)
  Jun 10 12:26:09.319: INFO: 	Container token-test ready: false, restart count 0
  Jun 10 12:26:09.319: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-46-40 before test
  Jun 10 12:26:09.326: INFO: nginx-ingress-controller-kubernetes-worker-vb28m from ingress-nginx-kubernetes-worker started at 2023-06-10 11:54:55 +0000 UTC (1 container statuses recorded)
  Jun 10 12:26:09.326: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 10 12:26:09.326: INFO: coredns-5c7f76ccb8-xmz95 from kube-system started at 2023-06-10 11:54:47 +0000 UTC (1 container statuses recorded)
  Jun 10 12:26:09.326: INFO: 	Container coredns ready: true, restart count 0
  Jun 10 12:26:09.326: INFO: kube-state-metrics-5b95b4459c-rtj7m from kube-system started at 2023-06-10 11:54:47 +0000 UTC (1 container statuses recorded)
  Jun 10 12:26:09.326: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jun 10 12:26:09.326: INFO: metrics-server-v0.5.2-6cf8c8b69c-ftmcl from kube-system started at 2023-06-10 11:54:47 +0000 UTC (2 container statuses recorded)
  Jun 10 12:26:09.326: INFO: 	Container metrics-server ready: true, restart count 0
  Jun 10 12:26:09.326: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jun 10 12:26:09.326: INFO: dashboard-metrics-scraper-6b8586b5c9-znc9p from kubernetes-dashboard started at 2023-06-10 11:54:47 +0000 UTC (1 container statuses recorded)
  Jun 10 12:26:09.326: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jun 10 12:26:09.326: INFO: kubernetes-dashboard-6869f4cd5f-xdbkn from kubernetes-dashboard started at 2023-06-10 11:54:47 +0000 UTC (1 container statuses recorded)
  Jun 10 12:26:09.326: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jun 10 12:26:09.326: INFO: sonobuoy-systemd-logs-daemon-set-31eb0473969f40ee-wg64x from sonobuoy started at 2023-06-10 12:06:59 +0000 UTC (2 container statuses recorded)
  Jun 10 12:26:09.326: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 10 12:26:09.326: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 10 12:26:09.326: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-89-0 before test
  Jun 10 12:26:09.332: INFO: nginx-ingress-controller-kubernetes-worker-889j8 from ingress-nginx-kubernetes-worker started at 2023-06-10 12:03:37 +0000 UTC (1 container statuses recorded)
  Jun 10 12:26:09.332: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 10 12:26:09.332: INFO: sonobuoy from sonobuoy started at 2023-06-10 12:06:57 +0000 UTC (1 container statuses recorded)
  Jun 10 12:26:09.332: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jun 10 12:26:09.332: INFO: sonobuoy-e2e-job-f362cde51af14fae from sonobuoy started at 2023-06-10 12:06:59 +0000 UTC (2 container statuses recorded)
  Jun 10 12:26:09.332: INFO: 	Container e2e ready: true, restart count 0
  Jun 10 12:26:09.332: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 10 12:26:09.332: INFO: sonobuoy-systemd-logs-daemon-set-31eb0473969f40ee-qjm5m from sonobuoy started at 2023-06-10 12:06:59 +0000 UTC (2 container statuses recorded)
  Jun 10 12:26:09.332: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 10 12:26:09.332: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 10 12:26:09.332: INFO: pod-service-account-mountsa-mountspec from svcaccounts-3796 started at 2023-06-10 12:24:34 +0000 UTC (1 container statuses recorded)
  Jun 10 12:26:09.332: INFO: 	Container token-test ready: false, restart count 0
  STEP: verifying the node has the label node ip-172-31-27-177 @ 06/10/23 12:26:09.35
  STEP: verifying the node has the label node ip-172-31-46-40 @ 06/10/23 12:26:09.366
  STEP: verifying the node has the label node ip-172-31-89-0 @ 06/10/23 12:26:09.38
  Jun 10 12:26:09.398: INFO: Pod default-http-backend-kubernetes-worker-65fc475d49-7vmxm requesting resource cpu=10m on Node ip-172-31-27-177
  Jun 10 12:26:09.398: INFO: Pod nginx-ingress-controller-kubernetes-worker-889j8 requesting resource cpu=0m on Node ip-172-31-89-0
  Jun 10 12:26:09.398: INFO: Pod nginx-ingress-controller-kubernetes-worker-vb28m requesting resource cpu=0m on Node ip-172-31-46-40
  Jun 10 12:26:09.398: INFO: Pod nginx-ingress-controller-kubernetes-worker-vn24d requesting resource cpu=0m on Node ip-172-31-27-177
  Jun 10 12:26:09.398: INFO: Pod calico-kube-controllers-86c9c69795-8gfrp requesting resource cpu=0m on Node ip-172-31-27-177
  Jun 10 12:26:09.398: INFO: Pod coredns-5c7f76ccb8-xmz95 requesting resource cpu=100m on Node ip-172-31-46-40
  Jun 10 12:26:09.399: INFO: Pod kube-state-metrics-5b95b4459c-rtj7m requesting resource cpu=0m on Node ip-172-31-46-40
  Jun 10 12:26:09.399: INFO: Pod metrics-server-v0.5.2-6cf8c8b69c-ftmcl requesting resource cpu=5m on Node ip-172-31-46-40
  Jun 10 12:26:09.399: INFO: Pod dashboard-metrics-scraper-6b8586b5c9-znc9p requesting resource cpu=0m on Node ip-172-31-46-40
  Jun 10 12:26:09.399: INFO: Pod kubernetes-dashboard-6869f4cd5f-xdbkn requesting resource cpu=0m on Node ip-172-31-46-40
  Jun 10 12:26:09.399: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-89-0
  Jun 10 12:26:09.399: INFO: Pod sonobuoy-e2e-job-f362cde51af14fae requesting resource cpu=0m on Node ip-172-31-89-0
  Jun 10 12:26:09.399: INFO: Pod sonobuoy-systemd-logs-daemon-set-31eb0473969f40ee-l8bg7 requesting resource cpu=0m on Node ip-172-31-27-177
  Jun 10 12:26:09.399: INFO: Pod sonobuoy-systemd-logs-daemon-set-31eb0473969f40ee-qjm5m requesting resource cpu=0m on Node ip-172-31-89-0
  Jun 10 12:26:09.399: INFO: Pod sonobuoy-systemd-logs-daemon-set-31eb0473969f40ee-wg64x requesting resource cpu=0m on Node ip-172-31-46-40
  Jun 10 12:26:09.399: INFO: Pod pod-service-account-defaultsa requesting resource cpu=0m on Node ip-172-31-27-177
  Jun 10 12:26:09.400: INFO: Pod pod-service-account-defaultsa-mountspec requesting resource cpu=0m on Node ip-172-31-27-177
  Jun 10 12:26:09.400: INFO: Pod pod-service-account-mountsa requesting resource cpu=0m on Node ip-172-31-27-177
  Jun 10 12:26:09.400: INFO: Pod pod-service-account-mountsa-mountspec requesting resource cpu=0m on Node ip-172-31-89-0
  Jun 10 12:26:09.400: INFO: Pod pod-service-account-nomountsa-mountspec requesting resource cpu=0m on Node ip-172-31-27-177
  STEP: Starting Pods to consume most of the cluster CPU. @ 06/10/23 12:26:09.4
  Jun 10 12:26:09.400: INFO: Creating a pod which consumes cpu=1393m on Node ip-172-31-27-177
  Jun 10 12:26:09.409: INFO: Creating a pod which consumes cpu=1326m on Node ip-172-31-46-40
  Jun 10 12:26:09.417: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-89-0
  STEP: Creating another pod that requires unavailable amount of CPU. @ 06/10/23 12:26:11.446
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-704ad70e-aef4-4671-b94e-c659d1cbff2a.17674bd65742c425], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1575/filler-pod-704ad70e-aef4-4671-b94e-c659d1cbff2a to ip-172-31-46-40] @ 06/10/23 12:26:11.451
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-704ad70e-aef4-4671-b94e-c659d1cbff2a.17674bd684046593], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 06/10/23 12:26:11.451
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-704ad70e-aef4-4671-b94e-c659d1cbff2a.17674bd6854d3245], Reason = [Created], Message = [Created container filler-pod-704ad70e-aef4-4671-b94e-c659d1cbff2a] @ 06/10/23 12:26:11.451
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-704ad70e-aef4-4671-b94e-c659d1cbff2a.17674bd689ff6c73], Reason = [Started], Message = [Started container filler-pod-704ad70e-aef4-4671-b94e-c659d1cbff2a] @ 06/10/23 12:26:11.451
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-75452ed3-58e9-4c92-ac42-f1b4e97a9acb.17674bd65784930c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1575/filler-pod-75452ed3-58e9-4c92-ac42-f1b4e97a9acb to ip-172-31-89-0] @ 06/10/23 12:26:11.451
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-75452ed3-58e9-4c92-ac42-f1b4e97a9acb.17674bd681206e9c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 06/10/23 12:26:11.451
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-75452ed3-58e9-4c92-ac42-f1b4e97a9acb.17674bd6827435e0], Reason = [Created], Message = [Created container filler-pod-75452ed3-58e9-4c92-ac42-f1b4e97a9acb] @ 06/10/23 12:26:11.451
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-75452ed3-58e9-4c92-ac42-f1b4e97a9acb.17674bd686e3331f], Reason = [Started], Message = [Started container filler-pod-75452ed3-58e9-4c92-ac42-f1b4e97a9acb] @ 06/10/23 12:26:11.451
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d1acfe0b-d393-4f29-bd14-d52baec9178e.17674bd65674707f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1575/filler-pod-d1acfe0b-d393-4f29-bd14-d52baec9178e to ip-172-31-27-177] @ 06/10/23 12:26:11.451
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d1acfe0b-d393-4f29-bd14-d52baec9178e.17674bd681231c6e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 06/10/23 12:26:11.451
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d1acfe0b-d393-4f29-bd14-d52baec9178e.17674bd68235b70a], Reason = [Created], Message = [Created container filler-pod-d1acfe0b-d393-4f29-bd14-d52baec9178e] @ 06/10/23 12:26:11.451
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d1acfe0b-d393-4f29-bd14-d52baec9178e.17674bd6870bd8f8], Reason = [Started], Message = [Started container filler-pod-d1acfe0b-d393-4f29-bd14-d52baec9178e] @ 06/10/23 12:26:11.451
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.17674bd6d01bb169], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/5 nodes are available: 2 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] @ 06/10/23 12:26:11.466
  STEP: removing the label node off the node ip-172-31-27-177 @ 06/10/23 12:26:12.466
  STEP: verifying the node doesn't have the label node @ 06/10/23 12:26:12.479
  STEP: removing the label node off the node ip-172-31-46-40 @ 06/10/23 12:26:12.483
  STEP: verifying the node doesn't have the label node @ 06/10/23 12:26:12.497
  STEP: removing the label node off the node ip-172-31-89-0 @ 06/10/23 12:26:12.501
  STEP: verifying the node doesn't have the label node @ 06/10/23 12:26:12.522
  Jun 10 12:26:12.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1575" for this suite. @ 06/10/23 12:26:12.535
• [3.267 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 06/10/23 12:26:12.544
  Jun 10 12:26:12.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename watch @ 06/10/23 12:26:12.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:26:12.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:26:12.568
  STEP: creating a new configmap @ 06/10/23 12:26:12.572
  STEP: modifying the configmap once @ 06/10/23 12:26:12.577
  STEP: modifying the configmap a second time @ 06/10/23 12:26:12.586
  STEP: deleting the configmap @ 06/10/23 12:26:12.596
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 06/10/23 12:26:12.603
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 06/10/23 12:26:12.605
  Jun 10 12:26:12.605: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8750  834382f5-33ec-4e84-aa36-f0b13767f3dd 11407 0 2023-06-10 12:26:12 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-10 12:26:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 12:26:12.605: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8750  834382f5-33ec-4e84-aa36-f0b13767f3dd 11408 0 2023-06-10 12:26:12 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-10 12:26:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 12:26:12.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8750" for this suite. @ 06/10/23 12:26:12.61
• [0.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 06/10/23 12:26:12.621
  Jun 10 12:26:12.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/10/23 12:26:12.623
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:26:12.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:26:12.648
  Jun 10 12:26:12.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 06/10/23 12:26:14.145
  Jun 10 12:26:14.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-9748 --namespace=crd-publish-openapi-9748 create -f -'
  Jun 10 12:26:14.916: INFO: stderr: ""
  Jun 10 12:26:14.916: INFO: stdout: "e2e-test-crd-publish-openapi-9377-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jun 10 12:26:14.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-9748 --namespace=crd-publish-openapi-9748 delete e2e-test-crd-publish-openapi-9377-crds test-foo'
  Jun 10 12:26:15.044: INFO: stderr: ""
  Jun 10 12:26:15.044: INFO: stdout: "e2e-test-crd-publish-openapi-9377-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Jun 10 12:26:15.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-9748 --namespace=crd-publish-openapi-9748 apply -f -'
  Jun 10 12:26:15.701: INFO: stderr: ""
  Jun 10 12:26:15.701: INFO: stdout: "e2e-test-crd-publish-openapi-9377-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jun 10 12:26:15.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-9748 --namespace=crd-publish-openapi-9748 delete e2e-test-crd-publish-openapi-9377-crds test-foo'
  Jun 10 12:26:15.790: INFO: stderr: ""
  Jun 10 12:26:15.790: INFO: stdout: "e2e-test-crd-publish-openapi-9377-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 06/10/23 12:26:15.79
  Jun 10 12:26:15.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-9748 --namespace=crd-publish-openapi-9748 create -f -'
  Jun 10 12:26:16.023: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 06/10/23 12:26:16.023
  Jun 10 12:26:16.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-9748 --namespace=crd-publish-openapi-9748 create -f -'
  Jun 10 12:26:16.272: INFO: rc: 1
  Jun 10 12:26:16.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-9748 --namespace=crd-publish-openapi-9748 apply -f -'
  Jun 10 12:26:16.517: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 06/10/23 12:26:16.517
  Jun 10 12:26:16.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-9748 --namespace=crd-publish-openapi-9748 create -f -'
  Jun 10 12:26:16.743: INFO: rc: 1
  Jun 10 12:26:16.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-9748 --namespace=crd-publish-openapi-9748 apply -f -'
  Jun 10 12:26:17.005: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 06/10/23 12:26:17.005
  Jun 10 12:26:17.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-9748 explain e2e-test-crd-publish-openapi-9377-crds'
  Jun 10 12:26:17.239: INFO: stderr: ""
  Jun 10 12:26:17.239: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9377-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 06/10/23 12:26:17.239
  Jun 10 12:26:17.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-9748 explain e2e-test-crd-publish-openapi-9377-crds.metadata'
  Jun 10 12:26:17.478: INFO: stderr: ""
  Jun 10 12:26:17.479: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9377-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Jun 10 12:26:17.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-9748 explain e2e-test-crd-publish-openapi-9377-crds.spec'
  Jun 10 12:26:17.719: INFO: stderr: ""
  Jun 10 12:26:17.719: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9377-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Jun 10 12:26:17.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-9748 explain e2e-test-crd-publish-openapi-9377-crds.spec.bars'
  Jun 10 12:26:18.039: INFO: stderr: ""
  Jun 10 12:26:18.039: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9377-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 06/10/23 12:26:18.039
  Jun 10 12:26:18.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-9748 explain e2e-test-crd-publish-openapi-9377-crds.spec.bars2'
  Jun 10 12:26:18.472: INFO: rc: 1
  Jun 10 12:26:19.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9748" for this suite. @ 06/10/23 12:26:19.938
• [7.323 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 06/10/23 12:26:19.946
  Jun 10 12:26:19.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename configmap @ 06/10/23 12:26:19.947
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:26:19.969
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:26:19.973
  STEP: Creating configMap configmap-5574/configmap-test-e9aab672-c510-4d1e-a52d-288e9625cb55 @ 06/10/23 12:26:19.977
  STEP: Creating a pod to test consume configMaps @ 06/10/23 12:26:19.984
  STEP: Saw pod success @ 06/10/23 12:26:24.012
  Jun 10 12:26:24.017: INFO: Trying to get logs from node ip-172-31-89-0 pod pod-configmaps-3216afdb-838b-4925-99cf-6f6fa3941c60 container env-test: <nil>
  STEP: delete the pod @ 06/10/23 12:26:24.024
  Jun 10 12:26:24.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5574" for this suite. @ 06/10/23 12:26:24.048
• [4.108 seconds]
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 06/10/23 12:26:24.055
  Jun 10 12:26:24.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename job @ 06/10/23 12:26:24.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:26:24.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:26:24.079
  STEP: Creating Indexed job @ 06/10/23 12:26:24.083
  STEP: Ensuring job reaches completions @ 06/10/23 12:26:24.091
  STEP: Ensuring pods with index for job exist @ 06/10/23 12:26:34.096
  Jun 10 12:26:34.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3581" for this suite. @ 06/10/23 12:26:34.104
• [10.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 06/10/23 12:26:34.112
  Jun 10 12:26:34.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 12:26:34.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:26:34.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:26:34.137
  STEP: Setting up server cert @ 06/10/23 12:26:34.165
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 12:26:35.071
  STEP: Deploying the webhook pod @ 06/10/23 12:26:35.082
  STEP: Wait for the deployment to be ready @ 06/10/23 12:26:35.097
  Jun 10 12:26:35.109: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/10/23 12:26:37.122
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 12:26:37.135
  Jun 10 12:26:38.135: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 06/10/23 12:26:38.141
  Jun 10 12:26:38.161: INFO: Waiting for webhook configuration to be ready...
  STEP: create a namespace for the webhook @ 06/10/23 12:26:38.272
  STEP: create a configmap should be unconditionally rejected by the webhook @ 06/10/23 12:26:38.287
  Jun 10 12:26:38.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4368" for this suite. @ 06/10/23 12:26:38.447
  STEP: Destroying namespace "webhook-markers-6630" for this suite. @ 06/10/23 12:26:38.459
  STEP: Destroying namespace "fail-closed-namespace-9774" for this suite. @ 06/10/23 12:26:38.48
• [4.379 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 06/10/23 12:26:38.493
  Jun 10 12:26:38.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename secrets @ 06/10/23 12:26:38.494
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:26:38.512
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:26:38.515
  STEP: Creating secret with name secret-test-3f19cff2-5230-4b55-9547-9e1a1e98d01d @ 06/10/23 12:26:38.52
  STEP: Creating a pod to test consume secrets @ 06/10/23 12:26:38.526
  STEP: Saw pod success @ 06/10/23 12:26:42.55
  Jun 10 12:26:42.554: INFO: Trying to get logs from node ip-172-31-89-0 pod pod-secrets-e1afa673-7c70-4f78-aa9b-5332e1966fc3 container secret-env-test: <nil>
  STEP: delete the pod @ 06/10/23 12:26:42.563
  Jun 10 12:26:42.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9868" for this suite. @ 06/10/23 12:26:42.586
• [4.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 06/10/23 12:26:42.597
  Jun 10 12:26:42.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename podtemplate @ 06/10/23 12:26:42.599
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:26:42.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:26:42.64
  Jun 10 12:26:42.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-2288" for this suite. @ 06/10/23 12:26:42.691
• [0.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 06/10/23 12:26:42.701
  Jun 10 12:26:42.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename taint-single-pod @ 06/10/23 12:26:42.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:26:42.719
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:26:42.724
  Jun 10 12:26:42.729: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun 10 12:27:42.758: INFO: Waiting for terminating namespaces to be deleted...
  Jun 10 12:27:42.764: INFO: Starting informer...
  STEP: Starting pod... @ 06/10/23 12:27:42.764
  Jun 10 12:27:42.983: INFO: Pod is running on ip-172-31-27-177. Tainting Node
  STEP: Trying to apply a taint on the Node @ 06/10/23 12:27:42.983
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 06/10/23 12:27:42.999
  STEP: Waiting short time to make sure Pod is queued for deletion @ 06/10/23 12:27:43.007
  Jun 10 12:27:43.007: INFO: Pod wasn't evicted. Proceeding
  Jun 10 12:27:43.008: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 06/10/23 12:27:43.042
  STEP: Waiting some time to make sure that toleration time passed. @ 06/10/23 12:27:43.06
  Jun 10 12:28:58.061: INFO: Pod wasn't evicted. Test successful
  Jun 10 12:28:58.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-8854" for this suite. @ 06/10/23 12:28:58.067
• [135.374 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 06/10/23 12:28:58.079
  Jun 10 12:28:58.079: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl @ 06/10/23 12:28:58.08
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:28:58.096
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:28:58.101
  STEP: creating all guestbook components @ 06/10/23 12:28:58.105
  Jun 10 12:28:58.106: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Jun 10 12:28:58.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4599 create -f -'
  Jun 10 12:28:58.858: INFO: stderr: ""
  Jun 10 12:28:58.859: INFO: stdout: "service/agnhost-replica created\n"
  Jun 10 12:28:58.859: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Jun 10 12:28:58.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4599 create -f -'
  Jun 10 12:28:59.711: INFO: stderr: ""
  Jun 10 12:28:59.711: INFO: stdout: "service/agnhost-primary created\n"
  Jun 10 12:28:59.711: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Jun 10 12:28:59.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4599 create -f -'
  Jun 10 12:29:00.058: INFO: stderr: ""
  Jun 10 12:29:00.058: INFO: stdout: "service/frontend created\n"
  Jun 10 12:29:00.058: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Jun 10 12:29:00.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4599 create -f -'
  Jun 10 12:29:00.327: INFO: stderr: ""
  Jun 10 12:29:00.327: INFO: stdout: "deployment.apps/frontend created\n"
  Jun 10 12:29:00.327: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jun 10 12:29:00.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4599 create -f -'
  Jun 10 12:29:00.598: INFO: stderr: ""
  Jun 10 12:29:00.598: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Jun 10 12:29:00.598: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jun 10 12:29:00.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4599 create -f -'
  Jun 10 12:29:00.947: INFO: stderr: ""
  Jun 10 12:29:00.947: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 06/10/23 12:29:00.947
  Jun 10 12:29:00.947: INFO: Waiting for all frontend pods to be Running.
  Jun 10 12:29:06.001: INFO: Waiting for frontend to serve content.
  Jun 10 12:29:06.012: INFO: Trying to add a new entry to the guestbook.
  Jun 10 12:29:06.026: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 06/10/23 12:29:06.039
  Jun 10 12:29:06.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4599 delete --grace-period=0 --force -f -'
  Jun 10 12:29:06.137: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 10 12:29:06.137: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 06/10/23 12:29:06.137
  Jun 10 12:29:06.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4599 delete --grace-period=0 --force -f -'
  Jun 10 12:29:06.238: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 10 12:29:06.238: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 06/10/23 12:29:06.238
  Jun 10 12:29:06.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4599 delete --grace-period=0 --force -f -'
  Jun 10 12:29:06.352: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 10 12:29:06.352: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 06/10/23 12:29:06.352
  Jun 10 12:29:06.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4599 delete --grace-period=0 --force -f -'
  Jun 10 12:29:06.434: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 10 12:29:06.434: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 06/10/23 12:29:06.434
  Jun 10 12:29:06.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4599 delete --grace-period=0 --force -f -'
  Jun 10 12:29:06.543: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 10 12:29:06.543: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 06/10/23 12:29:06.543
  Jun 10 12:29:06.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4599 delete --grace-period=0 --force -f -'
  Jun 10 12:29:06.657: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 10 12:29:06.657: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Jun 10 12:29:06.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4599" for this suite. @ 06/10/23 12:29:06.661
• [8.599 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 06/10/23 12:29:06.678
  Jun 10 12:29:06.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename certificates @ 06/10/23 12:29:06.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:29:06.702
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:29:06.706
  STEP: getting /apis @ 06/10/23 12:29:07.547
  STEP: getting /apis/certificates.k8s.io @ 06/10/23 12:29:07.552
  STEP: getting /apis/certificates.k8s.io/v1 @ 06/10/23 12:29:07.554
  STEP: creating @ 06/10/23 12:29:07.555
  STEP: getting @ 06/10/23 12:29:07.576
  STEP: listing @ 06/10/23 12:29:07.58
  STEP: watching @ 06/10/23 12:29:07.584
  Jun 10 12:29:07.584: INFO: starting watch
  STEP: patching @ 06/10/23 12:29:07.586
  STEP: updating @ 06/10/23 12:29:07.593
  Jun 10 12:29:07.602: INFO: waiting for watch events with expected annotations
  Jun 10 12:29:07.602: INFO: saw patched and updated annotations
  STEP: getting /approval @ 06/10/23 12:29:07.602
  STEP: patching /approval @ 06/10/23 12:29:07.607
  STEP: updating /approval @ 06/10/23 12:29:07.615
  STEP: getting /status @ 06/10/23 12:29:07.626
  STEP: patching /status @ 06/10/23 12:29:07.63
  STEP: updating /status @ 06/10/23 12:29:07.641
  STEP: deleting @ 06/10/23 12:29:07.652
  STEP: deleting a collection @ 06/10/23 12:29:07.666
  Jun 10 12:29:07.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-2950" for this suite. @ 06/10/23 12:29:07.691
• [1.022 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 06/10/23 12:29:07.701
  Jun 10 12:29:07.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename conformance-tests @ 06/10/23 12:29:07.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:29:07.717
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:29:07.73
  STEP: Getting node addresses @ 06/10/23 12:29:07.734
  Jun 10 12:29:07.734: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Jun 10 12:29:07.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-5955" for this suite. @ 06/10/23 12:29:07.747
• [0.056 seconds]
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 06/10/23 12:29:07.757
  Jun 10 12:29:07.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename replicaset @ 06/10/23 12:29:07.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:29:07.775
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:29:07.779
  STEP: Create a Replicaset @ 06/10/23 12:29:07.787
  STEP: Verify that the required pods have come up. @ 06/10/23 12:29:07.793
  Jun 10 12:29:07.798: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jun 10 12:29:12.804: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/10/23 12:29:12.804
  STEP: Getting /status @ 06/10/23 12:29:12.804
  Jun 10 12:29:12.811: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 06/10/23 12:29:12.811
  Jun 10 12:29:12.823: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 06/10/23 12:29:12.823
  Jun 10 12:29:12.827: INFO: Observed &ReplicaSet event: ADDED
  Jun 10 12:29:12.827: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 10 12:29:12.828: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 10 12:29:12.828: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 10 12:29:12.828: INFO: Found replicaset test-rs in namespace replicaset-2781 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jun 10 12:29:12.828: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 06/10/23 12:29:12.828
  Jun 10 12:29:12.828: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jun 10 12:29:12.838: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 06/10/23 12:29:12.838
  Jun 10 12:29:12.841: INFO: Observed &ReplicaSet event: ADDED
  Jun 10 12:29:12.841: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 10 12:29:12.841: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 10 12:29:12.842: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 10 12:29:12.842: INFO: Observed replicaset test-rs in namespace replicaset-2781 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 10 12:29:12.842: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 10 12:29:12.842: INFO: Found replicaset test-rs in namespace replicaset-2781 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Jun 10 12:29:12.842: INFO: Replicaset test-rs has a patched status
  Jun 10 12:29:12.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2781" for this suite. @ 06/10/23 12:29:12.849
• [5.102 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 06/10/23 12:29:12.862
  Jun 10 12:29:12.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 12:29:12.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:29:12.889
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:29:12.893
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-d41a344d-6a75-4cdb-a9b7-426619aeeafb @ 06/10/23 12:29:12.904
  STEP: Creating the pod @ 06/10/23 12:29:12.912
  STEP: Updating configmap projected-configmap-test-upd-d41a344d-6a75-4cdb-a9b7-426619aeeafb @ 06/10/23 12:29:14.967
  STEP: waiting to observe update in volume @ 06/10/23 12:29:14.973
  Jun 10 12:30:21.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5290" for this suite. @ 06/10/23 12:30:21.322
• [68.468 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 06/10/23 12:30:21.332
  Jun 10 12:30:21.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename var-expansion @ 06/10/23 12:30:21.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:30:21.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:30:21.357
  STEP: Creating a pod to test substitution in container's args @ 06/10/23 12:30:21.361
  STEP: Saw pod success @ 06/10/23 12:30:25.384
  Jun 10 12:30:25.388: INFO: Trying to get logs from node ip-172-31-27-177 pod var-expansion-376ec104-6947-4690-ad19-b1096ed6c370 container dapi-container: <nil>
  STEP: delete the pod @ 06/10/23 12:30:25.396
  Jun 10 12:30:25.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2990" for this suite. @ 06/10/23 12:30:25.419
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 06/10/23 12:30:25.431
  Jun 10 12:30:25.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 12:30:25.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:30:25.451
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:30:25.456
  STEP: Setting up server cert @ 06/10/23 12:30:25.483
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 12:30:26.086
  STEP: Deploying the webhook pod @ 06/10/23 12:30:26.095
  STEP: Wait for the deployment to be ready @ 06/10/23 12:30:26.109
  Jun 10 12:30:26.119: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/10/23 12:30:28.132
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 12:30:28.148
  Jun 10 12:30:29.149: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 06/10/23 12:30:29.153
  STEP: create a pod that should be updated by the webhook @ 06/10/23 12:30:29.173
  Jun 10 12:30:29.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6792" for this suite. @ 06/10/23 12:30:29.266
  STEP: Destroying namespace "webhook-markers-1379" for this suite. @ 06/10/23 12:30:29.276
• [3.852 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 06/10/23 12:30:29.283
  Jun 10 12:30:29.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename downward-api @ 06/10/23 12:30:29.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:30:29.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:30:29.306
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 12:30:29.313
  STEP: Saw pod success @ 06/10/23 12:30:33.337
  Jun 10 12:30:33.341: INFO: Trying to get logs from node ip-172-31-27-177 pod downwardapi-volume-999b2f3e-2aba-49dd-8c95-baa9ba24a0af container client-container: <nil>
  STEP: delete the pod @ 06/10/23 12:30:33.35
  Jun 10 12:30:33.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9155" for this suite. @ 06/10/23 12:30:33.376
• [4.100 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 06/10/23 12:30:33.384
  Jun 10 12:30:33.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename gc @ 06/10/23 12:30:33.385
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:30:33.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:30:33.415
  STEP: create the rc @ 06/10/23 12:30:33.419
  W0610 12:30:33.426338      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 06/10/23 12:30:38.432
  STEP: wait for all pods to be garbage collected @ 06/10/23 12:30:38.44
  STEP: Gathering metrics @ 06/10/23 12:30:43.449
  W0610 12:30:43.454518      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 10 12:30:43.454: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 10 12:30:43.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5940" for this suite. @ 06/10/23 12:30:43.458
• [10.082 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 06/10/23 12:30:43.467
  Jun 10 12:30:43.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir @ 06/10/23 12:30:43.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:30:43.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:30:43.491
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 06/10/23 12:30:43.496
  STEP: Saw pod success @ 06/10/23 12:30:47.521
  Jun 10 12:30:47.524: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-d61ceda7-559f-4c48-8eb7-88f2c88172fd container test-container: <nil>
  STEP: delete the pod @ 06/10/23 12:30:47.531
  Jun 10 12:30:47.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6121" for this suite. @ 06/10/23 12:30:47.554
• [4.095 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 06/10/23 12:30:47.563
  Jun 10 12:30:47.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename subpath @ 06/10/23 12:30:47.564
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:30:47.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:30:47.589
  STEP: Setting up data @ 06/10/23 12:30:47.594
  STEP: Creating pod pod-subpath-test-configmap-8hl9 @ 06/10/23 12:30:47.605
  STEP: Creating a pod to test atomic-volume-subpath @ 06/10/23 12:30:47.605
  STEP: Saw pod success @ 06/10/23 12:31:11.684
  Jun 10 12:31:11.688: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-subpath-test-configmap-8hl9 container test-container-subpath-configmap-8hl9: <nil>
  STEP: delete the pod @ 06/10/23 12:31:11.697
  STEP: Deleting pod pod-subpath-test-configmap-8hl9 @ 06/10/23 12:31:11.715
  Jun 10 12:31:11.716: INFO: Deleting pod "pod-subpath-test-configmap-8hl9" in namespace "subpath-6176"
  Jun 10 12:31:11.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6176" for this suite. @ 06/10/23 12:31:11.724
• [24.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 06/10/23 12:31:11.732
  Jun 10 12:31:11.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename configmap @ 06/10/23 12:31:11.733
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:31:11.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:31:11.755
  STEP: Creating configMap with name configmap-test-volume-5fb343ab-ce1f-4648-8132-e28ba6ff15e2 @ 06/10/23 12:31:11.76
  STEP: Creating a pod to test consume configMaps @ 06/10/23 12:31:11.766
  STEP: Saw pod success @ 06/10/23 12:31:15.798
  Jun 10 12:31:15.802: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-configmaps-ce44a214-cdf5-4b3c-af55-2f151e5a4eb3 container configmap-volume-test: <nil>
  STEP: delete the pod @ 06/10/23 12:31:15.81
  Jun 10 12:31:15.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4040" for this suite. @ 06/10/23 12:31:15.835
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 06/10/23 12:31:15.845
  Jun 10 12:31:15.845: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir @ 06/10/23 12:31:15.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:31:15.865
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:31:15.869
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 06/10/23 12:31:15.878
  STEP: Saw pod success @ 06/10/23 12:31:19.906
  Jun 10 12:31:19.909: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-c3e39f13-10cf-4487-bd10-3b35d47e62c7 container test-container: <nil>
  STEP: delete the pod @ 06/10/23 12:31:19.918
  Jun 10 12:31:19.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9583" for this suite. @ 06/10/23 12:31:19.94
• [4.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 06/10/23 12:31:19.955
  Jun 10 12:31:19.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl @ 06/10/23 12:31:19.956
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:31:19.972
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:31:19.978
  STEP: create deployment with httpd image @ 06/10/23 12:31:19.982
  Jun 10 12:31:19.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-2323 create -f -'
  Jun 10 12:31:20.270: INFO: stderr: ""
  Jun 10 12:31:20.270: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 06/10/23 12:31:20.27
  Jun 10 12:31:20.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-2323 diff -f -'
  Jun 10 12:31:20.537: INFO: rc: 1
  Jun 10 12:31:20.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-2323 delete -f -'
  Jun 10 12:31:20.621: INFO: stderr: ""
  Jun 10 12:31:20.621: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Jun 10 12:31:20.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2323" for this suite. @ 06/10/23 12:31:20.626
• [0.680 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 06/10/23 12:31:20.636
  Jun 10 12:31:20.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename watch @ 06/10/23 12:31:20.636
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:31:20.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:31:20.663
  STEP: creating a watch on configmaps @ 06/10/23 12:31:20.668
  STEP: creating a new configmap @ 06/10/23 12:31:20.67
  STEP: modifying the configmap once @ 06/10/23 12:31:20.676
  STEP: closing the watch once it receives two notifications @ 06/10/23 12:31:20.686
  Jun 10 12:31:20.687: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4268  ee17df97-7497-4b97-996f-7a6d40edba34 13258 0 2023-06-10 12:31:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-10 12:31:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 12:31:20.687: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4268  ee17df97-7497-4b97-996f-7a6d40edba34 13259 0 2023-06-10 12:31:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-10 12:31:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 06/10/23 12:31:20.687
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 06/10/23 12:31:20.698
  STEP: deleting the configmap @ 06/10/23 12:31:20.701
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 06/10/23 12:31:20.707
  Jun 10 12:31:20.707: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4268  ee17df97-7497-4b97-996f-7a6d40edba34 13260 0 2023-06-10 12:31:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-10 12:31:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 12:31:20.708: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4268  ee17df97-7497-4b97-996f-7a6d40edba34 13261 0 2023-06-10 12:31:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-10 12:31:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 12:31:20.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4268" for this suite. @ 06/10/23 12:31:20.712
• [0.087 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 06/10/23 12:31:20.723
  Jun 10 12:31:20.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 12:31:20.724
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:31:20.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:31:20.744
  STEP: Setting up server cert @ 06/10/23 12:31:20.778
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 12:31:21.125
  STEP: Deploying the webhook pod @ 06/10/23 12:31:21.135
  STEP: Wait for the deployment to be ready @ 06/10/23 12:31:21.149
  Jun 10 12:31:21.162: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/10/23 12:31:23.175
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 12:31:23.187
  Jun 10 12:31:24.187: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 06/10/23 12:31:24.267
  STEP: Creating a configMap that should be mutated @ 06/10/23 12:31:24.282
  STEP: Deleting the collection of validation webhooks @ 06/10/23 12:31:24.321
  STEP: Creating a configMap that should not be mutated @ 06/10/23 12:31:24.378
  Jun 10 12:31:24.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3475" for this suite. @ 06/10/23 12:31:24.444
  STEP: Destroying namespace "webhook-markers-4956" for this suite. @ 06/10/23 12:31:24.455
• [3.742 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 06/10/23 12:31:24.467
  Jun 10 12:31:24.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename var-expansion @ 06/10/23 12:31:24.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:31:24.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:31:24.489
  STEP: Creating a pod to test substitution in container's command @ 06/10/23 12:31:24.494
  STEP: Saw pod success @ 06/10/23 12:31:28.52
  Jun 10 12:31:28.525: INFO: Trying to get logs from node ip-172-31-27-177 pod var-expansion-522937c7-3c32-4998-9d57-09ed551cdedb container dapi-container: <nil>
  STEP: delete the pod @ 06/10/23 12:31:28.532
  Jun 10 12:31:28.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7359" for this suite. @ 06/10/23 12:31:28.554
• [4.093 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 06/10/23 12:31:28.562
  Jun 10 12:31:28.562: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename replication-controller @ 06/10/23 12:31:28.563
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:31:28.578
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:31:28.585
  STEP: Creating ReplicationController "e2e-rc-fnftw" @ 06/10/23 12:31:28.59
  Jun 10 12:31:28.596: INFO: Get Replication Controller "e2e-rc-fnftw" to confirm replicas
  Jun 10 12:31:29.601: INFO: Get Replication Controller "e2e-rc-fnftw" to confirm replicas
  Jun 10 12:31:29.611: INFO: Found 1 replicas for "e2e-rc-fnftw" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-fnftw" @ 06/10/23 12:31:29.611
  STEP: Updating a scale subresource @ 06/10/23 12:31:29.622
  STEP: Verifying replicas where modified for replication controller "e2e-rc-fnftw" @ 06/10/23 12:31:29.641
  Jun 10 12:31:29.641: INFO: Get Replication Controller "e2e-rc-fnftw" to confirm replicas
  Jun 10 12:31:30.657: INFO: Get Replication Controller "e2e-rc-fnftw" to confirm replicas
  Jun 10 12:31:30.662: INFO: Found 2 replicas for "e2e-rc-fnftw" replication controller
  Jun 10 12:31:30.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3039" for this suite. @ 06/10/23 12:31:30.667
• [2.113 seconds]
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 06/10/23 12:31:30.675
  Jun 10 12:31:30.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename downward-api @ 06/10/23 12:31:30.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:31:30.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:31:30.701
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 12:31:30.706
  STEP: Saw pod success @ 06/10/23 12:31:34.735
  Jun 10 12:31:34.739: INFO: Trying to get logs from node ip-172-31-27-177 pod downwardapi-volume-e699b8c1-de7c-410c-b7e2-cd62f59ffd77 container client-container: <nil>
  STEP: delete the pod @ 06/10/23 12:31:34.748
  Jun 10 12:31:34.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5288" for this suite. @ 06/10/23 12:31:34.769
• [4.102 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 06/10/23 12:31:34.778
  Jun 10 12:31:34.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename field-validation @ 06/10/23 12:31:34.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:31:34.797
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:31:34.801
  Jun 10 12:31:34.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  W0610 12:31:37.367937      18 warnings.go:70] unknown field "alpha"
  W0610 12:31:37.368113      18 warnings.go:70] unknown field "beta"
  W0610 12:31:37.368123      18 warnings.go:70] unknown field "delta"
  W0610 12:31:37.368130      18 warnings.go:70] unknown field "epsilon"
  W0610 12:31:37.368137      18 warnings.go:70] unknown field "gamma"
  Jun 10 12:31:37.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2459" for this suite. @ 06/10/23 12:31:37.41
• [2.639 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 06/10/23 12:31:37.418
  Jun 10 12:31:37.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl @ 06/10/23 12:31:37.419
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:31:37.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:31:37.442
  STEP: creating Agnhost RC @ 06/10/23 12:31:37.446
  Jun 10 12:31:37.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7967 create -f -'
  Jun 10 12:31:38.401: INFO: stderr: ""
  Jun 10 12:31:38.401: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 06/10/23 12:31:38.401
  Jun 10 12:31:39.406: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 10 12:31:39.406: INFO: Found 0 / 1
  Jun 10 12:31:40.405: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 10 12:31:40.405: INFO: Found 1 / 1
  Jun 10 12:31:40.405: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 06/10/23 12:31:40.405
  Jun 10 12:31:40.409: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 10 12:31:40.409: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jun 10 12:31:40.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7967 patch pod agnhost-primary-v5dx4 -p {"metadata":{"annotations":{"x":"y"}}}'
  Jun 10 12:31:40.495: INFO: stderr: ""
  Jun 10 12:31:40.495: INFO: stdout: "pod/agnhost-primary-v5dx4 patched\n"
  STEP: checking annotations @ 06/10/23 12:31:40.495
  Jun 10 12:31:40.500: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 10 12:31:40.500: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jun 10 12:31:40.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7967" for this suite. @ 06/10/23 12:31:40.507
• [3.096 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 06/10/23 12:31:40.514
  Jun 10 12:31:40.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir @ 06/10/23 12:31:40.515
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:31:40.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:31:40.538
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 06/10/23 12:31:40.542
  STEP: Saw pod success @ 06/10/23 12:31:44.57
  Jun 10 12:31:44.576: INFO: Trying to get logs from node ip-172-31-89-0 pod pod-2d255eb4-a136-4190-b0b6-0206b8ed450c container test-container: <nil>
  STEP: delete the pod @ 06/10/23 12:31:44.603
  Jun 10 12:31:44.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-781" for this suite. @ 06/10/23 12:31:44.63
• [4.123 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 06/10/23 12:31:44.64
  Jun 10 12:31:44.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-probe @ 06/10/23 12:31:44.643
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:31:44.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:31:44.667
  STEP: Creating pod liveness-b8e68dd8-f062-469a-9a06-67c73fcaa314 in namespace container-probe-7491 @ 06/10/23 12:31:44.672
  Jun 10 12:31:46.690: INFO: Started pod liveness-b8e68dd8-f062-469a-9a06-67c73fcaa314 in namespace container-probe-7491
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/10/23 12:31:46.69
  Jun 10 12:31:46.694: INFO: Initial restart count of pod liveness-b8e68dd8-f062-469a-9a06-67c73fcaa314 is 0
  Jun 10 12:35:47.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/10/23 12:35:47.334
  STEP: Destroying namespace "container-probe-7491" for this suite. @ 06/10/23 12:35:47.349
• [242.718 seconds]
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 06/10/23 12:35:47.359
  Jun 10 12:35:47.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename gc @ 06/10/23 12:35:47.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:35:47.382
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:35:47.386
  STEP: create the rc @ 06/10/23 12:35:47.396
  W0610 12:35:47.404381      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 06/10/23 12:35:53.414
  STEP: wait for the rc to be deleted @ 06/10/23 12:35:53.427
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 06/10/23 12:35:58.45
  STEP: Gathering metrics @ 06/10/23 12:36:28.464
  W0610 12:36:28.469050      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 10 12:36:28.469: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 10 12:36:28.469: INFO: Deleting pod "simpletest.rc-2pq9z" in namespace "gc-4461"
  Jun 10 12:36:28.482: INFO: Deleting pod "simpletest.rc-2r2xn" in namespace "gc-4461"
  Jun 10 12:36:28.500: INFO: Deleting pod "simpletest.rc-2r6kq" in namespace "gc-4461"
  Jun 10 12:36:28.513: INFO: Deleting pod "simpletest.rc-49j6m" in namespace "gc-4461"
  Jun 10 12:36:28.526: INFO: Deleting pod "simpletest.rc-4nnzv" in namespace "gc-4461"
  Jun 10 12:36:28.543: INFO: Deleting pod "simpletest.rc-4tbm4" in namespace "gc-4461"
  Jun 10 12:36:28.558: INFO: Deleting pod "simpletest.rc-52sn7" in namespace "gc-4461"
  Jun 10 12:36:28.575: INFO: Deleting pod "simpletest.rc-54s9q" in namespace "gc-4461"
  Jun 10 12:36:28.675: INFO: Deleting pod "simpletest.rc-5g26g" in namespace "gc-4461"
  Jun 10 12:36:28.688: INFO: Deleting pod "simpletest.rc-5ktzp" in namespace "gc-4461"
  Jun 10 12:36:28.703: INFO: Deleting pod "simpletest.rc-5mgtq" in namespace "gc-4461"
  Jun 10 12:36:28.718: INFO: Deleting pod "simpletest.rc-5nhd8" in namespace "gc-4461"
  Jun 10 12:36:28.742: INFO: Deleting pod "simpletest.rc-5ttmp" in namespace "gc-4461"
  Jun 10 12:36:28.753: INFO: Deleting pod "simpletest.rc-5ww6p" in namespace "gc-4461"
  Jun 10 12:36:28.769: INFO: Deleting pod "simpletest.rc-6bxsc" in namespace "gc-4461"
  Jun 10 12:36:28.783: INFO: Deleting pod "simpletest.rc-6kgss" in namespace "gc-4461"
  Jun 10 12:36:28.795: INFO: Deleting pod "simpletest.rc-6kpmm" in namespace "gc-4461"
  Jun 10 12:36:28.810: INFO: Deleting pod "simpletest.rc-74m8j" in namespace "gc-4461"
  Jun 10 12:36:28.826: INFO: Deleting pod "simpletest.rc-762hc" in namespace "gc-4461"
  Jun 10 12:36:28.839: INFO: Deleting pod "simpletest.rc-7p8p2" in namespace "gc-4461"
  Jun 10 12:36:28.857: INFO: Deleting pod "simpletest.rc-7q6k9" in namespace "gc-4461"
  Jun 10 12:36:28.869: INFO: Deleting pod "simpletest.rc-8fp66" in namespace "gc-4461"
  Jun 10 12:36:28.883: INFO: Deleting pod "simpletest.rc-8j87k" in namespace "gc-4461"
  Jun 10 12:36:28.899: INFO: Deleting pod "simpletest.rc-8sdzj" in namespace "gc-4461"
  Jun 10 12:36:28.915: INFO: Deleting pod "simpletest.rc-94g8v" in namespace "gc-4461"
  Jun 10 12:36:28.935: INFO: Deleting pod "simpletest.rc-9729p" in namespace "gc-4461"
  Jun 10 12:36:28.950: INFO: Deleting pod "simpletest.rc-9g2xt" in namespace "gc-4461"
  Jun 10 12:36:28.966: INFO: Deleting pod "simpletest.rc-9pcdw" in namespace "gc-4461"
  Jun 10 12:36:28.981: INFO: Deleting pod "simpletest.rc-9tl4m" in namespace "gc-4461"
  Jun 10 12:36:28.995: INFO: Deleting pod "simpletest.rc-b6zf8" in namespace "gc-4461"
  Jun 10 12:36:29.010: INFO: Deleting pod "simpletest.rc-bbm59" in namespace "gc-4461"
  Jun 10 12:36:29.029: INFO: Deleting pod "simpletest.rc-bdjgt" in namespace "gc-4461"
  Jun 10 12:36:29.045: INFO: Deleting pod "simpletest.rc-bg652" in namespace "gc-4461"
  Jun 10 12:36:29.060: INFO: Deleting pod "simpletest.rc-bm7w5" in namespace "gc-4461"
  Jun 10 12:36:29.077: INFO: Deleting pod "simpletest.rc-bscsk" in namespace "gc-4461"
  Jun 10 12:36:29.092: INFO: Deleting pod "simpletest.rc-bsscd" in namespace "gc-4461"
  Jun 10 12:36:29.107: INFO: Deleting pod "simpletest.rc-btsdb" in namespace "gc-4461"
  Jun 10 12:36:29.124: INFO: Deleting pod "simpletest.rc-cfl9k" in namespace "gc-4461"
  Jun 10 12:36:29.139: INFO: Deleting pod "simpletest.rc-dmsjc" in namespace "gc-4461"
  Jun 10 12:36:29.155: INFO: Deleting pod "simpletest.rc-dzmr7" in namespace "gc-4461"
  Jun 10 12:36:29.172: INFO: Deleting pod "simpletest.rc-f5dvg" in namespace "gc-4461"
  Jun 10 12:36:29.187: INFO: Deleting pod "simpletest.rc-f7gcn" in namespace "gc-4461"
  Jun 10 12:36:29.203: INFO: Deleting pod "simpletest.rc-fctc9" in namespace "gc-4461"
  Jun 10 12:36:29.231: INFO: Deleting pod "simpletest.rc-fr74t" in namespace "gc-4461"
  Jun 10 12:36:29.244: INFO: Deleting pod "simpletest.rc-fv7dr" in namespace "gc-4461"
  Jun 10 12:36:29.264: INFO: Deleting pod "simpletest.rc-g2tdm" in namespace "gc-4461"
  Jun 10 12:36:29.277: INFO: Deleting pod "simpletest.rc-gsn6j" in namespace "gc-4461"
  Jun 10 12:36:29.292: INFO: Deleting pod "simpletest.rc-gtmv9" in namespace "gc-4461"
  Jun 10 12:36:29.309: INFO: Deleting pod "simpletest.rc-gtzxl" in namespace "gc-4461"
  Jun 10 12:36:29.332: INFO: Deleting pod "simpletest.rc-h7dv4" in namespace "gc-4461"
  Jun 10 12:36:29.348: INFO: Deleting pod "simpletest.rc-hpxt9" in namespace "gc-4461"
  Jun 10 12:36:29.363: INFO: Deleting pod "simpletest.rc-hzlkg" in namespace "gc-4461"
  Jun 10 12:36:29.380: INFO: Deleting pod "simpletest.rc-j4rt9" in namespace "gc-4461"
  Jun 10 12:36:29.394: INFO: Deleting pod "simpletest.rc-j6pwk" in namespace "gc-4461"
  Jun 10 12:36:29.412: INFO: Deleting pod "simpletest.rc-jdf8s" in namespace "gc-4461"
  Jun 10 12:36:29.428: INFO: Deleting pod "simpletest.rc-jxhsl" in namespace "gc-4461"
  Jun 10 12:36:29.443: INFO: Deleting pod "simpletest.rc-k2z92" in namespace "gc-4461"
  Jun 10 12:36:29.456: INFO: Deleting pod "simpletest.rc-kdqsg" in namespace "gc-4461"
  Jun 10 12:36:29.473: INFO: Deleting pod "simpletest.rc-kldn7" in namespace "gc-4461"
  Jun 10 12:36:29.488: INFO: Deleting pod "simpletest.rc-kln7p" in namespace "gc-4461"
  Jun 10 12:36:29.505: INFO: Deleting pod "simpletest.rc-kpq2h" in namespace "gc-4461"
  Jun 10 12:36:29.525: INFO: Deleting pod "simpletest.rc-ld6w8" in namespace "gc-4461"
  Jun 10 12:36:29.538: INFO: Deleting pod "simpletest.rc-lpcxf" in namespace "gc-4461"
  Jun 10 12:36:29.550: INFO: Deleting pod "simpletest.rc-lqg82" in namespace "gc-4461"
  Jun 10 12:36:29.566: INFO: Deleting pod "simpletest.rc-lsc2q" in namespace "gc-4461"
  Jun 10 12:36:29.581: INFO: Deleting pod "simpletest.rc-m8865" in namespace "gc-4461"
  Jun 10 12:36:29.596: INFO: Deleting pod "simpletest.rc-mgcz8" in namespace "gc-4461"
  Jun 10 12:36:29.613: INFO: Deleting pod "simpletest.rc-mhwdf" in namespace "gc-4461"
  Jun 10 12:36:29.630: INFO: Deleting pod "simpletest.rc-mnhc4" in namespace "gc-4461"
  Jun 10 12:36:29.649: INFO: Deleting pod "simpletest.rc-mxlsv" in namespace "gc-4461"
  Jun 10 12:36:29.667: INFO: Deleting pod "simpletest.rc-nqqsb" in namespace "gc-4461"
  Jun 10 12:36:29.682: INFO: Deleting pod "simpletest.rc-nrmk8" in namespace "gc-4461"
  Jun 10 12:36:29.698: INFO: Deleting pod "simpletest.rc-pwdk8" in namespace "gc-4461"
  Jun 10 12:36:29.714: INFO: Deleting pod "simpletest.rc-q4szc" in namespace "gc-4461"
  Jun 10 12:36:29.768: INFO: Deleting pod "simpletest.rc-q7qwt" in namespace "gc-4461"
  Jun 10 12:36:29.817: INFO: Deleting pod "simpletest.rc-q8pcg" in namespace "gc-4461"
  Jun 10 12:36:29.866: INFO: Deleting pod "simpletest.rc-q9zqp" in namespace "gc-4461"
  Jun 10 12:36:29.920: INFO: Deleting pod "simpletest.rc-qn46k" in namespace "gc-4461"
  Jun 10 12:36:29.964: INFO: Deleting pod "simpletest.rc-qvm5k" in namespace "gc-4461"
  Jun 10 12:36:30.014: INFO: Deleting pod "simpletest.rc-r2xkq" in namespace "gc-4461"
  Jun 10 12:36:30.064: INFO: Deleting pod "simpletest.rc-rs9hs" in namespace "gc-4461"
  Jun 10 12:36:30.114: INFO: Deleting pod "simpletest.rc-s92xb" in namespace "gc-4461"
  Jun 10 12:36:30.169: INFO: Deleting pod "simpletest.rc-sbmhn" in namespace "gc-4461"
  Jun 10 12:36:30.218: INFO: Deleting pod "simpletest.rc-sljnn" in namespace "gc-4461"
  Jun 10 12:36:30.268: INFO: Deleting pod "simpletest.rc-smpq8" in namespace "gc-4461"
  Jun 10 12:36:30.321: INFO: Deleting pod "simpletest.rc-sts4n" in namespace "gc-4461"
  Jun 10 12:36:30.364: INFO: Deleting pod "simpletest.rc-svph5" in namespace "gc-4461"
  Jun 10 12:36:30.420: INFO: Deleting pod "simpletest.rc-tfttn" in namespace "gc-4461"
  Jun 10 12:36:30.463: INFO: Deleting pod "simpletest.rc-tmv7r" in namespace "gc-4461"
  Jun 10 12:36:30.518: INFO: Deleting pod "simpletest.rc-v287h" in namespace "gc-4461"
  Jun 10 12:36:30.568: INFO: Deleting pod "simpletest.rc-vgnpb" in namespace "gc-4461"
  Jun 10 12:36:30.616: INFO: Deleting pod "simpletest.rc-vwr8z" in namespace "gc-4461"
  Jun 10 12:36:30.666: INFO: Deleting pod "simpletest.rc-wc2w8" in namespace "gc-4461"
  Jun 10 12:36:30.717: INFO: Deleting pod "simpletest.rc-wx4jq" in namespace "gc-4461"
  Jun 10 12:36:30.765: INFO: Deleting pod "simpletest.rc-x8kv4" in namespace "gc-4461"
  Jun 10 12:36:30.814: INFO: Deleting pod "simpletest.rc-xprn7" in namespace "gc-4461"
  Jun 10 12:36:30.865: INFO: Deleting pod "simpletest.rc-z2fbn" in namespace "gc-4461"
  Jun 10 12:36:30.917: INFO: Deleting pod "simpletest.rc-zbkr2" in namespace "gc-4461"
  Jun 10 12:36:30.965: INFO: Deleting pod "simpletest.rc-zdmlc" in namespace "gc-4461"
  Jun 10 12:36:31.018: INFO: Deleting pod "simpletest.rc-zmsr5" in namespace "gc-4461"
  Jun 10 12:36:31.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4461" for this suite. @ 06/10/23 12:36:31.108
• [43.801 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 06/10/23 12:36:31.163
  Jun 10 12:36:31.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename downward-api @ 06/10/23 12:36:31.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:36:31.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:36:31.191
  STEP: Creating the pod @ 06/10/23 12:36:31.196
  Jun 10 12:36:41.782: INFO: Successfully updated pod "annotationupdate463a01de-9036-4812-99e6-1a14e287de6a"
  Jun 10 12:36:43.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1057" for this suite. @ 06/10/23 12:36:43.804
• [12.649 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 06/10/23 12:36:43.812
  Jun 10 12:36:43.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 12:36:43.814
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:36:43.833
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:36:43.837
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 12:36:43.841
  STEP: Saw pod success @ 06/10/23 12:36:47.869
  Jun 10 12:36:47.873: INFO: Trying to get logs from node ip-172-31-27-177 pod downwardapi-volume-ecc77d3e-9574-40c6-8914-9550e899874f container client-container: <nil>
  STEP: delete the pod @ 06/10/23 12:36:47.881
  Jun 10 12:36:47.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-43" for this suite. @ 06/10/23 12:36:47.905
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 06/10/23 12:36:47.916
  Jun 10 12:36:47.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename sched-preemption @ 06/10/23 12:36:47.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:36:47.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:36:47.938
  Jun 10 12:36:47.961: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun 10 12:37:47.982: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 06/10/23 12:37:47.986
  Jun 10 12:37:47.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename sched-preemption-path @ 06/10/23 12:37:47.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:37:48.004
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:37:48.01
  STEP: Finding an available node @ 06/10/23 12:37:48.014
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 06/10/23 12:37:48.014
  STEP: Explicitly delete pod here to free the resource it takes. @ 06/10/23 12:37:50.044
  Jun 10 12:37:50.056: INFO: found a healthy node: ip-172-31-27-177
  Jun 10 12:37:56.141: INFO: pods created so far: [1 1 1]
  Jun 10 12:37:56.141: INFO: length of pods created so far: 3
  Jun 10 12:37:58.152: INFO: pods created so far: [2 2 1]
  Jun 10 12:38:05.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 10 12:38:05.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-5679" for this suite. @ 06/10/23 12:38:05.253
  STEP: Destroying namespace "sched-preemption-8483" for this suite. @ 06/10/23 12:38:05.26
• [77.352 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 06/10/23 12:38:05.27
  Jun 10 12:38:05.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename containers @ 06/10/23 12:38:05.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:38:05.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:38:05.292
  STEP: Creating a pod to test override arguments @ 06/10/23 12:38:05.297
  STEP: Saw pod success @ 06/10/23 12:38:09.321
  Jun 10 12:38:09.324: INFO: Trying to get logs from node ip-172-31-89-0 pod client-containers-03b8e4a6-5ce8-4a9c-b3d9-837723c2e0b7 container agnhost-container: <nil>
  STEP: delete the pod @ 06/10/23 12:38:09.354
  Jun 10 12:38:09.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-3005" for this suite. @ 06/10/23 12:38:09.377
• [4.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 06/10/23 12:38:09.388
  Jun 10 12:38:09.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/10/23 12:38:09.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:38:09.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:38:09.429
  Jun 10 12:38:09.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:38:15.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8480" for this suite. @ 06/10/23 12:38:15.707
• [6.327 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 06/10/23 12:38:15.714
  Jun 10 12:38:15.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir @ 06/10/23 12:38:15.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:38:15.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:38:15.741
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 06/10/23 12:38:15.744
  STEP: Saw pod success @ 06/10/23 12:38:19.769
  Jun 10 12:38:19.772: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-3caf2e25-a0cf-4781-9601-94b2116d6868 container test-container: <nil>
  STEP: delete the pod @ 06/10/23 12:38:19.792
  Jun 10 12:38:19.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6945" for this suite. @ 06/10/23 12:38:19.816
• [4.108 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 06/10/23 12:38:19.823
  Jun 10 12:38:19.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename sched-pred @ 06/10/23 12:38:19.824
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:38:19.841
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:38:19.847
  Jun 10 12:38:19.850: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jun 10 12:38:19.861: INFO: Waiting for terminating namespaces to be deleted...
  Jun 10 12:38:19.864: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-27-177 before test
  Jun 10 12:38:19.871: INFO: default-http-backend-kubernetes-worker-65fc475d49-ndjwt from ingress-nginx-kubernetes-worker started at 2023-06-10 12:27:43 +0000 UTC (1 container statuses recorded)
  Jun 10 12:38:19.871: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jun 10 12:38:19.871: INFO: nginx-ingress-controller-kubernetes-worker-6c5bl from ingress-nginx-kubernetes-worker started at 2023-06-10 12:27:55 +0000 UTC (1 container statuses recorded)
  Jun 10 12:38:19.871: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 10 12:38:19.871: INFO: calico-kube-controllers-86c9c69795-vrx7l from kube-system started at 2023-06-10 12:27:43 +0000 UTC (1 container statuses recorded)
  Jun 10 12:38:19.871: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jun 10 12:38:19.871: INFO: sonobuoy-systemd-logs-daemon-set-31eb0473969f40ee-l8bg7 from sonobuoy started at 2023-06-10 12:06:59 +0000 UTC (2 container statuses recorded)
  Jun 10 12:38:19.871: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 10 12:38:19.871: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 10 12:38:19.871: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-46-40 before test
  Jun 10 12:38:19.877: INFO: nginx-ingress-controller-kubernetes-worker-vb28m from ingress-nginx-kubernetes-worker started at 2023-06-10 11:54:55 +0000 UTC (1 container statuses recorded)
  Jun 10 12:38:19.877: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 10 12:38:19.877: INFO: coredns-5c7f76ccb8-xmz95 from kube-system started at 2023-06-10 11:54:47 +0000 UTC (1 container statuses recorded)
  Jun 10 12:38:19.877: INFO: 	Container coredns ready: true, restart count 0
  Jun 10 12:38:19.877: INFO: kube-state-metrics-5b95b4459c-rtj7m from kube-system started at 2023-06-10 11:54:47 +0000 UTC (1 container statuses recorded)
  Jun 10 12:38:19.877: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jun 10 12:38:19.877: INFO: metrics-server-v0.5.2-6cf8c8b69c-ftmcl from kube-system started at 2023-06-10 11:54:47 +0000 UTC (2 container statuses recorded)
  Jun 10 12:38:19.877: INFO: 	Container metrics-server ready: true, restart count 0
  Jun 10 12:38:19.877: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jun 10 12:38:19.877: INFO: dashboard-metrics-scraper-6b8586b5c9-znc9p from kubernetes-dashboard started at 2023-06-10 11:54:47 +0000 UTC (1 container statuses recorded)
  Jun 10 12:38:19.877: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jun 10 12:38:19.877: INFO: kubernetes-dashboard-6869f4cd5f-xdbkn from kubernetes-dashboard started at 2023-06-10 11:54:47 +0000 UTC (1 container statuses recorded)
  Jun 10 12:38:19.877: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jun 10 12:38:19.877: INFO: sonobuoy-systemd-logs-daemon-set-31eb0473969f40ee-wg64x from sonobuoy started at 2023-06-10 12:06:59 +0000 UTC (2 container statuses recorded)
  Jun 10 12:38:19.877: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 10 12:38:19.877: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 10 12:38:19.877: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-89-0 before test
  Jun 10 12:38:19.883: INFO: nginx-ingress-controller-kubernetes-worker-889j8 from ingress-nginx-kubernetes-worker started at 2023-06-10 12:03:37 +0000 UTC (1 container statuses recorded)
  Jun 10 12:38:19.883: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 10 12:38:19.883: INFO: sonobuoy from sonobuoy started at 2023-06-10 12:06:57 +0000 UTC (1 container statuses recorded)
  Jun 10 12:38:19.883: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jun 10 12:38:19.883: INFO: sonobuoy-e2e-job-f362cde51af14fae from sonobuoy started at 2023-06-10 12:06:59 +0000 UTC (2 container statuses recorded)
  Jun 10 12:38:19.883: INFO: 	Container e2e ready: true, restart count 0
  Jun 10 12:38:19.883: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 10 12:38:19.883: INFO: sonobuoy-systemd-logs-daemon-set-31eb0473969f40ee-qjm5m from sonobuoy started at 2023-06-10 12:06:59 +0000 UTC (2 container statuses recorded)
  Jun 10 12:38:19.883: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 10 12:38:19.884: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 06/10/23 12:38:19.884
  STEP: Explicitly delete pod here to free the resource it takes. @ 06/10/23 12:38:21.909
  STEP: Trying to apply a random label on the found node. @ 06/10/23 12:38:21.923
  STEP: verifying the node has the label kubernetes.io/e2e-00eab5c9-24dc-418b-9286-3652eba68de5 95 @ 06/10/23 12:38:21.936
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 06/10/23 12:38:21.94
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.27.177 on the node which pod4 resides and expect not scheduled @ 06/10/23 12:38:23.963
  STEP: removing the label kubernetes.io/e2e-00eab5c9-24dc-418b-9286-3652eba68de5 off the node ip-172-31-27-177 @ 06/10/23 12:43:23.973
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-00eab5c9-24dc-418b-9286-3652eba68de5 @ 06/10/23 12:43:23.988
  Jun 10 12:43:23.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-7109" for this suite. @ 06/10/23 12:43:23.998
• [304.183 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 06/10/23 12:43:24.013
  Jun 10 12:43:24.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 06/10/23 12:43:24.014
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:43:24.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:43:24.05
  STEP: create the container to handle the HTTPGet hook request. @ 06/10/23 12:43:24.058
  STEP: create the pod with lifecycle hook @ 06/10/23 12:43:26.085
  STEP: delete the pod with lifecycle hook @ 06/10/23 12:43:28.103
  STEP: check prestop hook @ 06/10/23 12:43:30.121
  Jun 10 12:43:30.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-915" for this suite. @ 06/10/23 12:43:30.15
• [6.145 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 06/10/23 12:43:30.159
  Jun 10 12:43:30.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename secrets @ 06/10/23 12:43:30.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:43:30.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:43:30.182
  STEP: Creating secret with name secret-test-f24a828f-d2cb-4de4-bb27-b7f6e96f5d61 @ 06/10/23 12:43:30.186
  STEP: Creating a pod to test consume secrets @ 06/10/23 12:43:30.191
  STEP: Saw pod success @ 06/10/23 12:43:34.215
  Jun 10 12:43:34.218: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-secrets-8f2097d8-9d50-4178-b57c-a8c189e60c49 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/10/23 12:43:34.241
  Jun 10 12:43:34.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1500" for this suite. @ 06/10/23 12:43:34.265
• [4.113 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 06/10/23 12:43:34.274
  Jun 10 12:43:34.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename field-validation @ 06/10/23 12:43:34.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:43:34.294
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:43:34.298
  STEP: apply creating a deployment @ 06/10/23 12:43:34.303
  Jun 10 12:43:34.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-245" for this suite. @ 06/10/23 12:43:34.325
• [0.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 06/10/23 12:43:34.335
  Jun 10 12:43:34.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename gc @ 06/10/23 12:43:34.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:43:34.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:43:34.374
  STEP: create the deployment @ 06/10/23 12:43:34.381
  W0610 12:43:34.387744      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 06/10/23 12:43:34.387
  STEP: delete the deployment @ 06/10/23 12:43:34.898
  STEP: wait for all rs to be garbage collected @ 06/10/23 12:43:34.906
  STEP: expected 0 rs, got 1 rs @ 06/10/23 12:43:34.923
  STEP: expected 0 pods, got 2 pods @ 06/10/23 12:43:34.927
  STEP: Gathering metrics @ 06/10/23 12:43:35.44
  W0610 12:43:35.445736      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 10 12:43:35.446: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 10 12:43:35.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5425" for this suite. @ 06/10/23 12:43:35.451
• [1.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 06/10/23 12:43:35.463
  Jun 10 12:43:35.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename namespaces @ 06/10/23 12:43:35.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:43:35.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:43:35.488
  STEP: Updating Namespace "namespaces-8706" @ 06/10/23 12:43:35.492
  Jun 10 12:43:35.503: INFO: Namespace "namespaces-8706" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"78244405-d6bd-408b-af19-702c6f7dfe06", "kubernetes.io/metadata.name":"namespaces-8706", "namespaces-8706":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Jun 10 12:43:35.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8706" for this suite. @ 06/10/23 12:43:35.512
• [0.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 06/10/23 12:43:35.527
  Jun 10 12:43:35.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename var-expansion @ 06/10/23 12:43:35.528
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:43:35.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:43:35.551
  STEP: creating the pod with failed condition @ 06/10/23 12:43:35.555
  STEP: updating the pod @ 06/10/23 12:45:35.566
  Jun 10 12:45:36.081: INFO: Successfully updated pod "var-expansion-eae91e55-39a1-4f14-ba01-cc1b887234de"
  STEP: waiting for pod running @ 06/10/23 12:45:36.081
  STEP: deleting the pod gracefully @ 06/10/23 12:45:38.089
  Jun 10 12:45:38.089: INFO: Deleting pod "var-expansion-eae91e55-39a1-4f14-ba01-cc1b887234de" in namespace "var-expansion-601"
  Jun 10 12:45:38.099: INFO: Wait up to 5m0s for pod "var-expansion-eae91e55-39a1-4f14-ba01-cc1b887234de" to be fully deleted
  Jun 10 12:46:10.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-601" for this suite. @ 06/10/23 12:46:10.195
• [154.675 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 06/10/23 12:46:10.204
  Jun 10 12:46:10.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename secrets @ 06/10/23 12:46:10.205
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:46:10.229
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:46:10.234
  STEP: Creating secret with name secret-test-6cafc746-034b-42fb-b1e6-8c3c93d8436e @ 06/10/23 12:46:10.268
  STEP: Creating a pod to test consume secrets @ 06/10/23 12:46:10.277
  STEP: Saw pod success @ 06/10/23 12:46:14.303
  Jun 10 12:46:14.307: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-secrets-11310f25-e798-45fa-8e11-e66a7a9fe82e container secret-volume-test: <nil>
  STEP: delete the pod @ 06/10/23 12:46:14.327
  Jun 10 12:46:14.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2844" for this suite. @ 06/10/23 12:46:14.349
  STEP: Destroying namespace "secret-namespace-6974" for this suite. @ 06/10/23 12:46:14.359
• [4.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 06/10/23 12:46:14.371
  Jun 10 12:46:14.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename lease-test @ 06/10/23 12:46:14.373
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:46:14.388
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:46:14.399
  Jun 10 12:46:14.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-6134" for this suite. @ 06/10/23 12:46:14.474
• [0.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 06/10/23 12:46:14.486
  Jun 10 12:46:14.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 12:46:14.487
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:46:14.503
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:46:14.507
  STEP: Creating configMap with name configmap-projected-all-test-volume-ddb29e2b-01e8-46d0-aed4-16c596ce9c19 @ 06/10/23 12:46:14.511
  STEP: Creating secret with name secret-projected-all-test-volume-dde93586-a4cd-49c4-a1b9-436c83a8d835 @ 06/10/23 12:46:14.517
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 06/10/23 12:46:14.522
  STEP: Saw pod success @ 06/10/23 12:46:18.548
  Jun 10 12:46:18.552: INFO: Trying to get logs from node ip-172-31-27-177 pod projected-volume-17ab9dac-b496-4dc0-816c-d6f710c5d1b6 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 06/10/23 12:46:18.561
  Jun 10 12:46:18.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6608" for this suite. @ 06/10/23 12:46:18.585
• [4.107 seconds]
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 06/10/23 12:46:18.594
  Jun 10 12:46:18.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename runtimeclass @ 06/10/23 12:46:18.595
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:46:18.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:46:18.618
  Jun 10 12:46:20.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2133" for this suite. @ 06/10/23 12:46:20.66
• [2.073 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 06/10/23 12:46:20.67
  Jun 10 12:46:20.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename containers @ 06/10/23 12:46:20.671
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:46:20.692
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:46:20.699
  Jun 10 12:46:22.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-4238" for this suite. @ 06/10/23 12:46:22.739
• [2.076 seconds]
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 06/10/23 12:46:22.746
  Jun 10 12:46:22.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename services @ 06/10/23 12:46:22.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:46:22.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:46:22.767
  STEP: creating service in namespace services-9118 @ 06/10/23 12:46:22.775
  STEP: creating service affinity-nodeport-transition in namespace services-9118 @ 06/10/23 12:46:22.775
  STEP: creating replication controller affinity-nodeport-transition in namespace services-9118 @ 06/10/23 12:46:22.8
  I0610 12:46:22.817718      18 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-9118, replica count: 3
  I0610 12:46:25.868646      18 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 10 12:46:25.882: INFO: Creating new exec pod
  Jun 10 12:46:28.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-9118 exec execpod-affinityfdvmt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Jun 10 12:46:29.075: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Jun 10 12:46:29.075: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 12:46:29.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-9118 exec execpod-affinityfdvmt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.234 80'
  Jun 10 12:46:29.229: INFO: stderr: "+ nc -v -t -w 2 10.152.183.234 80\n+ echo hostName\nConnection to 10.152.183.234 80 port [tcp/http] succeeded!\n"
  Jun 10 12:46:29.229: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 12:46:29.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-9118 exec execpod-affinityfdvmt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.89.0 30954'
  Jun 10 12:46:29.390: INFO: stderr: "+ nc -v -t -w 2 172.31.89.0 30954\n+ echo hostName\nConnection to 172.31.89.0 30954 port [tcp/*] succeeded!\n"
  Jun 10 12:46:29.390: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 12:46:29.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-9118 exec execpod-affinityfdvmt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.46.40 30954'
  Jun 10 12:46:29.551: INFO: stderr: "+ nc -v -t -w 2 172.31.46.40 30954\n+ echo hostName\nConnection to 172.31.46.40 30954 port [tcp/*] succeeded!\n"
  Jun 10 12:46:29.551: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 12:46:29.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-9118 exec execpod-affinityfdvmt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.27.177:30954/ ; done'
  Jun 10 12:46:29.814: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n"
  Jun 10 12:46:29.815: INFO: stdout: "\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-bftxz\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-bftxz\naffinity-nodeport-transition-bftxz\naffinity-nodeport-transition-bftxz\naffinity-nodeport-transition-bftxz\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-mkhlf\naffinity-nodeport-transition-mkhlf\naffinity-nodeport-transition-mkhlf\naffinity-nodeport-transition-bftxz\naffinity-nodeport-transition-bftxz\naffinity-nodeport-transition-k8s4t"
  Jun 10 12:46:29.815: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:29.815: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:29.815: INFO: Received response from host: affinity-nodeport-transition-bftxz
  Jun 10 12:46:29.815: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:29.815: INFO: Received response from host: affinity-nodeport-transition-bftxz
  Jun 10 12:46:29.815: INFO: Received response from host: affinity-nodeport-transition-bftxz
  Jun 10 12:46:29.815: INFO: Received response from host: affinity-nodeport-transition-bftxz
  Jun 10 12:46:29.815: INFO: Received response from host: affinity-nodeport-transition-bftxz
  Jun 10 12:46:29.815: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:29.815: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:29.815: INFO: Received response from host: affinity-nodeport-transition-mkhlf
  Jun 10 12:46:29.815: INFO: Received response from host: affinity-nodeport-transition-mkhlf
  Jun 10 12:46:29.815: INFO: Received response from host: affinity-nodeport-transition-mkhlf
  Jun 10 12:46:29.815: INFO: Received response from host: affinity-nodeport-transition-bftxz
  Jun 10 12:46:29.815: INFO: Received response from host: affinity-nodeport-transition-bftxz
  Jun 10 12:46:29.815: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:29.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-9118 exec execpod-affinityfdvmt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.27.177:30954/ ; done'
  Jun 10 12:46:30.093: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.177:30954/\n"
  Jun 10 12:46:30.093: INFO: stdout: "\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t\naffinity-nodeport-transition-k8s4t"
  Jun 10 12:46:30.093: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:30.093: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:30.093: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:30.093: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:30.093: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:30.093: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:30.093: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:30.093: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:30.093: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:30.093: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:30.093: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:30.093: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:30.093: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:30.093: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:30.093: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:30.093: INFO: Received response from host: affinity-nodeport-transition-k8s4t
  Jun 10 12:46:30.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 10 12:46:30.097: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9118, will wait for the garbage collector to delete the pods @ 06/10/23 12:46:30.115
  Jun 10 12:46:30.178: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.453879ms
  Jun 10 12:46:30.279: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.013195ms
  STEP: Destroying namespace "services-9118" for this suite. @ 06/10/23 12:46:32.505
• [9.766 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 06/10/23 12:46:32.513
  Jun 10 12:46:32.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename field-validation @ 06/10/23 12:46:32.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:46:32.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:46:32.537
  Jun 10 12:46:32.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  W0610 12:46:35.106522      18 warnings.go:70] unknown field "alpha"
  W0610 12:46:35.106664      18 warnings.go:70] unknown field "beta"
  W0610 12:46:35.106790      18 warnings.go:70] unknown field "delta"
  W0610 12:46:35.106950      18 warnings.go:70] unknown field "epsilon"
  W0610 12:46:35.107098      18 warnings.go:70] unknown field "gamma"
  Jun 10 12:46:35.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3355" for this suite. @ 06/10/23 12:46:35.146
• [2.641 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 06/10/23 12:46:35.156
  Jun 10 12:46:35.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename proxy @ 06/10/23 12:46:35.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:46:35.175
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:46:35.18
  STEP: starting an echo server on multiple ports @ 06/10/23 12:46:35.199
  STEP: creating replication controller proxy-service-c96g9 in namespace proxy-9378 @ 06/10/23 12:46:35.199
  I0610 12:46:35.209841      18 runners.go:194] Created replication controller with name: proxy-service-c96g9, namespace: proxy-9378, replica count: 1
  I0610 12:46:36.260874      18 runners.go:194] proxy-service-c96g9 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0610 12:46:37.261917      18 runners.go:194] proxy-service-c96g9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0610 12:46:38.263022      18 runners.go:194] proxy-service-c96g9 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 10 12:46:38.267: INFO: setup took 3.082657038s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 06/10/23 12:46:38.267
  Jun 10 12:46:38.277: INFO: (0) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 9.796772ms)
  Jun 10 12:46:38.282: INFO: (0) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 15.39552ms)
  Jun 10 12:46:38.289: INFO: (0) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 20.900127ms)
  Jun 10 12:46:38.289: INFO: (0) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 21.172221ms)
  Jun 10 12:46:38.289: INFO: (0) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 21.905638ms)
  Jun 10 12:46:38.289: INFO: (0) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 21.997809ms)
  Jun 10 12:46:38.289: INFO: (0) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 21.938378ms)
  Jun 10 12:46:38.293: INFO: (0) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 25.638566ms)
  Jun 10 12:46:38.294: INFO: (0) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 26.318094ms)
  Jun 10 12:46:38.296: INFO: (0) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 28.300295ms)
  Jun 10 12:46:38.296: INFO: (0) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 29.229384ms)
  Jun 10 12:46:38.297: INFO: (0) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 29.549328ms)
  Jun 10 12:46:38.298: INFO: (0) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 29.75601ms)
  Jun 10 12:46:38.298: INFO: (0) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 30.166493ms)
  Jun 10 12:46:38.298: INFO: (0) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 30.870401ms)
  Jun 10 12:46:38.298: INFO: (0) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 30.7287ms)
  Jun 10 12:46:38.336: INFO: (1) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 37.4957ms)
  Jun 10 12:46:38.350: INFO: (1) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 51.071722ms)
  Jun 10 12:46:38.350: INFO: (1) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 51.443935ms)
  Jun 10 12:46:38.356: INFO: (1) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 56.833981ms)
  Jun 10 12:46:38.360: INFO: (1) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 60.919673ms)
  Jun 10 12:46:38.360: INFO: (1) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 61.421629ms)
  Jun 10 12:46:38.360: INFO: (1) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 61.329097ms)
  Jun 10 12:46:38.361: INFO: (1) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 61.508179ms)
  Jun 10 12:46:38.370: INFO: (1) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 71.039549ms)
  Jun 10 12:46:38.370: INFO: (1) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 70.559204ms)
  Jun 10 12:46:38.371: INFO: (1) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 72.213542ms)
  Jun 10 12:46:38.373: INFO: (1) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 73.585456ms)
  Jun 10 12:46:38.374: INFO: (1) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 74.9318ms)
  Jun 10 12:46:38.375: INFO: (1) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 75.700487ms)
  Jun 10 12:46:38.377: INFO: (1) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 78.331614ms)
  Jun 10 12:46:38.377: INFO: (1) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 77.631908ms)
  Jun 10 12:46:38.400: INFO: (2) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 22.772047ms)
  Jun 10 12:46:38.401: INFO: (2) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 23.08096ms)
  Jun 10 12:46:38.426: INFO: (2) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 48.158731ms)
  Jun 10 12:46:38.426: INFO: (2) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 47.875458ms)
  Jun 10 12:46:38.426: INFO: (2) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 47.965349ms)
  Jun 10 12:46:38.426: INFO: (2) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 48.205021ms)
  Jun 10 12:46:38.426: INFO: (2) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 48.10245ms)
  Jun 10 12:46:38.426: INFO: (2) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 48.223761ms)
  Jun 10 12:46:38.427: INFO: (2) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 49.547005ms)
  Jun 10 12:46:38.427: INFO: (2) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 49.721707ms)
  Jun 10 12:46:38.428: INFO: (2) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 49.668917ms)
  Jun 10 12:46:38.429: INFO: (2) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 51.90784ms)
  Jun 10 12:46:38.452: INFO: (2) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 74.366873ms)
  Jun 10 12:46:38.452: INFO: (2) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 74.538716ms)
  Jun 10 12:46:38.452: INFO: (2) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 74.694087ms)
  Jun 10 12:46:38.452: INFO: (2) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 74.821699ms)
  Jun 10 12:46:38.471: INFO: (3) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 18.360472ms)
  Jun 10 12:46:38.471: INFO: (3) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 18.362851ms)
  Jun 10 12:46:38.475: INFO: (3) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 22.259132ms)
  Jun 10 12:46:38.482: INFO: (3) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 29.297545ms)
  Jun 10 12:46:38.492: INFO: (3) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 39.37675ms)
  Jun 10 12:46:38.493: INFO: (3) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 39.103747ms)
  Jun 10 12:46:38.495: INFO: (3) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 41.37686ms)
  Jun 10 12:46:38.495: INFO: (3) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 41.630053ms)
  Jun 10 12:46:38.495: INFO: (3) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 42.087317ms)
  Jun 10 12:46:38.495: INFO: (3) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 41.931836ms)
  Jun 10 12:46:38.495: INFO: (3) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 41.591403ms)
  Jun 10 12:46:38.497: INFO: (3) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 43.413652ms)
  Jun 10 12:46:38.498: INFO: (3) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 44.426382ms)
  Jun 10 12:46:38.498: INFO: (3) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 44.473922ms)
  Jun 10 12:46:38.498: INFO: (3) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 44.910167ms)
  Jun 10 12:46:38.499: INFO: (3) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 46.19607ms)
  Jun 10 12:46:38.525: INFO: (4) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 25.161531ms)
  Jun 10 12:46:38.528: INFO: (4) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 28.397255ms)
  Jun 10 12:46:38.528: INFO: (4) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 28.587488ms)
  Jun 10 12:46:38.529: INFO: (4) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 29.258694ms)
  Jun 10 12:46:38.529: INFO: (4) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 29.275685ms)
  Jun 10 12:46:38.529: INFO: (4) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 29.860051ms)
  Jun 10 12:46:38.530: INFO: (4) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 29.983461ms)
  Jun 10 12:46:38.530: INFO: (4) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 29.77839ms)
  Jun 10 12:46:38.530: INFO: (4) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 30.072343ms)
  Jun 10 12:46:38.530: INFO: (4) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 30.344506ms)
  Jun 10 12:46:38.530: INFO: (4) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 30.822501ms)
  Jun 10 12:46:38.530: INFO: (4) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 30.825811ms)
  Jun 10 12:46:38.530: INFO: (4) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 30.440966ms)
  Jun 10 12:46:38.536: INFO: (4) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 35.549799ms)
  Jun 10 12:46:38.536: INFO: (4) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 35.62343ms)
  Jun 10 12:46:38.539: INFO: (4) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 38.846745ms)
  Jun 10 12:46:38.552: INFO: (5) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 12.052855ms)
  Jun 10 12:46:38.552: INFO: (5) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 12.201567ms)
  Jun 10 12:46:38.556: INFO: (5) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 16.613533ms)
  Jun 10 12:46:38.556: INFO: (5) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 16.711174ms)
  Jun 10 12:46:38.556: INFO: (5) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 16.489481ms)
  Jun 10 12:46:38.557: INFO: (5) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 18.443822ms)
  Jun 10 12:46:38.559: INFO: (5) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 19.842287ms)
  Jun 10 12:46:38.560: INFO: (5) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 20.140269ms)
  Jun 10 12:46:38.560: INFO: (5) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 20.461093ms)
  Jun 10 12:46:38.562: INFO: (5) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 22.355493ms)
  Jun 10 12:46:38.562: INFO: (5) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 22.810318ms)
  Jun 10 12:46:38.562: INFO: (5) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 22.552584ms)
  Jun 10 12:46:38.562: INFO: (5) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 22.870299ms)
  Jun 10 12:46:38.562: INFO: (5) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 22.99562ms)
  Jun 10 12:46:38.562: INFO: (5) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 22.868078ms)
  Jun 10 12:46:38.563: INFO: (5) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 23.478475ms)
  Jun 10 12:46:38.577: INFO: (6) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 13.56369ms)
  Jun 10 12:46:38.577: INFO: (6) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 13.833004ms)
  Jun 10 12:46:38.577: INFO: (6) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 14.076307ms)
  Jun 10 12:46:38.577: INFO: (6) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 14.320219ms)
  Jun 10 12:46:38.579: INFO: (6) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 15.137908ms)
  Jun 10 12:46:38.580: INFO: (6) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 16.972837ms)
  Jun 10 12:46:38.580: INFO: (6) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 16.830455ms)
  Jun 10 12:46:38.580: INFO: (6) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 16.942396ms)
  Jun 10 12:46:38.582: INFO: (6) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 18.19945ms)
  Jun 10 12:46:38.582: INFO: (6) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 18.414002ms)
  Jun 10 12:46:38.582: INFO: (6) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 18.121329ms)
  Jun 10 12:46:38.582: INFO: (6) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 18.828545ms)
  Jun 10 12:46:38.584: INFO: (6) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 20.023298ms)
  Jun 10 12:46:38.584: INFO: (6) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 20.24034ms)
  Jun 10 12:46:38.584: INFO: (6) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 20.715715ms)
  Jun 10 12:46:38.584: INFO: (6) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 20.753126ms)
  Jun 10 12:46:38.592: INFO: (7) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 6.916482ms)
  Jun 10 12:46:38.594: INFO: (7) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 8.924312ms)
  Jun 10 12:46:38.594: INFO: (7) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 9.177795ms)
  Jun 10 12:46:38.597: INFO: (7) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 12.300118ms)
  Jun 10 12:46:38.599: INFO: (7) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 13.925875ms)
  Jun 10 12:46:38.599: INFO: (7) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 14.120937ms)
  Jun 10 12:46:38.600: INFO: (7) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 14.849054ms)
  Jun 10 12:46:38.600: INFO: (7) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 15.357869ms)
  Jun 10 12:46:38.602: INFO: (7) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 16.793125ms)
  Jun 10 12:46:38.603: INFO: (7) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 18.408461ms)
  Jun 10 12:46:38.604: INFO: (7) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 19.106219ms)
  Jun 10 12:46:38.606: INFO: (7) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 21.314971ms)
  Jun 10 12:46:38.606: INFO: (7) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 21.291482ms)
  Jun 10 12:46:38.607: INFO: (7) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 22.13751ms)
  Jun 10 12:46:38.608: INFO: (7) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 23.02952ms)
  Jun 10 12:46:38.608: INFO: (7) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 22.789887ms)
  Jun 10 12:46:38.622: INFO: (8) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 13.269798ms)
  Jun 10 12:46:38.623: INFO: (8) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 14.911045ms)
  Jun 10 12:46:38.623: INFO: (8) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 14.662323ms)
  Jun 10 12:46:38.626: INFO: (8) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 17.531822ms)
  Jun 10 12:46:38.626: INFO: (8) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 17.483172ms)
  Jun 10 12:46:38.626: INFO: (8) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 17.600643ms)
  Jun 10 12:46:38.627: INFO: (8) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 18.797135ms)
  Jun 10 12:46:38.627: INFO: (8) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 18.546262ms)
  Jun 10 12:46:38.627: INFO: (8) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 18.033948ms)
  Jun 10 12:46:38.627: INFO: (8) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 18.478963ms)
  Jun 10 12:46:38.627: INFO: (8) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 19.109279ms)
  Jun 10 12:46:38.628: INFO: (8) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 19.564893ms)
  Jun 10 12:46:38.628: INFO: (8) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 19.962938ms)
  Jun 10 12:46:38.629: INFO: (8) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 20.426033ms)
  Jun 10 12:46:38.630: INFO: (8) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 21.12886ms)
  Jun 10 12:46:38.630: INFO: (8) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 21.708926ms)
  Jun 10 12:46:38.638: INFO: (9) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 7.911323ms)
  Jun 10 12:46:38.638: INFO: (9) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 7.69741ms)
  Jun 10 12:46:38.638: INFO: (9) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 7.640549ms)
  Jun 10 12:46:38.639: INFO: (9) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 8.801742ms)
  Jun 10 12:46:38.640: INFO: (9) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 9.460279ms)
  Jun 10 12:46:38.641: INFO: (9) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 10.424369ms)
  Jun 10 12:46:38.642: INFO: (9) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 11.067735ms)
  Jun 10 12:46:38.642: INFO: (9) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 11.226237ms)
  Jun 10 12:46:38.643: INFO: (9) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 12.49659ms)
  Jun 10 12:46:38.643: INFO: (9) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 12.788243ms)
  Jun 10 12:46:38.643: INFO: (9) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 12.864914ms)
  Jun 10 12:46:38.644: INFO: (9) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 13.657362ms)
  Jun 10 12:46:38.644: INFO: (9) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 13.54574ms)
  Jun 10 12:46:38.644: INFO: (9) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 13.677072ms)
  Jun 10 12:46:38.644: INFO: (9) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 13.577491ms)
  Jun 10 12:46:38.645: INFO: (9) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 14.471331ms)
  Jun 10 12:46:38.650: INFO: (10) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 5.226244ms)
  Jun 10 12:46:38.650: INFO: (10) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 5.487067ms)
  Jun 10 12:46:38.652: INFO: (10) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 7.143724ms)
  Jun 10 12:46:38.655: INFO: (10) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 9.215686ms)
  Jun 10 12:46:38.655: INFO: (10) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 9.322597ms)
  Jun 10 12:46:38.657: INFO: (10) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 11.057535ms)
  Jun 10 12:46:38.657: INFO: (10) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 11.49414ms)
  Jun 10 12:46:38.657: INFO: (10) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 12.175096ms)
  Jun 10 12:46:38.658: INFO: (10) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 13.147326ms)
  Jun 10 12:46:38.658: INFO: (10) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 12.976505ms)
  Jun 10 12:46:38.659: INFO: (10) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 14.280769ms)
  Jun 10 12:46:38.659: INFO: (10) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 14.010875ms)
  Jun 10 12:46:38.659: INFO: (10) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 14.092257ms)
  Jun 10 12:46:38.660: INFO: (10) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 14.201468ms)
  Jun 10 12:46:38.660: INFO: (10) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 14.667863ms)
  Jun 10 12:46:38.660: INFO: (10) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 14.975976ms)
  Jun 10 12:46:38.666: INFO: (11) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 5.700929ms)
  Jun 10 12:46:38.668: INFO: (11) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 6.850181ms)
  Jun 10 12:46:38.670: INFO: (11) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 8.751921ms)
  Jun 10 12:46:38.670: INFO: (11) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 9.388438ms)
  Jun 10 12:46:38.670: INFO: (11) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 9.517229ms)
  Jun 10 12:46:38.672: INFO: (11) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 10.941044ms)
  Jun 10 12:46:38.672: INFO: (11) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 11.363778ms)
  Jun 10 12:46:38.672: INFO: (11) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 11.042255ms)
  Jun 10 12:46:38.672: INFO: (11) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 11.122146ms)
  Jun 10 12:46:38.672: INFO: (11) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 11.338138ms)
  Jun 10 12:46:38.672: INFO: (11) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 11.209157ms)
  Jun 10 12:46:38.673: INFO: (11) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 11.834063ms)
  Jun 10 12:46:38.673: INFO: (11) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 12.406909ms)
  Jun 10 12:46:38.674: INFO: (11) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 12.51106ms)
  Jun 10 12:46:38.674: INFO: (11) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 13.536891ms)
  Jun 10 12:46:38.674: INFO: (11) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 13.216128ms)
  Jun 10 12:46:38.681: INFO: (12) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 6.73541ms)
  Jun 10 12:46:38.682: INFO: (12) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 8.002484ms)
  Jun 10 12:46:38.683: INFO: (12) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 8.755061ms)
  Jun 10 12:46:38.683: INFO: (12) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 8.773351ms)
  Jun 10 12:46:38.683: INFO: (12) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 9.132994ms)
  Jun 10 12:46:38.684: INFO: (12) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 9.138055ms)
  Jun 10 12:46:38.685: INFO: (12) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 10.161805ms)
  Jun 10 12:46:38.685: INFO: (12) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 10.537319ms)
  Jun 10 12:46:38.686: INFO: (12) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 11.455529ms)
  Jun 10 12:46:38.687: INFO: (12) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 12.094515ms)
  Jun 10 12:46:38.687: INFO: (12) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 12.144886ms)
  Jun 10 12:46:38.687: INFO: (12) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 12.882394ms)
  Jun 10 12:46:38.687: INFO: (12) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 12.739742ms)
  Jun 10 12:46:38.688: INFO: (12) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 13.114357ms)
  Jun 10 12:46:38.688: INFO: (12) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 13.141987ms)
  Jun 10 12:46:38.689: INFO: (12) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 13.851644ms)
  Jun 10 12:46:38.694: INFO: (13) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 5.703229ms)
  Jun 10 12:46:38.697: INFO: (13) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 7.938183ms)
  Jun 10 12:46:38.697: INFO: (13) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 8.375667ms)
  Jun 10 12:46:38.698: INFO: (13) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 9.472339ms)
  Jun 10 12:46:38.699: INFO: (13) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 10.225296ms)
  Jun 10 12:46:38.700: INFO: (13) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 11.346648ms)
  Jun 10 12:46:38.700: INFO: (13) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 11.002024ms)
  Jun 10 12:46:38.702: INFO: (13) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 12.236038ms)
  Jun 10 12:46:38.702: INFO: (13) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 12.41171ms)
  Jun 10 12:46:38.702: INFO: (13) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 12.841574ms)
  Jun 10 12:46:38.702: INFO: (13) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 13.234488ms)
  Jun 10 12:46:38.702: INFO: (13) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 13.4033ms)
  Jun 10 12:46:38.703: INFO: (13) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 14.208947ms)
  Jun 10 12:46:38.703: INFO: (13) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 14.369219ms)
  Jun 10 12:46:38.704: INFO: (13) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 14.761504ms)
  Jun 10 12:46:38.704: INFO: (13) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 14.785654ms)
  Jun 10 12:46:38.710: INFO: (14) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 5.74181ms)
  Jun 10 12:46:38.711: INFO: (14) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 6.248735ms)
  Jun 10 12:46:38.713: INFO: (14) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 8.326167ms)
  Jun 10 12:46:38.714: INFO: (14) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 9.036785ms)
  Jun 10 12:46:38.715: INFO: (14) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 10.809273ms)
  Jun 10 12:46:38.716: INFO: (14) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 10.809873ms)
  Jun 10 12:46:38.716: INFO: (14) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 11.306167ms)
  Jun 10 12:46:38.717: INFO: (14) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 11.948294ms)
  Jun 10 12:46:38.717: INFO: (14) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 12.52264ms)
  Jun 10 12:46:38.717: INFO: (14) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 12.320248ms)
  Jun 10 12:46:38.718: INFO: (14) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 13.371649ms)
  Jun 10 12:46:38.718: INFO: (14) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 13.302399ms)
  Jun 10 12:46:38.719: INFO: (14) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 13.987815ms)
  Jun 10 12:46:38.719: INFO: (14) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 14.758613ms)
  Jun 10 12:46:38.719: INFO: (14) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 14.701243ms)
  Jun 10 12:46:38.720: INFO: (14) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 15.502241ms)
  Jun 10 12:46:38.727: INFO: (15) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 6.68862ms)
  Jun 10 12:46:38.728: INFO: (15) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 7.359687ms)
  Jun 10 12:46:38.729: INFO: (15) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 7.894382ms)
  Jun 10 12:46:38.729: INFO: (15) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 8.308107ms)
  Jun 10 12:46:38.730: INFO: (15) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 10.088275ms)
  Jun 10 12:46:38.732: INFO: (15) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 10.483369ms)
  Jun 10 12:46:38.732: INFO: (15) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 11.233087ms)
  Jun 10 12:46:38.732: INFO: (15) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 11.088445ms)
  Jun 10 12:46:38.733: INFO: (15) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 11.979225ms)
  Jun 10 12:46:38.733: INFO: (15) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 12.173047ms)
  Jun 10 12:46:38.733: INFO: (15) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 12.329768ms)
  Jun 10 12:46:38.733: INFO: (15) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 12.239127ms)
  Jun 10 12:46:38.734: INFO: (15) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 12.725052ms)
  Jun 10 12:46:38.734: INFO: (15) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 13.094977ms)
  Jun 10 12:46:38.734: INFO: (15) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 13.608662ms)
  Jun 10 12:46:38.736: INFO: (15) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 15.468091ms)
  Jun 10 12:46:38.742: INFO: (16) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 5.76981ms)
  Jun 10 12:46:38.745: INFO: (16) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 7.73723ms)
  Jun 10 12:46:38.747: INFO: (16) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 10.112055ms)
  Jun 10 12:46:38.748: INFO: (16) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 10.333348ms)
  Jun 10 12:46:38.748: INFO: (16) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 10.602561ms)
  Jun 10 12:46:38.749: INFO: (16) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 11.53765ms)
  Jun 10 12:46:38.749: INFO: (16) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 11.642701ms)
  Jun 10 12:46:38.749: INFO: (16) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 11.951474ms)
  Jun 10 12:46:38.750: INFO: (16) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 13.200497ms)
  Jun 10 12:46:38.751: INFO: (16) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 13.899124ms)
  Jun 10 12:46:38.751: INFO: (16) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 13.5333ms)
  Jun 10 12:46:38.752: INFO: (16) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 14.576501ms)
  Jun 10 12:46:38.752: INFO: (16) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 14.810384ms)
  Jun 10 12:46:38.752: INFO: (16) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 14.632892ms)
  Jun 10 12:46:38.752: INFO: (16) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 15.047917ms)
  Jun 10 12:46:38.752: INFO: (16) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 15.662013ms)
  Jun 10 12:46:38.768: INFO: (17) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 15.811644ms)
  Jun 10 12:46:38.768: INFO: (17) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 15.639582ms)
  Jun 10 12:46:38.770: INFO: (17) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 17.111708ms)
  Jun 10 12:46:38.770: INFO: (17) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 17.188428ms)
  Jun 10 12:46:38.770: INFO: (17) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 17.298189ms)
  Jun 10 12:46:38.771: INFO: (17) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 18.143469ms)
  Jun 10 12:46:38.771: INFO: (17) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 17.746684ms)
  Jun 10 12:46:38.771: INFO: (17) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 18.530733ms)
  Jun 10 12:46:38.772: INFO: (17) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 19.051708ms)
  Jun 10 12:46:38.773: INFO: (17) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 20.397222ms)
  Jun 10 12:46:38.774: INFO: (17) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 21.039279ms)
  Jun 10 12:46:38.774: INFO: (17) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 20.966508ms)
  Jun 10 12:46:38.774: INFO: (17) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 21.156131ms)
  Jun 10 12:46:38.774: INFO: (17) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 20.857157ms)
  Jun 10 12:46:38.774: INFO: (17) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 21.097339ms)
  Jun 10 12:46:38.774: INFO: (17) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 21.628185ms)
  Jun 10 12:46:38.781: INFO: (18) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 5.84521ms)
  Jun 10 12:46:38.782: INFO: (18) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 7.345686ms)
  Jun 10 12:46:38.782: INFO: (18) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 7.525158ms)
  Jun 10 12:46:38.783: INFO: (18) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 8.411667ms)
  Jun 10 12:46:38.785: INFO: (18) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 10.064445ms)
  Jun 10 12:46:38.786: INFO: (18) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 11.010694ms)
  Jun 10 12:46:38.786: INFO: (18) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 11.55657ms)
  Jun 10 12:46:38.786: INFO: (18) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 11.394258ms)
  Jun 10 12:46:38.786: INFO: (18) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 11.157766ms)
  Jun 10 12:46:38.786: INFO: (18) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 11.476529ms)
  Jun 10 12:46:38.787: INFO: (18) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 12.110146ms)
  Jun 10 12:46:38.787: INFO: (18) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 12.202887ms)
  Jun 10 12:46:38.788: INFO: (18) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 12.760392ms)
  Jun 10 12:46:38.788: INFO: (18) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 13.41265ms)
  Jun 10 12:46:38.788: INFO: (18) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 13.690872ms)
  Jun 10 12:46:38.789: INFO: (18) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 13.904084ms)
  Jun 10 12:46:38.796: INFO: (19) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:160/proxy/: foo (200; 6.952463ms)
  Jun 10 12:46:38.797: INFO: (19) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:460/proxy/: tls baz (200; 8.071144ms)
  Jun 10 12:46:38.797: INFO: (19) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:160/proxy/: foo (200; 7.505608ms)
  Jun 10 12:46:38.798: INFO: (19) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:462/proxy/: tls qux (200; 8.842812ms)
  Jun 10 12:46:38.799: INFO: (19) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:1080/proxy/rewriteme">test<... (200; 9.829512ms)
  Jun 10 12:46:38.800: INFO: (19) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:162/proxy/: bar (200; 10.270217ms)
  Jun 10 12:46:38.800: INFO: (19) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname2/proxy/: tls qux (200; 11.167496ms)
  Jun 10 12:46:38.800: INFO: (19) /api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/https:proxy-service-c96g9-tslt7:443/proxy/tlsrewritem... (200; 11.041475ms)
  Jun 10 12:46:38.801: INFO: (19) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7/proxy/rewriteme">test</a> (200; 10.749282ms)
  Jun 10 12:46:38.801: INFO: (19) /api/v1/namespaces/proxy-9378/pods/proxy-service-c96g9-tslt7:162/proxy/: bar (200; 11.348488ms)
  Jun 10 12:46:38.801: INFO: (19) /api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-9378/pods/http:proxy-service-c96g9-tslt7:1080/proxy/rewriteme">... (200; 12.211147ms)
  Jun 10 12:46:38.802: INFO: (19) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname1/proxy/: foo (200; 12.589501ms)
  Jun 10 12:46:38.802: INFO: (19) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname2/proxy/: bar (200; 12.934105ms)
  Jun 10 12:46:38.803: INFO: (19) /api/v1/namespaces/proxy-9378/services/http:proxy-service-c96g9:portname2/proxy/: bar (200; 13.034146ms)
  Jun 10 12:46:38.803: INFO: (19) /api/v1/namespaces/proxy-9378/services/proxy-service-c96g9:portname1/proxy/: foo (200; 13.193778ms)
  Jun 10 12:46:38.803: INFO: (19) /api/v1/namespaces/proxy-9378/services/https:proxy-service-c96g9:tlsportname1/proxy/: tls baz (200; 13.363989ms)
  Jun 10 12:46:38.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-c96g9 in namespace proxy-9378, will wait for the garbage collector to delete the pods @ 06/10/23 12:46:38.808
  Jun 10 12:46:38.870: INFO: Deleting ReplicationController proxy-service-c96g9 took: 7.515688ms
  Jun 10 12:46:38.971: INFO: Terminating ReplicationController proxy-service-c96g9 pods took: 100.616936ms
  STEP: Destroying namespace "proxy-9378" for this suite. @ 06/10/23 12:46:40.472
• [5.324 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 06/10/23 12:46:40.481
  Jun 10 12:46:40.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 12:46:40.482
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:46:40.5
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:46:40.508
  STEP: Creating secret with name s-test-opt-del-f9627a45-2add-412c-a4e2-f2784c3e82cf @ 06/10/23 12:46:40.52
  STEP: Creating secret with name s-test-opt-upd-1204a54c-4b07-42d5-868d-2638237d78ff @ 06/10/23 12:46:40.528
  STEP: Creating the pod @ 06/10/23 12:46:40.535
  STEP: Deleting secret s-test-opt-del-f9627a45-2add-412c-a4e2-f2784c3e82cf @ 06/10/23 12:46:42.588
  STEP: Updating secret s-test-opt-upd-1204a54c-4b07-42d5-868d-2638237d78ff @ 06/10/23 12:46:42.597
  STEP: Creating secret with name s-test-opt-create-bf7963ef-3b74-4432-a8c4-cdbb626034ad @ 06/10/23 12:46:42.609
  STEP: waiting to observe update in volume @ 06/10/23 12:46:42.615
  Jun 10 12:46:46.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1756" for this suite. @ 06/10/23 12:46:46.661
• [6.189 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 06/10/23 12:46:46.672
  Jun 10 12:46:46.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename runtimeclass @ 06/10/23 12:46:46.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:46:46.688
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:46:46.694
  STEP: Deleting RuntimeClass runtimeclass-7831-delete-me @ 06/10/23 12:46:46.703
  STEP: Waiting for the RuntimeClass to disappear @ 06/10/23 12:46:46.711
  Jun 10 12:46:46.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7831" for this suite. @ 06/10/23 12:46:46.728
• [0.063 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 06/10/23 12:46:46.74
  Jun 10 12:46:46.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename pod-network-test @ 06/10/23 12:46:46.741
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:46:46.773
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:46:46.777
  STEP: Performing setup for networking test in namespace pod-network-test-1610 @ 06/10/23 12:46:46.781
  STEP: creating a selector @ 06/10/23 12:46:46.781
  STEP: Creating the service pods in kubernetes @ 06/10/23 12:46:46.781
  Jun 10 12:46:46.781: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 06/10/23 12:46:58.879
  Jun 10 12:47:00.914: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jun 10 12:47:00.914: INFO: Going to poll 192.168.109.43 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jun 10 12:47:00.918: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.109.43:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1610 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 12:47:00.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:47:00.919: INFO: ExecWithOptions: Clientset creation
  Jun 10 12:47:00.919: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1610/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.109.43%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun 10 12:47:01.006: INFO: Found all 1 expected endpoints: [netserver-0]
  Jun 10 12:47:01.006: INFO: Going to poll 192.168.92.33 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jun 10 12:47:01.010: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.92.33:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1610 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 12:47:01.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:47:01.011: INFO: ExecWithOptions: Clientset creation
  Jun 10 12:47:01.011: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1610/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.92.33%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun 10 12:47:01.115: INFO: Found all 1 expected endpoints: [netserver-1]
  Jun 10 12:47:01.115: INFO: Going to poll 192.168.149.121 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jun 10 12:47:01.120: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.149.121:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1610 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 12:47:01.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 12:47:01.121: INFO: ExecWithOptions: Clientset creation
  Jun 10 12:47:01.121: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1610/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.149.121%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun 10 12:47:01.202: INFO: Found all 1 expected endpoints: [netserver-2]
  Jun 10 12:47:01.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1610" for this suite. @ 06/10/23 12:47:01.208
• [14.476 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 06/10/23 12:47:01.22
  Jun 10 12:47:01.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename replication-controller @ 06/10/23 12:47:01.221
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:47:01.238
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:47:01.248
  STEP: Given a ReplicationController is created @ 06/10/23 12:47:01.252
  STEP: When the matched label of one of its pods change @ 06/10/23 12:47:01.26
  Jun 10 12:47:01.264: INFO: Pod name pod-release: Found 0 pods out of 1
  Jun 10 12:47:06.273: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 06/10/23 12:47:06.288
  Jun 10 12:47:07.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9099" for this suite. @ 06/10/23 12:47:07.302
• [6.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 06/10/23 12:47:07.317
  Jun 10 12:47:07.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename configmap @ 06/10/23 12:47:07.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:47:07.337
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:47:07.342
  STEP: Creating configMap that has name configmap-test-emptyKey-d6c01323-b543-4d90-bd7d-7b3eedf4021a @ 06/10/23 12:47:07.346
  Jun 10 12:47:07.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9701" for this suite. @ 06/10/23 12:47:07.353
• [0.044 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 06/10/23 12:47:07.363
  Jun 10 12:47:07.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 12:47:07.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:47:07.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:47:07.387
  STEP: Creating the pod @ 06/10/23 12:47:07.393
  Jun 10 12:47:09.966: INFO: Successfully updated pod "labelsupdate73fb9fb9-0edc-40e3-88c9-fbbb6f3bfbf1"
  Jun 10 12:47:11.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7984" for this suite. @ 06/10/23 12:47:11.987
• [4.631 seconds]
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 06/10/23 12:47:11.995
  Jun 10 12:47:11.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 12:47:11.996
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:47:12.015
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:47:12.019
  STEP: Creating configMap with name cm-test-opt-del-a42b1ad4-5b2c-415f-b410-b6be1c8e71fe @ 06/10/23 12:47:12.029
  STEP: Creating configMap with name cm-test-opt-upd-4ce4fd74-546e-4cdd-ad17-e3fad3757f00 @ 06/10/23 12:47:12.035
  STEP: Creating the pod @ 06/10/23 12:47:12.04
  STEP: Deleting configmap cm-test-opt-del-a42b1ad4-5b2c-415f-b410-b6be1c8e71fe @ 06/10/23 12:47:14.094
  STEP: Updating configmap cm-test-opt-upd-4ce4fd74-546e-4cdd-ad17-e3fad3757f00 @ 06/10/23 12:47:14.103
  STEP: Creating configMap with name cm-test-opt-create-480ad989-3582-41ab-a320-db7dae6ad30a @ 06/10/23 12:47:14.108
  STEP: waiting to observe update in volume @ 06/10/23 12:47:14.113
  Jun 10 12:48:30.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3720" for this suite. @ 06/10/23 12:48:30.539
• [78.551 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 06/10/23 12:48:30.546
  Jun 10 12:48:30.546: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 12:48:30.547
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:48:30.567
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:48:30.571
  STEP: Setting up server cert @ 06/10/23 12:48:30.601
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 12:48:31.089
  STEP: Deploying the webhook pod @ 06/10/23 12:48:31.098
  STEP: Wait for the deployment to be ready @ 06/10/23 12:48:31.115
  Jun 10 12:48:31.122: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 06/10/23 12:48:33.135
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 12:48:33.148
  Jun 10 12:48:34.148: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 06/10/23 12:48:34.153
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 06/10/23 12:48:34.175
  STEP: Creating a dummy validating-webhook-configuration object @ 06/10/23 12:48:34.193
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 06/10/23 12:48:34.203
  STEP: Creating a dummy mutating-webhook-configuration object @ 06/10/23 12:48:34.211
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 06/10/23 12:48:34.222
  Jun 10 12:48:34.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7144" for this suite. @ 06/10/23 12:48:34.306
  STEP: Destroying namespace "webhook-markers-5686" for this suite. @ 06/10/23 12:48:34.314
• [3.776 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 06/10/23 12:48:34.326
  Jun 10 12:48:34.326: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl @ 06/10/23 12:48:34.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:48:34.347
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:48:34.351
  STEP: validating cluster-info @ 06/10/23 12:48:34.355
  Jun 10 12:48:34.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-3924 cluster-info'
  Jun 10 12:48:34.435: INFO: stderr: ""
  Jun 10 12:48:34.435: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Jun 10 12:48:34.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3924" for this suite. @ 06/10/23 12:48:34.439
• [0.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 06/10/23 12:48:34.451
  Jun 10 12:48:34.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename discovery @ 06/10/23 12:48:34.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:48:34.468
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:48:34.473
  STEP: Setting up server cert @ 06/10/23 12:48:34.479
  Jun 10 12:48:35.027: INFO: Checking APIGroup: apiregistration.k8s.io
  Jun 10 12:48:35.028: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Jun 10 12:48:35.028: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Jun 10 12:48:35.028: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Jun 10 12:48:35.028: INFO: Checking APIGroup: apps
  Jun 10 12:48:35.030: INFO: PreferredVersion.GroupVersion: apps/v1
  Jun 10 12:48:35.030: INFO: Versions found [{apps/v1 v1}]
  Jun 10 12:48:35.030: INFO: apps/v1 matches apps/v1
  Jun 10 12:48:35.030: INFO: Checking APIGroup: events.k8s.io
  Jun 10 12:48:35.031: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Jun 10 12:48:35.031: INFO: Versions found [{events.k8s.io/v1 v1}]
  Jun 10 12:48:35.031: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Jun 10 12:48:35.031: INFO: Checking APIGroup: authentication.k8s.io
  Jun 10 12:48:35.032: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Jun 10 12:48:35.033: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Jun 10 12:48:35.033: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Jun 10 12:48:35.033: INFO: Checking APIGroup: authorization.k8s.io
  Jun 10 12:48:35.034: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Jun 10 12:48:35.034: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Jun 10 12:48:35.034: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Jun 10 12:48:35.034: INFO: Checking APIGroup: autoscaling
  Jun 10 12:48:35.035: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Jun 10 12:48:35.036: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Jun 10 12:48:35.036: INFO: autoscaling/v2 matches autoscaling/v2
  Jun 10 12:48:35.036: INFO: Checking APIGroup: batch
  Jun 10 12:48:35.037: INFO: PreferredVersion.GroupVersion: batch/v1
  Jun 10 12:48:35.037: INFO: Versions found [{batch/v1 v1}]
  Jun 10 12:48:35.037: INFO: batch/v1 matches batch/v1
  Jun 10 12:48:35.037: INFO: Checking APIGroup: certificates.k8s.io
  Jun 10 12:48:35.039: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Jun 10 12:48:35.039: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Jun 10 12:48:35.039: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Jun 10 12:48:35.039: INFO: Checking APIGroup: networking.k8s.io
  Jun 10 12:48:35.040: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Jun 10 12:48:35.040: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Jun 10 12:48:35.040: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Jun 10 12:48:35.040: INFO: Checking APIGroup: policy
  Jun 10 12:48:35.042: INFO: PreferredVersion.GroupVersion: policy/v1
  Jun 10 12:48:35.042: INFO: Versions found [{policy/v1 v1}]
  Jun 10 12:48:35.042: INFO: policy/v1 matches policy/v1
  Jun 10 12:48:35.042: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Jun 10 12:48:35.043: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Jun 10 12:48:35.043: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Jun 10 12:48:35.043: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Jun 10 12:48:35.043: INFO: Checking APIGroup: storage.k8s.io
  Jun 10 12:48:35.044: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Jun 10 12:48:35.044: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Jun 10 12:48:35.044: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Jun 10 12:48:35.044: INFO: Checking APIGroup: admissionregistration.k8s.io
  Jun 10 12:48:35.045: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Jun 10 12:48:35.045: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Jun 10 12:48:35.045: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Jun 10 12:48:35.045: INFO: Checking APIGroup: apiextensions.k8s.io
  Jun 10 12:48:35.046: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Jun 10 12:48:35.046: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Jun 10 12:48:35.046: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Jun 10 12:48:35.047: INFO: Checking APIGroup: scheduling.k8s.io
  Jun 10 12:48:35.048: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Jun 10 12:48:35.048: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Jun 10 12:48:35.048: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Jun 10 12:48:35.048: INFO: Checking APIGroup: coordination.k8s.io
  Jun 10 12:48:35.049: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Jun 10 12:48:35.049: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Jun 10 12:48:35.049: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Jun 10 12:48:35.049: INFO: Checking APIGroup: node.k8s.io
  Jun 10 12:48:35.051: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Jun 10 12:48:35.051: INFO: Versions found [{node.k8s.io/v1 v1}]
  Jun 10 12:48:35.051: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Jun 10 12:48:35.051: INFO: Checking APIGroup: discovery.k8s.io
  Jun 10 12:48:35.052: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Jun 10 12:48:35.052: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Jun 10 12:48:35.052: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Jun 10 12:48:35.052: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Jun 10 12:48:35.054: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Jun 10 12:48:35.054: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Jun 10 12:48:35.054: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Jun 10 12:48:35.054: INFO: Checking APIGroup: metrics.k8s.io
  Jun 10 12:48:35.056: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Jun 10 12:48:35.056: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Jun 10 12:48:35.056: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Jun 10 12:48:35.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-5149" for this suite. @ 06/10/23 12:48:35.061
• [0.618 seconds]
------------------------------
SS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 06/10/23 12:48:35.07
  Jun 10 12:48:35.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename ingressclass @ 06/10/23 12:48:35.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:48:35.089
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:48:35.093
  STEP: getting /apis @ 06/10/23 12:48:35.097
  STEP: getting /apis/networking.k8s.io @ 06/10/23 12:48:35.102
  STEP: getting /apis/networking.k8s.iov1 @ 06/10/23 12:48:35.104
  STEP: creating @ 06/10/23 12:48:35.105
  STEP: getting @ 06/10/23 12:48:35.135
  STEP: listing @ 06/10/23 12:48:35.138
  STEP: watching @ 06/10/23 12:48:35.142
  Jun 10 12:48:35.142: INFO: starting watch
  STEP: patching @ 06/10/23 12:48:35.144
  STEP: updating @ 06/10/23 12:48:35.15
  Jun 10 12:48:35.156: INFO: waiting for watch events with expected annotations
  Jun 10 12:48:35.156: INFO: saw patched and updated annotations
  STEP: deleting @ 06/10/23 12:48:35.157
  STEP: deleting a collection @ 06/10/23 12:48:35.172
  Jun 10 12:48:35.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-279" for this suite. @ 06/10/23 12:48:35.205
• [0.143 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 06/10/23 12:48:35.213
  Jun 10 12:48:35.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename downward-api @ 06/10/23 12:48:35.214
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:48:35.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:48:35.235
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 12:48:35.239
  STEP: Saw pod success @ 06/10/23 12:48:39.273
  Jun 10 12:48:39.280: INFO: Trying to get logs from node ip-172-31-89-0 pod downwardapi-volume-9eefb371-d87d-48d4-93d6-d551e428ca10 container client-container: <nil>
  STEP: delete the pod @ 06/10/23 12:48:39.289
  Jun 10 12:48:39.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2234" for this suite. @ 06/10/23 12:48:39.313
• [4.108 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 06/10/23 12:48:39.322
  Jun 10 12:48:39.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename podtemplate @ 06/10/23 12:48:39.323
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:48:39.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:48:39.358
  STEP: Create set of pod templates @ 06/10/23 12:48:39.364
  Jun 10 12:48:39.371: INFO: created test-podtemplate-1
  Jun 10 12:48:39.384: INFO: created test-podtemplate-2
  Jun 10 12:48:39.396: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 06/10/23 12:48:39.396
  STEP: delete collection of pod templates @ 06/10/23 12:48:39.4
  Jun 10 12:48:39.400: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 06/10/23 12:48:39.433
  Jun 10 12:48:39.433: INFO: requesting list of pod templates to confirm quantity
  Jun 10 12:48:39.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4046" for this suite. @ 06/10/23 12:48:39.449
• [0.144 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 06/10/23 12:48:39.467
  Jun 10 12:48:39.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename gc @ 06/10/23 12:48:39.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:48:39.506
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:48:39.521
  Jun 10 12:48:39.585: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d09b26a2-0741-4a3c-bf97-1eb260fe6c6f", Controller:(*bool)(0xc0036252ae), BlockOwnerDeletion:(*bool)(0xc0036252af)}}
  Jun 10 12:48:39.604: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"c3ef8746-2632-4f1d-9b93-71bffa1c9d8d", Controller:(*bool)(0xc0036254e6), BlockOwnerDeletion:(*bool)(0xc0036254e7)}}
  Jun 10 12:48:39.622: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"153242ba-6980-4e6f-8ca9-c590da2e722f", Controller:(*bool)(0xc006907466), BlockOwnerDeletion:(*bool)(0xc006907467)}}
  Jun 10 12:48:44.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5163" for this suite. @ 06/10/23 12:48:44.647
• [5.188 seconds]
------------------------------
S
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 06/10/23 12:48:44.656
  Jun 10 12:48:44.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename secrets @ 06/10/23 12:48:44.657
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:48:44.683
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:48:44.688
  STEP: Creating secret with name secret-test-46717e00-cb07-4f9a-824b-e0af8c27dc53 @ 06/10/23 12:48:44.693
  STEP: Creating a pod to test consume secrets @ 06/10/23 12:48:44.7
  STEP: Saw pod success @ 06/10/23 12:48:48.73
  Jun 10 12:48:48.734: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-secrets-cf6de791-b0aa-4d9d-aba1-85d9e81d1083 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/10/23 12:48:48.746
  Jun 10 12:48:48.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7718" for this suite. @ 06/10/23 12:48:48.773
• [4.125 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 06/10/23 12:48:48.782
  Jun 10 12:48:48.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir @ 06/10/23 12:48:48.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:48:48.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:48:48.807
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 06/10/23 12:48:48.811
  STEP: Saw pod success @ 06/10/23 12:48:52.838
  Jun 10 12:48:52.842: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-2bd5a7ab-9f8f-43e1-83c6-4640b46d78dc container test-container: <nil>
  STEP: delete the pod @ 06/10/23 12:48:52.85
  Jun 10 12:48:52.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6038" for this suite. @ 06/10/23 12:48:52.872
• [4.097 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 06/10/23 12:48:52.88
  Jun 10 12:48:52.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir @ 06/10/23 12:48:52.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:48:52.898
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:48:52.903
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 06/10/23 12:48:52.908
  STEP: Saw pod success @ 06/10/23 12:48:56.936
  Jun 10 12:48:56.940: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-01e001aa-5801-4f0e-ab91-82001f0ec0c7 container test-container: <nil>
  STEP: delete the pod @ 06/10/23 12:48:56.949
  Jun 10 12:48:56.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1882" for this suite. @ 06/10/23 12:48:56.971
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 06/10/23 12:48:56.981
  Jun 10 12:48:56.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename endpointslice @ 06/10/23 12:48:56.982
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:48:57.008
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:48:57.012
  Jun 10 12:49:01.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-3279" for this suite. @ 06/10/23 12:49:01.098
• [4.126 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 06/10/23 12:49:01.108
  Jun 10 12:49:01.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename resourcequota @ 06/10/23 12:49:01.109
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:49:01.131
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:49:01.136
  STEP: Creating a ResourceQuota with terminating scope @ 06/10/23 12:49:01.14
  STEP: Ensuring ResourceQuota status is calculated @ 06/10/23 12:49:01.147
  STEP: Creating a ResourceQuota with not terminating scope @ 06/10/23 12:49:03.152
  STEP: Ensuring ResourceQuota status is calculated @ 06/10/23 12:49:03.158
  STEP: Creating a long running pod @ 06/10/23 12:49:05.162
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 06/10/23 12:49:05.176
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 06/10/23 12:49:07.181
  STEP: Deleting the pod @ 06/10/23 12:49:09.185
  STEP: Ensuring resource quota status released the pod usage @ 06/10/23 12:49:09.2
  STEP: Creating a terminating pod @ 06/10/23 12:49:11.205
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 06/10/23 12:49:11.219
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 06/10/23 12:49:13.224
  STEP: Deleting the pod @ 06/10/23 12:49:15.228
  STEP: Ensuring resource quota status released the pod usage @ 06/10/23 12:49:15.247
  Jun 10 12:49:17.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3178" for this suite. @ 06/10/23 12:49:17.257
• [16.157 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 06/10/23 12:49:17.269
  Jun 10 12:49:17.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename services @ 06/10/23 12:49:17.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:49:17.289
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:49:17.293
  STEP: creating a collection of services @ 06/10/23 12:49:17.299
  Jun 10 12:49:17.299: INFO: Creating e2e-svc-a-jr8v4
  Jun 10 12:49:17.310: INFO: Creating e2e-svc-b-pc6c7
  Jun 10 12:49:17.322: INFO: Creating e2e-svc-c-gnj9x
  STEP: deleting service collection @ 06/10/23 12:49:17.341
  Jun 10 12:49:17.379: INFO: Collection of services has been deleted
  Jun 10 12:49:17.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4479" for this suite. @ 06/10/23 12:49:17.384
• [0.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 06/10/23 12:49:17.398
  Jun 10 12:49:17.398: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename containers @ 06/10/23 12:49:17.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:49:17.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:49:17.421
  STEP: Creating a pod to test override all @ 06/10/23 12:49:17.425
  STEP: Saw pod success @ 06/10/23 12:49:21.451
  Jun 10 12:49:21.455: INFO: Trying to get logs from node ip-172-31-27-177 pod client-containers-df6d68ca-61d4-4c05-9e2f-21e6cb401a2f container agnhost-container: <nil>
  STEP: delete the pod @ 06/10/23 12:49:21.463
  Jun 10 12:49:21.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-1487" for this suite. @ 06/10/23 12:49:21.488
• [4.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 06/10/23 12:49:21.501
  Jun 10 12:49:21.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename configmap @ 06/10/23 12:49:21.502
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:49:21.52
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:49:21.524
  STEP: Creating configMap with name cm-test-opt-del-a53ab187-5885-4fc4-8dbe-248bf0effafc @ 06/10/23 12:49:21.533
  STEP: Creating configMap with name cm-test-opt-upd-13d5f458-02e8-47db-be58-8d766330bec2 @ 06/10/23 12:49:21.54
  STEP: Creating the pod @ 06/10/23 12:49:21.546
  STEP: Deleting configmap cm-test-opt-del-a53ab187-5885-4fc4-8dbe-248bf0effafc @ 06/10/23 12:49:23.602
  STEP: Updating configmap cm-test-opt-upd-13d5f458-02e8-47db-be58-8d766330bec2 @ 06/10/23 12:49:23.609
  STEP: Creating configMap with name cm-test-opt-create-c05ec5aa-42ed-4f7b-9584-152bdc766ee4 @ 06/10/23 12:49:23.618
  STEP: waiting to observe update in volume @ 06/10/23 12:49:23.624
  Jun 10 12:50:36.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4319" for this suite. @ 06/10/23 12:50:36.007
• [74.513 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 06/10/23 12:50:36.015
  Jun 10 12:50:36.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename configmap @ 06/10/23 12:50:36.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:50:36.039
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:50:36.043
  STEP: Creating configMap with name configmap-test-volume-map-261c5570-b768-4d9c-843e-66ad463e2d8d @ 06/10/23 12:50:36.047
  STEP: Creating a pod to test consume configMaps @ 06/10/23 12:50:36.053
  STEP: Saw pod success @ 06/10/23 12:50:40.083
  Jun 10 12:50:40.088: INFO: Trying to get logs from node ip-172-31-89-0 pod pod-configmaps-25217251-f849-4fe1-8687-f1eda6377276 container agnhost-container: <nil>
  STEP: delete the pod @ 06/10/23 12:50:40.112
  Jun 10 12:50:40.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2436" for this suite. @ 06/10/23 12:50:40.14
• [4.135 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 06/10/23 12:50:40.15
  Jun 10 12:50:40.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename aggregator @ 06/10/23 12:50:40.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:50:40.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:50:40.192
  Jun 10 12:50:40.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Registering the sample API server. @ 06/10/23 12:50:40.198
  Jun 10 12:50:40.542: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Jun 10 12:50:40.599: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  Jun 10 12:50:42.670: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 10 12:50:44.678: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 10 12:50:46.677: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 10 12:50:48.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 10 12:50:50.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 10 12:50:52.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 10 12:50:54.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 10 12:50:56.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 10 12:50:58.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 10 12:51:00.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 10 12:51:02.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 12, 50, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 10 12:51:04.800: INFO: Waited 116.744726ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 06/10/23 12:51:04.854
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 06/10/23 12:51:04.859
  STEP: List APIServices @ 06/10/23 12:51:04.865
  Jun 10 12:51:04.873: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 06/10/23 12:51:04.873
  Jun 10 12:51:04.889: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 06/10/23 12:51:04.889
  Jun 10 12:51:04.903: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.June, 10, 12, 51, 4, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 06/10/23 12:51:04.905
  Jun 10 12:51:04.909: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-06-10 12:51:04 +0000 UTC Passed all checks passed}
  Jun 10 12:51:04.909: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 10 12:51:04.909: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 06/10/23 12:51:04.909
  Jun 10 12:51:04.926: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-149917022" @ 06/10/23 12:51:04.927
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 06/10/23 12:51:04.95
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 06/10/23 12:51:04.96
  STEP: Patch APIService Status @ 06/10/23 12:51:04.965
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 06/10/23 12:51:04.977
  Jun 10 12:51:04.982: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-06-10 12:51:04 +0000 UTC Passed all checks passed}
  Jun 10 12:51:04.982: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 10 12:51:04.982: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Jun 10 12:51:04.982: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 06/10/23 12:51:04.983
  STEP: Confirm that the generated APIService has been deleted @ 06/10/23 12:51:04.989
  Jun 10 12:51:04.989: INFO: Requesting list of APIServices to confirm quantity
  Jun 10 12:51:04.995: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Jun 10 12:51:04.995: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Jun 10 12:51:05.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-1990" for this suite. @ 06/10/23 12:51:05.152
• [25.009 seconds]
------------------------------
S
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 06/10/23 12:51:05.16
  Jun 10 12:51:05.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename services @ 06/10/23 12:51:05.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:51:05.181
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:51:05.193
  STEP: fetching services @ 06/10/23 12:51:05.197
  Jun 10 12:51:05.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-816" for this suite. @ 06/10/23 12:51:05.208
• [0.056 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 06/10/23 12:51:05.218
  Jun 10 12:51:05.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename secrets @ 06/10/23 12:51:05.219
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:51:05.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:51:05.243
  STEP: Creating secret with name secret-test-map-16062782-7717-4c7c-a34c-b4d7499000f7 @ 06/10/23 12:51:05.247
  STEP: Creating a pod to test consume secrets @ 06/10/23 12:51:05.256
  STEP: Saw pod success @ 06/10/23 12:51:09.283
  Jun 10 12:51:09.288: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-secrets-0a08ed1d-bd93-48f7-a4ab-00424fa9e66b container secret-volume-test: <nil>
  STEP: delete the pod @ 06/10/23 12:51:09.296
  Jun 10 12:51:09.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3572" for this suite. @ 06/10/23 12:51:09.316
• [4.106 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 06/10/23 12:51:09.324
  Jun 10 12:51:09.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename secrets @ 06/10/23 12:51:09.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:51:09.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:51:09.347
  STEP: creating a secret @ 06/10/23 12:51:09.351
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 06/10/23 12:51:09.356
  STEP: patching the secret @ 06/10/23 12:51:09.36
  STEP: deleting the secret using a LabelSelector @ 06/10/23 12:51:09.372
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 06/10/23 12:51:09.381
  Jun 10 12:51:09.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2387" for this suite. @ 06/10/23 12:51:09.391
• [0.075 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 06/10/23 12:51:09.4
  Jun 10 12:51:09.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename namespaces @ 06/10/23 12:51:09.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:51:09.423
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:51:09.427
  STEP: creating a Namespace @ 06/10/23 12:51:09.431
  STEP: patching the Namespace @ 06/10/23 12:51:09.448
  STEP: get the Namespace and ensuring it has the label @ 06/10/23 12:51:09.456
  Jun 10 12:51:09.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6957" for this suite. @ 06/10/23 12:51:09.466
  STEP: Destroying namespace "nspatchtest-f800bcca-1203-4b12-9167-c971e0a01bbc-4542" for this suite. @ 06/10/23 12:51:09.474
• [0.082 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 06/10/23 12:51:09.483
  Jun 10 12:51:09.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-runtime @ 06/10/23 12:51:09.484
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:51:09.503
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:51:09.508
  STEP: create the container @ 06/10/23 12:51:09.512
  W0610 12:51:09.523931      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 06/10/23 12:51:09.524
  STEP: get the container status @ 06/10/23 12:51:12.547
  STEP: the container should be terminated @ 06/10/23 12:51:12.551
  STEP: the termination message should be set @ 06/10/23 12:51:12.551
  Jun 10 12:51:12.551: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 06/10/23 12:51:12.552
  Jun 10 12:51:12.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8945" for this suite. @ 06/10/23 12:51:12.575
• [3.099 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 06/10/23 12:51:12.583
  Jun 10 12:51:12.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubelet-test @ 06/10/23 12:51:12.585
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:51:12.602
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:51:12.607
  Jun 10 12:51:16.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2703" for this suite. @ 06/10/23 12:51:16.637
• [4.062 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 06/10/23 12:51:16.647
  Jun 10 12:51:16.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename field-validation @ 06/10/23 12:51:16.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:51:16.67
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:51:16.675
  Jun 10 12:51:16.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  W0610 12:51:16.680714      18 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc000bb6060 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0610 12:51:19.260899      18 warnings.go:70] unknown field "alpha"
  W0610 12:51:19.261055      18 warnings.go:70] unknown field "beta"
  W0610 12:51:19.261188      18 warnings.go:70] unknown field "delta"
  W0610 12:51:19.261277      18 warnings.go:70] unknown field "epsilon"
  W0610 12:51:19.261368      18 warnings.go:70] unknown field "gamma"
  Jun 10 12:51:19.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1005" for this suite. @ 06/10/23 12:51:19.311
• [2.673 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 06/10/23 12:51:19.324
  Jun 10 12:51:19.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename crd-webhook @ 06/10/23 12:51:19.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:51:19.342
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:51:19.357
  STEP: Setting up server cert @ 06/10/23 12:51:19.366
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 06/10/23 12:51:19.859
  STEP: Deploying the custom resource conversion webhook pod @ 06/10/23 12:51:19.868
  STEP: Wait for the deployment to be ready @ 06/10/23 12:51:19.882
  Jun 10 12:51:19.899: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/10/23 12:51:21.913
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 12:51:21.923
  Jun 10 12:51:22.923: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jun 10 12:51:22.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Creating a v1 custom resource @ 06/10/23 12:51:25.523
  STEP: Create a v2 custom resource @ 06/10/23 12:51:25.543
  STEP: List CRs in v1 @ 06/10/23 12:51:25.604
  STEP: List CRs in v2 @ 06/10/23 12:51:25.61
  Jun 10 12:51:25.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-3389" for this suite. @ 06/10/23 12:51:26.2
• [6.885 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 06/10/23 12:51:26.209
  Jun 10 12:51:26.209: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 12:51:26.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:51:26.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:51:26.236
  STEP: Setting up server cert @ 06/10/23 12:51:26.265
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 12:51:26.797
  STEP: Deploying the webhook pod @ 06/10/23 12:51:26.804
  STEP: Wait for the deployment to be ready @ 06/10/23 12:51:26.818
  Jun 10 12:51:26.831: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/10/23 12:51:28.842
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 12:51:28.86
  Jun 10 12:51:29.860: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 06/10/23 12:51:29.865
  STEP: Registering slow webhook via the AdmissionRegistration API @ 06/10/23 12:51:29.865
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 06/10/23 12:51:29.883
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 06/10/23 12:51:30.896
  STEP: Registering slow webhook via the AdmissionRegistration API @ 06/10/23 12:51:30.897
  STEP: Having no error when timeout is longer than webhook latency @ 06/10/23 12:51:31.932
  STEP: Registering slow webhook via the AdmissionRegistration API @ 06/10/23 12:51:31.932
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 06/10/23 12:51:36.975
  STEP: Registering slow webhook via the AdmissionRegistration API @ 06/10/23 12:51:36.975
  Jun 10 12:51:42.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9678" for this suite. @ 06/10/23 12:51:42.094
  STEP: Destroying namespace "webhook-markers-3807" for this suite. @ 06/10/23 12:51:42.103
• [15.906 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 06/10/23 12:51:42.119
  Jun 10 12:51:42.119: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename prestop @ 06/10/23 12:51:42.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:51:42.139
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:51:42.143
  STEP: Creating server pod server in namespace prestop-2102 @ 06/10/23 12:51:42.148
  STEP: Waiting for pods to come up. @ 06/10/23 12:51:42.157
  STEP: Creating tester pod tester in namespace prestop-2102 @ 06/10/23 12:51:44.174
  STEP: Deleting pre-stop pod @ 06/10/23 12:51:46.195
  Jun 10 12:51:51.211: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Jun 10 12:51:51.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 06/10/23 12:51:51.218
  STEP: Destroying namespace "prestop-2102" for this suite. @ 06/10/23 12:51:51.231
• [9.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 06/10/23 12:51:51.239
  Jun 10 12:51:51.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 06/10/23 12:51:51.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:51:51.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:51:51.263
  STEP: create the container to handle the HTTPGet hook request. @ 06/10/23 12:51:51.272
  STEP: create the pod with lifecycle hook @ 06/10/23 12:51:53.297
  STEP: check poststart hook @ 06/10/23 12:51:55.315
  STEP: delete the pod with lifecycle hook @ 06/10/23 12:51:55.342
  Jun 10 12:51:57.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-5145" for this suite. @ 06/10/23 12:51:57.364
• [6.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 06/10/23 12:51:57.376
  Jun 10 12:51:57.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename configmap @ 06/10/23 12:51:57.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:51:57.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:51:57.4
  STEP: Creating configMap with name configmap-test-volume-33677bc5-7f58-40f4-bcb6-74d436fa4fa0 @ 06/10/23 12:51:57.404
  STEP: Creating a pod to test consume configMaps @ 06/10/23 12:51:57.41
  STEP: Saw pod success @ 06/10/23 12:52:01.44
  Jun 10 12:52:01.444: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-configmaps-aeea72f3-b63d-40ab-bbb0-3b8bf74b31e1 container agnhost-container: <nil>
  STEP: delete the pod @ 06/10/23 12:52:01.452
  Jun 10 12:52:01.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8512" for this suite. @ 06/10/23 12:52:01.477
• [4.109 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 06/10/23 12:52:01.487
  Jun 10 12:52:01.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename statefulset @ 06/10/23 12:52:01.488
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:52:01.577
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:52:01.582
  STEP: Creating service test in namespace statefulset-4077 @ 06/10/23 12:52:01.587
  STEP: Creating statefulset ss in namespace statefulset-4077 @ 06/10/23 12:52:01.603
  Jun 10 12:52:01.618: INFO: Found 0 stateful pods, waiting for 1
  Jun 10 12:52:11.627: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 06/10/23 12:52:11.635
  STEP: Getting /status @ 06/10/23 12:52:11.647
  Jun 10 12:52:11.652: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 06/10/23 12:52:11.652
  Jun 10 12:52:11.664: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 06/10/23 12:52:11.664
  Jun 10 12:52:11.667: INFO: Observed &StatefulSet event: ADDED
  Jun 10 12:52:11.667: INFO: Found Statefulset ss in namespace statefulset-4077 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 10 12:52:11.667: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 06/10/23 12:52:11.667
  Jun 10 12:52:11.668: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jun 10 12:52:11.676: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 06/10/23 12:52:11.676
  Jun 10 12:52:11.678: INFO: Observed &StatefulSet event: ADDED
  Jun 10 12:52:11.679: INFO: Deleting all statefulset in ns statefulset-4077
  Jun 10 12:52:11.683: INFO: Scaling statefulset ss to 0
  Jun 10 12:52:21.709: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 10 12:52:21.713: INFO: Deleting statefulset ss
  Jun 10 12:52:21.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4077" for this suite. @ 06/10/23 12:52:21.736
• [20.257 seconds]
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 06/10/23 12:52:21.744
  Jun 10 12:52:21.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 12:52:21.746
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:52:21.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:52:21.77
  STEP: Creating configMap with name projected-configmap-test-volume-d1e29767-b1dd-4963-bf84-9e7fb32f0579 @ 06/10/23 12:52:21.775
  STEP: Creating a pod to test consume configMaps @ 06/10/23 12:52:21.781
  STEP: Saw pod success @ 06/10/23 12:52:25.806
  Jun 10 12:52:25.809: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-projected-configmaps-1c12eb6d-277e-4d77-9c1a-830e779a5f57 container agnhost-container: <nil>
  STEP: delete the pod @ 06/10/23 12:52:25.817
  Jun 10 12:52:25.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5854" for this suite. @ 06/10/23 12:52:25.84
• [4.102 seconds]
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 06/10/23 12:52:25.846
  Jun 10 12:52:25.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubelet-test @ 06/10/23 12:52:25.847
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:52:25.864
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:52:25.87
  Jun 10 12:52:27.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7621" for this suite. @ 06/10/23 12:52:27.91
• [2.071 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 06/10/23 12:52:27.92
  Jun 10 12:52:27.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename replicaset @ 06/10/23 12:52:27.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:52:27.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:52:27.943
  STEP: Create a ReplicaSet @ 06/10/23 12:52:27.947
  STEP: Verify that the required pods have come up @ 06/10/23 12:52:27.954
  Jun 10 12:52:27.958: INFO: Pod name sample-pod: Found 0 pods out of 3
  Jun 10 12:52:32.965: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 06/10/23 12:52:32.965
  Jun 10 12:52:32.971: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 06/10/23 12:52:32.971
  STEP: DeleteCollection of the ReplicaSets @ 06/10/23 12:52:32.975
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 06/10/23 12:52:32.985
  Jun 10 12:52:32.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7533" for this suite. @ 06/10/23 12:52:32.997
• [5.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 06/10/23 12:52:33.012
  Jun 10 12:52:33.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename services @ 06/10/23 12:52:33.013
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:52:33.044
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:52:33.048
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-5848 @ 06/10/23 12:52:33.055
  STEP: changing the ExternalName service to type=ClusterIP @ 06/10/23 12:52:33.067
  STEP: creating replication controller externalname-service in namespace services-5848 @ 06/10/23 12:52:33.084
  I0610 12:52:33.101940      18 runners.go:194] Created replication controller with name: externalname-service, namespace: services-5848, replica count: 2
  I0610 12:52:36.153795      18 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 10 12:52:36.153: INFO: Creating new exec pod
  Jun 10 12:52:39.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5848 exec execpodwl5zc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun 10 12:52:39.337: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun 10 12:52:39.337: INFO: stdout: ""
  Jun 10 12:52:40.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5848 exec execpodwl5zc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun 10 12:52:40.501: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun 10 12:52:40.501: INFO: stdout: ""
  Jun 10 12:52:41.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5848 exec execpodwl5zc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun 10 12:52:41.489: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun 10 12:52:41.489: INFO: stdout: ""
  Jun 10 12:52:42.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5848 exec execpodwl5zc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun 10 12:52:42.493: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun 10 12:52:42.493: INFO: stdout: ""
  Jun 10 12:52:43.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5848 exec execpodwl5zc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun 10 12:52:43.551: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun 10 12:52:43.551: INFO: stdout: ""
  Jun 10 12:52:44.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5848 exec execpodwl5zc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun 10 12:52:44.550: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun 10 12:52:44.550: INFO: stdout: ""
  Jun 10 12:52:45.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5848 exec execpodwl5zc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun 10 12:52:45.559: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun 10 12:52:45.559: INFO: stdout: ""
  Jun 10 12:52:46.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5848 exec execpodwl5zc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun 10 12:52:46.544: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun 10 12:52:46.544: INFO: stdout: ""
  Jun 10 12:52:47.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5848 exec execpodwl5zc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun 10 12:52:47.496: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun 10 12:52:47.496: INFO: stdout: "externalname-service-59qcm"
  Jun 10 12:52:47.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5848 exec execpodwl5zc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.193 80'
  Jun 10 12:52:47.653: INFO: stderr: "+ nc -v -t -w 2 10.152.183.193 80\n+ echo hostName\nConnection to 10.152.183.193 80 port [tcp/http] succeeded!\n"
  Jun 10 12:52:47.653: INFO: stdout: ""
  Jun 10 12:52:48.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5848 exec execpodwl5zc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.193 80'
  Jun 10 12:52:48.800: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 10.152.183.193 80\nConnection to 10.152.183.193 80 port [tcp/http] succeeded!\n"
  Jun 10 12:52:48.800: INFO: stdout: "externalname-service-59qcm"
  Jun 10 12:52:48.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 10 12:52:48.805: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-5848" for this suite. @ 06/10/23 12:52:48.827
• [15.824 seconds]
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 06/10/23 12:52:48.836
  Jun 10 12:52:48.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 06/10/23 12:52:48.837
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:52:48.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:52:48.874
  STEP: create the container to handle the HTTPGet hook request. @ 06/10/23 12:52:48.883
  STEP: create the pod with lifecycle hook @ 06/10/23 12:52:50.911
  STEP: delete the pod with lifecycle hook @ 06/10/23 12:52:52.931
  STEP: check prestop hook @ 06/10/23 12:52:54.949
  Jun 10 12:52:54.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-6361" for this suite. @ 06/10/23 12:52:54.963
• [6.138 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 06/10/23 12:52:54.975
  Jun 10 12:52:54.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-probe @ 06/10/23 12:52:54.976
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:52:54.992
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:52:55
  STEP: Creating pod test-webserver-5908ba15-94b6-4056-bdae-d9a4d2325851 in namespace container-probe-5706 @ 06/10/23 12:52:55.007
  Jun 10 12:52:57.029: INFO: Started pod test-webserver-5908ba15-94b6-4056-bdae-d9a4d2325851 in namespace container-probe-5706
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/10/23 12:52:57.029
  Jun 10 12:52:57.033: INFO: Initial restart count of pod test-webserver-5908ba15-94b6-4056-bdae-d9a4d2325851 is 0
  Jun 10 12:56:57.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/10/23 12:56:57.667
  STEP: Destroying namespace "container-probe-5706" for this suite. @ 06/10/23 12:56:57.683
• [242.717 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 06/10/23 12:56:57.696
  Jun 10 12:56:57.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-probe @ 06/10/23 12:56:57.698
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 12:56:57.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 12:56:57.719
  STEP: Creating pod busybox-7187e3d5-88b5-4100-8382-4cb3505b1ea7 in namespace container-probe-1 @ 06/10/23 12:56:57.723
  Jun 10 12:56:59.743: INFO: Started pod busybox-7187e3d5-88b5-4100-8382-4cb3505b1ea7 in namespace container-probe-1
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/10/23 12:56:59.743
  Jun 10 12:56:59.747: INFO: Initial restart count of pod busybox-7187e3d5-88b5-4100-8382-4cb3505b1ea7 is 0
  Jun 10 13:01:00.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/10/23 13:01:00.415
  STEP: Destroying namespace "container-probe-1" for this suite. @ 06/10/23 13:01:00.431
• [242.743 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 06/10/23 13:01:00.439
  Jun 10 13:01:00.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir @ 06/10/23 13:01:00.441
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:01:00.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:01:00.463
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 06/10/23 13:01:00.468
  STEP: Saw pod success @ 06/10/23 13:01:04.491
  Jun 10 13:01:04.495: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-1b105acd-13e5-4950-8f2e-d6c906cddd5f container test-container: <nil>
  STEP: delete the pod @ 06/10/23 13:01:04.518
  Jun 10 13:01:04.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-346" for this suite. @ 06/10/23 13:01:04.544
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 06/10/23 13:01:04.555
  Jun 10 13:01:04.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename csiinlinevolumes @ 06/10/23 13:01:04.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:01:04.576
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:01:04.58
  STEP: creating @ 06/10/23 13:01:04.583
  STEP: getting @ 06/10/23 13:01:04.603
  STEP: listing @ 06/10/23 13:01:04.61
  STEP: deleting @ 06/10/23 13:01:04.614
  Jun 10 13:01:04.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-8592" for this suite. @ 06/10/23 13:01:04.641
• [0.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 06/10/23 13:01:04.649
  Jun 10 13:01:04.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename downward-api @ 06/10/23 13:01:04.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:01:04.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:01:04.671
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 13:01:04.675
  STEP: Saw pod success @ 06/10/23 13:01:08.698
  Jun 10 13:01:08.702: INFO: Trying to get logs from node ip-172-31-27-177 pod downwardapi-volume-a6535f5a-3e63-4b1d-ac5b-8f86a8191609 container client-container: <nil>
  STEP: delete the pod @ 06/10/23 13:01:08.711
  Jun 10 13:01:08.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9078" for this suite. @ 06/10/23 13:01:08.732
• [4.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 06/10/23 13:01:08.742
  Jun 10 13:01:08.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename dns @ 06/10/23 13:01:08.743
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:01:08.756
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:01:08.762
  STEP: Creating a test headless service @ 06/10/23 13:01:08.766
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1755 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1755;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1755 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1755;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1755.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1755.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1755.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1755.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1755.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1755.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1755.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1755.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1755.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1755.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1755.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1755.svc;check="$$(dig +notcp +noall +answer +search 95.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.95_udp@PTR;check="$$(dig +tcp +noall +answer +search 95.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.95_tcp@PTR;sleep 1; done
   @ 06/10/23 13:01:08.785
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1755 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1755;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1755 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1755;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1755.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1755.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1755.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1755.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1755.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1755.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1755.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1755.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1755.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1755.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1755.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1755.svc;check="$$(dig +notcp +noall +answer +search 95.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.95_udp@PTR;check="$$(dig +tcp +noall +answer +search 95.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.95_tcp@PTR;sleep 1; done
   @ 06/10/23 13:01:08.786
  STEP: creating a pod to probe DNS @ 06/10/23 13:01:08.787
  STEP: submitting the pod to kubernetes @ 06/10/23 13:01:08.787
  STEP: retrieving the pod @ 06/10/23 13:01:10.816
  STEP: looking for the results for each expected name from probers @ 06/10/23 13:01:10.82
  Jun 10 13:01:10.826: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1: the server could not find the requested resource (get pods dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1)
  Jun 10 13:01:10.830: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1: the server could not find the requested resource (get pods dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1)
  Jun 10 13:01:10.835: INFO: Unable to read wheezy_udp@dns-test-service.dns-1755 from pod dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1: the server could not find the requested resource (get pods dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1)
  Jun 10 13:01:10.839: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1755 from pod dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1: the server could not find the requested resource (get pods dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1)
  Jun 10 13:01:10.844: INFO: Unable to read wheezy_udp@dns-test-service.dns-1755.svc from pod dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1: the server could not find the requested resource (get pods dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1)
  Jun 10 13:01:10.849: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1755.svc from pod dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1: the server could not find the requested resource (get pods dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1)
  Jun 10 13:01:10.852: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1755.svc from pod dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1: the server could not find the requested resource (get pods dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1)
  Jun 10 13:01:10.857: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1755.svc from pod dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1: the server could not find the requested resource (get pods dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1)
  Jun 10 13:01:10.879: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1: the server could not find the requested resource (get pods dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1)
  Jun 10 13:01:10.883: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1: the server could not find the requested resource (get pods dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1)
  Jun 10 13:01:10.888: INFO: Unable to read jessie_udp@dns-test-service.dns-1755 from pod dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1: the server could not find the requested resource (get pods dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1)
  Jun 10 13:01:10.892: INFO: Unable to read jessie_tcp@dns-test-service.dns-1755 from pod dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1: the server could not find the requested resource (get pods dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1)
  Jun 10 13:01:10.897: INFO: Unable to read jessie_udp@dns-test-service.dns-1755.svc from pod dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1: the server could not find the requested resource (get pods dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1)
  Jun 10 13:01:10.902: INFO: Unable to read jessie_tcp@dns-test-service.dns-1755.svc from pod dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1: the server could not find the requested resource (get pods dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1)
  Jun 10 13:01:10.907: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1755.svc from pod dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1: the server could not find the requested resource (get pods dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1)
  Jun 10 13:01:10.912: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1755.svc from pod dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1: the server could not find the requested resource (get pods dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1)
  Jun 10 13:01:10.929: INFO: Lookups using dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1755 wheezy_tcp@dns-test-service.dns-1755 wheezy_udp@dns-test-service.dns-1755.svc wheezy_tcp@dns-test-service.dns-1755.svc wheezy_udp@_http._tcp.dns-test-service.dns-1755.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1755.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1755 jessie_tcp@dns-test-service.dns-1755 jessie_udp@dns-test-service.dns-1755.svc jessie_tcp@dns-test-service.dns-1755.svc jessie_udp@_http._tcp.dns-test-service.dns-1755.svc jessie_tcp@_http._tcp.dns-test-service.dns-1755.svc]

  Jun 10 13:01:16.046: INFO: DNS probes using dns-1755/dns-test-d24a3feb-e9ad-4f79-9193-fc66b544f1a1 succeeded

  Jun 10 13:01:16.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/10/23 13:01:16.05
  STEP: deleting the test service @ 06/10/23 13:01:16.065
  STEP: deleting the test headless service @ 06/10/23 13:01:16.106
  STEP: Destroying namespace "dns-1755" for this suite. @ 06/10/23 13:01:16.127
• [7.393 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 06/10/23 13:01:16.138
  Jun 10 13:01:16.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename runtimeclass @ 06/10/23 13:01:16.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:01:16.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:01:16.162
  STEP: getting /apis @ 06/10/23 13:01:16.166
  STEP: getting /apis/node.k8s.io @ 06/10/23 13:01:16.171
  STEP: getting /apis/node.k8s.io/v1 @ 06/10/23 13:01:16.173
  STEP: creating @ 06/10/23 13:01:16.175
  STEP: watching @ 06/10/23 13:01:16.193
  Jun 10 13:01:16.193: INFO: starting watch
  STEP: getting @ 06/10/23 13:01:16.2
  STEP: listing @ 06/10/23 13:01:16.204
  STEP: patching @ 06/10/23 13:01:16.208
  STEP: updating @ 06/10/23 13:01:16.214
  Jun 10 13:01:16.220: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 06/10/23 13:01:16.22
  STEP: deleting a collection @ 06/10/23 13:01:16.235
  Jun 10 13:01:16.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6404" for this suite. @ 06/10/23 13:01:16.259
• [0.128 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 06/10/23 13:01:16.27
  Jun 10 13:01:16.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename subpath @ 06/10/23 13:01:16.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:01:16.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:01:16.292
  STEP: Setting up data @ 06/10/23 13:01:16.296
  STEP: Creating pod pod-subpath-test-configmap-z7h8 @ 06/10/23 13:01:16.308
  STEP: Creating a pod to test atomic-volume-subpath @ 06/10/23 13:01:16.308
  STEP: Saw pod success @ 06/10/23 13:01:40.389
  Jun 10 13:01:40.393: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-subpath-test-configmap-z7h8 container test-container-subpath-configmap-z7h8: <nil>
  STEP: delete the pod @ 06/10/23 13:01:40.402
  STEP: Deleting pod pod-subpath-test-configmap-z7h8 @ 06/10/23 13:01:40.418
  Jun 10 13:01:40.418: INFO: Deleting pod "pod-subpath-test-configmap-z7h8" in namespace "subpath-8448"
  Jun 10 13:01:40.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8448" for this suite. @ 06/10/23 13:01:40.427
• [24.165 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 06/10/23 13:01:40.436
  Jun 10 13:01:40.436: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 06/10/23 13:01:40.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:01:40.456
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:01:40.46
  STEP: create the container to handle the HTTPGet hook request. @ 06/10/23 13:01:40.468
  STEP: create the pod with lifecycle hook @ 06/10/23 13:01:42.496
  STEP: check poststart hook @ 06/10/23 13:01:44.519
  STEP: delete the pod with lifecycle hook @ 06/10/23 13:01:44.546
  Jun 10 13:01:46.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1294" for this suite. @ 06/10/23 13:01:46.566
• [6.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 06/10/23 13:01:46.576
  Jun 10 13:01:46.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename disruption @ 06/10/23 13:01:46.576
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:01:46.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:01:46.599
  STEP: Creating a pdb that targets all three pods in a test replica set @ 06/10/23 13:01:46.603
  STEP: Waiting for the pdb to be processed @ 06/10/23 13:01:46.608
  STEP: First trying to evict a pod which shouldn't be evictable @ 06/10/23 13:01:48.627
  STEP: Waiting for all pods to be running @ 06/10/23 13:01:48.627
  Jun 10 13:01:48.631: INFO: pods: 0 < 3
  STEP: locating a running pod @ 06/10/23 13:01:50.637
  STEP: Updating the pdb to allow a pod to be evicted @ 06/10/23 13:01:50.649
  STEP: Waiting for the pdb to be processed @ 06/10/23 13:01:50.661
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 06/10/23 13:01:52.673
  STEP: Waiting for all pods to be running @ 06/10/23 13:01:52.673
  STEP: Waiting for the pdb to observed all healthy pods @ 06/10/23 13:01:52.677
  STEP: Patching the pdb to disallow a pod to be evicted @ 06/10/23 13:01:52.708
  STEP: Waiting for the pdb to be processed @ 06/10/23 13:01:52.738
  STEP: Waiting for all pods to be running @ 06/10/23 13:01:54.752
  STEP: locating a running pod @ 06/10/23 13:01:54.757
  STEP: Deleting the pdb to allow a pod to be evicted @ 06/10/23 13:01:54.768
  STEP: Waiting for the pdb to be deleted @ 06/10/23 13:01:54.775
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 06/10/23 13:01:54.78
  STEP: Waiting for all pods to be running @ 06/10/23 13:01:54.78
  Jun 10 13:01:54.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-6540" for this suite. @ 06/10/23 13:01:54.813
• [8.252 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 06/10/23 13:01:54.828
  Jun 10 13:01:54.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename secrets @ 06/10/23 13:01:54.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:01:54.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:01:54.856
  STEP: Creating secret with name secret-test-map-e639af48-2843-49fb-88b5-4c9106e70cd4 @ 06/10/23 13:01:54.86
  STEP: Creating a pod to test consume secrets @ 06/10/23 13:01:54.866
  STEP: Saw pod success @ 06/10/23 13:01:58.893
  Jun 10 13:01:58.898: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-secrets-0363d117-a7d9-42ad-b57c-eaf15f42a753 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/10/23 13:01:58.905
  Jun 10 13:01:58.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7571" for this suite. @ 06/10/23 13:01:58.929
• [4.108 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 06/10/23 13:01:58.937
  Jun 10 13:01:58.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename downward-api @ 06/10/23 13:01:58.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:01:58.953
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:01:58.959
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 13:01:58.963
  STEP: Saw pod success @ 06/10/23 13:02:02.99
  Jun 10 13:02:02.994: INFO: Trying to get logs from node ip-172-31-27-177 pod downwardapi-volume-6ca3a4ae-dbf4-42b5-96ca-80239b31564f container client-container: <nil>
  STEP: delete the pod @ 06/10/23 13:02:03.002
  Jun 10 13:02:03.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7482" for this suite. @ 06/10/23 13:02:03.026
• [4.097 seconds]
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 06/10/23 13:02:03.034
  Jun 10 13:02:03.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubelet-test @ 06/10/23 13:02:03.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:02:03.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:02:03.061
  Jun 10 13:02:05.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1013" for this suite. @ 06/10/23 13:02:05.103
• [2.077 seconds]
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 06/10/23 13:02:05.112
  Jun 10 13:02:05.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename sched-preemption @ 06/10/23 13:02:05.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:02:05.132
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:02:05.136
  Jun 10 13:02:05.157: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun 10 13:03:05.180: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 06/10/23 13:03:05.185
  Jun 10 13:03:05.210: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jun 10 13:03:05.225: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jun 10 13:03:05.253: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jun 10 13:03:05.263: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jun 10 13:03:05.288: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jun 10 13:03:05.299: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 06/10/23 13:03:05.299
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 06/10/23 13:03:07.342
  Jun 10 13:03:11.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-1630" for this suite. @ 06/10/23 13:03:11.474
• [66.370 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 06/10/23 13:03:11.484
  Jun 10 13:03:11.484: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl @ 06/10/23 13:03:11.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:03:11.5
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:03:11.506
  STEP: Starting the proxy @ 06/10/23 13:03:11.51
  Jun 10 13:03:11.510: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-2684 proxy --unix-socket=/tmp/kubectl-proxy-unix599373316/test'
  STEP: retrieving proxy /api/ output @ 06/10/23 13:03:11.567
  Jun 10 13:03:11.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2684" for this suite. @ 06/10/23 13:03:11.574
• [0.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 06/10/23 13:03:11.583
  Jun 10 13:03:11.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename svcaccounts @ 06/10/23 13:03:11.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:03:11.601
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:03:11.604
  STEP: Creating ServiceAccount "e2e-sa-mqc4s"  @ 06/10/23 13:03:11.608
  Jun 10 13:03:11.613: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-mqc4s"  @ 06/10/23 13:03:11.613
  Jun 10 13:03:11.623: INFO: AutomountServiceAccountToken: true
  Jun 10 13:03:11.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4706" for this suite. @ 06/10/23 13:03:11.628
• [0.056 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 06/10/23 13:03:11.64
  Jun 10 13:03:11.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir @ 06/10/23 13:03:11.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:03:11.66
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:03:11.668
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 06/10/23 13:03:11.672
  STEP: Saw pod success @ 06/10/23 13:03:15.697
  Jun 10 13:03:15.702: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-ed7be296-b8fe-43ce-ab44-34c55bc8d826 container test-container: <nil>
  STEP: delete the pod @ 06/10/23 13:03:15.71
  Jun 10 13:03:15.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3286" for this suite. @ 06/10/23 13:03:15.732
• [4.101 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 06/10/23 13:03:15.741
  Jun 10 13:03:15.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 13:03:15.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:03:15.757
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:03:15.763
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 13:03:15.767
  STEP: Saw pod success @ 06/10/23 13:03:19.809
  Jun 10 13:03:19.814: INFO: Trying to get logs from node ip-172-31-27-177 pod downwardapi-volume-2289a371-2a17-4ccf-bb0e-fc2b40bfb5af container client-container: <nil>
  STEP: delete the pod @ 06/10/23 13:03:19.822
  Jun 10 13:03:19.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2290" for this suite. @ 06/10/23 13:03:19.847
• [4.114 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 06/10/23 13:03:19.858
  Jun 10 13:03:19.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename downward-api @ 06/10/23 13:03:19.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:03:19.875
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:03:19.887
  STEP: Creating a pod to test downward api env vars @ 06/10/23 13:03:19.893
  STEP: Saw pod success @ 06/10/23 13:03:23.921
  Jun 10 13:03:23.925: INFO: Trying to get logs from node ip-172-31-27-177 pod downward-api-f8a1c9c5-df25-486a-96c9-7c4a44baab0d container dapi-container: <nil>
  STEP: delete the pod @ 06/10/23 13:03:23.935
  Jun 10 13:03:23.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9514" for this suite. @ 06/10/23 13:03:23.959
• [4.109 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 06/10/23 13:03:23.967
  Jun 10 13:03:23.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir @ 06/10/23 13:03:23.968
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:03:23.985
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:03:23.99
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 06/10/23 13:03:23.995
  STEP: Saw pod success @ 06/10/23 13:03:28.024
  Jun 10 13:03:28.028: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-092a4307-c583-49e2-ac16-2a5a9c4e4e0f container test-container: <nil>
  STEP: delete the pod @ 06/10/23 13:03:28.036
  Jun 10 13:03:28.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3925" for this suite. @ 06/10/23 13:03:28.061
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 06/10/23 13:03:28.072
  Jun 10 13:03:28.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename endpointslice @ 06/10/23 13:03:28.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:03:28.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:03:28.097
  STEP: referencing a single matching pod @ 06/10/23 13:03:33.19
  STEP: referencing matching pods with named port @ 06/10/23 13:03:38.2
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 06/10/23 13:03:43.211
  STEP: recreating EndpointSlices after they've been deleted @ 06/10/23 13:03:48.221
  Jun 10 13:03:48.249: INFO: EndpointSlice for Service endpointslice-8613/example-named-port not found
  Jun 10 13:03:58.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8613" for this suite. @ 06/10/23 13:03:58.264
• [30.200 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 06/10/23 13:03:58.272
  Jun 10 13:03:58.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 13:03:58.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:03:58.295
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:03:58.299
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 13:03:58.304
  STEP: Saw pod success @ 06/10/23 13:04:02.329
  Jun 10 13:04:02.333: INFO: Trying to get logs from node ip-172-31-27-177 pod downwardapi-volume-e07ae8d4-de73-4b5c-ad5b-efe60b8e0fcb container client-container: <nil>
  STEP: delete the pod @ 06/10/23 13:04:02.342
  Jun 10 13:04:02.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6757" for this suite. @ 06/10/23 13:04:02.365
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 06/10/23 13:04:02.376
  Jun 10 13:04:02.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename disruption @ 06/10/23 13:04:02.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:04:02.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:04:02.403
  STEP: Waiting for the pdb to be processed @ 06/10/23 13:04:02.414
  STEP: Waiting for all pods to be running @ 06/10/23 13:04:04.455
  Jun 10 13:04:04.465: INFO: running pods: 0 < 3
  Jun 10 13:04:06.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9725" for this suite. @ 06/10/23 13:04:06.479
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 06/10/23 13:04:06.488
  Jun 10 13:04:06.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename configmap @ 06/10/23 13:04:06.489
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:04:06.512
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:04:06.517
  STEP: creating a ConfigMap @ 06/10/23 13:04:06.521
  STEP: fetching the ConfigMap @ 06/10/23 13:04:06.527
  STEP: patching the ConfigMap @ 06/10/23 13:04:06.531
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 06/10/23 13:04:06.537
  STEP: deleting the ConfigMap by collection with a label selector @ 06/10/23 13:04:06.542
  STEP: listing all ConfigMaps in test namespace @ 06/10/23 13:04:06.551
  Jun 10 13:04:06.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1608" for this suite. @ 06/10/23 13:04:06.56
• [0.080 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 06/10/23 13:04:06.568
  Jun 10 13:04:06.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename daemonsets @ 06/10/23 13:04:06.569
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:04:06.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:04:06.59
  Jun 10 13:04:06.628: INFO: Create a RollingUpdate DaemonSet
  Jun 10 13:04:06.634: INFO: Check that daemon pods launch on every node of the cluster
  Jun 10 13:04:06.639: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:04:06.639: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:04:06.643: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:04:06.644: INFO: Node ip-172-31-27-177 is running 0 daemon pod, expected 1
  Jun 10 13:04:07.650: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:04:07.650: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:04:07.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:04:07.655: INFO: Node ip-172-31-27-177 is running 0 daemon pod, expected 1
  Jun 10 13:04:08.650: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:04:08.650: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:04:08.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 10 13:04:08.655: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Jun 10 13:04:08.655: INFO: Update the DaemonSet to trigger a rollout
  Jun 10 13:04:08.669: INFO: Updating DaemonSet daemon-set
  Jun 10 13:04:11.695: INFO: Roll back the DaemonSet before rollout is complete
  Jun 10 13:04:11.708: INFO: Updating DaemonSet daemon-set
  Jun 10 13:04:11.709: INFO: Make sure DaemonSet rollback is complete
  Jun 10 13:04:11.714: INFO: Wrong image for pod: daemon-set-26qfs. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Jun 10 13:04:11.715: INFO: Pod daemon-set-26qfs is not available
  Jun 10 13:04:11.721: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:04:11.721: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:04:12.732: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:04:12.732: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:04:13.727: INFO: Pod daemon-set-z7fzq is not available
  Jun 10 13:04:13.732: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:04:13.732: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 06/10/23 13:04:13.741
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6103, will wait for the garbage collector to delete the pods @ 06/10/23 13:04:13.742
  Jun 10 13:04:13.805: INFO: Deleting DaemonSet.extensions daemon-set took: 9.172363ms
  Jun 10 13:04:13.906: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.054259ms
  Jun 10 13:04:15.612: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:04:15.612: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 10 13:04:15.616: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24201"},"items":null}

  Jun 10 13:04:15.620: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24201"},"items":null}

  Jun 10 13:04:15.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6103" for this suite. @ 06/10/23 13:04:15.64
• [9.079 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 06/10/23 13:04:15.65
  Jun 10 13:04:15.650: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename replicaset @ 06/10/23 13:04:15.651
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:04:15.67
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:04:15.679
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 06/10/23 13:04:15.683
  Jun 10 13:04:15.694: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jun 10 13:04:20.704: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/10/23 13:04:20.704
  STEP: getting scale subresource @ 06/10/23 13:04:20.704
  STEP: updating a scale subresource @ 06/10/23 13:04:20.711
  STEP: verifying the replicaset Spec.Replicas was modified @ 06/10/23 13:04:20.719
  STEP: Patch a scale subresource @ 06/10/23 13:04:20.726
  Jun 10 13:04:20.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3720" for this suite. @ 06/10/23 13:04:20.767
• [5.140 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 06/10/23 13:04:20.791
  Jun 10 13:04:20.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl @ 06/10/23 13:04:20.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:04:20.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:04:20.83
  Jun 10 13:04:20.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-3758 version'
  Jun 10 13:04:20.908: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Jun 10 13:04:20.908: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.2\", GitCommit:\"7f6f68fdabc4df88cfea2dcf9a19b2b830f1e647\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:20:07Z\", GoVersion:\"go1.20.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.2\", GitCommit:\"7f6f68fdabc4df88cfea2dcf9a19b2b830f1e647\", GitTreeState:\"clean\", BuildDate:\"2023-05-18T02:06:41Z\", GoVersion:\"go1.20.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Jun 10 13:04:20.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3758" for this suite. @ 06/10/23 13:04:20.914
• [0.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 06/10/23 13:04:20.93
  Jun 10 13:04:20.930: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-probe @ 06/10/23 13:04:20.931
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:04:20.963
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:04:20.972
  STEP: Creating pod liveness-45dc1540-3b56-4dee-86e8-454583831fee in namespace container-probe-245 @ 06/10/23 13:04:20.976
  Jun 10 13:04:23.000: INFO: Started pod liveness-45dc1540-3b56-4dee-86e8-454583831fee in namespace container-probe-245
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/10/23 13:04:23
  Jun 10 13:04:23.004: INFO: Initial restart count of pod liveness-45dc1540-3b56-4dee-86e8-454583831fee is 0
  Jun 10 13:04:43.060: INFO: Restart count of pod container-probe-245/liveness-45dc1540-3b56-4dee-86e8-454583831fee is now 1 (20.056349456s elapsed)
  Jun 10 13:04:43.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/10/23 13:04:43.066
  STEP: Destroying namespace "container-probe-245" for this suite. @ 06/10/23 13:04:43.081
• [22.161 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 06/10/23 13:04:43.093
  Jun 10 13:04:43.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl @ 06/10/23 13:04:43.094
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:04:43.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:04:43.149
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 06/10/23 13:04:43.172
  Jun 10 13:04:43.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-702 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jun 10 13:04:43.276: INFO: stderr: ""
  Jun 10 13:04:43.276: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 06/10/23 13:04:43.276
  STEP: verifying the pod e2e-test-httpd-pod was created @ 06/10/23 13:04:48.328
  Jun 10 13:04:48.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-702 get pod e2e-test-httpd-pod -o json'
  Jun 10 13:04:48.406: INFO: stderr: ""
  Jun 10 13:04:48.406: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-06-10T13:04:43Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-702\",\n        \"resourceVersion\": \"24417\",\n        \"uid\": \"a3d6bdfe-a5a3-4f63-8e68-dea39387e7b5\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-j6mqc\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-27-177\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-j6mqc\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-10T13:04:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-10T13:04:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-10T13:04:44Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-10T13:04:43Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://7cb329959e45d98e32001dd0827db7fa694c2332440e9dc1916a687f03883da7\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-06-10T13:04:44Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.27.177\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.109.29\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.109.29\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-06-10T13:04:43Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 06/10/23 13:04:48.406
  Jun 10 13:04:48.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-702 replace -f -'
  Jun 10 13:04:49.563: INFO: stderr: ""
  Jun 10 13:04:49.563: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 06/10/23 13:04:49.563
  Jun 10 13:04:49.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-702 delete pods e2e-test-httpd-pod'
  Jun 10 13:04:50.978: INFO: stderr: ""
  Jun 10 13:04:50.978: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jun 10 13:04:50.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-702" for this suite. @ 06/10/23 13:04:50.983
• [7.900 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 06/10/23 13:04:50.994
  Jun 10 13:04:50.994: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename pods @ 06/10/23 13:04:50.995
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:04:51.011
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:04:51.019
  Jun 10 13:04:51.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: creating the pod @ 06/10/23 13:04:51.025
  STEP: submitting the pod to kubernetes @ 06/10/23 13:04:51.025
  Jun 10 13:04:53.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4725" for this suite. @ 06/10/23 13:04:53.151
• [2.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 06/10/23 13:04:53.16
  Jun 10 13:04:53.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename downward-api @ 06/10/23 13:04:53.162
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:04:53.186
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:04:53.195
  STEP: Creating a pod to test downward api env vars @ 06/10/23 13:04:53.2
  STEP: Saw pod success @ 06/10/23 13:04:57.235
  Jun 10 13:04:57.239: INFO: Trying to get logs from node ip-172-31-27-177 pod downward-api-3d84253b-ef07-4ff2-b1a1-2a0d7626b7ae container dapi-container: <nil>
  STEP: delete the pod @ 06/10/23 13:04:57.249
  Jun 10 13:04:57.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5854" for this suite. @ 06/10/23 13:04:57.276
• [4.124 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 06/10/23 13:04:57.286
  Jun 10 13:04:57.286: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename var-expansion @ 06/10/23 13:04:57.287
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:04:57.308
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:04:57.315
  Jun 10 13:04:59.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 10 13:04:59.351: INFO: Deleting pod "var-expansion-b2c61fc7-afeb-4533-bfa6-88b798efbc9f" in namespace "var-expansion-2643"
  Jun 10 13:04:59.360: INFO: Wait up to 5m0s for pod "var-expansion-b2c61fc7-afeb-4533-bfa6-88b798efbc9f" to be fully deleted
  STEP: Destroying namespace "var-expansion-2643" for this suite. @ 06/10/23 13:05:01.37
• [4.091 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 06/10/23 13:05:01.378
  Jun 10 13:05:01.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 13:05:01.379
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:05:01.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:05:01.405
  STEP: Setting up server cert @ 06/10/23 13:05:01.431
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 13:05:01.91
  STEP: Deploying the webhook pod @ 06/10/23 13:05:01.919
  STEP: Wait for the deployment to be ready @ 06/10/23 13:05:01.937
  Jun 10 13:05:01.945: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/10/23 13:05:03.956
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 13:05:03.967
  Jun 10 13:05:04.967: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun 10 13:05:04.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5901-crds.webhook.example.com via the AdmissionRegistration API @ 06/10/23 13:05:05.488
  STEP: Creating a custom resource that should be mutated by the webhook @ 06/10/23 13:05:05.506
  Jun 10 13:05:07.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5603" for this suite. @ 06/10/23 13:05:08.158
  STEP: Destroying namespace "webhook-markers-5535" for this suite. @ 06/10/23 13:05:08.167
• [6.796 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 06/10/23 13:05:08.175
  Jun 10 13:05:08.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename runtimeclass @ 06/10/23 13:05:08.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:05:08.193
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:05:08.2
  Jun 10 13:05:08.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7985" for this suite. @ 06/10/23 13:05:08.217
• [0.050 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 06/10/23 13:05:08.227
  Jun 10 13:05:08.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename downward-api @ 06/10/23 13:05:08.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:05:08.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:05:08.25
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 13:05:08.254
  STEP: Saw pod success @ 06/10/23 13:05:12.286
  Jun 10 13:05:12.292: INFO: Trying to get logs from node ip-172-31-27-177 pod downwardapi-volume-834289b2-4e75-4051-9cd9-b89137cded9e container client-container: <nil>
  STEP: delete the pod @ 06/10/23 13:05:12.303
  Jun 10 13:05:12.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7774" for this suite. @ 06/10/23 13:05:12.328
• [4.109 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 06/10/23 13:05:12.336
  Jun 10 13:05:12.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-runtime @ 06/10/23 13:05:12.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:05:12.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:05:12.358
  STEP: create the container @ 06/10/23 13:05:12.362
  W0610 13:05:12.371401      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 06/10/23 13:05:12.371
  STEP: get the container status @ 06/10/23 13:05:16.395
  STEP: the container should be terminated @ 06/10/23 13:05:16.398
  STEP: the termination message should be set @ 06/10/23 13:05:16.399
  Jun 10 13:05:16.399: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 06/10/23 13:05:16.399
  Jun 10 13:05:16.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7340" for this suite. @ 06/10/23 13:05:16.423
• [4.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 06/10/23 13:05:16.435
  Jun 10 13:05:16.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename dns @ 06/10/23 13:05:16.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:05:16.46
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:05:16.464
  STEP: Creating a test headless service @ 06/10/23 13:05:16.468
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9429.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9429.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9429.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9429.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9429.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9429.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9429.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9429.svc.cluster.local;sleep 1; done
   @ 06/10/23 13:05:16.476
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9429.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9429.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9429.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9429.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9429.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9429.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9429.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9429.svc.cluster.local;sleep 1; done
   @ 06/10/23 13:05:16.476
  STEP: creating a pod to probe DNS @ 06/10/23 13:05:16.476
  STEP: submitting the pod to kubernetes @ 06/10/23 13:05:16.476
  STEP: retrieving the pod @ 06/10/23 13:05:18.5
  STEP: looking for the results for each expected name from probers @ 06/10/23 13:05:18.504
  Jun 10 13:05:18.509: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9429.svc.cluster.local from pod dns-9429/dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079: the server could not find the requested resource (get pods dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079)
  Jun 10 13:05:18.514: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9429.svc.cluster.local from pod dns-9429/dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079: the server could not find the requested resource (get pods dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079)
  Jun 10 13:05:18.519: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9429.svc.cluster.local from pod dns-9429/dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079: the server could not find the requested resource (get pods dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079)
  Jun 10 13:05:18.524: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9429.svc.cluster.local from pod dns-9429/dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079: the server could not find the requested resource (get pods dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079)
  Jun 10 13:05:18.529: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9429.svc.cluster.local from pod dns-9429/dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079: the server could not find the requested resource (get pods dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079)
  Jun 10 13:05:18.533: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9429.svc.cluster.local from pod dns-9429/dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079: the server could not find the requested resource (get pods dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079)
  Jun 10 13:05:18.538: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9429.svc.cluster.local from pod dns-9429/dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079: the server could not find the requested resource (get pods dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079)
  Jun 10 13:05:18.543: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9429.svc.cluster.local from pod dns-9429/dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079: the server could not find the requested resource (get pods dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079)
  Jun 10 13:05:18.543: INFO: Lookups using dns-9429/dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9429.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9429.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9429.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9429.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9429.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9429.svc.cluster.local jessie_udp@dns-test-service-2.dns-9429.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9429.svc.cluster.local]

  Jun 10 13:05:23.581: INFO: DNS probes using dns-9429/dns-test-3db55fa1-1122-48b2-aa4b-0c90e8118079 succeeded

  Jun 10 13:05:23.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/10/23 13:05:23.586
  STEP: deleting the test headless service @ 06/10/23 13:05:23.608
  STEP: Destroying namespace "dns-9429" for this suite. @ 06/10/23 13:05:23.625
• [7.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 06/10/23 13:05:23.637
  Jun 10 13:05:23.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename services @ 06/10/23 13:05:23.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:05:23.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:05:23.665
  STEP: creating an Endpoint @ 06/10/23 13:05:23.673
  STEP: waiting for available Endpoint @ 06/10/23 13:05:23.68
  STEP: listing all Endpoints @ 06/10/23 13:05:23.682
  STEP: updating the Endpoint @ 06/10/23 13:05:23.686
  STEP: fetching the Endpoint @ 06/10/23 13:05:23.693
  STEP: patching the Endpoint @ 06/10/23 13:05:23.697
  STEP: fetching the Endpoint @ 06/10/23 13:05:23.708
  STEP: deleting the Endpoint by Collection @ 06/10/23 13:05:23.713
  STEP: waiting for Endpoint deletion @ 06/10/23 13:05:23.723
  STEP: fetching the Endpoint @ 06/10/23 13:05:23.725
  Jun 10 13:05:23.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6132" for this suite. @ 06/10/23 13:05:23.734
• [0.105 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 06/10/23 13:05:23.743
  Jun 10 13:05:23.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename svcaccounts @ 06/10/23 13:05:23.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:05:23.764
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:05:23.769
  STEP: reading a file in the container @ 06/10/23 13:05:25.803
  Jun 10 13:05:25.803: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-299 pod-service-account-307b19a3-2ced-4ee5-abfc-cf4338884936 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 06/10/23 13:05:25.965
  Jun 10 13:05:25.965: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-299 pod-service-account-307b19a3-2ced-4ee5-abfc-cf4338884936 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 06/10/23 13:05:26.104
  Jun 10 13:05:26.104: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-299 pod-service-account-307b19a3-2ced-4ee5-abfc-cf4338884936 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Jun 10 13:05:26.259: INFO: Got root ca configmap in namespace "svcaccounts-299"
  Jun 10 13:05:26.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-299" for this suite. @ 06/10/23 13:05:26.266
• [2.530 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 06/10/23 13:05:26.276
  Jun 10 13:05:26.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename services @ 06/10/23 13:05:26.277
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:05:26.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:05:26.308
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5148 @ 06/10/23 13:05:26.312
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 06/10/23 13:05:26.327
  STEP: creating service externalsvc in namespace services-5148 @ 06/10/23 13:05:26.328
  STEP: creating replication controller externalsvc in namespace services-5148 @ 06/10/23 13:05:26.342
  I0610 13:05:26.354252      18 runners.go:194] Created replication controller with name: externalsvc, namespace: services-5148, replica count: 2
  I0610 13:05:29.405289      18 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 06/10/23 13:05:29.41
  Jun 10 13:05:29.427: INFO: Creating new exec pod
  Jun 10 13:05:31.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5148 exec execpod5zv57 -- /bin/sh -x -c nslookup clusterip-service.services-5148.svc.cluster.local'
  Jun 10 13:05:31.695: INFO: stderr: "+ nslookup clusterip-service.services-5148.svc.cluster.local\n"
  Jun 10 13:05:31.695: INFO: stdout: "Server:\t\t10.152.183.119\nAddress:\t10.152.183.119#53\n\nclusterip-service.services-5148.svc.cluster.local\tcanonical name = externalsvc.services-5148.svc.cluster.local.\nName:\texternalsvc.services-5148.svc.cluster.local\nAddress: 10.152.183.212\n\n"
  Jun 10 13:05:31.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-5148, will wait for the garbage collector to delete the pods @ 06/10/23 13:05:31.7
  Jun 10 13:05:31.763: INFO: Deleting ReplicationController externalsvc took: 7.760656ms
  Jun 10 13:05:31.863: INFO: Terminating ReplicationController externalsvc pods took: 100.323858ms
  Jun 10 13:05:34.190: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-5148" for this suite. @ 06/10/23 13:05:34.204
• [7.940 seconds]
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 06/10/23 13:05:34.216
  Jun 10 13:05:34.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename containers @ 06/10/23 13:05:34.218
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:05:34.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:05:34.241
  STEP: Creating a pod to test override command @ 06/10/23 13:05:34.249
  STEP: Saw pod success @ 06/10/23 13:05:38.279
  Jun 10 13:05:38.282: INFO: Trying to get logs from node ip-172-31-27-177 pod client-containers-6add4984-3768-4bc8-8e23-6aa02b45fd8e container agnhost-container: <nil>
  STEP: delete the pod @ 06/10/23 13:05:38.291
  Jun 10 13:05:38.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-3420" for this suite. @ 06/10/23 13:05:38.313
• [4.105 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 06/10/23 13:05:38.323
  Jun 10 13:05:38.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename pod-network-test @ 06/10/23 13:05:38.324
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:05:38.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:05:38.346
  STEP: Performing setup for networking test in namespace pod-network-test-8831 @ 06/10/23 13:05:38.349
  STEP: creating a selector @ 06/10/23 13:05:38.35
  STEP: Creating the service pods in kubernetes @ 06/10/23 13:05:38.35
  Jun 10 13:05:38.350: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 06/10/23 13:06:00.545
  Jun 10 13:06:02.582: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jun 10 13:06:02.582: INFO: Going to poll 192.168.109.41 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jun 10 13:06:02.586: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.109.41 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8831 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 13:06:02.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:06:02.587: INFO: ExecWithOptions: Clientset creation
  Jun 10 13:06:02.587: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8831/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.109.41+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun 10 13:06:03.667: INFO: Found all 1 expected endpoints: [netserver-0]
  Jun 10 13:06:03.667: INFO: Going to poll 192.168.92.40 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jun 10 13:06:03.672: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.92.40 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8831 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 13:06:03.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:06:03.673: INFO: ExecWithOptions: Clientset creation
  Jun 10 13:06:03.673: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8831/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.92.40+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun 10 13:06:04.748: INFO: Found all 1 expected endpoints: [netserver-1]
  Jun 10 13:06:04.748: INFO: Going to poll 192.168.149.82 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jun 10 13:06:04.754: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.149.82 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8831 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 13:06:04.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:06:04.755: INFO: ExecWithOptions: Clientset creation
  Jun 10 13:06:04.755: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8831/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.149.82+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun 10 13:06:05.826: INFO: Found all 1 expected endpoints: [netserver-2]
  Jun 10 13:06:05.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-8831" for this suite. @ 06/10/23 13:06:05.832
• [27.517 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 06/10/23 13:06:05.841
  Jun 10 13:06:05.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 13:06:05.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:06:05.86
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:06:05.865
  STEP: Creating configMap with name projected-configmap-test-volume-a593b5b2-a5e6-42e0-b872-5870e04ff469 @ 06/10/23 13:06:05.87
  STEP: Creating a pod to test consume configMaps @ 06/10/23 13:06:05.876
  STEP: Saw pod success @ 06/10/23 13:06:09.901
  Jun 10 13:06:09.905: INFO: Trying to get logs from node ip-172-31-89-0 pod pod-projected-configmaps-cb8c0100-638e-477e-9ced-d00ee7ee1f12 container agnhost-container: <nil>
  STEP: delete the pod @ 06/10/23 13:06:09.93
  Jun 10 13:06:09.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2947" for this suite. @ 06/10/23 13:06:09.954
• [4.119 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 06/10/23 13:06:09.961
  Jun 10 13:06:09.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename svcaccounts @ 06/10/23 13:06:09.962
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:06:09.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:06:09.982
  Jun 10 13:06:09.989: INFO: Got root ca configmap in namespace "svcaccounts-1990"
  Jun 10 13:06:09.997: INFO: Deleted root ca configmap in namespace "svcaccounts-1990"
  STEP: waiting for a new root ca configmap created @ 06/10/23 13:06:10.498
  Jun 10 13:06:10.503: INFO: Recreated root ca configmap in namespace "svcaccounts-1990"
  Jun 10 13:06:10.508: INFO: Updated root ca configmap in namespace "svcaccounts-1990"
  STEP: waiting for the root ca configmap reconciled @ 06/10/23 13:06:11.009
  Jun 10 13:06:11.014: INFO: Reconciled root ca configmap in namespace "svcaccounts-1990"
  Jun 10 13:06:11.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1990" for this suite. @ 06/10/23 13:06:11.02
• [1.067 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 06/10/23 13:06:11.033
  Jun 10 13:06:11.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl-logs @ 06/10/23 13:06:11.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:06:11.055
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:06:11.065
  STEP: creating an pod @ 06/10/23 13:06:11.07
  Jun 10 13:06:11.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-logs-2288 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Jun 10 13:06:11.179: INFO: stderr: ""
  Jun 10 13:06:11.180: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 06/10/23 13:06:11.18
  Jun 10 13:06:11.180: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  Jun 10 13:06:13.201: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 06/10/23 13:06:13.201
  Jun 10 13:06:13.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-logs-2288 logs logs-generator logs-generator'
  Jun 10 13:06:13.294: INFO: stderr: ""
  Jun 10 13:06:13.294: INFO: stdout: "I0610 13:06:12.076712       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/fsb 524\nI0610 13:06:12.276784       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/hp7 570\nI0610 13:06:12.477384       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/j4h2 245\nI0610 13:06:12.677754       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/qk8m 521\nI0610 13:06:12.877113       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/2phd 366\nI0610 13:06:13.077435       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/sh5n 480\nI0610 13:06:13.276717       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/s4p 453\n"
  STEP: limiting log lines @ 06/10/23 13:06:13.294
  Jun 10 13:06:13.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-logs-2288 logs logs-generator logs-generator --tail=1'
  Jun 10 13:06:13.379: INFO: stderr: ""
  Jun 10 13:06:13.379: INFO: stdout: "I0610 13:06:13.276717       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/s4p 453\n"
  Jun 10 13:06:13.379: INFO: got output "I0610 13:06:13.276717       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/s4p 453\n"
  STEP: limiting log bytes @ 06/10/23 13:06:13.379
  Jun 10 13:06:13.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-logs-2288 logs logs-generator logs-generator --limit-bytes=1'
  Jun 10 13:06:13.478: INFO: stderr: ""
  Jun 10 13:06:13.478: INFO: stdout: "I"
  Jun 10 13:06:13.478: INFO: got output "I"
  STEP: exposing timestamps @ 06/10/23 13:06:13.478
  Jun 10 13:06:13.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-logs-2288 logs logs-generator logs-generator --tail=1 --timestamps'
  Jun 10 13:06:13.566: INFO: stderr: ""
  Jun 10 13:06:13.566: INFO: stdout: "2023-06-10T13:06:13.477326002Z I0610 13:06:13.477182       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/ltl 278\n"
  Jun 10 13:06:13.566: INFO: got output "2023-06-10T13:06:13.477326002Z I0610 13:06:13.477182       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/ltl 278\n"
  STEP: restricting to a time range @ 06/10/23 13:06:13.566
  Jun 10 13:06:16.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-logs-2288 logs logs-generator logs-generator --since=1s'
  Jun 10 13:06:16.160: INFO: stderr: ""
  Jun 10 13:06:16.160: INFO: stdout: "I0610 13:06:15.277529       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/xm9 560\nI0610 13:06:15.476763       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/tsrr 428\nI0610 13:06:15.677109       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/f5vm 250\nI0610 13:06:15.877479       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/mjf 328\nI0610 13:06:16.077638       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/km6r 239\n"
  Jun 10 13:06:16.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-logs-2288 logs logs-generator logs-generator --since=24h'
  Jun 10 13:06:16.251: INFO: stderr: ""
  Jun 10 13:06:16.251: INFO: stdout: "I0610 13:06:12.076712       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/fsb 524\nI0610 13:06:12.276784       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/hp7 570\nI0610 13:06:12.477384       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/j4h2 245\nI0610 13:06:12.677754       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/qk8m 521\nI0610 13:06:12.877113       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/2phd 366\nI0610 13:06:13.077435       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/sh5n 480\nI0610 13:06:13.276717       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/s4p 453\nI0610 13:06:13.477182       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/ltl 278\nI0610 13:06:13.677537       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/c4k 244\nI0610 13:06:13.877041       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/6xm7 526\nI0610 13:06:14.077562       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/2qd 336\nI0610 13:06:14.276793       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/8d7 313\nI0610 13:06:14.477218       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/4475 527\nI0610 13:06:14.677664       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/nqs 242\nI0610 13:06:14.876804       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/gpc 367\nI0610 13:06:15.077303       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/xgx 364\nI0610 13:06:15.277529       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/xm9 560\nI0610 13:06:15.476763       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/tsrr 428\nI0610 13:06:15.677109       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/f5vm 250\nI0610 13:06:15.877479       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/mjf 328\nI0610 13:06:16.077638       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/km6r 239\n"
  Jun 10 13:06:16.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-logs-2288 delete pod logs-generator'
  Jun 10 13:06:17.768: INFO: stderr: ""
  Jun 10 13:06:17.769: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Jun 10 13:06:17.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-2288" for this suite. @ 06/10/23 13:06:17.774
• [6.748 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 06/10/23 13:06:17.782
  Jun 10 13:06:17.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename resourcequota @ 06/10/23 13:06:17.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:06:17.802
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:06:17.806
  STEP: Creating resourceQuota "e2e-rq-status-cgst6" @ 06/10/23 13:06:17.814
  Jun 10 13:06:17.823: INFO: Resource quota "e2e-rq-status-cgst6" reports spec: hard cpu limit of 500m
  Jun 10 13:06:17.824: INFO: Resource quota "e2e-rq-status-cgst6" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-cgst6" /status @ 06/10/23 13:06:17.824
  STEP: Confirm /status for "e2e-rq-status-cgst6" resourceQuota via watch @ 06/10/23 13:06:17.834
  Jun 10 13:06:17.836: INFO: observed resourceQuota "e2e-rq-status-cgst6" in namespace "resourcequota-5472" with hard status: v1.ResourceList(nil)
  Jun 10 13:06:17.836: INFO: Found resourceQuota "e2e-rq-status-cgst6" in namespace "resourcequota-5472" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jun 10 13:06:17.836: INFO: ResourceQuota "e2e-rq-status-cgst6" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 06/10/23 13:06:17.84
  Jun 10 13:06:17.846: INFO: Resource quota "e2e-rq-status-cgst6" reports spec: hard cpu limit of 1
  Jun 10 13:06:17.846: INFO: Resource quota "e2e-rq-status-cgst6" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-cgst6" /status @ 06/10/23 13:06:17.846
  STEP: Confirm /status for "e2e-rq-status-cgst6" resourceQuota via watch @ 06/10/23 13:06:17.853
  Jun 10 13:06:17.855: INFO: observed resourceQuota "e2e-rq-status-cgst6" in namespace "resourcequota-5472" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jun 10 13:06:17.855: INFO: Found resourceQuota "e2e-rq-status-cgst6" in namespace "resourcequota-5472" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jun 10 13:06:17.855: INFO: ResourceQuota "e2e-rq-status-cgst6" /status was patched
  STEP: Get "e2e-rq-status-cgst6" /status @ 06/10/23 13:06:17.855
  Jun 10 13:06:17.860: INFO: Resourcequota "e2e-rq-status-cgst6" reports status: hard cpu of 1
  Jun 10 13:06:17.860: INFO: Resourcequota "e2e-rq-status-cgst6" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-cgst6" /status before checking Spec is unchanged @ 06/10/23 13:06:17.865
  Jun 10 13:06:17.872: INFO: Resourcequota "e2e-rq-status-cgst6" reports status: hard cpu of 2
  Jun 10 13:06:17.873: INFO: Resourcequota "e2e-rq-status-cgst6" reports status: hard memory of 2Gi
  Jun 10 13:06:17.875: INFO: Found resourceQuota "e2e-rq-status-cgst6" in namespace "resourcequota-5472" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  Jun 10 13:06:22.884: INFO: ResourceQuota "e2e-rq-status-cgst6" Spec was unchanged and /status reset
  Jun 10 13:06:22.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5472" for this suite. @ 06/10/23 13:06:22.89
• [5.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 06/10/23 13:06:22.902
  Jun 10 13:06:22.902: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename configmap @ 06/10/23 13:06:22.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:06:22.919
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:06:22.927
  STEP: Creating configMap with name configmap-test-upd-1e89246e-cd08-4f82-a0cc-59f23e96dad2 @ 06/10/23 13:06:22.937
  STEP: Creating the pod @ 06/10/23 13:06:22.943
  STEP: Updating configmap configmap-test-upd-1e89246e-cd08-4f82-a0cc-59f23e96dad2 @ 06/10/23 13:06:24.983
  STEP: waiting to observe update in volume @ 06/10/23 13:06:24.989
  Jun 10 13:07:39.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5376" for this suite. @ 06/10/23 13:07:39.407
• [76.512 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 06/10/23 13:07:39.418
  Jun 10 13:07:39.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename deployment @ 06/10/23 13:07:39.419
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:07:39.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:07:39.441
  Jun 10 13:07:39.457: INFO: Pod name rollover-pod: Found 0 pods out of 1
  Jun 10 13:07:44.462: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/10/23 13:07:44.462
  Jun 10 13:07:44.462: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  Jun 10 13:07:46.466: INFO: Creating deployment "test-rollover-deployment"
  Jun 10 13:07:46.476: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  Jun 10 13:07:48.485: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Jun 10 13:07:48.493: INFO: Ensure that both replica sets have 1 created replica
  Jun 10 13:07:48.500: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Jun 10 13:07:48.513: INFO: Updating deployment test-rollover-deployment
  Jun 10 13:07:48.513: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  Jun 10 13:07:50.521: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Jun 10 13:07:50.528: INFO: Make sure deployment "test-rollover-deployment" is complete
  Jun 10 13:07:50.537: INFO: all replica sets need to contain the pod-template-hash label
  Jun 10 13:07:50.537: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 13, 7, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 13, 7, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 13, 7, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 13, 7, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 10 13:07:52.546: INFO: all replica sets need to contain the pod-template-hash label
  Jun 10 13:07:52.546: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 13, 7, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 13, 7, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 13, 7, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 13, 7, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 10 13:07:54.546: INFO: all replica sets need to contain the pod-template-hash label
  Jun 10 13:07:54.547: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 13, 7, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 13, 7, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 13, 7, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 13, 7, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 10 13:07:56.546: INFO: all replica sets need to contain the pod-template-hash label
  Jun 10 13:07:56.546: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 13, 7, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 13, 7, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 13, 7, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 13, 7, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 10 13:07:58.546: INFO: all replica sets need to contain the pod-template-hash label
  Jun 10 13:07:58.546: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 13, 7, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 13, 7, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 13, 7, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 13, 7, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 10 13:08:00.546: INFO: 
  Jun 10 13:08:00.546: INFO: Ensure that both old replica sets have no replicas
  Jun 10 13:08:00.557: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-503  a134a74c-0e95-4206-8a5a-9382d7173b20 25736 2 2023-06-10 13:07:46 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-10 13:07:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 13:07:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002be2e68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-10 13:07:46 +0000 UTC,LastTransitionTime:2023-06-10 13:07:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-06-10 13:07:59 +0000 UTC,LastTransitionTime:2023-06-10 13:07:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun 10 13:08:00.561: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-503  ebddc4a8-8e9c-4df7-a7d6-8f3253900dde 25726 2 2023-06-10 13:07:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment a134a74c-0e95-4206-8a5a-9382d7173b20 0xc002be3337 0xc002be3338}] [] [{kube-controller-manager Update apps/v1 2023-06-10 13:07:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a134a74c-0e95-4206-8a5a-9382d7173b20\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 13:07:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002be33e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun 10 13:08:00.561: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Jun 10 13:08:00.562: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-503  76e197f5-b821-4eff-a033-da7678ab5de7 25735 2 2023-06-10 13:07:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment a134a74c-0e95-4206-8a5a-9382d7173b20 0xc002be3207 0xc002be3208}] [] [{e2e.test Update apps/v1 2023-06-10 13:07:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 13:07:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a134a74c-0e95-4206-8a5a-9382d7173b20\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-10 13:07:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002be32c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 10 13:08:00.562: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-503  a20a5e8d-7ce3-48cb-9345-8dfc69c59e40 25689 2 2023-06-10 13:07:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment a134a74c-0e95-4206-8a5a-9382d7173b20 0xc002be3457 0xc002be3458}] [] [{kube-controller-manager Update apps/v1 2023-06-10 13:07:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a134a74c-0e95-4206-8a5a-9382d7173b20\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 13:07:48 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002be3508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 10 13:08:00.566: INFO: Pod "test-rollover-deployment-57777854c9-hmksz" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-hmksz test-rollover-deployment-57777854c9- deployment-503  a65a7e28-1085-4a77-86bb-7fb47a61a5ce 25702 0 2023-06-10 13:07:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 ebddc4a8-8e9c-4df7-a7d6-8f3253900dde 0xc0050f6677 0xc0050f6678}] [] [{kube-controller-manager Update v1 2023-06-10 13:07:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebddc4a8-8e9c-4df7-a7d6-8f3253900dde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:07:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.109.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q6lgw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q6lgw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:07:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:07:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:07:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:07:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.177,PodIP:192.168.109.46,StartTime:2023-06-10 13:07:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-10 13:07:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://a6e27ce04dabddb2763dd501c953fea7b24f1fee2e0524639dc5ec475dcad20e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.109.46,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:08:00.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-503" for this suite. @ 06/10/23 13:08:00.571
• [21.161 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 06/10/23 13:08:00.579
  Jun 10 13:08:00.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename job @ 06/10/23 13:08:00.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:08:00.601
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:08:00.605
  STEP: Creating a job @ 06/10/23 13:08:00.609
  STEP: Ensuring job reaches completions @ 06/10/23 13:08:00.617
  Jun 10 13:08:10.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9544" for this suite. @ 06/10/23 13:08:10.628
• [10.060 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 06/10/23 13:08:10.642
  Jun 10 13:08:10.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename job @ 06/10/23 13:08:10.643
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:08:10.662
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:08:10.667
  STEP: Creating a suspended job @ 06/10/23 13:08:10.675
  STEP: Patching the Job @ 06/10/23 13:08:10.683
  STEP: Watching for Job to be patched @ 06/10/23 13:08:10.705
  Jun 10 13:08:10.707: INFO: Event ADDED observed for Job e2e-bljhr in namespace job-8871 with labels: map[e2e-job-label:e2e-bljhr] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jun 10 13:08:10.707: INFO: Event MODIFIED observed for Job e2e-bljhr in namespace job-8871 with labels: map[e2e-job-label:e2e-bljhr] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jun 10 13:08:10.707: INFO: Event MODIFIED found for Job e2e-bljhr in namespace job-8871 with labels: map[e2e-bljhr:patched e2e-job-label:e2e-bljhr] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 06/10/23 13:08:10.707
  STEP: Watching for Job to be updated @ 06/10/23 13:08:10.719
  Jun 10 13:08:10.722: INFO: Event MODIFIED found for Job e2e-bljhr in namespace job-8871 with labels: map[e2e-bljhr:patched e2e-job-label:e2e-bljhr] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 10 13:08:10.722: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 06/10/23 13:08:10.722
  Jun 10 13:08:10.727: INFO: Job: e2e-bljhr as labels: map[e2e-bljhr:patched e2e-job-label:e2e-bljhr]
  STEP: Waiting for job to complete @ 06/10/23 13:08:10.728
  STEP: Delete a job collection with a labelselector @ 06/10/23 13:08:20.732
  STEP: Watching for Job to be deleted @ 06/10/23 13:08:20.742
  Jun 10 13:08:20.744: INFO: Event MODIFIED observed for Job e2e-bljhr in namespace job-8871 with labels: map[e2e-bljhr:patched e2e-job-label:e2e-bljhr] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 10 13:08:20.744: INFO: Event MODIFIED observed for Job e2e-bljhr in namespace job-8871 with labels: map[e2e-bljhr:patched e2e-job-label:e2e-bljhr] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 10 13:08:20.745: INFO: Event MODIFIED observed for Job e2e-bljhr in namespace job-8871 with labels: map[e2e-bljhr:patched e2e-job-label:e2e-bljhr] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 10 13:08:20.745: INFO: Event MODIFIED observed for Job e2e-bljhr in namespace job-8871 with labels: map[e2e-bljhr:patched e2e-job-label:e2e-bljhr] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 10 13:08:20.745: INFO: Event MODIFIED observed for Job e2e-bljhr in namespace job-8871 with labels: map[e2e-bljhr:patched e2e-job-label:e2e-bljhr] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 10 13:08:20.745: INFO: Event DELETED found for Job e2e-bljhr in namespace job-8871 with labels: map[e2e-bljhr:patched e2e-job-label:e2e-bljhr] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 06/10/23 13:08:20.745
  Jun 10 13:08:20.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8871" for this suite. @ 06/10/23 13:08:20.76
• [10.135 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 06/10/23 13:08:20.778
  Jun 10 13:08:20.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename init-container @ 06/10/23 13:08:20.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:08:20.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:08:20.808
  STEP: creating the pod @ 06/10/23 13:08:20.815
  Jun 10 13:08:20.815: INFO: PodSpec: initContainers in spec.initContainers
  Jun 10 13:08:24.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6145" for this suite. @ 06/10/23 13:08:24.985
• [4.215 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 06/10/23 13:08:24.994
  Jun 10 13:08:24.994: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename replication-controller @ 06/10/23 13:08:24.995
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:08:25.013
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:08:25.017
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 06/10/23 13:08:25.021
  STEP: When a replication controller with a matching selector is created @ 06/10/23 13:08:27.043
  STEP: Then the orphan pod is adopted @ 06/10/23 13:08:27.048
  Jun 10 13:08:28.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2937" for this suite. @ 06/10/23 13:08:28.063
• [3.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 06/10/23 13:08:28.073
  Jun 10 13:08:28.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename proxy @ 06/10/23 13:08:28.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:08:28.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:08:28.099
  Jun 10 13:08:28.102: INFO: Creating pod...
  Jun 10 13:08:30.123: INFO: Creating service...
  Jun 10 13:08:30.134: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-296/pods/agnhost/proxy/some/path/with/DELETE
  Jun 10 13:08:30.151: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jun 10 13:08:30.151: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-296/pods/agnhost/proxy/some/path/with/GET
  Jun 10 13:08:30.159: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jun 10 13:08:30.159: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-296/pods/agnhost/proxy/some/path/with/HEAD
  Jun 10 13:08:30.165: INFO: http.Client request:HEAD | StatusCode:200
  Jun 10 13:08:30.165: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-296/pods/agnhost/proxy/some/path/with/OPTIONS
  Jun 10 13:08:30.176: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jun 10 13:08:30.176: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-296/pods/agnhost/proxy/some/path/with/PATCH
  Jun 10 13:08:30.181: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jun 10 13:08:30.181: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-296/pods/agnhost/proxy/some/path/with/POST
  Jun 10 13:08:30.186: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jun 10 13:08:30.187: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-296/pods/agnhost/proxy/some/path/with/PUT
  Jun 10 13:08:30.192: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jun 10 13:08:30.192: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-296/services/test-service/proxy/some/path/with/DELETE
  Jun 10 13:08:30.199: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jun 10 13:08:30.199: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-296/services/test-service/proxy/some/path/with/GET
  Jun 10 13:08:30.210: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jun 10 13:08:30.210: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-296/services/test-service/proxy/some/path/with/HEAD
  Jun 10 13:08:30.218: INFO: http.Client request:HEAD | StatusCode:200
  Jun 10 13:08:30.218: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-296/services/test-service/proxy/some/path/with/OPTIONS
  Jun 10 13:08:30.224: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jun 10 13:08:30.224: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-296/services/test-service/proxy/some/path/with/PATCH
  Jun 10 13:08:30.231: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jun 10 13:08:30.231: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-296/services/test-service/proxy/some/path/with/POST
  Jun 10 13:08:30.237: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jun 10 13:08:30.237: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-296/services/test-service/proxy/some/path/with/PUT
  Jun 10 13:08:30.243: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jun 10 13:08:30.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-296" for this suite. @ 06/10/23 13:08:30.248
• [2.184 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 06/10/23 13:08:30.259
  Jun 10 13:08:30.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-probe @ 06/10/23 13:08:30.261
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:08:30.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:08:30.285
  STEP: Creating pod test-grpc-cf169277-7a1a-46ec-8936-57031c1ca65a in namespace container-probe-4284 @ 06/10/23 13:08:30.29
  Jun 10 13:08:32.311: INFO: Started pod test-grpc-cf169277-7a1a-46ec-8936-57031c1ca65a in namespace container-probe-4284
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/10/23 13:08:32.311
  Jun 10 13:08:32.315: INFO: Initial restart count of pod test-grpc-cf169277-7a1a-46ec-8936-57031c1ca65a is 0
  Jun 10 13:09:46.512: INFO: Restart count of pod container-probe-4284/test-grpc-cf169277-7a1a-46ec-8936-57031c1ca65a is now 1 (1m14.196908405s elapsed)
  Jun 10 13:09:46.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/10/23 13:09:46.517
  STEP: Destroying namespace "container-probe-4284" for this suite. @ 06/10/23 13:09:46.532
• [76.280 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 06/10/23 13:09:46.54
  Jun 10 13:09:46.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/10/23 13:09:46.541
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:09:46.556
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:09:46.561
  STEP: set up a multi version CRD @ 06/10/23 13:09:46.566
  Jun 10 13:09:46.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: rename a version @ 06/10/23 13:09:50.616
  STEP: check the new version name is served @ 06/10/23 13:09:50.637
  STEP: check the old version name is removed @ 06/10/23 13:09:51.554
  STEP: check the other version is not changed @ 06/10/23 13:09:52.308
  Jun 10 13:09:55.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5429" for this suite. @ 06/10/23 13:09:55.461
• [8.930 seconds]
------------------------------
SS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 06/10/23 13:09:55.47
  Jun 10 13:09:55.470: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename sysctl @ 06/10/23 13:09:55.472
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:09:55.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:09:55.5
  STEP: Creating a pod with one valid and two invalid sysctls @ 06/10/23 13:09:55.504
  Jun 10 13:09:55.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-3190" for this suite. @ 06/10/23 13:09:55.515
• [0.051 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 06/10/23 13:09:55.522
  Jun 10 13:09:55.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename runtimeclass @ 06/10/23 13:09:55.523
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:09:55.544
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:09:55.549
  Jun 10 13:09:55.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4669" for this suite. @ 06/10/23 13:09:55.592
• [0.078 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 06/10/23 13:09:55.602
  Jun 10 13:09:55.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename configmap @ 06/10/23 13:09:55.603
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:09:55.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:09:55.626
  STEP: Creating configMap with name configmap-test-volume-11ea4227-d9d7-4999-b0a9-031237303acc @ 06/10/23 13:09:55.63
  STEP: Creating a pod to test consume configMaps @ 06/10/23 13:09:55.636
  STEP: Saw pod success @ 06/10/23 13:09:59.663
  Jun 10 13:09:59.666: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-configmaps-b36002cd-b7dc-451e-9426-50ccb3a28502 container agnhost-container: <nil>
  STEP: delete the pod @ 06/10/23 13:09:59.686
  Jun 10 13:09:59.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1029" for this suite. @ 06/10/23 13:09:59.71
• [4.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 06/10/23 13:09:59.726
  Jun 10 13:09:59.726: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename sched-pred @ 06/10/23 13:09:59.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:09:59.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:09:59.748
  Jun 10 13:09:59.752: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jun 10 13:09:59.761: INFO: Waiting for terminating namespaces to be deleted...
  Jun 10 13:09:59.765: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-27-177 before test
  Jun 10 13:09:59.772: INFO: default-http-backend-kubernetes-worker-65fc475d49-ndjwt from ingress-nginx-kubernetes-worker started at 2023-06-10 12:27:43 +0000 UTC (1 container statuses recorded)
  Jun 10 13:09:59.772: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jun 10 13:09:59.772: INFO: nginx-ingress-controller-kubernetes-worker-6c5bl from ingress-nginx-kubernetes-worker started at 2023-06-10 12:27:55 +0000 UTC (1 container statuses recorded)
  Jun 10 13:09:59.772: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 10 13:09:59.772: INFO: calico-kube-controllers-86c9c69795-vrx7l from kube-system started at 2023-06-10 12:27:43 +0000 UTC (1 container statuses recorded)
  Jun 10 13:09:59.772: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jun 10 13:09:59.773: INFO: test-runtimeclass-runtimeclass-4669-preconfigured-handler-jn45j from runtimeclass-4669 started at 2023-06-10 13:09:55 +0000 UTC (1 container statuses recorded)
  Jun 10 13:09:59.773: INFO: 	Container test ready: false, restart count 0
  Jun 10 13:09:59.773: INFO: sonobuoy-systemd-logs-daemon-set-31eb0473969f40ee-l8bg7 from sonobuoy started at 2023-06-10 12:06:59 +0000 UTC (2 container statuses recorded)
  Jun 10 13:09:59.773: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 10 13:09:59.773: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 10 13:09:59.773: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-46-40 before test
  Jun 10 13:09:59.781: INFO: nginx-ingress-controller-kubernetes-worker-vb28m from ingress-nginx-kubernetes-worker started at 2023-06-10 11:54:55 +0000 UTC (1 container statuses recorded)
  Jun 10 13:09:59.781: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 10 13:09:59.781: INFO: coredns-5c7f76ccb8-xmz95 from kube-system started at 2023-06-10 11:54:47 +0000 UTC (1 container statuses recorded)
  Jun 10 13:09:59.781: INFO: 	Container coredns ready: true, restart count 0
  Jun 10 13:09:59.781: INFO: kube-state-metrics-5b95b4459c-rtj7m from kube-system started at 2023-06-10 11:54:47 +0000 UTC (1 container statuses recorded)
  Jun 10 13:09:59.781: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jun 10 13:09:59.781: INFO: metrics-server-v0.5.2-6cf8c8b69c-ftmcl from kube-system started at 2023-06-10 11:54:47 +0000 UTC (2 container statuses recorded)
  Jun 10 13:09:59.781: INFO: 	Container metrics-server ready: true, restart count 0
  Jun 10 13:09:59.781: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jun 10 13:09:59.781: INFO: dashboard-metrics-scraper-6b8586b5c9-znc9p from kubernetes-dashboard started at 2023-06-10 11:54:47 +0000 UTC (1 container statuses recorded)
  Jun 10 13:09:59.781: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jun 10 13:09:59.781: INFO: kubernetes-dashboard-6869f4cd5f-xdbkn from kubernetes-dashboard started at 2023-06-10 11:54:47 +0000 UTC (1 container statuses recorded)
  Jun 10 13:09:59.781: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jun 10 13:09:59.781: INFO: sonobuoy-systemd-logs-daemon-set-31eb0473969f40ee-wg64x from sonobuoy started at 2023-06-10 12:06:59 +0000 UTC (2 container statuses recorded)
  Jun 10 13:09:59.781: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 10 13:09:59.781: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 10 13:09:59.781: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-89-0 before test
  Jun 10 13:09:59.789: INFO: nginx-ingress-controller-kubernetes-worker-889j8 from ingress-nginx-kubernetes-worker started at 2023-06-10 12:03:37 +0000 UTC (1 container statuses recorded)
  Jun 10 13:09:59.789: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 10 13:09:59.789: INFO: sonobuoy from sonobuoy started at 2023-06-10 12:06:57 +0000 UTC (1 container statuses recorded)
  Jun 10 13:09:59.789: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jun 10 13:09:59.789: INFO: sonobuoy-e2e-job-f362cde51af14fae from sonobuoy started at 2023-06-10 12:06:59 +0000 UTC (2 container statuses recorded)
  Jun 10 13:09:59.789: INFO: 	Container e2e ready: true, restart count 0
  Jun 10 13:09:59.789: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 10 13:09:59.789: INFO: sonobuoy-systemd-logs-daemon-set-31eb0473969f40ee-qjm5m from sonobuoy started at 2023-06-10 12:06:59 +0000 UTC (2 container statuses recorded)
  Jun 10 13:09:59.789: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 10 13:09:59.789: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 06/10/23 13:09:59.789
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.17674e3ac6cafead], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] @ 06/10/23 13:09:59.83
  Jun 10 13:10:00.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-5668" for this suite. @ 06/10/23 13:10:00.833
• [1.116 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 06/10/23 13:10:00.843
  Jun 10 13:10:00.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-runtime @ 06/10/23 13:10:00.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:10:00.882
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:10:00.886
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 06/10/23 13:10:00.901
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 06/10/23 13:10:14.974
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 06/10/23 13:10:14.978
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 06/10/23 13:10:14.985
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 06/10/23 13:10:14.985
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 06/10/23 13:10:15.011
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 06/10/23 13:10:18.032
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 06/10/23 13:10:20.046
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 06/10/23 13:10:20.054
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 06/10/23 13:10:20.054
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 06/10/23 13:10:20.079
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 06/10/23 13:10:21.088
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 06/10/23 13:10:24.107
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 06/10/23 13:10:24.115
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 06/10/23 13:10:24.115
  Jun 10 13:10:24.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2422" for this suite. @ 06/10/23 13:10:24.156
• [23.320 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 06/10/23 13:10:24.166
  Jun 10 13:10:24.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl @ 06/10/23 13:10:24.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:10:24.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:10:24.191
  STEP: creating a replication controller @ 06/10/23 13:10:24.195
  Jun 10 13:10:24.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 create -f -'
  Jun 10 13:10:24.506: INFO: stderr: ""
  Jun 10 13:10:24.506: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 06/10/23 13:10:24.506
  Jun 10 13:10:24.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 10 13:10:24.597: INFO: stderr: ""
  Jun 10 13:10:24.597: INFO: stdout: "update-demo-nautilus-k5dwr update-demo-nautilus-v5wr7 "
  Jun 10 13:10:24.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods update-demo-nautilus-k5dwr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 10 13:10:24.677: INFO: stderr: ""
  Jun 10 13:10:24.677: INFO: stdout: ""
  Jun 10 13:10:24.677: INFO: update-demo-nautilus-k5dwr is created but not running
  Jun 10 13:10:29.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 10 13:10:29.760: INFO: stderr: ""
  Jun 10 13:10:29.760: INFO: stdout: "update-demo-nautilus-k5dwr update-demo-nautilus-v5wr7 "
  Jun 10 13:10:29.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods update-demo-nautilus-k5dwr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 10 13:10:29.840: INFO: stderr: ""
  Jun 10 13:10:29.840: INFO: stdout: "true"
  Jun 10 13:10:29.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods update-demo-nautilus-k5dwr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 10 13:10:29.922: INFO: stderr: ""
  Jun 10 13:10:29.922: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 10 13:10:29.922: INFO: validating pod update-demo-nautilus-k5dwr
  Jun 10 13:10:29.928: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 10 13:10:29.928: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 10 13:10:29.928: INFO: update-demo-nautilus-k5dwr is verified up and running
  Jun 10 13:10:29.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods update-demo-nautilus-v5wr7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 10 13:10:30.014: INFO: stderr: ""
  Jun 10 13:10:30.014: INFO: stdout: "true"
  Jun 10 13:10:30.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods update-demo-nautilus-v5wr7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 10 13:10:30.095: INFO: stderr: ""
  Jun 10 13:10:30.095: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 10 13:10:30.095: INFO: validating pod update-demo-nautilus-v5wr7
  Jun 10 13:10:30.101: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 10 13:10:30.101: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 10 13:10:30.101: INFO: update-demo-nautilus-v5wr7 is verified up and running
  STEP: scaling down the replication controller @ 06/10/23 13:10:30.101
  Jun 10 13:10:30.103: INFO: scanned /root for discovery docs: <nil>
  Jun 10 13:10:30.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  Jun 10 13:10:31.204: INFO: stderr: ""
  Jun 10 13:10:31.204: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 06/10/23 13:10:31.204
  Jun 10 13:10:31.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 10 13:10:31.299: INFO: stderr: ""
  Jun 10 13:10:31.299: INFO: stdout: "update-demo-nautilus-k5dwr update-demo-nautilus-v5wr7 "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 06/10/23 13:10:31.299
  Jun 10 13:10:36.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 10 13:10:36.381: INFO: stderr: ""
  Jun 10 13:10:36.381: INFO: stdout: "update-demo-nautilus-v5wr7 "
  Jun 10 13:10:36.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods update-demo-nautilus-v5wr7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 10 13:10:36.462: INFO: stderr: ""
  Jun 10 13:10:36.462: INFO: stdout: "true"
  Jun 10 13:10:36.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods update-demo-nautilus-v5wr7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 10 13:10:36.541: INFO: stderr: ""
  Jun 10 13:10:36.541: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 10 13:10:36.541: INFO: validating pod update-demo-nautilus-v5wr7
  Jun 10 13:10:36.546: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 10 13:10:36.546: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 10 13:10:36.546: INFO: update-demo-nautilus-v5wr7 is verified up and running
  STEP: scaling up the replication controller @ 06/10/23 13:10:36.547
  Jun 10 13:10:36.548: INFO: scanned /root for discovery docs: <nil>
  Jun 10 13:10:36.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  Jun 10 13:10:37.650: INFO: stderr: ""
  Jun 10 13:10:37.650: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 06/10/23 13:10:37.65
  Jun 10 13:10:37.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 10 13:10:37.736: INFO: stderr: ""
  Jun 10 13:10:37.736: INFO: stdout: "update-demo-nautilus-k4lmb update-demo-nautilus-v5wr7 "
  Jun 10 13:10:37.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods update-demo-nautilus-k4lmb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 10 13:10:37.815: INFO: stderr: ""
  Jun 10 13:10:37.815: INFO: stdout: ""
  Jun 10 13:10:37.815: INFO: update-demo-nautilus-k4lmb is created but not running
  Jun 10 13:10:42.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 10 13:10:42.899: INFO: stderr: ""
  Jun 10 13:10:42.899: INFO: stdout: "update-demo-nautilus-k4lmb update-demo-nautilus-v5wr7 "
  Jun 10 13:10:42.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods update-demo-nautilus-k4lmb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 10 13:10:42.984: INFO: stderr: ""
  Jun 10 13:10:42.984: INFO: stdout: "true"
  Jun 10 13:10:42.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods update-demo-nautilus-k4lmb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 10 13:10:43.063: INFO: stderr: ""
  Jun 10 13:10:43.064: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 10 13:10:43.064: INFO: validating pod update-demo-nautilus-k4lmb
  Jun 10 13:10:43.069: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 10 13:10:43.069: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 10 13:10:43.069: INFO: update-demo-nautilus-k4lmb is verified up and running
  Jun 10 13:10:43.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods update-demo-nautilus-v5wr7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 10 13:10:43.149: INFO: stderr: ""
  Jun 10 13:10:43.149: INFO: stdout: "true"
  Jun 10 13:10:43.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods update-demo-nautilus-v5wr7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 10 13:10:43.229: INFO: stderr: ""
  Jun 10 13:10:43.230: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 10 13:10:43.230: INFO: validating pod update-demo-nautilus-v5wr7
  Jun 10 13:10:43.237: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 10 13:10:43.237: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 10 13:10:43.237: INFO: update-demo-nautilus-v5wr7 is verified up and running
  STEP: using delete to clean up resources @ 06/10/23 13:10:43.237
  Jun 10 13:10:43.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 delete --grace-period=0 --force -f -'
  Jun 10 13:10:43.325: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 10 13:10:43.325: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jun 10 13:10:43.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get rc,svc -l name=update-demo --no-headers'
  Jun 10 13:10:43.439: INFO: stderr: "No resources found in kubectl-7591 namespace.\n"
  Jun 10 13:10:43.439: INFO: stdout: ""
  Jun 10 13:10:43.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-7591 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jun 10 13:10:43.564: INFO: stderr: ""
  Jun 10 13:10:43.564: INFO: stdout: ""
  Jun 10 13:10:43.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7591" for this suite. @ 06/10/23 13:10:43.568
• [19.410 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 06/10/23 13:10:43.576
  Jun 10 13:10:43.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename cronjob @ 06/10/23 13:10:43.577
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:10:43.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:10:43.603
  STEP: Creating a ReplaceConcurrent cronjob @ 06/10/23 13:10:43.613
  STEP: Ensuring a job is scheduled @ 06/10/23 13:10:43.619
  STEP: Ensuring exactly one is scheduled @ 06/10/23 13:11:01.625
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 06/10/23 13:11:01.63
  STEP: Ensuring the job is replaced with a new one @ 06/10/23 13:11:01.635
  STEP: Removing cronjob @ 06/10/23 13:12:01.639
  Jun 10 13:12:01.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3264" for this suite. @ 06/10/23 13:12:01.651
• [78.082 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 06/10/23 13:12:01.659
  Jun 10 13:12:01.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename downward-api @ 06/10/23 13:12:01.661
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:12:01.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:12:01.688
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 13:12:01.693
  STEP: Saw pod success @ 06/10/23 13:12:05.721
  Jun 10 13:12:05.725: INFO: Trying to get logs from node ip-172-31-89-0 pod downwardapi-volume-c72c5770-aba3-4910-ab2f-02ff2e0f4f10 container client-container: <nil>
  STEP: delete the pod @ 06/10/23 13:12:05.748
  Jun 10 13:12:05.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9109" for this suite. @ 06/10/23 13:12:05.772
• [4.121 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 06/10/23 13:12:05.781
  Jun 10 13:12:05.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 13:12:05.782
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:12:05.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:12:05.804
  STEP: Setting up server cert @ 06/10/23 13:12:05.836
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 13:12:06.401
  STEP: Deploying the webhook pod @ 06/10/23 13:12:06.411
  STEP: Wait for the deployment to be ready @ 06/10/23 13:12:06.424
  Jun 10 13:12:06.433: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/10/23 13:12:08.444
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 13:12:08.456
  Jun 10 13:12:09.456: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 06/10/23 13:12:09.462
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/10/23 13:12:09.48
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 06/10/23 13:12:09.49
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/10/23 13:12:09.503
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 06/10/23 13:12:09.514
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/10/23 13:12:09.523
  Jun 10 13:12:09.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2796" for this suite. @ 06/10/23 13:12:09.579
  STEP: Destroying namespace "webhook-markers-7067" for this suite. @ 06/10/23 13:12:09.586
• [3.816 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 06/10/23 13:12:09.604
  Jun 10 13:12:09.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename downward-api @ 06/10/23 13:12:09.605
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:12:09.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:12:09.629
  STEP: Creating a pod to test downward api env vars @ 06/10/23 13:12:09.633
  STEP: Saw pod success @ 06/10/23 13:12:13.664
  Jun 10 13:12:13.668: INFO: Trying to get logs from node ip-172-31-89-0 pod downward-api-6bf6c786-3c38-47a0-ad70-1b2783f6790d container dapi-container: <nil>
  STEP: delete the pod @ 06/10/23 13:12:13.677
  Jun 10 13:12:13.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3647" for this suite. @ 06/10/23 13:12:13.698
• [4.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 06/10/23 13:12:13.71
  Jun 10 13:12:13.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename job @ 06/10/23 13:12:13.711
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:12:13.73
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:12:13.734
  STEP: Creating a job @ 06/10/23 13:12:13.738
  STEP: Ensuring active pods == parallelism @ 06/10/23 13:12:13.746
  STEP: Orphaning one of the Job's Pods @ 06/10/23 13:12:15.751
  Jun 10 13:12:16.269: INFO: Successfully updated pod "adopt-release-p6lkg"
  STEP: Checking that the Job readopts the Pod @ 06/10/23 13:12:16.269
  STEP: Removing the labels from the Job's Pod @ 06/10/23 13:12:18.278
  Jun 10 13:12:18.792: INFO: Successfully updated pod "adopt-release-p6lkg"
  STEP: Checking that the Job releases the Pod @ 06/10/23 13:12:18.792
  Jun 10 13:12:20.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3899" for this suite. @ 06/10/23 13:12:20.808
• [7.106 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 06/10/23 13:12:20.816
  Jun 10 13:12:20.816: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename resourcequota @ 06/10/23 13:12:20.817
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:12:20.834
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:12:20.847
  STEP: Counting existing ResourceQuota @ 06/10/23 13:12:20.851
  STEP: Creating a ResourceQuota @ 06/10/23 13:12:25.856
  STEP: Ensuring resource quota status is calculated @ 06/10/23 13:12:25.861
  STEP: Creating a Pod that fits quota @ 06/10/23 13:12:27.867
  STEP: Ensuring ResourceQuota status captures the pod usage @ 06/10/23 13:12:27.884
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 06/10/23 13:12:29.889
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 06/10/23 13:12:29.892
  STEP: Ensuring a pod cannot update its resource requirements @ 06/10/23 13:12:29.894
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 06/10/23 13:12:29.899
  STEP: Deleting the pod @ 06/10/23 13:12:31.903
  STEP: Ensuring resource quota status released the pod usage @ 06/10/23 13:12:31.919
  Jun 10 13:12:33.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2751" for this suite. @ 06/10/23 13:12:33.93
• [13.121 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 06/10/23 13:12:33.938
  Jun 10 13:12:33.938: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename disruption @ 06/10/23 13:12:33.939
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:12:33.96
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:12:33.963
  STEP: Creating a kubernetes client @ 06/10/23 13:12:33.966
  Jun 10 13:12:33.966: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename disruption-2 @ 06/10/23 13:12:33.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:12:33.986
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:12:33.989
  STEP: Waiting for the pdb to be processed @ 06/10/23 13:12:34
  STEP: Waiting for the pdb to be processed @ 06/10/23 13:12:36.014
  STEP: Waiting for the pdb to be processed @ 06/10/23 13:12:38.028
  STEP: listing a collection of PDBs across all namespaces @ 06/10/23 13:12:40.038
  STEP: listing a collection of PDBs in namespace disruption-4033 @ 06/10/23 13:12:40.042
  STEP: deleting a collection of PDBs @ 06/10/23 13:12:40.045
  STEP: Waiting for the PDB collection to be deleted @ 06/10/23 13:12:40.058
  Jun 10 13:12:40.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 10 13:12:40.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-5356" for this suite. @ 06/10/23 13:12:40.071
  STEP: Destroying namespace "disruption-4033" for this suite. @ 06/10/23 13:12:40.078
• [6.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 06/10/23 13:12:40.092
  Jun 10 13:12:40.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 13:12:40.093
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:12:40.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:12:40.116
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 13:12:40.12
  STEP: Saw pod success @ 06/10/23 13:12:44.152
  Jun 10 13:12:44.156: INFO: Trying to get logs from node ip-172-31-27-177 pod downwardapi-volume-68625723-fc4a-4bcb-a84e-e419246cc5de container client-container: <nil>
  STEP: delete the pod @ 06/10/23 13:12:44.18
  Jun 10 13:12:44.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-504" for this suite. @ 06/10/23 13:12:44.201
• [4.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 06/10/23 13:12:44.21
  Jun 10 13:12:44.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename replication-controller @ 06/10/23 13:12:44.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:12:44.231
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:12:44.235
  STEP: Creating replication controller my-hostname-basic-7394fbad-71d6-4801-8bba-b06d842c3604 @ 06/10/23 13:12:44.24
  Jun 10 13:12:44.252: INFO: Pod name my-hostname-basic-7394fbad-71d6-4801-8bba-b06d842c3604: Found 0 pods out of 1
  Jun 10 13:12:49.256: INFO: Pod name my-hostname-basic-7394fbad-71d6-4801-8bba-b06d842c3604: Found 1 pods out of 1
  Jun 10 13:12:49.256: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-7394fbad-71d6-4801-8bba-b06d842c3604" are running
  Jun 10 13:12:49.260: INFO: Pod "my-hostname-basic-7394fbad-71d6-4801-8bba-b06d842c3604-w5wmw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-10 13:12:44 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-10 13:12:45 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-10 13:12:45 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-10 13:12:44 +0000 UTC Reason: Message:}])
  Jun 10 13:12:49.260: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 06/10/23 13:12:49.26
  Jun 10 13:12:49.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1476" for this suite. @ 06/10/23 13:12:49.278
• [5.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 06/10/23 13:12:49.296
  Jun 10 13:12:49.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 13:12:49.297
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:12:49.33
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:12:49.333
  STEP: Setting up server cert @ 06/10/23 13:12:49.37
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 13:12:49.834
  STEP: Deploying the webhook pod @ 06/10/23 13:12:49.845
  STEP: Wait for the deployment to be ready @ 06/10/23 13:12:49.859
  Jun 10 13:12:49.868: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/10/23 13:12:51.881
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 13:12:51.894
  Jun 10 13:12:52.894: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 06/10/23 13:12:52.899
  STEP: create a configmap that should be updated by the webhook @ 06/10/23 13:12:52.916
  Jun 10 13:12:52.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4697" for this suite. @ 06/10/23 13:12:52.986
  STEP: Destroying namespace "webhook-markers-4920" for this suite. @ 06/10/23 13:12:52.994
• [3.710 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 06/10/23 13:12:53.009
  Jun 10 13:12:53.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir @ 06/10/23 13:12:53.01
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:12:53.03
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:12:53.034
  STEP: Creating a pod to test emptydir volume type on node default medium @ 06/10/23 13:12:53.037
  STEP: Saw pod success @ 06/10/23 13:12:57.063
  Jun 10 13:12:57.068: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-7efd040b-2949-4721-af9a-0520da44e06d container test-container: <nil>
  STEP: delete the pod @ 06/10/23 13:12:57.075
  Jun 10 13:12:57.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5527" for this suite. @ 06/10/23 13:12:57.098
• [4.095 seconds]
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 06/10/23 13:12:57.105
  Jun 10 13:12:57.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename sched-preemption @ 06/10/23 13:12:57.107
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:12:57.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:12:57.13
  Jun 10 13:12:57.149: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun 10 13:13:57.169: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 06/10/23 13:13:57.173
  Jun 10 13:13:57.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename sched-preemption-path @ 06/10/23 13:13:57.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:13:57.196
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:13:57.2
  Jun 10 13:13:57.221: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Jun 10 13:13:57.226: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Jun 10 13:13:57.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 10 13:13:57.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-9918" for this suite. @ 06/10/23 13:13:57.334
  STEP: Destroying namespace "sched-preemption-6545" for this suite. @ 06/10/23 13:13:57.346
• [60.250 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 06/10/23 13:13:57.356
  Jun 10 13:13:57.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename dns @ 06/10/23 13:13:57.357
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:13:57.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:13:57.382
  STEP: Creating a test headless service @ 06/10/23 13:13:57.386
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-127.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-127.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-127.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-127.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-127.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-127.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-127.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-127.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-127.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-127.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 175.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.175_tcp@PTR;sleep 1; done
   @ 06/10/23 13:13:57.408
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-127.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-127.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-127.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-127.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-127.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-127.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-127.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-127.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-127.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-127.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 175.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.175_tcp@PTR;sleep 1; done
   @ 06/10/23 13:13:57.408
  STEP: creating a pod to probe DNS @ 06/10/23 13:13:57.408
  STEP: submitting the pod to kubernetes @ 06/10/23 13:13:57.408
  STEP: retrieving the pod @ 06/10/23 13:13:59.447
  STEP: looking for the results for each expected name from probers @ 06/10/23 13:13:59.451
  Jun 10 13:13:59.457: INFO: Unable to read wheezy_udp@dns-test-service.dns-127.svc.cluster.local from pod dns-127/dns-test-f6f4bdbf-385a-42a2-917f-245720562525: the server could not find the requested resource (get pods dns-test-f6f4bdbf-385a-42a2-917f-245720562525)
  Jun 10 13:13:59.460: INFO: Unable to read wheezy_tcp@dns-test-service.dns-127.svc.cluster.local from pod dns-127/dns-test-f6f4bdbf-385a-42a2-917f-245720562525: the server could not find the requested resource (get pods dns-test-f6f4bdbf-385a-42a2-917f-245720562525)
  Jun 10 13:13:59.465: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-127.svc.cluster.local from pod dns-127/dns-test-f6f4bdbf-385a-42a2-917f-245720562525: the server could not find the requested resource (get pods dns-test-f6f4bdbf-385a-42a2-917f-245720562525)
  Jun 10 13:13:59.469: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-127.svc.cluster.local from pod dns-127/dns-test-f6f4bdbf-385a-42a2-917f-245720562525: the server could not find the requested resource (get pods dns-test-f6f4bdbf-385a-42a2-917f-245720562525)
  Jun 10 13:13:59.488: INFO: Unable to read jessie_udp@dns-test-service.dns-127.svc.cluster.local from pod dns-127/dns-test-f6f4bdbf-385a-42a2-917f-245720562525: the server could not find the requested resource (get pods dns-test-f6f4bdbf-385a-42a2-917f-245720562525)
  Jun 10 13:13:59.492: INFO: Unable to read jessie_tcp@dns-test-service.dns-127.svc.cluster.local from pod dns-127/dns-test-f6f4bdbf-385a-42a2-917f-245720562525: the server could not find the requested resource (get pods dns-test-f6f4bdbf-385a-42a2-917f-245720562525)
  Jun 10 13:13:59.496: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-127.svc.cluster.local from pod dns-127/dns-test-f6f4bdbf-385a-42a2-917f-245720562525: the server could not find the requested resource (get pods dns-test-f6f4bdbf-385a-42a2-917f-245720562525)
  Jun 10 13:13:59.500: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-127.svc.cluster.local from pod dns-127/dns-test-f6f4bdbf-385a-42a2-917f-245720562525: the server could not find the requested resource (get pods dns-test-f6f4bdbf-385a-42a2-917f-245720562525)
  Jun 10 13:13:59.517: INFO: Lookups using dns-127/dns-test-f6f4bdbf-385a-42a2-917f-245720562525 failed for: [wheezy_udp@dns-test-service.dns-127.svc.cluster.local wheezy_tcp@dns-test-service.dns-127.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-127.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-127.svc.cluster.local jessie_udp@dns-test-service.dns-127.svc.cluster.local jessie_tcp@dns-test-service.dns-127.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-127.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-127.svc.cluster.local]

  Jun 10 13:14:04.584: INFO: DNS probes using dns-127/dns-test-f6f4bdbf-385a-42a2-917f-245720562525 succeeded

  Jun 10 13:14:04.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/10/23 13:14:04.589
  STEP: deleting the test service @ 06/10/23 13:14:04.607
  STEP: deleting the test headless service @ 06/10/23 13:14:04.628
  STEP: Destroying namespace "dns-127" for this suite. @ 06/10/23 13:14:04.646
• [7.298 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 06/10/23 13:14:04.657
  Jun 10 13:14:04.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename downward-api @ 06/10/23 13:14:04.658
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:14:04.687
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:14:04.691
  STEP: Creating a pod to test downward api env vars @ 06/10/23 13:14:04.694
  STEP: Saw pod success @ 06/10/23 13:14:08.717
  Jun 10 13:14:08.721: INFO: Trying to get logs from node ip-172-31-27-177 pod downward-api-baf8fb53-2f4f-4f47-af40-7b1723b1bed3 container dapi-container: <nil>
  STEP: delete the pod @ 06/10/23 13:14:08.73
  Jun 10 13:14:08.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4961" for this suite. @ 06/10/23 13:14:08.755
• [4.105 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 06/10/23 13:14:08.762
  Jun 10 13:14:08.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename gc @ 06/10/23 13:14:08.763
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:14:08.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:14:08.788
  STEP: create the deployment @ 06/10/23 13:14:08.792
  W0610 13:14:08.798417      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 06/10/23 13:14:08.798
  STEP: delete the deployment @ 06/10/23 13:14:09.31
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 06/10/23 13:14:09.319
  STEP: Gathering metrics @ 06/10/23 13:14:09.847
  W0610 13:14:09.851877      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 10 13:14:09.852: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 10 13:14:09.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1032" for this suite. @ 06/10/23 13:14:09.857
• [1.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 06/10/23 13:14:09.869
  Jun 10 13:14:09.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename crd-watch @ 06/10/23 13:14:09.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:14:09.888
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:14:09.892
  Jun 10 13:14:09.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Creating first CR  @ 06/10/23 13:14:12.449
  Jun 10 13:14:12.457: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-10T13:14:12Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-10T13:14:12Z]] name:name1 resourceVersion:28033 uid:62d4ff1e-172d-447e-bbba-eabb1d45519c] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 06/10/23 13:14:22.461
  Jun 10 13:14:22.468: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-10T13:14:22Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-10T13:14:22Z]] name:name2 resourceVersion:28093 uid:fc7aa7f3-e9a5-40f2-8781-b6b871a12aef] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 06/10/23 13:14:32.471
  Jun 10 13:14:32.481: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-10T13:14:12Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-10T13:14:32Z]] name:name1 resourceVersion:28113 uid:62d4ff1e-172d-447e-bbba-eabb1d45519c] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 06/10/23 13:14:42.485
  Jun 10 13:14:42.492: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-10T13:14:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-10T13:14:42Z]] name:name2 resourceVersion:28133 uid:fc7aa7f3-e9a5-40f2-8781-b6b871a12aef] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 06/10/23 13:14:52.494
  Jun 10 13:14:52.504: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-10T13:14:12Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-10T13:14:32Z]] name:name1 resourceVersion:28152 uid:62d4ff1e-172d-447e-bbba-eabb1d45519c] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 06/10/23 13:15:02.504
  Jun 10 13:15:02.517: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-10T13:14:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-10T13:14:42Z]] name:name2 resourceVersion:28172 uid:fc7aa7f3-e9a5-40f2-8781-b6b871a12aef] num:map[num1:9223372036854775807 num2:1000000]]}
  Jun 10 13:15:13.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-2123" for this suite. @ 06/10/23 13:15:13.041
• [63.180 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 06/10/23 13:15:13.055
  Jun 10 13:15:13.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename downward-api @ 06/10/23 13:15:13.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:15:13.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:15:13.079
  STEP: Creating the pod @ 06/10/23 13:15:13.083
  Jun 10 13:15:15.642: INFO: Successfully updated pod "labelsupdate5a1a3afe-e046-4323-bbe5-23ae95e821ba"
  Jun 10 13:15:17.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9561" for this suite. @ 06/10/23 13:15:17.664
• [4.617 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 06/10/23 13:15:17.672
  Jun 10 13:15:17.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 13:15:17.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:15:17.694
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:15:17.697
  STEP: Creating projection with secret that has name projected-secret-test-0bc2b10b-0612-43e8-aebf-606c3a0926ed @ 06/10/23 13:15:17.706
  STEP: Creating a pod to test consume secrets @ 06/10/23 13:15:17.712
  STEP: Saw pod success @ 06/10/23 13:15:21.738
  Jun 10 13:15:21.741: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-projected-secrets-ea1b401b-9abb-47e9-8e39-4efd0b95f206 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/10/23 13:15:21.749
  Jun 10 13:15:21.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-270" for this suite. @ 06/10/23 13:15:21.768
• [4.103 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 06/10/23 13:15:21.776
  Jun 10 13:15:21.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 13:15:21.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:15:21.799
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:15:21.802
  STEP: Setting up server cert @ 06/10/23 13:15:21.832
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 13:15:22.199
  STEP: Deploying the webhook pod @ 06/10/23 13:15:22.208
  STEP: Wait for the deployment to be ready @ 06/10/23 13:15:22.223
  Jun 10 13:15:22.231: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/10/23 13:15:24.243
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 13:15:24.254
  Jun 10 13:15:25.254: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 06/10/23 13:15:25.258
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 06/10/23 13:15:25.28
  STEP: Creating a configMap that should not be mutated @ 06/10/23 13:15:25.289
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 06/10/23 13:15:25.302
  STEP: Creating a configMap that should be mutated @ 06/10/23 13:15:25.311
  Jun 10 13:15:25.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6995" for this suite. @ 06/10/23 13:15:25.421
  STEP: Destroying namespace "webhook-markers-2297" for this suite. @ 06/10/23 13:15:25.438
• [3.679 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 06/10/23 13:15:25.457
  Jun 10 13:15:25.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename var-expansion @ 06/10/23 13:15:25.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:15:25.482
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:15:25.486
  Jun 10 13:15:27.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 10 13:15:27.517: INFO: Deleting pod "var-expansion-17b43b75-4e83-4d8c-9feb-467983ab7331" in namespace "var-expansion-7310"
  Jun 10 13:15:27.524: INFO: Wait up to 5m0s for pod "var-expansion-17b43b75-4e83-4d8c-9feb-467983ab7331" to be fully deleted
  STEP: Destroying namespace "var-expansion-7310" for this suite. @ 06/10/23 13:15:29.532
• [4.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 06/10/23 13:15:29.551
  Jun 10 13:15:29.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename svcaccounts @ 06/10/23 13:15:29.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:15:29.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:15:29.578
  STEP: creating a ServiceAccount @ 06/10/23 13:15:29.582
  STEP: watching for the ServiceAccount to be added @ 06/10/23 13:15:29.592
  STEP: patching the ServiceAccount @ 06/10/23 13:15:29.595
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 06/10/23 13:15:29.602
  STEP: deleting the ServiceAccount @ 06/10/23 13:15:29.606
  Jun 10 13:15:29.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1950" for this suite. @ 06/10/23 13:15:29.627
• [0.085 seconds]
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 06/10/23 13:15:29.637
  Jun 10 13:15:29.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename replication-controller @ 06/10/23 13:15:29.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:15:29.657
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:15:29.661
  Jun 10 13:15:29.665: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 06/10/23 13:15:30.68
  STEP: Checking rc "condition-test" has the desired failure condition set @ 06/10/23 13:15:30.686
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 06/10/23 13:15:31.694
  Jun 10 13:15:31.704: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 06/10/23 13:15:31.704
  Jun 10 13:15:32.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-140" for this suite. @ 06/10/23 13:15:32.718
• [3.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 06/10/23 13:15:32.734
  Jun 10 13:15:32.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename downward-api @ 06/10/23 13:15:32.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:15:32.757
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:15:32.761
  STEP: Creating a pod to test downward api env vars @ 06/10/23 13:15:32.765
  STEP: Saw pod success @ 06/10/23 13:15:36.789
  Jun 10 13:15:36.793: INFO: Trying to get logs from node ip-172-31-27-177 pod downward-api-7cce3dbb-a938-4101-86c7-116e9470f4a5 container dapi-container: <nil>
  STEP: delete the pod @ 06/10/23 13:15:36.801
  Jun 10 13:15:36.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7227" for this suite. @ 06/10/23 13:15:36.825
• [4.100 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 06/10/23 13:15:36.834
  Jun 10 13:15:36.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename field-validation @ 06/10/23 13:15:36.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:15:36.857
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:15:36.86
  Jun 10 13:15:36.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:15:39.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5335" for this suite. @ 06/10/23 13:15:39.473
• [2.646 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 06/10/23 13:15:39.482
  Jun 10 13:15:39.482: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename replication-controller @ 06/10/23 13:15:39.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:15:39.502
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:15:39.509
  STEP: creating a ReplicationController @ 06/10/23 13:15:39.517
  STEP: waiting for RC to be added @ 06/10/23 13:15:39.524
  STEP: waiting for available Replicas @ 06/10/23 13:15:39.524
  STEP: patching ReplicationController @ 06/10/23 13:15:40.712
  STEP: waiting for RC to be modified @ 06/10/23 13:15:40.722
  STEP: patching ReplicationController status @ 06/10/23 13:15:40.722
  STEP: waiting for RC to be modified @ 06/10/23 13:15:40.73
  STEP: waiting for available Replicas @ 06/10/23 13:15:40.73
  STEP: fetching ReplicationController status @ 06/10/23 13:15:40.735
  STEP: patching ReplicationController scale @ 06/10/23 13:15:40.739
  STEP: waiting for RC to be modified @ 06/10/23 13:15:40.748
  STEP: waiting for ReplicationController's scale to be the max amount @ 06/10/23 13:15:40.748
  STEP: fetching ReplicationController; ensuring that it's patched @ 06/10/23 13:15:41.904
  STEP: updating ReplicationController status @ 06/10/23 13:15:41.907
  STEP: waiting for RC to be modified @ 06/10/23 13:15:41.915
  STEP: listing all ReplicationControllers @ 06/10/23 13:15:41.916
  STEP: checking that ReplicationController has expected values @ 06/10/23 13:15:41.923
  STEP: deleting ReplicationControllers by collection @ 06/10/23 13:15:41.923
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 06/10/23 13:15:41.932
  Jun 10 13:15:41.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0610 13:15:41.993628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-7837" for this suite. @ 06/10/23 13:15:41.998
• [2.524 seconds]
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 06/10/23 13:15:42.006
  Jun 10 13:15:42.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 13:15:42.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:15:42.029
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:15:42.032
  STEP: Creating projection with secret that has name projected-secret-test-ed3abb4e-fe59-49a4-9c4d-7411371b9dcc @ 06/10/23 13:15:42.037
  STEP: Creating a pod to test consume secrets @ 06/10/23 13:15:42.043
  E0610 13:15:42.994506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:15:43.994979      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:15:44.995900      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:15:45.996550      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:15:46.07
  Jun 10 13:15:46.074: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-projected-secrets-587e0f07-6777-4ca6-9948-50d7bd217bd1 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/10/23 13:15:46.082
  Jun 10 13:15:46.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3478" for this suite. @ 06/10/23 13:15:46.106
• [4.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 06/10/23 13:15:46.114
  Jun 10 13:15:46.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/10/23 13:15:46.115
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:15:46.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:15:46.137
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 06/10/23 13:15:46.141
  Jun 10 13:15:46.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  E0610 13:15:46.996612      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:15:47.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  E0610 13:15:47.997385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:15:48.997510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:15:49.997501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:15:50.997737      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:15:51.998191      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:15:52.998454      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:15:53.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7341" for this suite. @ 06/10/23 13:15:53.937
• [7.831 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 06/10/23 13:15:53.946
  Jun 10 13:15:53.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename security-context @ 06/10/23 13:15:53.947
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:15:53.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:15:53.97
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 06/10/23 13:15:53.975
  E0610 13:15:53.998851      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:15:54.999410      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:15:56.000005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:15:57.000147      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:15:58.000915      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:15:58.003
  Jun 10 13:15:58.008: INFO: Trying to get logs from node ip-172-31-27-177 pod security-context-7f7101f3-3ba5-4e99-8750-1174a9a4fb35 container test-container: <nil>
  STEP: delete the pod @ 06/10/23 13:15:58.028
  Jun 10 13:15:58.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-9085" for this suite. @ 06/10/23 13:15:58.052
• [4.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 06/10/23 13:15:58.066
  Jun 10 13:15:58.066: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename init-container @ 06/10/23 13:15:58.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:15:58.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:15:58.093
  STEP: creating the pod @ 06/10/23 13:15:58.097
  Jun 10 13:15:58.097: INFO: PodSpec: initContainers in spec.initContainers
  E0610 13:15:59.001179      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:00.002159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:01.002426      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:16:01.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6724" for this suite. @ 06/10/23 13:16:01.787
• [3.728 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 06/10/23 13:16:01.795
  Jun 10 13:16:01.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename csiinlinevolumes @ 06/10/23 13:16:01.796
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:16:01.814
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:16:01.818
  STEP: creating @ 06/10/23 13:16:01.823
  STEP: getting @ 06/10/23 13:16:01.844
  STEP: listing in namespace @ 06/10/23 13:16:01.848
  STEP: patching @ 06/10/23 13:16:01.867
  STEP: deleting @ 06/10/23 13:16:01.878
  Jun 10 13:16:01.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-5461" for this suite. @ 06/10/23 13:16:01.896
• [0.109 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 06/10/23 13:16:01.906
  Jun 10 13:16:01.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename ingress @ 06/10/23 13:16:01.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:16:01.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:16:01.931
  STEP: getting /apis @ 06/10/23 13:16:01.935
  STEP: getting /apis/networking.k8s.io @ 06/10/23 13:16:01.94
  STEP: getting /apis/networking.k8s.iov1 @ 06/10/23 13:16:01.941
  STEP: creating @ 06/10/23 13:16:01.943
  STEP: getting @ 06/10/23 13:16:01.967
  STEP: listing @ 06/10/23 13:16:01.975
  STEP: watching @ 06/10/23 13:16:01.982
  Jun 10 13:16:01.983: INFO: starting watch
  STEP: cluster-wide listing @ 06/10/23 13:16:01.99
  STEP: cluster-wide watching @ 06/10/23 13:16:01.999
  Jun 10 13:16:01.999: INFO: starting watch
  STEP: patching @ 06/10/23 13:16:02.003
  E0610 13:16:02.003932      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating @ 06/10/23 13:16:02.01
  Jun 10 13:16:02.025: INFO: waiting for watch events with expected annotations
  Jun 10 13:16:02.026: INFO: saw patched and updated annotations
  STEP: patching /status @ 06/10/23 13:16:02.026
  STEP: updating /status @ 06/10/23 13:16:02.039
  STEP: get /status @ 06/10/23 13:16:02.055
  STEP: deleting @ 06/10/23 13:16:02.064
  STEP: deleting a collection @ 06/10/23 13:16:02.088
  Jun 10 13:16:02.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-7987" for this suite. @ 06/10/23 13:16:02.118
• [0.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 06/10/23 13:16:02.128
  Jun 10 13:16:02.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename security-context @ 06/10/23 13:16:02.129
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:16:02.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:16:02.171
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 06/10/23 13:16:02.175
  E0610 13:16:03.004606      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:04.004709      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:05.005483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:06.005601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:16:06.209
  Jun 10 13:16:06.213: INFO: Trying to get logs from node ip-172-31-27-177 pod security-context-f71841ae-6b92-4c16-bd9e-6dfee4962bf7 container test-container: <nil>
  STEP: delete the pod @ 06/10/23 13:16:06.221
  Jun 10 13:16:06.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-4499" for this suite. @ 06/10/23 13:16:06.244
• [4.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 06/10/23 13:16:06.253
  Jun 10 13:16:06.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename deployment @ 06/10/23 13:16:06.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:16:06.274
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:16:06.278
  Jun 10 13:16:06.282: INFO: Creating deployment "webserver-deployment"
  Jun 10 13:16:06.290: INFO: Waiting for observed generation 1
  E0610 13:16:07.006287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:08.006975      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:16:08.299: INFO: Waiting for all required pods to come up
  Jun 10 13:16:08.304: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 06/10/23 13:16:08.304
  E0610 13:16:09.007260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:10.008143      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:16:10.315: INFO: Waiting for deployment "webserver-deployment" to complete
  Jun 10 13:16:10.323: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Jun 10 13:16:10.335: INFO: Updating deployment webserver-deployment
  Jun 10 13:16:10.335: INFO: Waiting for observed generation 2
  E0610 13:16:11.008338      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:12.008425      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:16:12.349: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Jun 10 13:16:12.354: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Jun 10 13:16:12.360: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jun 10 13:16:12.374: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Jun 10 13:16:12.374: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Jun 10 13:16:12.379: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jun 10 13:16:12.391: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Jun 10 13:16:12.391: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Jun 10 13:16:12.404: INFO: Updating deployment webserver-deployment
  Jun 10 13:16:12.404: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Jun 10 13:16:12.415: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Jun 10 13:16:12.431: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Jun 10 13:16:12.457: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-3579  fe674d26-306a-45b8-9b0e-4256428f0517 29267 3 2023-06-10 13:16:06 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-10 13:16:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 13:16:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f537a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-06-10 13:16:10 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-10 13:16:12 +0000 UTC,LastTransitionTime:2023-06-10 13:16:12 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Jun 10 13:16:12.469: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-3579  fa5d6b81-802b-44e8-b701-bfc579b34356 29264 3 2023-06-10 13:16:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment fe674d26-306a-45b8-9b0e-4256428f0517 0xc002f53c67 0xc002f53c68}] [] [{kube-controller-manager Update apps/v1 2023-06-10 13:16:10 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-10 13:16:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe674d26-306a-45b8-9b0e-4256428f0517\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f53d08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 10 13:16:12.469: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Jun 10 13:16:12.469: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-3579  c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4 29261 3 2023-06-10 13:16:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment fe674d26-306a-45b8-9b0e-4256428f0517 0xc002f53b77 0xc002f53b78}] [] [{kube-controller-manager Update apps/v1 2023-06-10 13:16:10 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-10 13:16:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe674d26-306a-45b8-9b0e-4256428f0517\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f53c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Jun 10 13:16:12.501: INFO: Pod "webserver-deployment-67bd4bf6dc-2r4kr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2r4kr webserver-deployment-67bd4bf6dc- deployment-3579  7c3fd3b9-873e-48ac-83e2-a7171f599884 29280 0 2023-06-10 13:16:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4 0xc004004697 0xc004004698}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j5m8j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j5m8j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.501: INFO: Pod "webserver-deployment-67bd4bf6dc-6s2hp" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6s2hp webserver-deployment-67bd4bf6dc- deployment-3579  0bb9dac3-ddde-4104-b0db-6e2b7d84302b 29100 0 2023-06-10 13:16:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4 0xc0040047d7 0xc0040047d8}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:16:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.149.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l542s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l542s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-89-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.89.0,PodIP:192.168.149.107,StartTime:2023-06-10 13:16:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-10 13:16:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7d73970d913bf7e9cddc140195a30d69fb5f3803c57aadf37375f48ca87069fd,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.149.107,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.501: INFO: Pod "webserver-deployment-67bd4bf6dc-8glm7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8glm7 webserver-deployment-67bd4bf6dc- deployment-3579  f7425181-63f5-4052-98dc-ff290d5900a2 29279 0 2023-06-10 13:16:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4 0xc0040049c7 0xc0040049c8}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:16:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6zfbt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zfbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.177,PodIP:,StartTime:2023-06-10 13:16:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.502: INFO: Pod "webserver-deployment-67bd4bf6dc-8gmm6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8gmm6 webserver-deployment-67bd4bf6dc- deployment-3579  f59005cc-b81c-4643-9e5f-44ce8fe8ca3c 29282 0 2023-06-10 13:16:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4 0xc004004b97 0xc004004b98}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ggqtx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ggqtx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.502: INFO: Pod "webserver-deployment-67bd4bf6dc-8mvcz" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8mvcz webserver-deployment-67bd4bf6dc- deployment-3579  74c01440-b0c5-425f-aa6e-c544c08b9746 29113 0 2023-06-10 13:16:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4 0xc004004cd7 0xc004004cd8}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:16:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.92.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8qb6j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8qb6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-46-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.46.40,PodIP:192.168.92.46,StartTime:2023-06-10 13:16:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-10 13:16:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b26eacc3fa57b1295748bf9236adcee87fae144d6e994b36682f5972dd0a5793,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.92.46,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.502: INFO: Pod "webserver-deployment-67bd4bf6dc-99rqs" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-99rqs webserver-deployment-67bd4bf6dc- deployment-3579  44b2f791-8813-4810-a03b-3ea4a9fdc62b 29284 0 2023-06-10 13:16:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4 0xc004004ec7 0xc004004ec8}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-762nk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-762nk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.503: INFO: Pod "webserver-deployment-67bd4bf6dc-hgcgj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hgcgj webserver-deployment-67bd4bf6dc- deployment-3579  6571a06c-d9a1-42dd-9a27-4e36c86f916c 29274 0 2023-06-10 13:16:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4 0xc004005007 0xc004005008}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c9vlz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c9vlz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.503: INFO: Pod "webserver-deployment-67bd4bf6dc-l8nlf" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-l8nlf webserver-deployment-67bd4bf6dc- deployment-3579  13d1335c-88ae-42f6-b54e-3ced6ec8b894 29103 0 2023-06-10 13:16:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4 0xc004005170 0xc004005171}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:16:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.149.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-28mhb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-28mhb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-89-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.89.0,PodIP:192.168.149.115,StartTime:2023-06-10 13:16:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-10 13:16:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f6753a267e2634ca533f355269950bc3b3d27c299467a73238818f97ceec6371,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.149.115,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.504: INFO: Pod "webserver-deployment-67bd4bf6dc-nfx6m" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nfx6m webserver-deployment-67bd4bf6dc- deployment-3579  c8504ad4-04b1-4ea4-baaa-b822f1e8400f 29109 0 2023-06-10 13:16:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4 0xc004005357 0xc004005358}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:16:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.149.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v2jnc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v2jnc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-89-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.89.0,PodIP:192.168.149.85,StartTime:2023-06-10 13:16:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-10 13:16:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2720e7fc716df3489ab4dd1cf12a934132c319436169784031e6003ebb97988f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.149.85,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.504: INFO: Pod "webserver-deployment-67bd4bf6dc-nq82f" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nq82f webserver-deployment-67bd4bf6dc- deployment-3579  b3f26c93-5691-4354-a112-d60148173a26 29278 0 2023-06-10 13:16:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4 0xc004005567 0xc004005568}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gdmkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gdmkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-89-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.505: INFO: Pod "webserver-deployment-67bd4bf6dc-qfcqv" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-qfcqv webserver-deployment-67bd4bf6dc- deployment-3579  9b159187-ed51-43a8-b5f9-8df8a126de5e 29130 0 2023-06-10 13:16:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4 0xc0040056d0 0xc0040056d1}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:16:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.109.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x4xcn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x4xcn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.177,PodIP:192.168.109.5,StartTime:2023-06-10 13:16:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-10 13:16:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b248889402107a357647a9624d9020a4842dfa255e852d8c13769f15c7a69f1a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.109.5,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.505: INFO: Pod "webserver-deployment-67bd4bf6dc-ts8qs" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-ts8qs webserver-deployment-67bd4bf6dc- deployment-3579  08fc8b76-7ef1-4bfb-973a-34d961cdbf4d 29102 0 2023-06-10 13:16:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4 0xc0040058b7 0xc0040058b8}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:16:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.92.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bxnks,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bxnks,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-46-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.46.40,PodIP:192.168.92.43,StartTime:2023-06-10 13:16:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-10 13:16:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a5897e011c02e5480938349267240262e6516cf2348f3b7607d0a21fe068815c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.92.43,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.506: INFO: Pod "webserver-deployment-67bd4bf6dc-vvjvk" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vvjvk webserver-deployment-67bd4bf6dc- deployment-3579  b4c7257f-1b34-49c0-89b8-21780ad3e113 29108 0 2023-06-10 13:16:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4 0xc004005aa7 0xc004005aa8}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:16:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.92.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jw52t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jw52t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-46-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.46.40,PodIP:192.168.92.38,StartTime:2023-06-10 13:16:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-10 13:16:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a5182c3db23f40e3d4ac51c40b7dc47fb73d243bf6bc72c6b9bc4ec282ebd20a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.92.38,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.506: INFO: Pod "webserver-deployment-67bd4bf6dc-zjq6c" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-zjq6c webserver-deployment-67bd4bf6dc- deployment-3579  ca09e169-2820-490e-a292-09c1dbf9d1b7 29137 0 2023-06-10 13:16:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4 0xc004005cb7 0xc004005cb8}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4746c78-bde8-4e0c-9a73-d32e8c1cd7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:16:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.109.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fb7hn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fb7hn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.177,PodIP:192.168.109.60,StartTime:2023-06-10 13:16:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-10 13:16:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b2b00b66204c18184dc7e2c023390d88edabe0efe0f35c1c729f7b449f0e7df9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.109.60,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.507: INFO: Pod "webserver-deployment-7b75d79cf5-5gbqj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-5gbqj webserver-deployment-7b75d79cf5- deployment-3579  8d0ae8bc-e210-44ea-a561-d569a4039344 29275 0 2023-06-10 13:16:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 fa5d6b81-802b-44e8-b701-bfc579b34356 0xc004005ea7 0xc004005ea8}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa5d6b81-802b-44e8-b701-bfc579b34356\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ncwd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ncwd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.507: INFO: Pod "webserver-deployment-7b75d79cf5-98h6z" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-98h6z webserver-deployment-7b75d79cf5- deployment-3579  16e8ded4-c35a-4d41-a559-9c23a4415539 29252 0 2023-06-10 13:16:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 fa5d6b81-802b-44e8-b701-bfc579b34356 0xc004005ff7 0xc004005ff8}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa5d6b81-802b-44e8-b701-bfc579b34356\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:16:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.149.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5xqtq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5xqtq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-89-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.89.0,PodIP:192.168.149.106,StartTime:2023-06-10 13:16:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.149.106,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.508: INFO: Pod "webserver-deployment-7b75d79cf5-9fxv4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-9fxv4 webserver-deployment-7b75d79cf5- deployment-3579  1c775b8b-cc0e-4105-b66f-3fdd80281221 29256 0 2023-06-10 13:16:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 fa5d6b81-802b-44e8-b701-bfc579b34356 0xc0040ce210 0xc0040ce211}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa5d6b81-802b-44e8-b701-bfc579b34356\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:16:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.92.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dkfj4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dkfj4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-46-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.46.40,PodIP:192.168.92.42,StartTime:2023-06-10 13:16:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.92.42,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.509: INFO: Pod "webserver-deployment-7b75d79cf5-bnx5w" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-bnx5w webserver-deployment-7b75d79cf5- deployment-3579  87e3f389-83f1-46be-bc58-64319ff5672d 29196 0 2023-06-10 13:16:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 fa5d6b81-802b-44e8-b701-bfc579b34356 0xc0040ce427 0xc0040ce428}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa5d6b81-802b-44e8-b701-bfc579b34356\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:16:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cpbfx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cpbfx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.177,PodIP:,StartTime:2023-06-10 13:16:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.509: INFO: Pod "webserver-deployment-7b75d79cf5-h5lgm" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-h5lgm webserver-deployment-7b75d79cf5- deployment-3579  74c36ec9-2535-4d82-8510-001396657f72 29258 0 2023-06-10 13:16:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 fa5d6b81-802b-44e8-b701-bfc579b34356 0xc0040ce677 0xc0040ce678}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa5d6b81-802b-44e8-b701-bfc579b34356\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:16:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.149.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8bzv7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8bzv7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-89-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.89.0,PodIP:192.168.149.113,StartTime:2023-06-10 13:16:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.149.113,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.510: INFO: Pod "webserver-deployment-7b75d79cf5-lc968" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lc968 webserver-deployment-7b75d79cf5- deployment-3579  ecec605b-f004-4715-aa44-52a072dce8da 29273 0 2023-06-10 13:16:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 fa5d6b81-802b-44e8-b701-bfc579b34356 0xc0040ce8c0 0xc0040ce8c1}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa5d6b81-802b-44e8-b701-bfc579b34356\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gg7zv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gg7zv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-46-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.510: INFO: Pod "webserver-deployment-7b75d79cf5-ngh86" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-ngh86 webserver-deployment-7b75d79cf5- deployment-3579  23e801e7-ba78-4498-8b1d-283a4bafdc21 29277 0 2023-06-10 13:16:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 fa5d6b81-802b-44e8-b701-bfc579b34356 0xc0040cebd0 0xc0040cebd1}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa5d6b81-802b-44e8-b701-bfc579b34356\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fzhhr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fzhhr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.511: INFO: Pod "webserver-deployment-7b75d79cf5-zq295" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-zq295 webserver-deployment-7b75d79cf5- deployment-3579  c6589204-4e53-44fa-a37c-cdbb3c5a5fa6 29246 0 2023-06-10 13:16:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 fa5d6b81-802b-44e8-b701-bfc579b34356 0xc0040cee37 0xc0040cee38}] [] [{kube-controller-manager Update v1 2023-06-10 13:16:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa5d6b81-802b-44e8-b701-bfc579b34356\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:16:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.109.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kxjzs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kxjzs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:16:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.177,PodIP:192.168.109.23,StartTime:2023-06-10 13:16:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.109.23,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:16:12.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3579" for this suite. @ 06/10/23 13:16:12.553
• [6.346 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 06/10/23 13:16:12.601
  Jun 10 13:16:12.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename configmap @ 06/10/23 13:16:12.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:16:12.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:16:12.732
  STEP: Creating configMap with name configmap-test-upd-232bc8d4-5099-4846-9882-f8c8526cec5e @ 06/10/23 13:16:12.752
  STEP: Creating the pod @ 06/10/23 13:16:12.758
  E0610 13:16:13.009376      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:14.009552      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 06/10/23 13:16:14.792
  STEP: Waiting for pod with binary data @ 06/10/23 13:16:14.801
  Jun 10 13:16:14.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3056" for this suite. @ 06/10/23 13:16:14.813
• [2.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 06/10/23 13:16:14.822
  Jun 10 13:16:14.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename job @ 06/10/23 13:16:14.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:16:14.841
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:16:14.846
  STEP: Creating a job @ 06/10/23 13:16:14.851
  STEP: Ensure pods equal to parallelism count is attached to the job @ 06/10/23 13:16:14.858
  E0610 13:16:15.009632      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:16.009792      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 06/10/23 13:16:16.864
  STEP: updating /status @ 06/10/23 13:16:16.874
  STEP: get /status @ 06/10/23 13:16:16.906
  Jun 10 13:16:16.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4224" for this suite. @ 06/10/23 13:16:16.916
• [2.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 06/10/23 13:16:16.928
  Jun 10 13:16:16.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename tables @ 06/10/23 13:16:16.929
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:16:16.947
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:16:16.952
  Jun 10 13:16:16.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-7135" for this suite. @ 06/10/23 13:16:16.965
• [0.044 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 06/10/23 13:16:16.972
  Jun 10 13:16:16.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename csistoragecapacity @ 06/10/23 13:16:16.973
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:16:16.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:16:16.996
  STEP: getting /apis @ 06/10/23 13:16:16.999
  STEP: getting /apis/storage.k8s.io @ 06/10/23 13:16:17.005
  STEP: getting /apis/storage.k8s.io/v1 @ 06/10/23 13:16:17.007
  STEP: creating @ 06/10/23 13:16:17.008
  E0610 13:16:17.009855      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: watching @ 06/10/23 13:16:17.029
  Jun 10 13:16:17.029: INFO: starting watch
  STEP: getting @ 06/10/23 13:16:17.038
  STEP: listing in namespace @ 06/10/23 13:16:17.042
  STEP: listing across namespaces @ 06/10/23 13:16:17.046
  STEP: patching @ 06/10/23 13:16:17.05
  STEP: updating @ 06/10/23 13:16:17.057
  Jun 10 13:16:17.063: INFO: waiting for watch events with expected annotations in namespace
  Jun 10 13:16:17.063: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 06/10/23 13:16:17.063
  STEP: deleting a collection @ 06/10/23 13:16:17.079
  Jun 10 13:16:17.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-1898" for this suite. @ 06/10/23 13:16:17.102
• [0.138 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 06/10/23 13:16:17.112
  Jun 10 13:16:17.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename pods @ 06/10/23 13:16:17.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:16:17.132
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:16:17.137
  STEP: creating a Pod with a static label @ 06/10/23 13:16:17.149
  STEP: watching for Pod to be ready @ 06/10/23 13:16:17.158
  Jun 10 13:16:17.161: INFO: observed Pod pod-test in namespace pods-9874 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Jun 10 13:16:17.165: INFO: observed Pod pod-test in namespace pods-9874 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:16:17 +0000 UTC  }]
  Jun 10 13:16:17.183: INFO: observed Pod pod-test in namespace pods-9874 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:16:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:16:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:16:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:16:17 +0000 UTC  }]
  E0610 13:16:18.010553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:16:18.999: INFO: Found Pod pod-test in namespace pods-9874 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:16:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:16:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:16:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:16:17 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 06/10/23 13:16:19.009
  E0610 13:16:19.010624      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting the Pod and ensuring that it's patched @ 06/10/23 13:16:19.022
  STEP: replacing the Pod's status Ready condition to False @ 06/10/23 13:16:19.045
  STEP: check the Pod again to ensure its Ready conditions are False @ 06/10/23 13:16:19.068
  STEP: deleting the Pod via a Collection with a LabelSelector @ 06/10/23 13:16:19.068
  STEP: watching for the Pod to be deleted @ 06/10/23 13:16:19.091
  Jun 10 13:16:19.094: INFO: observed event type MODIFIED
  E0610 13:16:20.011632      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:16:21.006: INFO: observed event type MODIFIED
  E0610 13:16:21.012510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:16:21.466: INFO: observed event type MODIFIED
  Jun 10 13:16:22.010: INFO: observed event type MODIFIED
  E0610 13:16:22.013241      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:16:22.030: INFO: observed event type MODIFIED
  Jun 10 13:16:22.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9874" for this suite. @ 06/10/23 13:16:22.047
• [4.949 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 06/10/23 13:16:22.062
  Jun 10 13:16:22.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/10/23 13:16:22.063
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:16:22.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:16:22.085
  Jun 10 13:16:22.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  E0610 13:16:23.013811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 06/10/23 13:16:23.626
  Jun 10 13:16:23.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-6181 --namespace=crd-publish-openapi-6181 create -f -'
  E0610 13:16:24.014915      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:16:24.399: INFO: stderr: ""
  Jun 10 13:16:24.400: INFO: stdout: "e2e-test-crd-publish-openapi-2000-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jun 10 13:16:24.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-6181 --namespace=crd-publish-openapi-6181 delete e2e-test-crd-publish-openapi-2000-crds test-cr'
  Jun 10 13:16:24.488: INFO: stderr: ""
  Jun 10 13:16:24.489: INFO: stdout: "e2e-test-crd-publish-openapi-2000-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Jun 10 13:16:24.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-6181 --namespace=crd-publish-openapi-6181 apply -f -'
  Jun 10 13:16:24.738: INFO: stderr: ""
  Jun 10 13:16:24.738: INFO: stdout: "e2e-test-crd-publish-openapi-2000-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jun 10 13:16:24.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-6181 --namespace=crd-publish-openapi-6181 delete e2e-test-crd-publish-openapi-2000-crds test-cr'
  Jun 10 13:16:24.828: INFO: stderr: ""
  Jun 10 13:16:24.828: INFO: stdout: "e2e-test-crd-publish-openapi-2000-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 06/10/23 13:16:24.828
  Jun 10 13:16:24.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-6181 explain e2e-test-crd-publish-openapi-2000-crds'
  E0610 13:16:25.014983      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:16:25.624: INFO: stderr: ""
  Jun 10 13:16:25.624: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-2000-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0610 13:16:26.015562      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:27.016381      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:16:27.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6181" for this suite. @ 06/10/23 13:16:27.191
• [5.139 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 06/10/23 13:16:27.202
  Jun 10 13:16:27.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-probe @ 06/10/23 13:16:27.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:16:27.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:16:27.227
  STEP: Creating pod test-grpc-0bd349bd-d2b7-466a-9862-bd12416f4483 in namespace container-probe-5311 @ 06/10/23 13:16:27.23
  E0610 13:16:28.016495      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:29.016625      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:16:29.251: INFO: Started pod test-grpc-0bd349bd-d2b7-466a-9862-bd12416f4483 in namespace container-probe-5311
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/10/23 13:16:29.251
  Jun 10 13:16:29.255: INFO: Initial restart count of pod test-grpc-0bd349bd-d2b7-466a-9862-bd12416f4483 is 0
  E0610 13:16:30.016767      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:31.016893      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:32.017832      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:33.018731      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:34.019027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:35.020148      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:36.020348      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:37.020419      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:38.020475      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:39.020594      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:40.020719      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:41.021060      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:42.021193      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:43.021790      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:44.022678      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:45.023005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:46.023154      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:47.023191      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:48.024130      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:49.024220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:50.024367      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:51.024706      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:52.024811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:53.024861      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:54.025012      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:55.025100      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:56.025968      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:57.026099      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:58.026568      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:16:59.026679      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:00.027010      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:01.027150      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:02.027567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:03.028156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:04.029200      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:05.029302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:06.030375      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:07.030480      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:08.031498      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:09.032009      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:10.032130      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:11.032252      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:12.032495      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:13.032582      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:14.032686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:15.032788      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:16.032913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:17.033022      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:18.033164      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:19.033266      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:20.033391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:21.033484      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:22.033589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:23.034199      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:24.034306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:25.034412      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:26.034578      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:27.034659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:28.035403      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:29.035762      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:30.035890      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:31.036051      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:32.036201      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:33.036793      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:34.036868      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:35.036960      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:36.037108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:37.038126      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:38.038953      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:39.038998      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:40.039898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:41.039890      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:42.040327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:43.040434      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:44.040674      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:45.040791      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:46.041724      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:47.042653      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:48.043100      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:49.044202      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:50.045319      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:51.045531      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:52.045616      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:53.046161      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:54.046290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:55.046612      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:56.047292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:57.047364      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:58.048111      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:17:59.048216      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:00.048354      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:01.048587      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:02.049585      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:03.049595      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:04.050102      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:05.050386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:06.051159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:07.052207      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:08.052880      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:09.053017      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:10.053894      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:11.054023      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:12.055090      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:13.055596      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:14.056248      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:15.056330      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:16.057309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:17.057401      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:18.057500      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:19.058379      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:20.058459      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:21.058561      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:22.059028      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:23.059663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:24.060753      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:25.060861      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:26.060892      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:27.061367      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:28.061556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:29.061975      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:30.062082      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:31.062179      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:32.063168      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:33.063640      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:34.064070      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:35.064181      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:36.064279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:37.064788      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:38.065583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:39.065690      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:40.065816      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:41.066333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:42.067333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:43.068049      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:44.068205      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:45.068560      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:46.069293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:47.069585      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:48.070196      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:49.070299      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:50.070952      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:51.071264      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:52.071791      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:53.072678      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:54.073322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:55.073536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:56.074590      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:57.074769      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:58.075418      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:18:59.075750      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:00.076773      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:01.077069      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:02.077947      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:03.078083      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:04.078650      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:05.079544      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:06.080077      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:07.080483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:08.080600      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:09.080702      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:10.081289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:11.081391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:12.081614      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:13.082362      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:14.082545      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:15.082767      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:16.083411      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:17.083530      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:18.084070      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:19.084249      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:20.085243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:21.085362      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:22.085462      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:23.085565      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:24.085695      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:25.085791      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:26.086409      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:27.086520      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:28.086643      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:29.086936      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:30.087459      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:31.088385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:32.088636      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:33.089268      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:34.090090      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:35.090194      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:36.090503      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:37.090611      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:38.091459      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:39.091544      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:40.092474      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:41.092882      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:42.093676      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:43.094070      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:44.094664      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:45.094846      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:46.095570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:47.096234      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:48.096700      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:49.096810      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:50.097306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:51.097735      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:52.098129      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:53.098718      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:54.098831      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:55.098993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:56.099987      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:57.100238      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:58.101067      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:19:59.101163      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:00.102222      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:01.102319      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:02.102435      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:03.102525      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:04.103441      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:05.104070      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:06.104230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:07.104478      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:08.105323      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:09.105428      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:10.105859      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:11.105962      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:12.106844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:13.107312      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:14.107410      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:15.107514      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:16.107692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:17.107787      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:18.107846      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:19.107933      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:20.108780      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:21.108893      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:22.109827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:23.110734      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:24.110783      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:25.111007      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:26.112231      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:27.116268      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:28.117309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:29.117514      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:20:29.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/10/23 13:20:29.854
  STEP: Destroying namespace "container-probe-5311" for this suite. @ 06/10/23 13:20:29.871
• [242.677 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 06/10/23 13:20:29.88
  Jun 10 13:20:29.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename cronjob @ 06/10/23 13:20:29.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:20:29.9
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:20:29.904
  STEP: Creating a cronjob @ 06/10/23 13:20:29.907
  STEP: Ensuring more than one job is running at a time @ 06/10/23 13:20:29.915
  E0610 13:20:30.118389      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:31.118468      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:32.118973      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:33.119014      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:34.119855      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:35.119955      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:36.120072      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:37.120265      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:38.121391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:39.121490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:40.122251      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:41.122563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:42.123433      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:43.124001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:44.124928      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:45.125207      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:46.125535      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:47.125671      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:48.126757      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:49.127322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:50.127671      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:51.127938      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:52.128618      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:53.128698      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:54.128776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:55.128886      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:56.129747      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:57.130111      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:58.130475      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:20:59.130624      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:00.131454      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:01.132238      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:02.133063      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:03.133756      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:04.134807      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:05.135002      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:06.135724      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:07.135831      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:08.136477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:09.136587      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:10.136684      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:11.136877      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:12.137848      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:13.138799      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:14.139350      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:15.139484      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:16.139584      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:17.139694      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:18.140421      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:19.140564      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:20.140673      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:21.140741      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:22.141589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:23.142075      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:24.143045      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:25.144274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:26.144290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:27.144400      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:28.145382      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:29.145440      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:30.146462      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:31.146539      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:32.146580      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:33.146693      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:34.147699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:35.147825      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:36.148148      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:37.148235      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:38.149378      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:39.149448      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:40.150027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:41.150097      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:42.150847      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:43.151600      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:44.152276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:45.152391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:46.152769      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:47.152882      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:48.152974      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:49.153348      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:50.153435      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:51.153844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:52.153931      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:53.154613      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:54.155406      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:55.155501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:56.156031      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:57.156230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:58.157107      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:21:59.157216      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:00.157936      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:01.158030      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 06/10/23 13:22:01.92
  STEP: Removing cronjob @ 06/10/23 13:22:01.924
  Jun 10 13:22:01.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1744" for this suite. @ 06/10/23 13:22:01.935
• [92.063 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 06/10/23 13:22:01.946
  Jun 10 13:22:01.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 13:22:01.947
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:22:01.985
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:22:01.989
  STEP: Setting up server cert @ 06/10/23 13:22:02.024
  E0610 13:22:02.158600      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 13:22:02.752
  STEP: Deploying the webhook pod @ 06/10/23 13:22:02.762
  STEP: Wait for the deployment to be ready @ 06/10/23 13:22:02.775
  Jun 10 13:22:02.783: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0610 13:22:03.159431      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:04.160244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/10/23 13:22:04.794
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 13:22:04.806
  E0610 13:22:05.161343      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:22:05.806: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun 10 13:22:05.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  E0610 13:22:06.161418      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2549-crds.webhook.example.com via the AdmissionRegistration API @ 06/10/23 13:22:06.324
  STEP: Creating a custom resource that should be mutated by the webhook @ 06/10/23 13:22:06.345
  E0610 13:22:07.161553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:08.162148      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:22:08.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4631" for this suite. @ 06/10/23 13:22:08.961
  STEP: Destroying namespace "webhook-markers-331" for this suite. @ 06/10/23 13:22:08.969
• [7.037 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 06/10/23 13:22:08.984
  Jun 10 13:22:08.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir @ 06/10/23 13:22:08.986
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:22:09.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:22:09.017
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 06/10/23 13:22:09.021
  E0610 13:22:09.163195      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:10.163343      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:11.164015      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:12.164131      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:22:13.045
  Jun 10 13:22:13.049: INFO: Trying to get logs from node ip-172-31-89-0 pod pod-b16dac1e-64e4-4c3e-bbab-ae7e9e053638 container test-container: <nil>
  STEP: delete the pod @ 06/10/23 13:22:13.072
  E0610 13:22:13.164831      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:22:13.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3662" for this suite. @ 06/10/23 13:22:13.188
• [4.212 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 06/10/23 13:22:13.198
  Jun 10 13:22:13.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 13:22:13.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:22:13.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:22:13.231
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 13:22:13.235
  E0610 13:22:14.164996      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:15.165093      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:16.165284      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:17.165313      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:22:17.26
  Jun 10 13:22:17.263: INFO: Trying to get logs from node ip-172-31-89-0 pod downwardapi-volume-1c725abf-20be-4047-a588-ea21e5e79c90 container client-container: <nil>
  STEP: delete the pod @ 06/10/23 13:22:17.27
  Jun 10 13:22:17.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-256" for this suite. @ 06/10/23 13:22:17.293
• [4.102 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 06/10/23 13:22:17.303
  Jun 10 13:22:17.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename statefulset @ 06/10/23 13:22:17.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:22:17.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:22:17.325
  STEP: Creating service test in namespace statefulset-4765 @ 06/10/23 13:22:17.329
  STEP: Creating stateful set ss in namespace statefulset-4765 @ 06/10/23 13:22:17.336
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4765 @ 06/10/23 13:22:17.344
  Jun 10 13:22:17.348: INFO: Found 0 stateful pods, waiting for 1
  E0610 13:22:18.165556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:19.166504      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:20.166685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:21.166996      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:22.167009      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:23.167820      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:24.168260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:25.168510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:26.168663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:27.168827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:22:27.354: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 06/10/23 13:22:27.354
  Jun 10 13:22:27.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4765 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 10 13:22:27.524: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 10 13:22:27.524: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 10 13:22:27.524: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 10 13:22:27.529: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0610 13:22:28.168868      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:29.168958      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:30.169140      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:31.169265      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:32.169346      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:33.170067      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:34.170171      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:35.170368      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:36.170611      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:37.170889      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:22:37.534: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jun 10 13:22:37.534: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 10 13:22:37.557: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Jun 10 13:22:37.557: INFO: ss-0  ip-172-31-89-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:17 +0000 UTC  }]
  Jun 10 13:22:37.557: INFO: 
  Jun 10 13:22:37.557: INFO: StatefulSet ss has not reached scale 3, at 1
  E0610 13:22:38.171281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:22:38.563: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994784829s
  E0610 13:22:39.172256      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:22:39.568: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988967622s
  E0610 13:22:40.173087      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:22:40.574: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982940364s
  E0610 13:22:41.173159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:22:41.581: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976912055s
  E0610 13:22:42.173250      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:22:42.587: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971004238s
  E0610 13:22:43.174009      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:22:43.592: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965251532s
  E0610 13:22:44.174230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:22:44.598: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.95987772s
  E0610 13:22:45.175296      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:22:45.603: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.95377536s
  E0610 13:22:46.176273      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:22:46.608: INFO: Verifying statefulset ss doesn't scale past 3 for another 948.969864ms
  E0610 13:22:47.176379      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4765 @ 06/10/23 13:22:47.609
  Jun 10 13:22:47.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4765 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 10 13:22:47.776: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 10 13:22:47.776: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 10 13:22:47.776: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 10 13:22:47.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4765 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 10 13:22:47.932: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jun 10 13:22:47.932: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 10 13:22:47.932: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 10 13:22:47.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4765 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 10 13:22:48.101: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jun 10 13:22:48.101: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 10 13:22:48.101: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 10 13:22:48.106: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  E0610 13:22:48.177231      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:49.177400      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:50.177531      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:51.177627      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:52.177729      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:53.177848      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:54.177964      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:55.178061      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:56.178186      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:22:57.178301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:22:58.112: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 10 13:22:58.112: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 10 13:22:58.112: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 06/10/23 13:22:58.112
  Jun 10 13:22:58.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4765 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0610 13:22:58.178989      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:22:58.275: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 10 13:22:58.275: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 10 13:22:58.275: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 10 13:22:58.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4765 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 10 13:22:58.441: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 10 13:22:58.441: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 10 13:22:58.441: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 10 13:22:58.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4765 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 10 13:22:58.607: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 10 13:22:58.607: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 10 13:22:58.607: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 10 13:22:58.607: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 10 13:22:58.612: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0610 13:22:59.179032      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:00.180274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:01.180338      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:02.180438      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:03.180536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:04.180646      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:05.180748      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:06.180855      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:07.180969      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:08.181064      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:08.621: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jun 10 13:23:08.621: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jun 10 13:23:08.621: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jun 10 13:23:08.634: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Jun 10 13:23:08.634: INFO: ss-0  ip-172-31-89-0    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:17 +0000 UTC  }]
  Jun 10 13:23:08.634: INFO: ss-1  ip-172-31-27-177  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:37 +0000 UTC  }]
  Jun 10 13:23:08.634: INFO: ss-2  ip-172-31-46-40   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:37 +0000 UTC  }]
  Jun 10 13:23:08.635: INFO: 
  Jun 10 13:23:08.635: INFO: StatefulSet ss has not reached scale 0, at 3
  E0610 13:23:09.181240      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:09.640: INFO: POD   NODE              PHASE      GRACE  CONDITIONS
  Jun 10 13:23:09.640: INFO: ss-0  ip-172-31-89-0    Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:17 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:58 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:58 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:17 +0000 UTC  }]
  Jun 10 13:23:09.641: INFO: ss-1  ip-172-31-27-177  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:37 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:58 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:58 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:37 +0000 UTC  }]
  Jun 10 13:23:09.641: INFO: ss-2  ip-172-31-46-40   Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:37 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:58 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:58 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-10 13:22:37 +0000 UTC  }]
  Jun 10 13:23:09.641: INFO: 
  Jun 10 13:23:09.641: INFO: StatefulSet ss has not reached scale 0, at 3
  E0610 13:23:10.181978      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:10.646: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.989906728s
  E0610 13:23:11.182824      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:11.650: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.98515971s
  E0610 13:23:12.182944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:12.654: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.980960299s
  E0610 13:23:13.183683      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:13.659: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.97607109s
  E0610 13:23:14.184277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:14.665: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.970320601s
  E0610 13:23:15.185041      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:15.669: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.965924167s
  E0610 13:23:16.186076      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:16.674: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.961655375s
  E0610 13:23:17.186799      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:17.678: INFO: Verifying statefulset ss doesn't scale past 0 for another 956.846287ms
  E0610 13:23:18.186984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4765 @ 06/10/23 13:23:18.678
  Jun 10 13:23:18.683: INFO: Scaling statefulset ss to 0
  Jun 10 13:23:18.695: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 10 13:23:18.699: INFO: Deleting all statefulset in ns statefulset-4765
  Jun 10 13:23:18.703: INFO: Scaling statefulset ss to 0
  Jun 10 13:23:18.716: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 10 13:23:18.719: INFO: Deleting statefulset ss
  Jun 10 13:23:18.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4765" for this suite. @ 06/10/23 13:23:18.738
• [61.444 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 06/10/23 13:23:18.747
  Jun 10 13:23:18.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename downward-api @ 06/10/23 13:23:18.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:23:18.77
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:23:18.773
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 13:23:18.777
  E0610 13:23:19.188107      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:20.188216      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:21.188297      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:22.188415      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:23:22.808
  Jun 10 13:23:22.812: INFO: Trying to get logs from node ip-172-31-27-177 pod downwardapi-volume-8f8003ba-504c-4ec7-86fc-a24475a43f9d container client-container: <nil>
  STEP: delete the pod @ 06/10/23 13:23:22.836
  Jun 10 13:23:22.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3756" for this suite. @ 06/10/23 13:23:22.857
• [4.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 06/10/23 13:23:22.868
  Jun 10 13:23:22.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename pods @ 06/10/23 13:23:22.869
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:23:22.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:23:22.894
  STEP: Create a pod @ 06/10/23 13:23:22.898
  E0610 13:23:23.188949      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:24.189072      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 06/10/23 13:23:24.917
  Jun 10 13:23:24.926: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Jun 10 13:23:24.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6253" for this suite. @ 06/10/23 13:23:24.931
• [2.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 06/10/23 13:23:24.943
  Jun 10 13:23:24.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename controllerrevisions @ 06/10/23 13:23:24.944
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:23:24.963
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:23:24.966
  STEP: Creating DaemonSet "e2e-znncx-daemon-set" @ 06/10/23 13:23:24.992
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/10/23 13:23:25
  Jun 10 13:23:25.009: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:23:25.009: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:23:25.016: INFO: Number of nodes with available pods controlled by daemonset e2e-znncx-daemon-set: 0
  Jun 10 13:23:25.016: INFO: Node ip-172-31-27-177 is running 0 daemon pod, expected 1
  E0610 13:23:25.190055      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:26.021: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:23:26.021: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:23:26.026: INFO: Number of nodes with available pods controlled by daemonset e2e-znncx-daemon-set: 0
  Jun 10 13:23:26.026: INFO: Node ip-172-31-27-177 is running 0 daemon pod, expected 1
  E0610 13:23:26.190799      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:27.023: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:23:27.023: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:23:27.027: INFO: Number of nodes with available pods controlled by daemonset e2e-znncx-daemon-set: 3
  Jun 10 13:23:27.027: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-znncx-daemon-set
  STEP: Confirm DaemonSet "e2e-znncx-daemon-set" successfully created with "daemonset-name=e2e-znncx-daemon-set" label @ 06/10/23 13:23:27.031
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-znncx-daemon-set" @ 06/10/23 13:23:27.039
  Jun 10 13:23:27.044: INFO: Located ControllerRevision: "e2e-znncx-daemon-set-6ffc76ddfb"
  STEP: Patching ControllerRevision "e2e-znncx-daemon-set-6ffc76ddfb" @ 06/10/23 13:23:27.047
  Jun 10 13:23:27.055: INFO: e2e-znncx-daemon-set-6ffc76ddfb has been patched
  STEP: Create a new ControllerRevision @ 06/10/23 13:23:27.055
  Jun 10 13:23:27.061: INFO: Created ControllerRevision: e2e-znncx-daemon-set-b5b98d994
  STEP: Confirm that there are two ControllerRevisions @ 06/10/23 13:23:27.061
  Jun 10 13:23:27.061: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun 10 13:23:27.065: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-znncx-daemon-set-6ffc76ddfb" @ 06/10/23 13:23:27.065
  STEP: Confirm that there is only one ControllerRevision @ 06/10/23 13:23:27.077
  Jun 10 13:23:27.077: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun 10 13:23:27.081: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-znncx-daemon-set-b5b98d994" @ 06/10/23 13:23:27.085
  Jun 10 13:23:27.094: INFO: e2e-znncx-daemon-set-b5b98d994 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 06/10/23 13:23:27.094
  W0610 13:23:27.104286      18 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 06/10/23 13:23:27.104
  Jun 10 13:23:27.104: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0610 13:23:27.191303      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:28.117: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun 10 13:23:28.121: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-znncx-daemon-set-b5b98d994=updated" @ 06/10/23 13:23:28.122
  STEP: Confirm that there is only one ControllerRevision @ 06/10/23 13:23:28.133
  Jun 10 13:23:28.133: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun 10 13:23:28.137: INFO: Found 1 ControllerRevisions
  Jun 10 13:23:28.141: INFO: ControllerRevision "e2e-znncx-daemon-set-55d94485ff" has revision 3
  STEP: Deleting DaemonSet "e2e-znncx-daemon-set" @ 06/10/23 13:23:28.145
  STEP: deleting DaemonSet.extensions e2e-znncx-daemon-set in namespace controllerrevisions-3409, will wait for the garbage collector to delete the pods @ 06/10/23 13:23:28.145
  E0610 13:23:28.191908      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:28.209: INFO: Deleting DaemonSet.extensions e2e-znncx-daemon-set took: 7.74745ms
  Jun 10 13:23:28.309: INFO: Terminating DaemonSet.extensions e2e-znncx-daemon-set pods took: 100.653119ms
  E0610 13:23:29.192628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:30.014: INFO: Number of nodes with available pods controlled by daemonset e2e-znncx-daemon-set: 0
  Jun 10 13:23:30.015: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-znncx-daemon-set
  Jun 10 13:23:30.019: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31284"},"items":null}

  Jun 10 13:23:30.022: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31285"},"items":null}

  Jun 10 13:23:30.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-3409" for this suite. @ 06/10/23 13:23:30.041
• [5.105 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 06/10/23 13:23:30.049
  Jun 10 13:23:30.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 13:23:30.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:23:30.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:23:30.075
  STEP: Creating secret with name projected-secret-test-3b45dced-c279-4a74-a672-2e1700aeabf7 @ 06/10/23 13:23:30.079
  STEP: Creating a pod to test consume secrets @ 06/10/23 13:23:30.085
  E0610 13:23:30.193138      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:31.193510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:32.194460      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:33.194572      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:23:34.108
  Jun 10 13:23:34.112: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-projected-secrets-a90f4f33-f5a1-451e-b19a-392a57e44bd9 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/10/23 13:23:34.12
  Jun 10 13:23:34.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8073" for this suite. @ 06/10/23 13:23:34.143
• [4.101 seconds]
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 06/10/23 13:23:34.151
  Jun 10 13:23:34.151: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename init-container @ 06/10/23 13:23:34.152
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:23:34.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:23:34.173
  STEP: creating the pod @ 06/10/23 13:23:34.176
  Jun 10 13:23:34.176: INFO: PodSpec: initContainers in spec.initContainers
  E0610 13:23:34.194699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:35.194819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:36.195449      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:37.195553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:38.195653      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:39.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-810" for this suite. @ 06/10/23 13:23:39.189
  E0610 13:23:39.195771      18 retrywatcher.go:130] "Watch failed" err="context canceled"
• [5.045 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 06/10/23 13:23:39.197
  Jun 10 13:23:39.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl @ 06/10/23 13:23:39.198
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:23:39.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:23:39.226
  STEP: creating a replication controller @ 06/10/23 13:23:39.229
  Jun 10 13:23:39.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4604 create -f -'
  E0610 13:23:40.195935      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:40.258: INFO: stderr: ""
  Jun 10 13:23:40.258: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 06/10/23 13:23:40.258
  Jun 10 13:23:40.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4604 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 10 13:23:40.355: INFO: stderr: ""
  Jun 10 13:23:40.355: INFO: stdout: "update-demo-nautilus-kggzc update-demo-nautilus-nl726 "
  Jun 10 13:23:40.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4604 get pods update-demo-nautilus-kggzc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 10 13:23:40.438: INFO: stderr: ""
  Jun 10 13:23:40.438: INFO: stdout: ""
  Jun 10 13:23:40.438: INFO: update-demo-nautilus-kggzc is created but not running
  E0610 13:23:41.196314      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:42.196533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:43.196834      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:44.196988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:45.197113      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:45.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4604 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 10 13:23:45.521: INFO: stderr: ""
  Jun 10 13:23:45.521: INFO: stdout: "update-demo-nautilus-kggzc update-demo-nautilus-nl726 "
  Jun 10 13:23:45.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4604 get pods update-demo-nautilus-kggzc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 10 13:23:45.598: INFO: stderr: ""
  Jun 10 13:23:45.598: INFO: stdout: "true"
  Jun 10 13:23:45.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4604 get pods update-demo-nautilus-kggzc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 10 13:23:45.680: INFO: stderr: ""
  Jun 10 13:23:45.680: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 10 13:23:45.680: INFO: validating pod update-demo-nautilus-kggzc
  Jun 10 13:23:45.686: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 10 13:23:45.686: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 10 13:23:45.686: INFO: update-demo-nautilus-kggzc is verified up and running
  Jun 10 13:23:45.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4604 get pods update-demo-nautilus-nl726 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 10 13:23:45.769: INFO: stderr: ""
  Jun 10 13:23:45.769: INFO: stdout: "true"
  Jun 10 13:23:45.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4604 get pods update-demo-nautilus-nl726 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 10 13:23:45.849: INFO: stderr: ""
  Jun 10 13:23:45.849: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 10 13:23:45.849: INFO: validating pod update-demo-nautilus-nl726
  Jun 10 13:23:45.855: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 10 13:23:45.855: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 10 13:23:45.855: INFO: update-demo-nautilus-nl726 is verified up and running
  STEP: using delete to clean up resources @ 06/10/23 13:23:45.855
  Jun 10 13:23:45.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4604 delete --grace-period=0 --force -f -'
  Jun 10 13:23:45.942: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 10 13:23:45.942: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jun 10 13:23:45.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4604 get rc,svc -l name=update-demo --no-headers'
  Jun 10 13:23:46.066: INFO: stderr: "No resources found in kubectl-4604 namespace.\n"
  Jun 10 13:23:46.066: INFO: stdout: ""
  Jun 10 13:23:46.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-4604 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  E0610 13:23:46.197533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:23:46.200: INFO: stderr: ""
  Jun 10 13:23:46.200: INFO: stdout: ""
  Jun 10 13:23:46.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4604" for this suite. @ 06/10/23 13:23:46.206
• [7.016 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 06/10/23 13:23:46.214
  Jun 10 13:23:46.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename svcaccounts @ 06/10/23 13:23:46.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:23:46.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:23:46.25
  STEP: Creating a pod to test service account token:  @ 06/10/23 13:23:46.254
  E0610 13:23:47.198134      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:48.198533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:49.198698      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:50.200623      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:23:50.277
  Jun 10 13:23:50.281: INFO: Trying to get logs from node ip-172-31-27-177 pod test-pod-b704d053-4b5f-4090-9812-a3d6c1a947e5 container agnhost-container: <nil>
  STEP: delete the pod @ 06/10/23 13:23:50.289
  Jun 10 13:23:50.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-674" for this suite. @ 06/10/23 13:23:50.309
• [4.103 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 06/10/23 13:23:50.318
  Jun 10 13:23:50.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 13:23:50.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:23:50.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:23:50.345
  STEP: Creating configMap with name projected-configmap-test-volume-map-ff26a837-d078-4b02-bb66-8858740e6b65 @ 06/10/23 13:23:50.349
  STEP: Creating a pod to test consume configMaps @ 06/10/23 13:23:50.355
  E0610 13:23:51.200732      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:52.201358      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:53.201560      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:54.201749      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:23:54.381
  Jun 10 13:23:54.385: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-projected-configmaps-bcb23b09-37f9-46fe-a871-2d44cc8d0398 container agnhost-container: <nil>
  STEP: delete the pod @ 06/10/23 13:23:54.393
  Jun 10 13:23:54.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6298" for this suite. @ 06/10/23 13:23:54.417
• [4.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 06/10/23 13:23:54.43
  Jun 10 13:23:54.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename services @ 06/10/23 13:23:54.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:23:54.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:23:54.456
  STEP: creating service nodeport-test with type=NodePort in namespace services-5916 @ 06/10/23 13:23:54.461
  STEP: creating replication controller nodeport-test in namespace services-5916 @ 06/10/23 13:23:54.476
  I0610 13:23:54.487536      18 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-5916, replica count: 2
  E0610 13:23:55.201890      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:56.202392      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:57.202487      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0610 13:23:57.538994      18 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 10 13:23:57.539: INFO: Creating new exec pod
  E0610 13:23:58.202822      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:23:59.203140      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:00.203767      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:00.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5916 exec execpodhg7mh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jun 10 13:24:00.733: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jun 10 13:24:00.733: INFO: stdout: ""
  E0610 13:24:01.204304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:01.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5916 exec execpodhg7mh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jun 10 13:24:01.889: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jun 10 13:24:01.889: INFO: stdout: ""
  E0610 13:24:02.205390      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:02.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5916 exec execpodhg7mh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jun 10 13:24:02.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jun 10 13:24:02.890: INFO: stdout: "nodeport-test-x26ff"
  Jun 10 13:24:02.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5916 exec execpodhg7mh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.150 80'
  Jun 10 13:24:03.048: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.150 80\nConnection to 10.152.183.150 80 port [tcp/http] succeeded!\n"
  Jun 10 13:24:03.048: INFO: stdout: ""
  E0610 13:24:03.206026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:04.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5916 exec execpodhg7mh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.150 80'
  E0610 13:24:04.206427      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:04.213: INFO: stderr: "+ nc -v -t -w 2 10.152.183.150 80\nConnection to 10.152.183.150 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun 10 13:24:04.213: INFO: stdout: "nodeport-test-s8dh9"
  Jun 10 13:24:04.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5916 exec execpodhg7mh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.46.40 30987'
  Jun 10 13:24:04.381: INFO: stderr: "+ nc -v -t -w 2 172.31.46.40 30987\n+ echo hostName\nConnection to 172.31.46.40 30987 port [tcp/*] succeeded!\n"
  Jun 10 13:24:04.381: INFO: stdout: "nodeport-test-x26ff"
  Jun 10 13:24:04.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5916 exec execpodhg7mh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.89.0 30987'
  Jun 10 13:24:04.554: INFO: stderr: "+ nc -v -t -w 2 172.31.89.0 30987\n+ echo hostName\nConnection to 172.31.89.0 30987 port [tcp/*] succeeded!\n"
  Jun 10 13:24:04.554: INFO: stdout: "nodeport-test-x26ff"
  Jun 10 13:24:04.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5916" for this suite. @ 06/10/23 13:24:04.56
• [10.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 06/10/23 13:24:04.568
  Jun 10 13:24:04.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 13:24:04.568
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:24:04.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:24:04.599
  STEP: Setting up server cert @ 06/10/23 13:24:04.629
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 13:24:05.036
  STEP: Deploying the webhook pod @ 06/10/23 13:24:05.044
  STEP: Wait for the deployment to be ready @ 06/10/23 13:24:05.058
  Jun 10 13:24:05.066: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0610 13:24:05.206471      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:06.206615      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/10/23 13:24:07.082
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 13:24:07.094
  E0610 13:24:07.207021      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:08.094: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 06/10/23 13:24:08.1
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 06/10/23 13:24:08.101
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 06/10/23 13:24:08.101
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 06/10/23 13:24:08.101
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 06/10/23 13:24:08.103
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 06/10/23 13:24:08.103
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 06/10/23 13:24:08.104
  Jun 10 13:24:08.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9143" for this suite. @ 06/10/23 13:24:08.149
  STEP: Destroying namespace "webhook-markers-2244" for this suite. @ 06/10/23 13:24:08.157
• [3.598 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 06/10/23 13:24:08.166
  Jun 10 13:24:08.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl @ 06/10/23 13:24:08.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:24:08.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:24:08.188
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 06/10/23 13:24:08.192
  Jun 10 13:24:08.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-6414 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  E0610 13:24:08.207497      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:08.276: INFO: stderr: ""
  Jun 10 13:24:08.276: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 06/10/23 13:24:08.276
  Jun 10 13:24:08.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-6414 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Jun 10 13:24:08.368: INFO: stderr: ""
  Jun 10 13:24:08.368: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 06/10/23 13:24:08.368
  Jun 10 13:24:08.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-6414 delete pods e2e-test-httpd-pod'
  E0610 13:24:09.208137      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:10.208307      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:11.208411      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:11.242: INFO: stderr: ""
  Jun 10 13:24:11.242: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jun 10 13:24:11.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6414" for this suite. @ 06/10/23 13:24:11.251
• [3.091 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 06/10/23 13:24:11.258
  Jun 10 13:24:11.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir @ 06/10/23 13:24:11.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:24:11.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:24:11.285
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 06/10/23 13:24:11.291
  E0610 13:24:12.208449      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:13.208559      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:14.208873      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:15.208980      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:24:15.317
  Jun 10 13:24:15.321: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-0f08c155-4294-40da-b4ac-2b23e2218323 container test-container: <nil>
  STEP: delete the pod @ 06/10/23 13:24:15.33
  Jun 10 13:24:15.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-177" for this suite. @ 06/10/23 13:24:15.351
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 06/10/23 13:24:15.363
  Jun 10 13:24:15.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename events @ 06/10/23 13:24:15.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:24:15.385
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:24:15.389
  STEP: Create set of events @ 06/10/23 13:24:15.396
  Jun 10 13:24:15.402: INFO: created test-event-1
  Jun 10 13:24:15.408: INFO: created test-event-2
  Jun 10 13:24:15.414: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 06/10/23 13:24:15.414
  STEP: delete collection of events @ 06/10/23 13:24:15.418
  Jun 10 13:24:15.418: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 06/10/23 13:24:15.443
  Jun 10 13:24:15.443: INFO: requesting list of events to confirm quantity
  Jun 10 13:24:15.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8663" for this suite. @ 06/10/23 13:24:15.451
• [0.096 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 06/10/23 13:24:15.46
  Jun 10 13:24:15.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename configmap @ 06/10/23 13:24:15.461
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:24:15.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:24:15.483
  STEP: Creating configMap configmap-4024/configmap-test-4ef34d10-430f-482e-8b88-e427701dea69 @ 06/10/23 13:24:15.487
  STEP: Creating a pod to test consume configMaps @ 06/10/23 13:24:15.492
  E0610 13:24:16.209922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:17.210057      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:18.210198      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:19.210287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:24:19.517
  Jun 10 13:24:19.520: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-configmaps-b4b9f22c-fb01-4965-8e1a-7bcb1358b7f0 container env-test: <nil>
  STEP: delete the pod @ 06/10/23 13:24:19.53
  Jun 10 13:24:19.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4024" for this suite. @ 06/10/23 13:24:19.553
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 06/10/23 13:24:19.562
  Jun 10 13:24:19.562: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename resourcequota @ 06/10/23 13:24:19.563
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:24:19.581
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:24:19.585
  STEP: Creating a ResourceQuota with best effort scope @ 06/10/23 13:24:19.589
  STEP: Ensuring ResourceQuota status is calculated @ 06/10/23 13:24:19.595
  E0610 13:24:20.210702      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:21.211049      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 06/10/23 13:24:21.6
  STEP: Ensuring ResourceQuota status is calculated @ 06/10/23 13:24:21.605
  E0610 13:24:22.211669      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:23.211829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 06/10/23 13:24:23.61
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 06/10/23 13:24:23.626
  E0610 13:24:24.212307      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:25.212459      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 06/10/23 13:24:25.632
  E0610 13:24:26.213220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:27.213696      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 06/10/23 13:24:27.636
  STEP: Ensuring resource quota status released the pod usage @ 06/10/23 13:24:27.654
  E0610 13:24:28.214225      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:29.214325      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 06/10/23 13:24:29.659
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 06/10/23 13:24:29.671
  E0610 13:24:30.214393      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:31.214806      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 06/10/23 13:24:31.676
  E0610 13:24:32.214999      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:33.215113      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 06/10/23 13:24:33.683
  STEP: Ensuring resource quota status released the pod usage @ 06/10/23 13:24:33.694
  E0610 13:24:34.215202      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:35.215349      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:35.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8745" for this suite. @ 06/10/23 13:24:35.704
• [16.151 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 06/10/23 13:24:35.713
  Jun 10 13:24:35.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename services @ 06/10/23 13:24:35.714
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:24:35.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:24:35.738
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-5246 @ 06/10/23 13:24:35.742
  STEP: changing the ExternalName service to type=NodePort @ 06/10/23 13:24:35.749
  STEP: creating replication controller externalname-service in namespace services-5246 @ 06/10/23 13:24:35.769
  I0610 13:24:35.777780      18 runners.go:194] Created replication controller with name: externalname-service, namespace: services-5246, replica count: 2
  E0610 13:24:36.216196      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:37.216714      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:38.217082      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0610 13:24:38.828607      18 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 10 13:24:38.828: INFO: Creating new exec pod
  E0610 13:24:39.217570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:40.217756      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:41.217917      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:41.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5246 exec execpodlvvph -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun 10 13:24:42.015: INFO: stderr: "+ + nc -v -t -w 2 externalname-service 80\necho hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun 10 13:24:42.015: INFO: stdout: "externalname-service-l6nwm"
  Jun 10 13:24:42.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5246 exec execpodlvvph -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.174 80'
  Jun 10 13:24:42.184: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.174 80\nConnection to 10.152.183.174 80 port [tcp/http] succeeded!\n"
  Jun 10 13:24:42.184: INFO: stdout: ""
  E0610 13:24:42.218546      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:43.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5246 exec execpodlvvph -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.174 80'
  E0610 13:24:43.219031      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:43.343: INFO: stderr: "+ nc -v -t -w 2 10.152.183.174 80\nConnection to 10.152.183.174 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun 10 13:24:43.343: INFO: stdout: ""
  Jun 10 13:24:44.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5246 exec execpodlvvph -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.174 80'
  E0610 13:24:44.219712      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:44.340: INFO: stderr: "+ nc -v -t -w 2 10.152.183.174 80\n+ echo hostName\nConnection to 10.152.183.174 80 port [tcp/http] succeeded!\n"
  Jun 10 13:24:44.340: INFO: stdout: "externalname-service-l6nwm"
  Jun 10 13:24:44.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5246 exec execpodlvvph -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.89.0 30660'
  Jun 10 13:24:44.500: INFO: stderr: "+ nc -v -t -w 2 172.31.89.0 30660\n+ echo hostName\nConnection to 172.31.89.0 30660 port [tcp/*] succeeded!\n"
  Jun 10 13:24:44.500: INFO: stdout: ""
  E0610 13:24:45.220613      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:45.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5246 exec execpodlvvph -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.89.0 30660'
  Jun 10 13:24:45.652: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.89.0 30660\nConnection to 172.31.89.0 30660 port [tcp/*] succeeded!\n"
  Jun 10 13:24:45.652: INFO: stdout: "externalname-service-dqn58"
  Jun 10 13:24:45.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5246 exec execpodlvvph -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.27.177 30660'
  Jun 10 13:24:45.808: INFO: stderr: "+ nc -v -t -w 2 172.31.27.177 30660\nConnection to 172.31.27.177 30660 port [tcp/*] succeeded!\n+ echo hostName\n"
  Jun 10 13:24:45.808: INFO: stdout: ""
  E0610 13:24:46.221407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:46.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5246 exec execpodlvvph -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.27.177 30660'
  Jun 10 13:24:46.965: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.27.177 30660\nConnection to 172.31.27.177 30660 port [tcp/*] succeeded!\n"
  Jun 10 13:24:46.965: INFO: stdout: ""
  E0610 13:24:47.221550      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:47.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-5246 exec execpodlvvph -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.27.177 30660'
  Jun 10 13:24:47.967: INFO: stderr: "+ nc -v -t -w 2 172.31.27.177 30660\n+ echo hostName\nConnection to 172.31.27.177 30660 port [tcp/*] succeeded!\n"
  Jun 10 13:24:47.967: INFO: stdout: "externalname-service-dqn58"
  Jun 10 13:24:47.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 10 13:24:47.972: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-5246" for this suite. @ 06/10/23 13:24:47.994
• [12.288 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 06/10/23 13:24:48.001
  Jun 10 13:24:48.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename services @ 06/10/23 13:24:48.002
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:24:48.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:24:48.037
  STEP: creating service in namespace services-9202 @ 06/10/23 13:24:48.041
  STEP: creating service affinity-clusterip in namespace services-9202 @ 06/10/23 13:24:48.041
  STEP: creating replication controller affinity-clusterip in namespace services-9202 @ 06/10/23 13:24:48.053
  I0610 13:24:48.060763      18 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-9202, replica count: 3
  E0610 13:24:48.222359      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:49.222913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:50.223018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0610 13:24:51.112225      18 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 10 13:24:51.120: INFO: Creating new exec pod
  E0610 13:24:51.223830      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:52.224323      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:53.224786      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:54.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-9202 exec execpod-affinitybnq9k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  E0610 13:24:54.227579      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:24:54.376: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Jun 10 13:24:54.376: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 13:24:54.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-9202 exec execpod-affinitybnq9k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.206 80'
  Jun 10 13:24:54.684: INFO: stderr: "+ nc -v -t -w 2 10.152.183.206 80\n+ echo hostName\nConnection to 10.152.183.206 80 port [tcp/http] succeeded!\n"
  Jun 10 13:24:54.684: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 13:24:54.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-9202 exec execpod-affinitybnq9k -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.206:80/ ; done'
  Jun 10 13:24:54.967: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.206:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.206:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.206:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.206:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.206:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.206:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.206:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.206:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.206:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.206:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.206:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.206:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.206:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.206:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.206:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.206:80/\n"
  Jun 10 13:24:54.967: INFO: stdout: "\naffinity-clusterip-726qj\naffinity-clusterip-726qj\naffinity-clusterip-726qj\naffinity-clusterip-726qj\naffinity-clusterip-726qj\naffinity-clusterip-726qj\naffinity-clusterip-726qj\naffinity-clusterip-726qj\naffinity-clusterip-726qj\naffinity-clusterip-726qj\naffinity-clusterip-726qj\naffinity-clusterip-726qj\naffinity-clusterip-726qj\naffinity-clusterip-726qj\naffinity-clusterip-726qj\naffinity-clusterip-726qj"
  Jun 10 13:24:54.967: INFO: Received response from host: affinity-clusterip-726qj
  Jun 10 13:24:54.967: INFO: Received response from host: affinity-clusterip-726qj
  Jun 10 13:24:54.967: INFO: Received response from host: affinity-clusterip-726qj
  Jun 10 13:24:54.967: INFO: Received response from host: affinity-clusterip-726qj
  Jun 10 13:24:54.967: INFO: Received response from host: affinity-clusterip-726qj
  Jun 10 13:24:54.967: INFO: Received response from host: affinity-clusterip-726qj
  Jun 10 13:24:54.967: INFO: Received response from host: affinity-clusterip-726qj
  Jun 10 13:24:54.967: INFO: Received response from host: affinity-clusterip-726qj
  Jun 10 13:24:54.967: INFO: Received response from host: affinity-clusterip-726qj
  Jun 10 13:24:54.967: INFO: Received response from host: affinity-clusterip-726qj
  Jun 10 13:24:54.967: INFO: Received response from host: affinity-clusterip-726qj
  Jun 10 13:24:54.967: INFO: Received response from host: affinity-clusterip-726qj
  Jun 10 13:24:54.967: INFO: Received response from host: affinity-clusterip-726qj
  Jun 10 13:24:54.967: INFO: Received response from host: affinity-clusterip-726qj
  Jun 10 13:24:54.967: INFO: Received response from host: affinity-clusterip-726qj
  Jun 10 13:24:54.967: INFO: Received response from host: affinity-clusterip-726qj
  Jun 10 13:24:54.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 10 13:24:54.972: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-9202, will wait for the garbage collector to delete the pods @ 06/10/23 13:24:54.984
  Jun 10 13:24:55.047: INFO: Deleting ReplicationController affinity-clusterip took: 8.903662ms
  Jun 10 13:24:55.148: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.08044ms
  E0610 13:24:55.226308      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:56.226820      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-9202" for this suite. @ 06/10/23 13:24:57.17
• [9.177 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 06/10/23 13:24:57.178
  Jun 10 13:24:57.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir @ 06/10/23 13:24:57.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:24:57.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:24:57.206
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 06/10/23 13:24:57.211
  E0610 13:24:57.227802      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:58.228215      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:24:59.228663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:00.228764      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:01.228887      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:25:01.238
  Jun 10 13:25:01.242: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-9e78558c-ccc6-4fa9-bd1c-82ad06786adf container test-container: <nil>
  STEP: delete the pod @ 06/10/23 13:25:01.25
  Jun 10 13:25:01.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-680" for this suite. @ 06/10/23 13:25:01.272
• [4.101 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 06/10/23 13:25:01.28
  Jun 10 13:25:01.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename secrets @ 06/10/23 13:25:01.282
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:25:01.303
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:25:01.307
  STEP: Creating secret with name secret-test-e5197044-2c53-44b9-840a-817b62a6ca05 @ 06/10/23 13:25:01.31
  STEP: Creating a pod to test consume secrets @ 06/10/23 13:25:01.316
  E0610 13:25:02.229773      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:03.230372      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:04.230483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:05.231023      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:25:05.34
  Jun 10 13:25:05.344: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-secrets-c74da18f-4261-48fc-88eb-b5894d34e198 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/10/23 13:25:05.353
  Jun 10 13:25:05.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7772" for this suite. @ 06/10/23 13:25:05.379
• [4.108 seconds]
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 06/10/23 13:25:05.388
  Jun 10 13:25:05.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename limitrange @ 06/10/23 13:25:05.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:25:05.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:25:05.414
  STEP: Creating LimitRange "e2e-limitrange-tq4w9" in namespace "limitrange-9854" @ 06/10/23 13:25:05.424
  STEP: Creating another limitRange in another namespace @ 06/10/23 13:25:05.431
  Jun 10 13:25:05.461: INFO: Namespace "e2e-limitrange-tq4w9-1233" created
  Jun 10 13:25:05.461: INFO: Creating LimitRange "e2e-limitrange-tq4w9" in namespace "e2e-limitrange-tq4w9-1233"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-tq4w9" @ 06/10/23 13:25:05.467
  Jun 10 13:25:05.473: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-tq4w9" in "limitrange-9854" namespace @ 06/10/23 13:25:05.474
  Jun 10 13:25:05.482: INFO: LimitRange "e2e-limitrange-tq4w9" has been patched
  STEP: Delete LimitRange "e2e-limitrange-tq4w9" by Collection with labelSelector: "e2e-limitrange-tq4w9=patched" @ 06/10/23 13:25:05.483
  STEP: Confirm that the limitRange "e2e-limitrange-tq4w9" has been deleted @ 06/10/23 13:25:05.491
  Jun 10 13:25:05.492: INFO: Requesting list of LimitRange to confirm quantity
  Jun 10 13:25:05.495: INFO: Found 0 LimitRange with label "e2e-limitrange-tq4w9=patched"
  Jun 10 13:25:05.495: INFO: LimitRange "e2e-limitrange-tq4w9" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-tq4w9" @ 06/10/23 13:25:05.495
  Jun 10 13:25:05.499: INFO: Found 1 limitRange
  Jun 10 13:25:05.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-9854" for this suite. @ 06/10/23 13:25:05.505
  STEP: Destroying namespace "e2e-limitrange-tq4w9-1233" for this suite. @ 06/10/23 13:25:05.513
• [0.132 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 06/10/23 13:25:05.521
  Jun 10 13:25:05.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 13:25:05.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:25:05.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:25:05.546
  STEP: Creating projection with secret that has name projected-secret-test-5e93b637-c92a-42bb-ae65-2aa4c238b4b2 @ 06/10/23 13:25:05.55
  STEP: Creating a pod to test consume secrets @ 06/10/23 13:25:05.556
  E0610 13:25:06.231155      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:07.231275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:08.231869      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:09.231984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:25:09.583
  Jun 10 13:25:09.586: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-projected-secrets-3c832d9e-1f5e-49f2-904f-2a063fe7913f container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/10/23 13:25:09.594
  Jun 10 13:25:09.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7935" for this suite. @ 06/10/23 13:25:09.617
• [4.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 06/10/23 13:25:09.629
  Jun 10 13:25:09.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename pods @ 06/10/23 13:25:09.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:25:09.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:25:09.653
  STEP: Create set of pods @ 06/10/23 13:25:09.656
  Jun 10 13:25:09.667: INFO: created test-pod-1
  Jun 10 13:25:09.675: INFO: created test-pod-2
  Jun 10 13:25:09.682: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 06/10/23 13:25:09.682
  E0610 13:25:10.232236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:11.232331      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 06/10/23 13:25:11.735
  Jun 10 13:25:11.740: INFO: Pod quantity 3 is different from expected quantity 0
  E0610 13:25:12.232416      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:25:12.746: INFO: Pod quantity 3 is different from expected quantity 0
  E0610 13:25:13.232726      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:25:13.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4081" for this suite. @ 06/10/23 13:25:13.751
• [4.130 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 06/10/23 13:25:13.761
  Jun 10 13:25:13.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 13:25:13.762
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:25:13.782
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:25:13.786
  STEP: Setting up server cert @ 06/10/23 13:25:13.812
  E0610 13:25:14.232869      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 13:25:14.455
  STEP: Deploying the webhook pod @ 06/10/23 13:25:14.465
  STEP: Wait for the deployment to be ready @ 06/10/23 13:25:14.479
  Jun 10 13:25:14.486: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0610 13:25:15.232998      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:16.233112      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/10/23 13:25:16.5
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 13:25:16.511
  E0610 13:25:17.233213      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:25:17.514: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun 10 13:25:17.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1913-crds.webhook.example.com via the AdmissionRegistration API @ 06/10/23 13:25:18.033
  STEP: Creating a custom resource while v1 is storage version @ 06/10/23 13:25:18.051
  E0610 13:25:18.233694      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:19.233780      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 06/10/23 13:25:20.114
  STEP: Patching the custom resource while v2 is storage version @ 06/10/23 13:25:20.133
  Jun 10 13:25:20.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0610 13:25:20.234026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-4763" for this suite. @ 06/10/23 13:25:20.728
  STEP: Destroying namespace "webhook-markers-1191" for this suite. @ 06/10/23 13:25:20.736
• [6.983 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 06/10/23 13:25:20.745
  Jun 10 13:25:20.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 13:25:20.746
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:25:20.768
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:25:20.772
  STEP: Creating projection with secret that has name projected-secret-test-map-2ea44fd3-9b0b-4dc7-9d52-56503078eb51 @ 06/10/23 13:25:20.775
  STEP: Creating a pod to test consume secrets @ 06/10/23 13:25:20.78
  E0610 13:25:21.235102      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:22.235210      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:23.235684      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:24.236422      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:25:24.805
  Jun 10 13:25:24.809: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-projected-secrets-827e208a-c776-4255-a3d2-f340151ceb34 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/10/23 13:25:24.817
  Jun 10 13:25:24.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9504" for this suite. @ 06/10/23 13:25:24.839
• [4.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 06/10/23 13:25:24.86
  Jun 10 13:25:24.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename limitrange @ 06/10/23 13:25:24.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:25:24.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:25:24.883
  STEP: Creating a LimitRange @ 06/10/23 13:25:24.887
  STEP: Setting up watch @ 06/10/23 13:25:24.887
  STEP: Submitting a LimitRange @ 06/10/23 13:25:24.992
  STEP: Verifying LimitRange creation was observed @ 06/10/23 13:25:24.998
  STEP: Fetching the LimitRange to ensure it has proper values @ 06/10/23 13:25:24.998
  Jun 10 13:25:25.002: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jun 10 13:25:25.002: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 06/10/23 13:25:25.003
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 06/10/23 13:25:25.009
  Jun 10 13:25:25.014: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jun 10 13:25:25.014: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 06/10/23 13:25:25.014
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 06/10/23 13:25:25.021
  Jun 10 13:25:25.025: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Jun 10 13:25:25.025: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 06/10/23 13:25:25.025
  STEP: Failing to create a Pod with more than max resources @ 06/10/23 13:25:25.027
  STEP: Updating a LimitRange @ 06/10/23 13:25:25.03
  STEP: Verifying LimitRange updating is effective @ 06/10/23 13:25:25.035
  E0610 13:25:25.237389      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:26.237563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 06/10/23 13:25:27.04
  STEP: Failing to create a Pod with more than max resources @ 06/10/23 13:25:27.048
  STEP: Deleting a LimitRange @ 06/10/23 13:25:27.05
  STEP: Verifying the LimitRange was deleted @ 06/10/23 13:25:27.061
  E0610 13:25:27.237671      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:28.238152      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:29.238290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:30.238387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:31.238748      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:25:32.065: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 06/10/23 13:25:32.065
  Jun 10 13:25:32.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-7743" for this suite. @ 06/10/23 13:25:32.087
• [7.234 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 06/10/23 13:25:32.103
  Jun 10 13:25:32.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename field-validation @ 06/10/23 13:25:32.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:25:32.124
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:25:32.128
  STEP: apply creating a deployment @ 06/10/23 13:25:32.131
  Jun 10 13:25:32.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3077" for this suite. @ 06/10/23 13:25:32.155
• [0.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 06/10/23 13:25:32.168
  Jun 10 13:25:32.168: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl @ 06/10/23 13:25:32.169
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:25:32.19
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:25:32.193
  STEP: validating api versions @ 06/10/23 13:25:32.197
  Jun 10 13:25:32.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-8073 api-versions'
  E0610 13:25:32.239298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:25:32.276: INFO: stderr: ""
  Jun 10 13:25:32.276: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Jun 10 13:25:32.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8073" for this suite. @ 06/10/23 13:25:32.282
• [0.121 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 06/10/23 13:25:32.29
  Jun 10 13:25:32.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 13:25:32.291
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:25:32.311
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:25:32.314
  STEP: Creating configMap with name projected-configmap-test-volume-bae519fd-b80d-459e-a689-c12ec53aa4d1 @ 06/10/23 13:25:32.318
  STEP: Creating a pod to test consume configMaps @ 06/10/23 13:25:32.324
  E0610 13:25:33.239578      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:34.239702      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:35.240440      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:36.240872      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:25:36.352
  Jun 10 13:25:36.356: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-projected-configmaps-05bb4102-81d0-4a5b-be95-c68a15a61712 container agnhost-container: <nil>
  STEP: delete the pod @ 06/10/23 13:25:36.363
  Jun 10 13:25:36.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5800" for this suite. @ 06/10/23 13:25:36.382
• [4.100 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 06/10/23 13:25:36.391
  Jun 10 13:25:36.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename services @ 06/10/23 13:25:36.392
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:25:36.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:25:36.416
  Jun 10 13:25:36.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6115" for this suite. @ 06/10/23 13:25:36.429
• [0.044 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 06/10/23 13:25:36.436
  Jun 10 13:25:36.436: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename deployment @ 06/10/23 13:25:36.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:25:36.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:25:36.461
  STEP: creating a Deployment @ 06/10/23 13:25:36.469
  STEP: waiting for Deployment to be created @ 06/10/23 13:25:36.475
  STEP: waiting for all Replicas to be Ready @ 06/10/23 13:25:36.477
  Jun 10 13:25:36.479: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 10 13:25:36.479: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 10 13:25:36.492: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 10 13:25:36.492: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 10 13:25:36.512: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 10 13:25:36.512: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 10 13:25:36.554: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 10 13:25:36.554: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0610 13:25:37.241708      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:25:38.193: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jun 10 13:25:38.193: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jun 10 13:25:38.214: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 06/10/23 13:25:38.214
  W0610 13:25:38.227539      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jun 10 13:25:38.229: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 06/10/23 13:25:38.23
  Jun 10 13:25:38.232: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 0
  Jun 10 13:25:38.232: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 0
  Jun 10 13:25:38.232: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 0
  Jun 10 13:25:38.232: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 0
  Jun 10 13:25:38.232: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 0
  Jun 10 13:25:38.232: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 0
  Jun 10 13:25:38.232: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 0
  Jun 10 13:25:38.232: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 0
  Jun 10 13:25:38.233: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1
  Jun 10 13:25:38.233: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1
  Jun 10 13:25:38.233: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2
  Jun 10 13:25:38.233: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2
  Jun 10 13:25:38.233: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2
  Jun 10 13:25:38.233: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2
  E0610 13:25:38.242221      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:25:38.242: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2
  Jun 10 13:25:38.242: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2
  Jun 10 13:25:38.262: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2
  Jun 10 13:25:38.262: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2
  Jun 10 13:25:38.287: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1
  Jun 10 13:25:38.287: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1
  Jun 10 13:25:38.308: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1
  Jun 10 13:25:38.308: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1
  Jun 10 13:25:39.233: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2
  Jun 10 13:25:39.233: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2
  E0610 13:25:39.242321      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:25:39.265: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1
  STEP: listing Deployments @ 06/10/23 13:25:39.266
  Jun 10 13:25:39.270: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 06/10/23 13:25:39.271
  Jun 10 13:25:39.284: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 06/10/23 13:25:39.285
  Jun 10 13:25:39.294: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 10 13:25:39.302: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 10 13:25:39.326: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 10 13:25:39.360: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 10 13:25:39.391: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 10 13:25:39.400: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 10 13:25:40.214: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0610 13:25:40.243063      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:25:40.247: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 10 13:25:40.377: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 10 13:25:40.385: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 10 13:25:41.204: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 06/10/23 13:25:41.238
  E0610 13:25:41.244074      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: fetching the DeploymentStatus @ 06/10/23 13:25:41.247
  Jun 10 13:25:41.254: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1
  Jun 10 13:25:41.255: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1
  Jun 10 13:25:41.255: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1
  Jun 10 13:25:41.255: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1
  Jun 10 13:25:41.255: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1
  Jun 10 13:25:41.256: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 1
  Jun 10 13:25:41.256: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2
  Jun 10 13:25:41.256: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 3
  Jun 10 13:25:41.256: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2
  Jun 10 13:25:41.256: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 2
  Jun 10 13:25:41.256: INFO: observed Deployment test-deployment in namespace deployment-8079 with ReadyReplicas 3
  STEP: deleting the Deployment @ 06/10/23 13:25:41.256
  Jun 10 13:25:41.270: INFO: observed event type MODIFIED
  Jun 10 13:25:41.270: INFO: observed event type MODIFIED
  Jun 10 13:25:41.270: INFO: observed event type MODIFIED
  Jun 10 13:25:41.270: INFO: observed event type MODIFIED
  Jun 10 13:25:41.270: INFO: observed event type MODIFIED
  Jun 10 13:25:41.271: INFO: observed event type MODIFIED
  Jun 10 13:25:41.271: INFO: observed event type MODIFIED
  Jun 10 13:25:41.271: INFO: observed event type MODIFIED
  Jun 10 13:25:41.271: INFO: observed event type MODIFIED
  Jun 10 13:25:41.272: INFO: observed event type MODIFIED
  Jun 10 13:25:41.272: INFO: observed event type MODIFIED
  Jun 10 13:25:41.272: INFO: observed event type MODIFIED
  Jun 10 13:25:41.277: INFO: Log out all the ReplicaSets if there is no deployment created
  Jun 10 13:25:41.281: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-8079  856b7ffe-7392-4aed-a756-8e03cc0a89aa 32918 3 2023-06-10 13:25:36 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment ef225800-5079-44cb-bb19-66e95e74068e 0xc0045c6e17 0xc0045c6e18}] [] [{kube-controller-manager Update apps/v1 2023-06-10 13:25:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef225800-5079-44cb-bb19-66e95e74068e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 13:25:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045c6ea0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jun 10 13:25:41.285: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-8079  e222190a-e76c-4c10-9483-70310b85cf73 33018 4 2023-06-10 13:25:38 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment ef225800-5079-44cb-bb19-66e95e74068e 0xc0045c6f07 0xc0045c6f08}] [] [{kube-controller-manager Update apps/v1 2023-06-10 13:25:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef225800-5079-44cb-bb19-66e95e74068e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 13:25:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045c6f90 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jun 10 13:25:41.292: INFO: pod: "test-deployment-5b5dcbcd95-ttxjz":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-ttxjz test-deployment-5b5dcbcd95- deployment-8079  46d648df-bd73-4345-9290-a41f51bb4b12 33013 0 2023-06-10 13:25:39 +0000 UTC 2023-06-10 13:25:42 +0000 UTC 0xc0045c7318 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 e222190a-e76c-4c10-9483-70310b85cf73 0xc0045c7347 0xc0045c7348}] [] [{kube-controller-manager Update v1 2023-06-10 13:25:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e222190a-e76c-4c10-9483-70310b85cf73\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:25:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.149.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wxbr8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wxbr8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-89-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.89.0,PodIP:192.168.149.114,StartTime:2023-06-10 13:25:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-10 13:25:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://4fc79f39e56a4d6e2fee0674d894db2ee36e0200188e3cf6f49d6e56e96d9d97,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.149.114,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jun 10 13:25:41.293: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-8079  ac2992f3-453c-4b36-91bd-c46d6cd034b5 33009 2 2023-06-10 13:25:39 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment ef225800-5079-44cb-bb19-66e95e74068e 0xc0045c6ff7 0xc0045c6ff8}] [] [{kube-controller-manager Update apps/v1 2023-06-10 13:25:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef225800-5079-44cb-bb19-66e95e74068e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 13:25:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045c7080 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Jun 10 13:25:41.298: INFO: pod: "test-deployment-6fc78d85c6-57w62":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-57w62 test-deployment-6fc78d85c6- deployment-8079  24cf114a-5664-4559-8948-830a4d998d6e 33008 0 2023-06-10 13:25:40 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 ac2992f3-453c-4b36-91bd-c46d6cd034b5 0xc0044aabc7 0xc0044aabc8}] [] [{kube-controller-manager Update v1 2023-06-10 13:25:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac2992f3-453c-4b36-91bd-c46d6cd034b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:25:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.149.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tg465,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tg465,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-89-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.89.0,PodIP:192.168.149.87,StartTime:2023-06-10 13:25:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-10 13:25:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://079d95e3ffd13acba29640ce75729e6d82910a2af8cf559e2a391bcc773f2d17,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.149.87,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jun 10 13:25:41.298: INFO: pod: "test-deployment-6fc78d85c6-zcnnt":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-zcnnt test-deployment-6fc78d85c6- deployment-8079  b1ac8ce9-2a56-43ce-9376-74387b78e0e2 32972 0 2023-06-10 13:25:39 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 ac2992f3-453c-4b36-91bd-c46d6cd034b5 0xc0044aadc7 0xc0044aadc8}] [] [{kube-controller-manager Update v1 2023-06-10 13:25:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac2992f3-453c-4b36-91bd-c46d6cd034b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:25:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.109.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j2kbt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j2kbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.177,PodIP:192.168.109.63,StartTime:2023-06-10 13:25:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-10 13:25:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d5cfe49e2fd354d1fab88a1a3013ee8bdf5fa69d7ca895b34e390860249281c9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.109.63,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jun 10 13:25:41.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8079" for this suite. @ 06/10/23 13:25:41.306
• [4.882 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 06/10/23 13:25:41.322
  Jun 10 13:25:41.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename deployment @ 06/10/23 13:25:41.323
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:25:41.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:25:41.345
  Jun 10 13:25:41.348: INFO: Creating simple deployment test-new-deployment
  Jun 10 13:25:41.375: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0610 13:25:42.244137      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:43.244949      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 06/10/23 13:25:43.392
  STEP: updating a scale subresource @ 06/10/23 13:25:43.396
  STEP: verifying the deployment Spec.Replicas was modified @ 06/10/23 13:25:43.402
  STEP: Patch a scale subresource @ 06/10/23 13:25:43.405
  Jun 10 13:25:43.426: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-8306  f45b6350-2d9d-4723-9772-74587779c9dd 33101 3 2023-06-10 13:25:41 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-06-10 13:25:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 13:25:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00410ad08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-10 13:25:43 +0000 UTC,LastTransitionTime:2023-06-10 13:25:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-06-10 13:25:43 +0000 UTC,LastTransitionTime:2023-06-10 13:25:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun 10 13:25:43.434: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-8306  f58b4322-fd3d-4238-9c1b-02412f88c494 33105 2 2023-06-10 13:25:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment f45b6350-2d9d-4723-9772-74587779c9dd 0xc003396257 0xc003396258}] [] [{kube-controller-manager Update apps/v1 2023-06-10 13:25:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f45b6350-2d9d-4723-9772-74587779c9dd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 13:25:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033962e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun 10 13:25:43.438: INFO: Pod "test-new-deployment-67bd4bf6dc-k55kt" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-k55kt test-new-deployment-67bd4bf6dc- deployment-8306  05ff27b2-c838-49c2-8d84-5e1fd704c675 33095 0 2023-06-10 13:25:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc f58b4322-fd3d-4238-9c1b-02412f88c494 0xc00410b127 0xc00410b128}] [] [{kube-controller-manager Update v1 2023-06-10 13:25:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f58b4322-fd3d-4238-9c1b-02412f88c494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:25:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.109.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6rcrd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6rcrd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.177,PodIP:192.168.109.8,StartTime:2023-06-10 13:25:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-10 13:25:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6a746897724b6dbdf3bb2311cfd7cd442c42edc45dfb823dbdb84e932584a66b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.109.8,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:25:43.439: INFO: Pod "test-new-deployment-67bd4bf6dc-xzkzj" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-xzkzj test-new-deployment-67bd4bf6dc- deployment-8306  be74a3f6-711a-40ac-b9b2-901c974e7ace 33104 0 2023-06-10 13:25:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc f58b4322-fd3d-4238-9c1b-02412f88c494 0xc00410b317 0xc00410b318}] [] [{kube-controller-manager Update v1 2023-06-10 13:25:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f58b4322-fd3d-4238-9c1b-02412f88c494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w42hx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w42hx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-89-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:25:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:25:43.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8306" for this suite. @ 06/10/23 13:25:43.445
• [2.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 06/10/23 13:25:43.462
  Jun 10 13:25:43.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename statefulset @ 06/10/23 13:25:43.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:25:43.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:25:43.489
  STEP: Creating service test in namespace statefulset-4088 @ 06/10/23 13:25:43.494
  Jun 10 13:25:43.517: INFO: Found 0 stateful pods, waiting for 1
  E0610 13:25:44.245005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:45.245129      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:46.245221      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:47.245436      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:48.245707      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:49.246008      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:50.246260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:51.246502      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:52.246613      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:53.246720      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:25:53.522: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 06/10/23 13:25:53.529
  W0610 13:25:53.540906      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jun 10 13:25:53.548: INFO: Found 1 stateful pods, waiting for 2
  E0610 13:25:54.247217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:55.247214      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:56.247334      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:57.247485      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:58.247612      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:25:59.247810      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:00.248117      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:01.248254      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:02.248300      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:03.248407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:26:03.553: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 10 13:26:03.553: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 06/10/23 13:26:03.56
  STEP: Delete all of the StatefulSets @ 06/10/23 13:26:03.564
  STEP: Verify that StatefulSets have been deleted @ 06/10/23 13:26:03.574
  Jun 10 13:26:03.578: INFO: Deleting all statefulset in ns statefulset-4088
  Jun 10 13:26:03.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4088" for this suite. @ 06/10/23 13:26:03.6
• [20.150 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 06/10/23 13:26:03.613
  Jun 10 13:26:03.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 13:26:03.614
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:26:03.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:26:03.649
  STEP: Creating configMap with name projected-configmap-test-volume-map-e65a5031-bb36-4c33-b95c-8b05a32a50bf @ 06/10/23 13:26:03.653
  STEP: Creating a pod to test consume configMaps @ 06/10/23 13:26:03.658
  E0610 13:26:04.248547      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:05.248590      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:06.249561      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:07.249763      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:26:07.683
  Jun 10 13:26:07.687: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-projected-configmaps-ab22f404-edfa-4b43-92b8-9e07eb967945 container agnhost-container: <nil>
  STEP: delete the pod @ 06/10/23 13:26:07.694
  Jun 10 13:26:07.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7909" for this suite. @ 06/10/23 13:26:07.717
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 06/10/23 13:26:07.73
  Jun 10 13:26:07.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename var-expansion @ 06/10/23 13:26:07.731
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:26:07.75
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:26:07.753
  STEP: Creating a pod to test substitution in volume subpath @ 06/10/23 13:26:07.757
  E0610 13:26:08.250763      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:09.251035      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:10.251505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:11.252345      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:26:11.784
  Jun 10 13:26:11.788: INFO: Trying to get logs from node ip-172-31-27-177 pod var-expansion-9bb258ff-0f4d-477e-95d6-723dc953a4b1 container dapi-container: <nil>
  STEP: delete the pod @ 06/10/23 13:26:11.796
  Jun 10 13:26:11.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6075" for this suite. @ 06/10/23 13:26:11.816
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 06/10/23 13:26:11.826
  Jun 10 13:26:11.826: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename daemonsets @ 06/10/23 13:26:11.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:26:11.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:26:11.857
  STEP: Creating simple DaemonSet "daemon-set" @ 06/10/23 13:26:11.882
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/10/23 13:26:11.888
  Jun 10 13:26:11.893: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:26:11.893: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:26:11.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:26:11.897: INFO: Node ip-172-31-27-177 is running 0 daemon pod, expected 1
  E0610 13:26:12.252601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:26:12.902: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:26:12.902: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:26:12.907: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:26:12.907: INFO: Node ip-172-31-27-177 is running 0 daemon pod, expected 1
  E0610 13:26:13.252631      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:26:13.904: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:26:13.904: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:26:13.908: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 10 13:26:13.908: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 06/10/23 13:26:13.911
  Jun 10 13:26:13.929: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:26:13.929: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:26:13.932: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 10 13:26:13.933: INFO: Node ip-172-31-46-40 is running 0 daemon pod, expected 1
  E0610 13:26:14.253503      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:26:14.940: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:26:14.940: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:26:14.944: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 10 13:26:14.944: INFO: Node ip-172-31-46-40 is running 0 daemon pod, expected 1
  E0610 13:26:15.254050      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:26:15.937: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:26:15.937: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:26:15.942: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 10 13:26:15.942: INFO: Node ip-172-31-46-40 is running 0 daemon pod, expected 1
  E0610 13:26:16.255113      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:26:16.938: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:26:16.938: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:26:16.942: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 10 13:26:16.942: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 06/10/23 13:26:16.946
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9534, will wait for the garbage collector to delete the pods @ 06/10/23 13:26:16.946
  Jun 10 13:26:17.008: INFO: Deleting DaemonSet.extensions daemon-set took: 6.85145ms
  Jun 10 13:26:17.108: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.442497ms
  E0610 13:26:17.255403      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:18.264385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:26:18.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:26:18.513: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 10 13:26:18.516: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33565"},"items":null}

  Jun 10 13:26:18.520: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33565"},"items":null}

  Jun 10 13:26:18.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9534" for this suite. @ 06/10/23 13:26:18.54
• [6.721 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 06/10/23 13:26:18.548
  Jun 10 13:26:18.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/10/23 13:26:18.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:26:18.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:26:18.574
  Jun 10 13:26:18.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  E0610 13:26:19.264433      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 06/10/23 13:26:19.998
  Jun 10 13:26:19.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-192 --namespace=crd-publish-openapi-192 create -f -'
  E0610 13:26:20.264746      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:26:20.846: INFO: stderr: ""
  Jun 10 13:26:20.846: INFO: stdout: "e2e-test-crd-publish-openapi-5960-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jun 10 13:26:20.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-192 --namespace=crd-publish-openapi-192 delete e2e-test-crd-publish-openapi-5960-crds test-cr'
  Jun 10 13:26:20.964: INFO: stderr: ""
  Jun 10 13:26:20.964: INFO: stdout: "e2e-test-crd-publish-openapi-5960-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Jun 10 13:26:20.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-192 --namespace=crd-publish-openapi-192 apply -f -'
  E0610 13:26:21.265463      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:26:21.795: INFO: stderr: ""
  Jun 10 13:26:21.795: INFO: stdout: "e2e-test-crd-publish-openapi-5960-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jun 10 13:26:21.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-192 --namespace=crd-publish-openapi-192 delete e2e-test-crd-publish-openapi-5960-crds test-cr'
  Jun 10 13:26:21.883: INFO: stderr: ""
  Jun 10 13:26:21.883: INFO: stdout: "e2e-test-crd-publish-openapi-5960-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 06/10/23 13:26:21.883
  Jun 10 13:26:21.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-192 explain e2e-test-crd-publish-openapi-5960-crds'
  Jun 10 13:26:22.119: INFO: stderr: ""
  Jun 10 13:26:22.119: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-5960-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0610 13:26:22.265909      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:23.276628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:26:23.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-192" for this suite. @ 06/10/23 13:26:23.562
• [5.025 seconds]
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 06/10/23 13:26:23.573
  Jun 10 13:26:23.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename security-context-test @ 06/10/23 13:26:23.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:26:23.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:26:23.607
  E0610 13:26:24.273792      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:25.273920      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:26.274790      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:27.275003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:26:27.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8848" for this suite. @ 06/10/23 13:26:27.64
• [4.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 06/10/23 13:26:27.648
  Jun 10 13:26:27.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename resourcequota @ 06/10/23 13:26:27.649
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:26:27.67
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:26:27.673
  E0610 13:26:28.276173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:29.276693      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:30.276652      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:31.276844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:32.278731      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:33.279564      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:34.280359      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:35.281152      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:36.281912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:37.282327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:38.282574      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:39.283038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:40.283165      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:41.283861      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:42.284086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:43.284911      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:44.285482      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 06/10/23 13:26:44.682
  E0610 13:26:45.285798      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:46.285896      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:47.286789      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:48.287703      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:49.288140      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 06/10/23 13:26:49.687
  STEP: Ensuring resource quota status is calculated @ 06/10/23 13:26:49.695
  E0610 13:26:50.288301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:51.288421      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 06/10/23 13:26:51.7
  STEP: Ensuring resource quota status captures configMap creation @ 06/10/23 13:26:51.713
  E0610 13:26:52.288511      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:53.288969      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 06/10/23 13:26:53.717
  STEP: Ensuring resource quota status released usage @ 06/10/23 13:26:53.724
  E0610 13:26:54.289373      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:55.289451      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:26:55.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7428" for this suite. @ 06/10/23 13:26:55.736
• [28.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 06/10/23 13:26:55.749
  Jun 10 13:26:55.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename server-version @ 06/10/23 13:26:55.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:26:55.77
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:26:55.774
  STEP: Request ServerVersion @ 06/10/23 13:26:55.777
  STEP: Confirm major version @ 06/10/23 13:26:55.779
  Jun 10 13:26:55.779: INFO: Major version: 1
  STEP: Confirm minor version @ 06/10/23 13:26:55.779
  Jun 10 13:26:55.779: INFO: cleanMinorVersion: 27
  Jun 10 13:26:55.780: INFO: Minor version: 27
  Jun 10 13:26:55.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-3270" for this suite. @ 06/10/23 13:26:55.784
• [0.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 06/10/23 13:26:55.793
  Jun 10 13:26:55.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename svcaccounts @ 06/10/23 13:26:55.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:26:55.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:26:55.812
  Jun 10 13:26:55.841: INFO: created pod
  E0610 13:26:56.289749      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:57.289875      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:58.290782      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:26:59.290898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:26:59.856
  E0610 13:27:00.291478      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:01.292245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:02.292334      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:03.292449      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:04.292548      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:05.292661      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:06.292765      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:07.292867      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:08.293100      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:09.293479      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:10.293733      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:11.293808      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:12.293916      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:13.294687      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:14.295012      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:15.295119      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:16.295563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:17.295667      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:18.295768      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:19.296043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:20.296911      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:21.297588      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:22.297700      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:23.297798      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:24.297900      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:25.298009      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:26.298356      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:27.298441      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:28.299005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:29.299111      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:27:29.857: INFO: polling logs
  Jun 10 13:27:29.865: INFO: Pod logs: 
  I0610 13:26:56.635620       1 log.go:198] OK: Got token
  I0610 13:26:56.635733       1 log.go:198] validating with in-cluster discovery
  I0610 13:26:56.636107       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0610 13:26:56.636155       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-2929:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1686404216, NotBefore:1686403616, IssuedAt:1686403616, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2929", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"1c49164c-c17d-491e-83a2-1aca085e1faf"}}}
  I0610 13:26:56.649951       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0610 13:26:56.658307       1 log.go:198] OK: Validated signature on JWT
  I0610 13:26:56.658564       1 log.go:198] OK: Got valid claims from token!
  I0610 13:26:56.658616       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-2929:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1686404216, NotBefore:1686403616, IssuedAt:1686403616, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2929", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"1c49164c-c17d-491e-83a2-1aca085e1faf"}}}

  Jun 10 13:27:29.865: INFO: completed pod
  Jun 10 13:27:29.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2929" for this suite. @ 06/10/23 13:27:29.876
• [34.091 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 06/10/23 13:27:29.885
  Jun 10 13:27:29.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename resourcequota @ 06/10/23 13:27:29.886
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:27:29.905
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:27:29.908
  STEP: Discovering how many secrets are in namespace by default @ 06/10/23 13:27:29.911
  E0610 13:27:30.300159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:31.300787      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:32.300856      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:33.301630      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:34.301737      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 06/10/23 13:27:34.917
  E0610 13:27:35.301829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:36.302554      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:37.302664      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:38.302772      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:39.302927      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 06/10/23 13:27:39.922
  STEP: Ensuring resource quota status is calculated @ 06/10/23 13:27:39.928
  E0610 13:27:40.303009      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:41.303035      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 06/10/23 13:27:41.933
  STEP: Ensuring resource quota status captures secret creation @ 06/10/23 13:27:41.945
  E0610 13:27:42.303150      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:43.303906      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 06/10/23 13:27:43.95
  STEP: Ensuring resource quota status released usage @ 06/10/23 13:27:43.957
  E0610 13:27:44.304011      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:45.304433      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:27:45.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6413" for this suite. @ 06/10/23 13:27:45.967
• [16.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 06/10/23 13:27:45.975
  Jun 10 13:27:45.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir-wrapper @ 06/10/23 13:27:45.976
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:27:46.003
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:27:46.006
  STEP: Creating 50 configmaps @ 06/10/23 13:27:46.01
  STEP: Creating RC which spawns configmap-volume pods @ 06/10/23 13:27:46.26
  E0610 13:27:46.304931      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:27:46.384: INFO: Pod name wrapped-volume-race-849b25aa-b299-45a7-b137-e405f718db06: Found 3 pods out of 5
  E0610 13:27:47.305753      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:48.305785      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:49.306928      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:50.306996      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:51.308144      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:27:51.391: INFO: Pod name wrapped-volume-race-849b25aa-b299-45a7-b137-e405f718db06: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 06/10/23 13:27:51.392
  STEP: Creating RC which spawns configmap-volume pods @ 06/10/23 13:27:51.417
  Jun 10 13:27:51.434: INFO: Pod name wrapped-volume-race-6b42d644-6771-4207-ac8e-6f51a9f98e1b: Found 0 pods out of 5
  E0610 13:27:52.308521      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:53.308638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:54.308711      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:55.308835      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:56.308912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:27:56.443: INFO: Pod name wrapped-volume-race-6b42d644-6771-4207-ac8e-6f51a9f98e1b: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 06/10/23 13:27:56.443
  STEP: Creating RC which spawns configmap-volume pods @ 06/10/23 13:27:56.468
  Jun 10 13:27:56.486: INFO: Pod name wrapped-volume-race-7379cbe9-d88c-4c3a-b5ed-0425d26da6d7: Found 0 pods out of 5
  E0610 13:27:57.309709      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:58.309887      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:27:59.309972      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:00.310455      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:01.310941      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:28:01.495: INFO: Pod name wrapped-volume-race-7379cbe9-d88c-4c3a-b5ed-0425d26da6d7: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 06/10/23 13:28:01.496
  Jun 10 13:28:01.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-7379cbe9-d88c-4c3a-b5ed-0425d26da6d7 in namespace emptydir-wrapper-7886, will wait for the garbage collector to delete the pods @ 06/10/23 13:28:01.522
  Jun 10 13:28:01.585: INFO: Deleting ReplicationController wrapped-volume-race-7379cbe9-d88c-4c3a-b5ed-0425d26da6d7 took: 8.067351ms
  Jun 10 13:28:01.685: INFO: Terminating ReplicationController wrapped-volume-race-7379cbe9-d88c-4c3a-b5ed-0425d26da6d7 pods took: 100.337725ms
  E0610 13:28:02.312132      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:03.312785      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-6b42d644-6771-4207-ac8e-6f51a9f98e1b in namespace emptydir-wrapper-7886, will wait for the garbage collector to delete the pods @ 06/10/23 13:28:04.186
  Jun 10 13:28:04.252: INFO: Deleting ReplicationController wrapped-volume-race-6b42d644-6771-4207-ac8e-6f51a9f98e1b took: 10.064058ms
  E0610 13:28:04.313457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:28:04.352: INFO: Terminating ReplicationController wrapped-volume-race-6b42d644-6771-4207-ac8e-6f51a9f98e1b pods took: 100.517297ms
  E0610 13:28:05.313716      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:06.314479      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-849b25aa-b299-45a7-b137-e405f718db06 in namespace emptydir-wrapper-7886, will wait for the garbage collector to delete the pods @ 06/10/23 13:28:06.953
  Jun 10 13:28:07.020: INFO: Deleting ReplicationController wrapped-volume-race-849b25aa-b299-45a7-b137-e405f718db06 took: 8.309323ms
  Jun 10 13:28:07.221: INFO: Terminating ReplicationController wrapped-volume-race-849b25aa-b299-45a7-b137-e405f718db06 pods took: 200.752332ms
  E0610 13:28:07.314852      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:08.315523      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:09.316128      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 06/10/23 13:28:09.722
  STEP: Destroying namespace "emptydir-wrapper-7886" for this suite. @ 06/10/23 13:28:10.079
• [24.112 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 06/10/23 13:28:10.088
  Jun 10 13:28:10.088: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename namespaces @ 06/10/23 13:28:10.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:28:10.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:28:10.115
  STEP: Creating namespace "e2e-ns-v9f4g" @ 06/10/23 13:28:10.12
  Jun 10 13:28:10.141: INFO: Namespace "e2e-ns-v9f4g-1022" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-v9f4g-1022" @ 06/10/23 13:28:10.141
  Jun 10 13:28:10.152: INFO: Namespace "e2e-ns-v9f4g-1022" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-v9f4g-1022" @ 06/10/23 13:28:10.152
  Jun 10 13:28:10.162: INFO: Namespace "e2e-ns-v9f4g-1022" has []v1.FinalizerName{"kubernetes"}
  Jun 10 13:28:10.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7361" for this suite. @ 06/10/23 13:28:10.167
  STEP: Destroying namespace "e2e-ns-v9f4g-1022" for this suite. @ 06/10/23 13:28:10.174
• [0.094 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 06/10/23 13:28:10.183
  Jun 10 13:28:10.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename deployment @ 06/10/23 13:28:10.184
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:28:10.203
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:28:10.207
  Jun 10 13:28:10.210: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Jun 10 13:28:10.221: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0610 13:28:10.316827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:11.317166      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:12.317300      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:13.317396      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:14.317514      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:28:15.226: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/10/23 13:28:15.227
  Jun 10 13:28:15.227: INFO: Creating deployment "test-rolling-update-deployment"
  Jun 10 13:28:15.234: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Jun 10 13:28:15.242: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0610 13:28:15.318015      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:16.318123      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:28:17.252: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Jun 10 13:28:17.256: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Jun 10 13:28:17.266: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8426  412c5f80-9f73-47eb-89b4-4f4282332689 34591 1 2023-06-10 13:28:15 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-06-10 13:28:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 13:28:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00442b348 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-10 13:28:15 +0000 UTC,LastTransitionTime:2023-06-10 13:28:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-06-10 13:28:16 +0000 UTC,LastTransitionTime:2023-06-10 13:28:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun 10 13:28:17.270: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-8426  f8ec5b29-1621-40b2-a790-78504c36de56 34581 1 2023-06-10 13:28:15 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 412c5f80-9f73-47eb-89b4-4f4282332689 0xc003dda8e7 0xc003dda8e8}] [] [{kube-controller-manager Update apps/v1 2023-06-10 13:28:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"412c5f80-9f73-47eb-89b4-4f4282332689\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 13:28:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dda998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun 10 13:28:17.270: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Jun 10 13:28:17.270: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8426  25b4ca9f-91e3-45ff-bf02-78313a529d1e 34590 2 2023-06-10 13:28:10 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 412c5f80-9f73-47eb-89b4-4f4282332689 0xc003dda7b7 0xc003dda7b8}] [] [{e2e.test Update apps/v1 2023-06-10 13:28:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 13:28:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"412c5f80-9f73-47eb-89b4-4f4282332689\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-10 13:28:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003dda878 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 10 13:28:17.274: INFO: Pod "test-rolling-update-deployment-656d657cd8-qqnrs" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-qqnrs test-rolling-update-deployment-656d657cd8- deployment-8426  0c26915e-54b0-4a17-979f-bdfc5533c0c8 34580 0 2023-06-10 13:28:15 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 f8ec5b29-1621-40b2-a790-78504c36de56 0xc00442b777 0xc00442b778}] [] [{kube-controller-manager Update v1 2023-06-10 13:28:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8ec5b29-1621-40b2-a790-78504c36de56\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:28:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.109.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vlh9v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vlh9v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:28:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:28:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:28:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:28:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.177,PodIP:192.168.109.4,StartTime:2023-06-10 13:28:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-10 13:28:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://7119b5171c34271c37b259a5dbdcc2ad334fe3cf2ff30abd14a8ad191529e9cd,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.109.4,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:28:17.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8426" for this suite. @ 06/10/23 13:28:17.279
• [7.106 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 06/10/23 13:28:17.292
  Jun 10 13:28:17.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename deployment @ 06/10/23 13:28:17.292
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:28:17.311
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:28:17.314
  E0610 13:28:17.318851      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a Deployment @ 06/10/23 13:28:17.322
  Jun 10 13:28:17.322: INFO: Creating simple deployment test-deployment-cvz9z
  Jun 10 13:28:17.336: INFO: deployment "test-deployment-cvz9z" doesn't have the required revision set
  E0610 13:28:18.319013      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:19.320138      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Getting /status @ 06/10/23 13:28:19.351
  Jun 10 13:28:19.356: INFO: Deployment test-deployment-cvz9z has Conditions: [{Available True 2023-06-10 13:28:18 +0000 UTC 2023-06-10 13:28:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-06-10 13:28:18 +0000 UTC 2023-06-10 13:28:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvz9z-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 06/10/23 13:28:19.356
  Jun 10 13:28:19.368: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 13, 28, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 13, 28, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 10, 13, 28, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 10, 13, 28, 17, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-cvz9z-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 06/10/23 13:28:19.368
  Jun 10 13:28:19.371: INFO: Observed &Deployment event: ADDED
  Jun 10 13:28:19.371: INFO: Observed Deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-10 13:28:17 +0000 UTC 2023-06-10 13:28:17 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvz9z-5994cf9475"}
  Jun 10 13:28:19.371: INFO: Observed &Deployment event: MODIFIED
  Jun 10 13:28:19.372: INFO: Observed Deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-10 13:28:17 +0000 UTC 2023-06-10 13:28:17 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvz9z-5994cf9475"}
  Jun 10 13:28:19.372: INFO: Observed Deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-10 13:28:17 +0000 UTC 2023-06-10 13:28:17 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jun 10 13:28:19.372: INFO: Observed &Deployment event: MODIFIED
  Jun 10 13:28:19.372: INFO: Observed Deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-10 13:28:17 +0000 UTC 2023-06-10 13:28:17 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jun 10 13:28:19.372: INFO: Observed Deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-10 13:28:17 +0000 UTC 2023-06-10 13:28:17 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-cvz9z-5994cf9475" is progressing.}
  Jun 10 13:28:19.373: INFO: Observed &Deployment event: MODIFIED
  Jun 10 13:28:19.373: INFO: Observed Deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-10 13:28:18 +0000 UTC 2023-06-10 13:28:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jun 10 13:28:19.373: INFO: Observed Deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-10 13:28:18 +0000 UTC 2023-06-10 13:28:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvz9z-5994cf9475" has successfully progressed.}
  Jun 10 13:28:19.373: INFO: Observed &Deployment event: MODIFIED
  Jun 10 13:28:19.373: INFO: Observed Deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-10 13:28:18 +0000 UTC 2023-06-10 13:28:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jun 10 13:28:19.373: INFO: Observed Deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-10 13:28:18 +0000 UTC 2023-06-10 13:28:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvz9z-5994cf9475" has successfully progressed.}
  Jun 10 13:28:19.373: INFO: Found Deployment test-deployment-cvz9z in namespace deployment-8819 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 10 13:28:19.373: INFO: Deployment test-deployment-cvz9z has an updated status
  STEP: patching the Statefulset Status @ 06/10/23 13:28:19.373
  Jun 10 13:28:19.374: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jun 10 13:28:19.384: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 06/10/23 13:28:19.384
  Jun 10 13:28:19.386: INFO: Observed &Deployment event: ADDED
  Jun 10 13:28:19.386: INFO: Observed deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-10 13:28:17 +0000 UTC 2023-06-10 13:28:17 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvz9z-5994cf9475"}
  Jun 10 13:28:19.386: INFO: Observed &Deployment event: MODIFIED
  Jun 10 13:28:19.386: INFO: Observed deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-10 13:28:17 +0000 UTC 2023-06-10 13:28:17 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvz9z-5994cf9475"}
  Jun 10 13:28:19.386: INFO: Observed deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-10 13:28:17 +0000 UTC 2023-06-10 13:28:17 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jun 10 13:28:19.387: INFO: Observed &Deployment event: MODIFIED
  Jun 10 13:28:19.387: INFO: Observed deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-10 13:28:17 +0000 UTC 2023-06-10 13:28:17 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jun 10 13:28:19.387: INFO: Observed deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-10 13:28:17 +0000 UTC 2023-06-10 13:28:17 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-cvz9z-5994cf9475" is progressing.}
  Jun 10 13:28:19.387: INFO: Observed &Deployment event: MODIFIED
  Jun 10 13:28:19.387: INFO: Observed deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-10 13:28:18 +0000 UTC 2023-06-10 13:28:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jun 10 13:28:19.388: INFO: Observed deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-10 13:28:18 +0000 UTC 2023-06-10 13:28:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvz9z-5994cf9475" has successfully progressed.}
  Jun 10 13:28:19.388: INFO: Observed &Deployment event: MODIFIED
  Jun 10 13:28:19.388: INFO: Observed deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-10 13:28:18 +0000 UTC 2023-06-10 13:28:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jun 10 13:28:19.388: INFO: Observed deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-10 13:28:18 +0000 UTC 2023-06-10 13:28:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvz9z-5994cf9475" has successfully progressed.}
  Jun 10 13:28:19.388: INFO: Observed deployment test-deployment-cvz9z in namespace deployment-8819 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 10 13:28:19.389: INFO: Observed &Deployment event: MODIFIED
  Jun 10 13:28:19.389: INFO: Found deployment test-deployment-cvz9z in namespace deployment-8819 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Jun 10 13:28:19.389: INFO: Deployment test-deployment-cvz9z has a patched status
  Jun 10 13:28:19.393: INFO: Deployment "test-deployment-cvz9z":
  &Deployment{ObjectMeta:{test-deployment-cvz9z  deployment-8819  e7e8b891-5dac-4772-8a13-8f7841f6cc89 34629 1 2023-06-10 13:28:17 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-10 13:28:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-06-10 13:28:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-06-10 13:28:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043cb8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-cvz9z-5994cf9475",LastUpdateTime:2023-06-10 13:28:19 +0000 UTC,LastTransitionTime:2023-06-10 13:28:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun 10 13:28:19.398: INFO: New ReplicaSet "test-deployment-cvz9z-5994cf9475" of Deployment "test-deployment-cvz9z":
  &ReplicaSet{ObjectMeta:{test-deployment-cvz9z-5994cf9475  deployment-8819  df3b3137-cbf0-4556-b75d-60ed5840667f 34624 1 2023-06-10 13:28:17 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-cvz9z e7e8b891-5dac-4772-8a13-8f7841f6cc89 0xc0043cbcb0 0xc0043cbcb1}] [] [{kube-controller-manager Update apps/v1 2023-06-10 13:28:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7e8b891-5dac-4772-8a13-8f7841f6cc89\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-10 13:28:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043cbd58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun 10 13:28:19.404: INFO: Pod "test-deployment-cvz9z-5994cf9475-jtxks" is available:
  &Pod{ObjectMeta:{test-deployment-cvz9z-5994cf9475-jtxks test-deployment-cvz9z-5994cf9475- deployment-8819  2be37b73-b9eb-4b6e-98e9-f982ceff1bce 34623 0 2023-06-10 13:28:17 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-cvz9z-5994cf9475 df3b3137-cbf0-4556-b75d-60ed5840667f 0xc004bac110 0xc004bac111}] [] [{kube-controller-manager Update v1 2023-06-10 13:28:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df3b3137-cbf0-4556-b75d-60ed5840667f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-10 13:28:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.109.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z7428,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z7428,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:28:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:28:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:28:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-10 13:28:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.177,PodIP:192.168.109.52,StartTime:2023-06-10 13:28:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-10 13:28:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9aa2edbf8fe581350cb7faa312bbeb5dc15be17e9dc4fb4dc661c20d60306504,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.109.52,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 10 13:28:19.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8819" for this suite. @ 06/10/23 13:28:19.41
• [2.127 seconds]
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 06/10/23 13:28:19.419
  Jun 10 13:28:19.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename events @ 06/10/23 13:28:19.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:28:19.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:28:19.442
  STEP: Create set of events @ 06/10/23 13:28:19.446
  STEP: get a list of Events with a label in the current namespace @ 06/10/23 13:28:19.467
  STEP: delete a list of events @ 06/10/23 13:28:19.472
  Jun 10 13:28:19.472: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 06/10/23 13:28:19.496
  Jun 10 13:28:19.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1142" for this suite. @ 06/10/23 13:28:19.504
• [0.094 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 06/10/23 13:28:19.514
  Jun 10 13:28:19.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir-wrapper @ 06/10/23 13:28:19.515
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:28:19.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:28:19.542
  E0610 13:28:20.320845      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:21.320985      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:28:21.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 06/10/23 13:28:21.586
  STEP: Cleaning up the configmap @ 06/10/23 13:28:21.596
  STEP: Cleaning up the pod @ 06/10/23 13:28:21.604
  STEP: Destroying namespace "emptydir-wrapper-7171" for this suite. @ 06/10/23 13:28:21.624
• [2.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 06/10/23 13:28:21.635
  Jun 10 13:28:21.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename resourcequota @ 06/10/23 13:28:21.636
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:28:21.657
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:28:21.661
  STEP: Counting existing ResourceQuota @ 06/10/23 13:28:21.667
  E0610 13:28:22.321291      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:23.322117      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:24.323024      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:25.323537      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:26.323646      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 06/10/23 13:28:26.671
  STEP: Ensuring resource quota status is calculated @ 06/10/23 13:28:26.677
  E0610 13:28:27.324130      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:28.324183      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 06/10/23 13:28:28.682
  STEP: Ensuring resource quota status captures replication controller creation @ 06/10/23 13:28:28.701
  E0610 13:28:29.324304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:30.325378      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 06/10/23 13:28:30.706
  STEP: Ensuring resource quota status released usage @ 06/10/23 13:28:30.714
  E0610 13:28:31.326151      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:32.326536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:28:32.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1775" for this suite. @ 06/10/23 13:28:32.723
• [11.096 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 06/10/23 13:28:32.733
  Jun 10 13:28:32.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename resourcequota @ 06/10/23 13:28:32.734
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:28:32.753
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:28:32.757
  STEP: Creating a ResourceQuota @ 06/10/23 13:28:32.765
  STEP: Getting a ResourceQuota @ 06/10/23 13:28:32.771
  STEP: Updating a ResourceQuota @ 06/10/23 13:28:32.775
  STEP: Verifying a ResourceQuota was modified @ 06/10/23 13:28:32.78
  STEP: Deleting a ResourceQuota @ 06/10/23 13:28:32.784
  STEP: Verifying the deleted ResourceQuota @ 06/10/23 13:28:32.791
  Jun 10 13:28:32.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4903" for this suite. @ 06/10/23 13:28:32.799
• [0.073 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 06/10/23 13:28:32.807
  Jun 10 13:28:32.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 13:28:32.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:28:32.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:28:32.83
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 13:28:32.833
  E0610 13:28:33.327174      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:34.327280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:35.328069      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:36.328200      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:28:36.858
  Jun 10 13:28:36.862: INFO: Trying to get logs from node ip-172-31-27-177 pod downwardapi-volume-6dc2effb-4ccb-46b6-b161-2ff367158ccf container client-container: <nil>
  STEP: delete the pod @ 06/10/23 13:28:36.873
  Jun 10 13:28:36.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1202" for this suite. @ 06/10/23 13:28:36.897
• [4.097 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 06/10/23 13:28:36.905
  Jun 10 13:28:36.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename emptydir @ 06/10/23 13:28:36.906
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:28:36.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:28:36.926
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 06/10/23 13:28:36.93
  E0610 13:28:37.329182      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:38.329772      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:39.330108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:40.330263      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:28:40.954
  Jun 10 13:28:40.959: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-c4de6f8e-b9a9-4e6f-b9ba-e4a15d6b58f4 container test-container: <nil>
  STEP: delete the pod @ 06/10/23 13:28:40.966
  Jun 10 13:28:40.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4652" for this suite. @ 06/10/23 13:28:40.989
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 06/10/23 13:28:40.999
  Jun 10 13:28:40.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename security-context-test @ 06/10/23 13:28:41
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:28:41.023
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:28:41.026
  E0610 13:28:41.330734      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:42.331081      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:43.331659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:44.331804      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:28:45.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-324" for this suite. @ 06/10/23 13:28:45.059
• [4.067 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 06/10/23 13:28:45.067
  Jun 10 13:28:45.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename security-context-test @ 06/10/23 13:28:45.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:28:45.087
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:28:45.09
  E0610 13:28:45.332858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:46.333561      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:47.334003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:48.334280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:28:49.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7289" for this suite. @ 06/10/23 13:28:49.132
• [4.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 06/10/23 13:28:49.142
  Jun 10 13:28:49.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename cronjob @ 06/10/23 13:28:49.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:28:49.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:28:49.168
  STEP: Creating a ForbidConcurrent cronjob @ 06/10/23 13:28:49.171
  STEP: Ensuring a job is scheduled @ 06/10/23 13:28:49.178
  E0610 13:28:49.334382      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:50.334452      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:51.335228      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:52.336086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:53.336159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:54.336215      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:55.336289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:56.336464      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:57.336536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:58.336621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:28:59.336840      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:00.337067      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 06/10/23 13:29:01.182
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 06/10/23 13:29:01.186
  STEP: Ensuring no more jobs are scheduled @ 06/10/23 13:29:01.19
  E0610 13:29:01.337159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:02.337460      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:03.337928      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:04.338074      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:05.338125      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:06.338262      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:07.339220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:08.339321      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:09.339753      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:10.339825      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:11.340257      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:12.340728      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:13.341444      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:14.341546      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:15.341659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:16.342522      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:17.343318      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:18.343559      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:19.344007      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:20.344112      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:21.344845      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:22.344947      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:23.346005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:24.346104      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:25.347022      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:26.347210      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:27.347323      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:28.347425      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:29.348103      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:30.348213      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:31.348828      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:32.348950      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:33.349581      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:34.349678      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:35.350171      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:36.350507      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:37.351286      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:38.351838      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:39.352370      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:40.352445      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:41.352571      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:42.352659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:43.353553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:44.353664      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:45.353790      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:46.353876      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:47.354467      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:48.354532      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:49.354614      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:50.354744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:51.355077      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:52.355167      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:53.355511      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:54.356153      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:55.356277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:56.356350      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:57.357188      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:58.357987      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:29:59.358831      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:00.359221      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:01.360106      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:02.360369      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:03.360879      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:04.364007      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:05.364734      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:06.364841      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:07.365511      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:08.365757      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:09.365965      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:10.366335      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:11.366417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:12.366741      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:13.367474      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:14.367651      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:15.368166      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:16.368481      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:17.368675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:18.368859      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:19.368904      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:20.369260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:21.370309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:22.370577      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:23.371449      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:24.371566      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:25.372567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:26.372847      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:27.373283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:28.373406      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:29.373568      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:30.373738      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:31.374613      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:32.374921      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:33.375018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:34.375126      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:35.376048      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:36.376157      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:37.376554      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:38.377069      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:39.377626      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:40.377827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:41.378524      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:42.379346      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:43.379493      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:44.379671      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:45.380491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:46.381239      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:47.381327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:48.381446      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:49.382210      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:50.382330      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:51.383116      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:52.384059      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:53.384889      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:54.385200      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:55.385819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:56.386135      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:57.386673      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:58.387181      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:30:59.388087      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:00.388782      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:01.389727      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:02.390083      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:03.390705      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:04.390998      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:05.391385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:06.391487      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:07.391956      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:08.392057      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:09.392821      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:10.392943      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:11.393916      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:12.394229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:13.394916      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:14.394988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:15.395391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:16.395515      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:17.396561      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:18.396671      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:19.397259      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:20.397440      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:21.398109      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:22.398224      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:23.398597      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:24.399257      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:25.399363      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:26.400073      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:27.400162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:28.400264      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:29.400629      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:30.400904      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:31.401361      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:32.401477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:33.402478      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:34.402673      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:35.402776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:36.402981      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:37.403532      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:38.404050      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:39.405114      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:40.405369      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:41.405880      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:42.406616      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:43.407328      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:44.407438      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:45.408361      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:46.408468      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:47.409357      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:48.409467      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:49.410564      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:50.410955      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:51.411706      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:52.412067      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:53.412252      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:54.412373      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:55.413340      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:56.413759      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:57.413866      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:58.414942      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:31:59.415921      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:00.416140      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:01.417127      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:02.417230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:03.418231      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:04.418331      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:05.418453      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:06.418524      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:07.419565      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:08.419723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:09.419759      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:10.419885      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:11.420477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:12.421284      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:13.421711      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:14.421899      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:15.422825      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:16.423018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:17.423360      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:18.423873      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:19.424353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:20.424659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:21.425184      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:22.425274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:23.425654      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:24.425986      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:25.426926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:26.427008      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:27.427756      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:28.427981      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:29.428408      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:30.428618      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:31.428713      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:32.428832      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:33.429691      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:34.429815      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:35.430697      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:36.431348      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:37.432030      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:38.432144      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:39.432387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:40.432722      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:41.433199      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:42.433490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:43.433528      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:44.433635      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:45.434513      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:46.434693      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:47.435555      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:48.435682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:49.435824      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:50.436094      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:51.436163      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:52.436276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:53.437253      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:54.437959      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:55.438744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:56.438829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:57.439622      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:58.440009      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:32:59.440551      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:00.440699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:01.441290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:02.441413      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:03.441492      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:04.442099      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:05.442220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:06.442317      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:07.442557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:08.442984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:09.443583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:10.444084      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:11.444584      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:12.444939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:13.445059      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:14.445165      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:15.446231      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:16.446335      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:17.446742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:18.446984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:19.447419      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:20.448075      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:21.448190      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:22.448468      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:23.449383      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:24.449831      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:25.450479      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:26.450597      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:27.451352      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:28.451648      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:29.452115      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:30.452417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:31.452667      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:32.453069      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:33.453830      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:34.453922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:35.454425      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:36.454767      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:37.455583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:38.456041      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:39.456549      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:40.456670      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:41.457173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:42.457276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:43.457448      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:44.457840      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:45.458930      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:46.459013      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:47.459123      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:48.459217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:49.459761      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:50.459974      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:51.460803      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:52.461129      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:53.461657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:54.461757      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:55.461867      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:56.461961      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:57.462593      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:58.462610      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:33:59.463431      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:00.463550      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 06/10/23 13:34:01.198
  Jun 10 13:34:01.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4342" for this suite. @ 06/10/23 13:34:01.209
• [312.074 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 06/10/23 13:34:01.217
  Jun 10 13:34:01.217: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename secrets @ 06/10/23 13:34:01.218
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:34:01.26
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:34:01.263
  STEP: Creating secret with name s-test-opt-del-7ae674e3-fff2-4ee3-8ba0-9d4e02216a0c @ 06/10/23 13:34:01.271
  STEP: Creating secret with name s-test-opt-upd-20655353-aad6-47b4-bc8b-6170596eb155 @ 06/10/23 13:34:01.277
  STEP: Creating the pod @ 06/10/23 13:34:01.281
  E0610 13:34:01.464521      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:02.464627      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-7ae674e3-fff2-4ee3-8ba0-9d4e02216a0c @ 06/10/23 13:34:03.346
  STEP: Updating secret s-test-opt-upd-20655353-aad6-47b4-bc8b-6170596eb155 @ 06/10/23 13:34:03.353
  STEP: Creating secret with name s-test-opt-create-396820db-742f-46c1-84b3-49c933f2fe53 @ 06/10/23 13:34:03.358
  STEP: waiting to observe update in volume @ 06/10/23 13:34:03.365
  E0610 13:34:03.465476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:04.465853      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:05.466604      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:06.466769      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:07.467538      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:08.467658      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:09.468416      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:10.468581      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:11.468663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:12.468790      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:13.469189      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:14.469369      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:15.469585      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:16.469817      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:17.470704      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:18.471040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:19.471065      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:20.471148      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:21.472125      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:22.472353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:23.473120      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:24.473316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:25.473682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:26.473858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:27.473870      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:28.474036      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:29.474139      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:30.474206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:31.474312      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:32.474380      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:33.474980      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:34.475171      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:35.476231      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:36.477247      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:37.477776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:38.477868      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:39.478213      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:40.478325      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:41.478442      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:42.479012      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:43.479120      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:44.480233      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:45.480314      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:46.480446      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:47.480800      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:48.480955      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:49.481080      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:50.481162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:51.482145      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:52.482275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:53.482521      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:54.482622      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:55.482747      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:56.483026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:57.484215      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:58.484328      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:34:59.484444      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:00.485156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:01.485249      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:02.485359      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:03.485588      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:04.486545      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:05.486648      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:06.487018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:07.487103      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:08.487204      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:09.487377      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:35:09.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9722" for this suite. @ 06/10/23 13:35:09.721
• [68.511 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 06/10/23 13:35:09.729
  Jun 10 13:35:09.729: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename disruption @ 06/10/23 13:35:09.73
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:35:09.754
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:35:09.758
  STEP: Waiting for the pdb to be processed @ 06/10/23 13:35:09.767
  E0610 13:35:10.488187      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:11.488294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating PodDisruptionBudget status @ 06/10/23 13:35:11.775
  STEP: Waiting for all pods to be running @ 06/10/23 13:35:11.785
  Jun 10 13:35:11.788: INFO: running pods: 0 < 1
  E0610 13:35:12.488385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:13.488493      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:35:13.795: INFO: running pods: 0 < 1
  E0610 13:35:14.488697      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:15.489175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 06/10/23 13:35:15.794
  STEP: Waiting for the pdb to be processed @ 06/10/23 13:35:15.806
  STEP: Patching PodDisruptionBudget status @ 06/10/23 13:35:15.814
  STEP: Waiting for the pdb to be processed @ 06/10/23 13:35:15.824
  Jun 10 13:35:15.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-6346" for this suite. @ 06/10/23 13:35:15.833
• [6.112 seconds]
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 06/10/23 13:35:15.842
  Jun 10 13:35:15.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubelet-test @ 06/10/23 13:35:15.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:35:15.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:35:15.866
  Jun 10 13:35:15.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8461" for this suite. @ 06/10/23 13:35:15.905
• [0.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 06/10/23 13:35:15.919
  Jun 10 13:35:15.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/10/23 13:35:15.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:35:15.94
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:35:15.944
  Jun 10 13:35:15.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  E0610 13:35:16.489758      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 06/10/23 13:35:17.443
  Jun 10 13:35:17.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-1919 --namespace=crd-publish-openapi-1919 create -f -'
  E0610 13:35:17.490684      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:35:18.326: INFO: stderr: ""
  Jun 10 13:35:18.326: INFO: stdout: "e2e-test-crd-publish-openapi-6951-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jun 10 13:35:18.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-1919 --namespace=crd-publish-openapi-1919 delete e2e-test-crd-publish-openapi-6951-crds test-cr'
  Jun 10 13:35:18.448: INFO: stderr: ""
  Jun 10 13:35:18.448: INFO: stdout: "e2e-test-crd-publish-openapi-6951-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Jun 10 13:35:18.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-1919 --namespace=crd-publish-openapi-1919 apply -f -'
  E0610 13:35:18.491382      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:19.492345      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:35:19.544: INFO: stderr: ""
  Jun 10 13:35:19.544: INFO: stdout: "e2e-test-crd-publish-openapi-6951-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jun 10 13:35:19.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-1919 --namespace=crd-publish-openapi-1919 delete e2e-test-crd-publish-openapi-6951-crds test-cr'
  Jun 10 13:35:19.658: INFO: stderr: ""
  Jun 10 13:35:19.658: INFO: stdout: "e2e-test-crd-publish-openapi-6951-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 06/10/23 13:35:19.658
  Jun 10 13:35:19.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=crd-publish-openapi-1919 explain e2e-test-crd-publish-openapi-6951-crds'
  Jun 10 13:35:19.896: INFO: stderr: ""
  Jun 10 13:35:19.896: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-6951-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0610 13:35:20.493336      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:21.494410      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:35:21.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1919" for this suite. @ 06/10/23 13:35:21.876
• [5.966 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 06/10/23 13:35:21.885
  Jun 10 13:35:21.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename sched-pred @ 06/10/23 13:35:21.886
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:35:21.908
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:35:21.911
  Jun 10 13:35:21.915: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jun 10 13:35:21.925: INFO: Waiting for terminating namespaces to be deleted...
  Jun 10 13:35:21.928: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-27-177 before test
  Jun 10 13:35:21.934: INFO: default-http-backend-kubernetes-worker-65fc475d49-ndjwt from ingress-nginx-kubernetes-worker started at 2023-06-10 12:27:43 +0000 UTC (1 container statuses recorded)
  Jun 10 13:35:21.934: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jun 10 13:35:21.934: INFO: nginx-ingress-controller-kubernetes-worker-6c5bl from ingress-nginx-kubernetes-worker started at 2023-06-10 12:27:55 +0000 UTC (1 container statuses recorded)
  Jun 10 13:35:21.934: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 10 13:35:21.934: INFO: calico-kube-controllers-86c9c69795-vrx7l from kube-system started at 2023-06-10 12:27:43 +0000 UTC (1 container statuses recorded)
  Jun 10 13:35:21.934: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jun 10 13:35:21.934: INFO: sonobuoy-systemd-logs-daemon-set-31eb0473969f40ee-l8bg7 from sonobuoy started at 2023-06-10 12:06:59 +0000 UTC (2 container statuses recorded)
  Jun 10 13:35:21.934: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 10 13:35:21.934: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 10 13:35:21.934: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-46-40 before test
  Jun 10 13:35:21.941: INFO: nginx-ingress-controller-kubernetes-worker-vb28m from ingress-nginx-kubernetes-worker started at 2023-06-10 11:54:55 +0000 UTC (1 container statuses recorded)
  Jun 10 13:35:21.941: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 10 13:35:21.941: INFO: coredns-5c7f76ccb8-xmz95 from kube-system started at 2023-06-10 11:54:47 +0000 UTC (1 container statuses recorded)
  Jun 10 13:35:21.941: INFO: 	Container coredns ready: true, restart count 0
  Jun 10 13:35:21.941: INFO: kube-state-metrics-5b95b4459c-rtj7m from kube-system started at 2023-06-10 11:54:47 +0000 UTC (1 container statuses recorded)
  Jun 10 13:35:21.941: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jun 10 13:35:21.941: INFO: metrics-server-v0.5.2-6cf8c8b69c-ftmcl from kube-system started at 2023-06-10 11:54:47 +0000 UTC (2 container statuses recorded)
  Jun 10 13:35:21.942: INFO: 	Container metrics-server ready: true, restart count 0
  Jun 10 13:35:21.942: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jun 10 13:35:21.942: INFO: dashboard-metrics-scraper-6b8586b5c9-znc9p from kubernetes-dashboard started at 2023-06-10 11:54:47 +0000 UTC (1 container statuses recorded)
  Jun 10 13:35:21.942: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jun 10 13:35:21.942: INFO: kubernetes-dashboard-6869f4cd5f-xdbkn from kubernetes-dashboard started at 2023-06-10 11:54:47 +0000 UTC (1 container statuses recorded)
  Jun 10 13:35:21.942: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jun 10 13:35:21.942: INFO: sonobuoy-systemd-logs-daemon-set-31eb0473969f40ee-wg64x from sonobuoy started at 2023-06-10 12:06:59 +0000 UTC (2 container statuses recorded)
  Jun 10 13:35:21.942: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 10 13:35:21.942: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 10 13:35:21.942: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-89-0 before test
  Jun 10 13:35:21.948: INFO: pod-0 from disruption-6346 started at 2023-06-10 13:35:11 +0000 UTC (1 container statuses recorded)
  Jun 10 13:35:21.948: INFO: 	Container donothing ready: false, restart count 0
  Jun 10 13:35:21.948: INFO: nginx-ingress-controller-kubernetes-worker-889j8 from ingress-nginx-kubernetes-worker started at 2023-06-10 12:03:37 +0000 UTC (1 container statuses recorded)
  Jun 10 13:35:21.948: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 10 13:35:21.948: INFO: sonobuoy from sonobuoy started at 2023-06-10 12:06:57 +0000 UTC (1 container statuses recorded)
  Jun 10 13:35:21.948: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jun 10 13:35:21.948: INFO: sonobuoy-e2e-job-f362cde51af14fae from sonobuoy started at 2023-06-10 12:06:59 +0000 UTC (2 container statuses recorded)
  Jun 10 13:35:21.948: INFO: 	Container e2e ready: true, restart count 0
  Jun 10 13:35:21.948: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 10 13:35:21.948: INFO: sonobuoy-systemd-logs-daemon-set-31eb0473969f40ee-qjm5m from sonobuoy started at 2023-06-10 12:06:59 +0000 UTC (2 container statuses recorded)
  Jun 10 13:35:21.948: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 10 13:35:21.948: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 06/10/23 13:35:21.948
  E0610 13:35:22.495398      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:23.495514      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 06/10/23 13:35:23.97
  STEP: Trying to apply a random label on the found node. @ 06/10/23 13:35:23.984
  STEP: verifying the node has the label kubernetes.io/e2e-a768c1fc-f125-4cd6-9387-5f3ac44dafe9 42 @ 06/10/23 13:35:23.994
  STEP: Trying to relaunch the pod, now with labels. @ 06/10/23 13:35:23.998
  E0610 13:35:24.495570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:25.495661      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-a768c1fc-f125-4cd6-9387-5f3ac44dafe9 off the node ip-172-31-27-177 @ 06/10/23 13:35:26.017
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-a768c1fc-f125-4cd6-9387-5f3ac44dafe9 @ 06/10/23 13:35:26.031
  Jun 10 13:35:26.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9035" for this suite. @ 06/10/23 13:35:26.044
• [4.167 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 06/10/23 13:35:26.053
  Jun 10 13:35:26.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/10/23 13:35:26.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:35:26.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:35:26.078
  Jun 10 13:35:26.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  E0610 13:35:26.496198      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:35:26.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9193" for this suite. @ 06/10/23 13:35:26.641
• [0.598 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 06/10/23 13:35:26.651
  Jun 10 13:35:26.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 06/10/23 13:35:26.652
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:35:26.671
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:35:26.674
  STEP: Setting up the test @ 06/10/23 13:35:26.678
  STEP: Creating hostNetwork=false pod @ 06/10/23 13:35:26.678
  E0610 13:35:27.496365      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:28.497378      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 06/10/23 13:35:28.705
  E0610 13:35:29.497463      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:30.497742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 06/10/23 13:35:30.725
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 06/10/23 13:35:30.725
  Jun 10 13:35:30.725: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9175 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 13:35:30.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:35:30.726: INFO: ExecWithOptions: Clientset creation
  Jun 10 13:35:30.726: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9175/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jun 10 13:35:30.793: INFO: Exec stderr: ""
  Jun 10 13:35:30.793: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9175 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 13:35:30.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:35:30.794: INFO: ExecWithOptions: Clientset creation
  Jun 10 13:35:30.794: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9175/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jun 10 13:35:30.865: INFO: Exec stderr: ""
  Jun 10 13:35:30.865: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9175 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 13:35:30.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:35:30.866: INFO: ExecWithOptions: Clientset creation
  Jun 10 13:35:30.866: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9175/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jun 10 13:35:30.935: INFO: Exec stderr: ""
  Jun 10 13:35:30.935: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9175 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 13:35:30.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:35:30.936: INFO: ExecWithOptions: Clientset creation
  Jun 10 13:35:30.937: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9175/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jun 10 13:35:31.008: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 06/10/23 13:35:31.009
  Jun 10 13:35:31.009: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9175 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 13:35:31.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:35:31.009: INFO: ExecWithOptions: Clientset creation
  Jun 10 13:35:31.010: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9175/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jun 10 13:35:31.081: INFO: Exec stderr: ""
  Jun 10 13:35:31.081: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9175 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 13:35:31.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:35:31.082: INFO: ExecWithOptions: Clientset creation
  Jun 10 13:35:31.083: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9175/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jun 10 13:35:31.178: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 06/10/23 13:35:31.178
  Jun 10 13:35:31.178: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9175 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 13:35:31.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:35:31.179: INFO: ExecWithOptions: Clientset creation
  Jun 10 13:35:31.180: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9175/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jun 10 13:35:31.265: INFO: Exec stderr: ""
  Jun 10 13:35:31.265: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9175 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 13:35:31.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:35:31.266: INFO: ExecWithOptions: Clientset creation
  Jun 10 13:35:31.266: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9175/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jun 10 13:35:31.338: INFO: Exec stderr: ""
  Jun 10 13:35:31.338: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9175 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 13:35:31.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:35:31.340: INFO: ExecWithOptions: Clientset creation
  Jun 10 13:35:31.340: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9175/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jun 10 13:35:31.418: INFO: Exec stderr: ""
  Jun 10 13:35:31.418: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9175 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 13:35:31.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:35:31.419: INFO: ExecWithOptions: Clientset creation
  Jun 10 13:35:31.419: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9175/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jun 10 13:35:31.489: INFO: Exec stderr: ""
  Jun 10 13:35:31.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-9175" for this suite. @ 06/10/23 13:35:31.494
  E0610 13:35:31.498745      18 retrywatcher.go:130] "Watch failed" err="context canceled"
• [4.851 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 06/10/23 13:35:31.506
  Jun 10 13:35:31.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/10/23 13:35:31.508
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:35:31.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:35:31.532
  STEP: set up a multi version CRD @ 06/10/23 13:35:31.536
  Jun 10 13:35:31.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  E0610 13:35:32.498959      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:33.499557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:34.500168      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 06/10/23 13:35:35.342
  STEP: check the unserved version gets removed @ 06/10/23 13:35:35.366
  E0610 13:35:35.500696      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 06/10/23 13:35:36.364
  E0610 13:35:36.501510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:37.501854      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:38.502535      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:39.503544      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:35:39.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7400" for this suite. @ 06/10/23 13:35:39.517
• [8.020 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 06/10/23 13:35:39.528
  Jun 10 13:35:39.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename pods @ 06/10/23 13:35:39.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:35:39.549
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:35:39.554
  STEP: creating the pod @ 06/10/23 13:35:39.558
  STEP: submitting the pod to kubernetes @ 06/10/23 13:35:39.559
  STEP: verifying QOS class is set on the pod @ 06/10/23 13:35:39.569
  Jun 10 13:35:39.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6153" for this suite. @ 06/10/23 13:35:39.579
• [0.059 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 06/10/23 13:35:39.588
  Jun 10 13:35:39.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename statefulset @ 06/10/23 13:35:39.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:35:39.61
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:35:39.614
  STEP: Creating service test in namespace statefulset-9886 @ 06/10/23 13:35:39.618
  STEP: Looking for a node to schedule stateful set and pod @ 06/10/23 13:35:39.626
  STEP: Creating pod with conflicting port in namespace statefulset-9886 @ 06/10/23 13:35:39.632
  STEP: Waiting until pod test-pod will start running in namespace statefulset-9886 @ 06/10/23 13:35:39.645
  E0610 13:35:40.507184      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:41.507223      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-9886 @ 06/10/23 13:35:41.657
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9886 @ 06/10/23 13:35:41.664
  Jun 10 13:35:41.680: INFO: Observed stateful pod in namespace: statefulset-9886, name: ss-0, uid: e8f73475-6804-4a73-9fca-49fd82a62703, status phase: Pending. Waiting for statefulset controller to delete.
  Jun 10 13:35:41.698: INFO: Observed stateful pod in namespace: statefulset-9886, name: ss-0, uid: e8f73475-6804-4a73-9fca-49fd82a62703, status phase: Failed. Waiting for statefulset controller to delete.
  Jun 10 13:35:41.709: INFO: Observed stateful pod in namespace: statefulset-9886, name: ss-0, uid: e8f73475-6804-4a73-9fca-49fd82a62703, status phase: Failed. Waiting for statefulset controller to delete.
  Jun 10 13:35:41.713: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9886
  STEP: Removing pod with conflicting port in namespace statefulset-9886 @ 06/10/23 13:35:41.714
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9886 and will be in running state @ 06/10/23 13:35:41.735
  E0610 13:35:42.508113      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:43.508253      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:35:43.743: INFO: Deleting all statefulset in ns statefulset-9886
  Jun 10 13:35:43.747: INFO: Scaling statefulset ss to 0
  E0610 13:35:44.508652      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:45.508987      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:46.509255      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:47.509582      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:48.510042      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:49.510195      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:50.510287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:51.510399      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:52.510477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:53.510553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:35:53.769: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 10 13:35:53.773: INFO: Deleting statefulset ss
  Jun 10 13:35:53.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9886" for this suite. @ 06/10/23 13:35:53.793
• [14.211 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 06/10/23 13:35:53.802
  Jun 10 13:35:53.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-probe @ 06/10/23 13:35:53.803
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:35:53.822
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:35:53.826
  E0610 13:35:54.514058      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:55.514097      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:56.514293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:57.515110      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:58.516065      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:35:59.516235      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:00.516358      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:01.517327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:02.517456      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:03.517560      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:04.518461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:05.519208      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:06.519531      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:07.519648      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:08.520718      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:09.520810      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:10.521585      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:11.521701      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:12.521740      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:13.522684      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:14.522800      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:15.522992      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:16.523089      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:17.523193      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:18.523302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:19.524071      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:20.525109      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:21.525226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:22.526205      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:23.526993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:24.527876      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:25.528746      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:26.528850      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:27.529855      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:28.530436      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:29.530551      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:30.530662      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:31.531601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:32.531703      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:33.531971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:34.532093      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:35.532829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:36.533020      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:37.533673      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:38.534592      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:39.534682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:40.535454      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:41.536116      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:42.536240      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:43.537281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:44.538107      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:45.538933      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:46.539988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:47.540170      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:48.541261      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:49.541361      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:50.541922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:51.542981      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:52.544090      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:53.544997      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:36:53.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-140" for this suite. @ 06/10/23 13:36:53.852
• [60.057 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 06/10/23 13:36:53.86
  Jun 10 13:36:53.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 13:36:53.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:36:53.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:36:53.884
  STEP: Setting up server cert @ 06/10/23 13:36:53.909
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 13:36:54.459
  STEP: Deploying the webhook pod @ 06/10/23 13:36:54.47
  STEP: Wait for the deployment to be ready @ 06/10/23 13:36:54.486
  Jun 10 13:36:54.493: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0610 13:36:54.545622      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:55.545737      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/10/23 13:36:56.505
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 13:36:56.517
  E0610 13:36:56.546431      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:36:57.517: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 06/10/23 13:36:57.521
  STEP: create a pod that should be denied by the webhook @ 06/10/23 13:36:57.539
  E0610 13:36:57.547125      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a pod that causes the webhook to hang @ 06/10/23 13:36:57.557
  E0610 13:36:58.548133      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:36:59.548534      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:00.549504      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:01.549584      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:02.549674      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:03.550084      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:04.550563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:05.550709      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:06.551008      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:07.552096      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 06/10/23 13:37:07.57
  STEP: create a configmap that should be admitted by the webhook @ 06/10/23 13:37:07.586
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 06/10/23 13:37:07.599
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 06/10/23 13:37:07.61
  STEP: create a namespace that bypass the webhook @ 06/10/23 13:37:07.618
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 06/10/23 13:37:07.636
  Jun 10 13:37:07.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-615" for this suite. @ 06/10/23 13:37:07.711
  STEP: Destroying namespace "webhook-markers-8011" for this suite. @ 06/10/23 13:37:07.723
  STEP: Destroying namespace "exempted-namespace-5456" for this suite. @ 06/10/23 13:37:07.733
• [13.881 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 06/10/23 13:37:07.744
  Jun 10 13:37:07.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename downward-api @ 06/10/23 13:37:07.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:37:07.764
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:37:07.768
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 13:37:07.772
  E0610 13:37:08.552245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:09.552335      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:10.552734      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:11.552902      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:37:11.803
  Jun 10 13:37:11.807: INFO: Trying to get logs from node ip-172-31-89-0 pod downwardapi-volume-d499f0e4-f742-4307-bfcb-42eddc8152d0 container client-container: <nil>
  STEP: delete the pod @ 06/10/23 13:37:11.83
  Jun 10 13:37:11.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4366" for this suite. @ 06/10/23 13:37:11.854
• [4.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 06/10/23 13:37:11.866
  Jun 10 13:37:11.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename webhook @ 06/10/23 13:37:11.867
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:37:11.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:37:11.888
  STEP: Setting up server cert @ 06/10/23 13:37:11.92
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/10/23 13:37:12.436
  STEP: Deploying the webhook pod @ 06/10/23 13:37:12.443
  STEP: Wait for the deployment to be ready @ 06/10/23 13:37:12.457
  Jun 10 13:37:12.476: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0610 13:37:12.553345      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:13.553419      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/10/23 13:37:14.488
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 13:37:14.507
  E0610 13:37:14.554483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:37:15.508: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 06/10/23 13:37:15.512
  STEP: Creating a custom resource definition that should be denied by the webhook @ 06/10/23 13:37:15.53
  Jun 10 13:37:15.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:37:15.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0610 13:37:15.554545      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-3570" for this suite. @ 06/10/23 13:37:15.608
  STEP: Destroying namespace "webhook-markers-1777" for this suite. @ 06/10/23 13:37:15.619
• [3.761 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 06/10/23 13:37:15.629
  Jun 10 13:37:15.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename namespaces @ 06/10/23 13:37:15.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:37:15.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:37:15.651
  STEP: Creating a test namespace @ 06/10/23 13:37:15.655
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:37:15.673
  STEP: Creating a service in the namespace @ 06/10/23 13:37:15.681
  STEP: Deleting the namespace @ 06/10/23 13:37:15.691
  STEP: Waiting for the namespace to be removed. @ 06/10/23 13:37:15.702
  E0610 13:37:16.555018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:17.556130      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:18.556234      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:19.556352      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:20.556414      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:21.556584      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 06/10/23 13:37:21.707
  STEP: Verifying there is no service in the namespace @ 06/10/23 13:37:21.729
  Jun 10 13:37:21.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9094" for this suite. @ 06/10/23 13:37:21.738
  STEP: Destroying namespace "nsdeletetest-5307" for this suite. @ 06/10/23 13:37:21.748
  Jun 10 13:37:21.753: INFO: Namespace nsdeletetest-5307 was already deleted
  STEP: Destroying namespace "nsdeletetest-3534" for this suite. @ 06/10/23 13:37:21.753
• [6.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 06/10/23 13:37:21.761
  Jun 10 13:37:21.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename secrets @ 06/10/23 13:37:21.763
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:37:21.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:37:21.784
  STEP: creating secret secrets-7493/secret-test-92439ab1-eea9-48ec-864e-c758790b5f39 @ 06/10/23 13:37:21.788
  STEP: Creating a pod to test consume secrets @ 06/10/23 13:37:21.795
  E0610 13:37:22.557510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:23.557565      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:24.557659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:25.557859      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:37:25.823
  Jun 10 13:37:25.827: INFO: Trying to get logs from node ip-172-31-89-0 pod pod-configmaps-c6f77f6d-4ee7-44a4-bae9-fa786f199112 container env-test: <nil>
  STEP: delete the pod @ 06/10/23 13:37:25.837
  Jun 10 13:37:25.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7493" for this suite. @ 06/10/23 13:37:25.86
• [4.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 06/10/23 13:37:25.869
  Jun 10 13:37:25.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename svc-latency @ 06/10/23 13:37:25.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:37:25.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:37:25.892
  Jun 10 13:37:25.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-939 @ 06/10/23 13:37:25.897
  I0610 13:37:25.904475      18 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-939, replica count: 1
  E0610 13:37:26.558005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0610 13:37:26.955554      18 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0610 13:37:27.558046      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0610 13:37:27.956052      18 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 10 13:37:28.068: INFO: Created: latency-svc-jb9kf
  Jun 10 13:37:28.082: INFO: Got endpoints: latency-svc-jb9kf [25.097816ms]
  Jun 10 13:37:28.100: INFO: Created: latency-svc-m6z7v
  Jun 10 13:37:28.112: INFO: Got endpoints: latency-svc-m6z7v [29.816733ms]
  Jun 10 13:37:28.117: INFO: Created: latency-svc-xqmpr
  Jun 10 13:37:28.127: INFO: Created: latency-svc-ltvz7
  Jun 10 13:37:28.130: INFO: Got endpoints: latency-svc-xqmpr [45.879921ms]
  Jun 10 13:37:28.138: INFO: Got endpoints: latency-svc-ltvz7 [54.365584ms]
  Jun 10 13:37:28.148: INFO: Created: latency-svc-znl8q
  Jun 10 13:37:28.151: INFO: Got endpoints: latency-svc-znl8q [68.22732ms]
  Jun 10 13:37:28.155: INFO: Created: latency-svc-qf9wm
  Jun 10 13:37:28.167: INFO: Got endpoints: latency-svc-qf9wm [83.569091ms]
  Jun 10 13:37:28.169: INFO: Created: latency-svc-vpp7s
  Jun 10 13:37:28.174: INFO: Created: latency-svc-h6hwc
  Jun 10 13:37:28.176: INFO: Got endpoints: latency-svc-vpp7s [92.580139ms]
  Jun 10 13:37:28.181: INFO: Got endpoints: latency-svc-h6hwc [97.009663ms]
  Jun 10 13:37:28.187: INFO: Created: latency-svc-b4vxk
  Jun 10 13:37:28.193: INFO: Created: latency-svc-kxxsk
  Jun 10 13:37:28.197: INFO: Got endpoints: latency-svc-b4vxk [114.523914ms]
  Jun 10 13:37:28.205: INFO: Got endpoints: latency-svc-kxxsk [121.277481ms]
  Jun 10 13:37:28.205: INFO: Created: latency-svc-29xjz
  Jun 10 13:37:28.214: INFO: Got endpoints: latency-svc-29xjz [131.635772ms]
  Jun 10 13:37:28.218: INFO: Created: latency-svc-x8svr
  Jun 10 13:37:28.226: INFO: Got endpoints: latency-svc-x8svr [142.456329ms]
  Jun 10 13:37:28.303: INFO: Created: latency-svc-8b6lm
  Jun 10 13:37:28.306: INFO: Created: latency-svc-dvj5w
  Jun 10 13:37:28.307: INFO: Created: latency-svc-s4g9f
  Jun 10 13:37:28.308: INFO: Created: latency-svc-5cw4v
  Jun 10 13:37:28.319: INFO: Created: latency-svc-n2s7n
  Jun 10 13:37:28.320: INFO: Created: latency-svc-7fpvl
  Jun 10 13:37:28.320: INFO: Created: latency-svc-892k7
  Jun 10 13:37:28.320: INFO: Created: latency-svc-rktvt
  Jun 10 13:37:28.321: INFO: Created: latency-svc-fk2sf
  Jun 10 13:37:28.321: INFO: Created: latency-svc-899qc
  Jun 10 13:37:28.321: INFO: Created: latency-svc-d79wx
  Jun 10 13:37:28.321: INFO: Created: latency-svc-h7p7g
  Jun 10 13:37:28.321: INFO: Created: latency-svc-4jdl5
  Jun 10 13:37:28.323: INFO: Created: latency-svc-7bthc
  Jun 10 13:37:28.323: INFO: Created: latency-svc-dph92
  Jun 10 13:37:28.330: INFO: Got endpoints: latency-svc-5cw4v [103.733628ms]
  Jun 10 13:37:28.330: INFO: Got endpoints: latency-svc-8b6lm [246.692612ms]
  Jun 10 13:37:28.340: INFO: Got endpoints: latency-svc-s4g9f [201.906972ms]
  Jun 10 13:37:28.352: INFO: Got endpoints: latency-svc-dph92 [201.621099ms]
  Jun 10 13:37:28.354: INFO: Got endpoints: latency-svc-7bthc [270.503235ms]
  Jun 10 13:37:28.370: INFO: Created: latency-svc-dgjw4
  Jun 10 13:37:28.378: INFO: Got endpoints: latency-svc-dvj5w [180.871085ms]
  Jun 10 13:37:28.378: INFO: Got endpoints: latency-svc-n2s7n [294.259188ms]
  Jun 10 13:37:28.379: INFO: Got endpoints: latency-svc-4jdl5 [211.602376ms]
  Jun 10 13:37:28.397: INFO: Created: latency-svc-8n8kw
  Jun 10 13:37:28.397: INFO: Got endpoints: latency-svc-fk2sf [284.660084ms]
  Jun 10 13:37:28.397: INFO: Got endpoints: latency-svc-7fpvl [182.29671ms]
  Jun 10 13:37:28.398: INFO: Got endpoints: latency-svc-h7p7g [221.535954ms]
  Jun 10 13:37:28.401: INFO: Got endpoints: latency-svc-899qc [220.818717ms]
  Jun 10 13:37:28.402: INFO: Got endpoints: latency-svc-892k7 [196.056014ms]
  Jun 10 13:37:28.415: INFO: Got endpoints: latency-svc-d79wx [331.626715ms]
  Jun 10 13:37:28.416: INFO: Created: latency-svc-tnns4
  Jun 10 13:37:28.420: INFO: Got endpoints: latency-svc-rktvt [290.142448ms]
  Jun 10 13:37:28.428: INFO: Got endpoints: latency-svc-dgjw4 [98.077153ms]
  Jun 10 13:37:28.429: INFO: Got endpoints: latency-svc-tnns4 [88.365207ms]
  Jun 10 13:37:28.429: INFO: Got endpoints: latency-svc-8n8kw [98.143573ms]
  Jun 10 13:37:28.499: INFO: Created: latency-svc-m2nnm
  Jun 10 13:37:28.500: INFO: Created: latency-svc-g97nt
  Jun 10 13:37:28.502: INFO: Created: latency-svc-f266n
  Jun 10 13:37:28.502: INFO: Created: latency-svc-nr57j
  Jun 10 13:37:28.502: INFO: Created: latency-svc-p72wf
  Jun 10 13:37:28.508: INFO: Created: latency-svc-tvxm6
  Jun 10 13:37:28.508: INFO: Created: latency-svc-k5kmj
  Jun 10 13:37:28.509: INFO: Created: latency-svc-9v5lx
  Jun 10 13:37:28.509: INFO: Created: latency-svc-p9l9q
  Jun 10 13:37:28.511: INFO: Created: latency-svc-7jxvt
  Jun 10 13:37:28.511: INFO: Created: latency-svc-jxbhv
  Jun 10 13:37:28.513: INFO: Created: latency-svc-48mbn
  Jun 10 13:37:28.513: INFO: Created: latency-svc-2j5vl
  Jun 10 13:37:28.513: INFO: Created: latency-svc-67w2m
  Jun 10 13:37:28.513: INFO: Created: latency-svc-f5txs
  Jun 10 13:37:28.539: INFO: Got endpoints: latency-svc-g97nt [137.61106ms]
  Jun 10 13:37:28.539: INFO: Got endpoints: latency-svc-m2nnm [141.514249ms]
  Jun 10 13:37:28.550: INFO: Got endpoints: latency-svc-9v5lx [121.577013ms]
  Jun 10 13:37:28.551: INFO: Got endpoints: latency-svc-7jxvt [148.970412ms]
  E0610 13:37:28.558588      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:37:28.567: INFO: Got endpoints: latency-svc-nr57j [212.351344ms]
  Jun 10 13:37:28.571: INFO: Got endpoints: latency-svc-f266n [174.351901ms]
  Jun 10 13:37:28.572: INFO: Got endpoints: latency-svc-jxbhv [174.102798ms]
  Jun 10 13:37:28.572: INFO: Got endpoints: latency-svc-p72wf [192.872404ms]
  Jun 10 13:37:28.574: INFO: Created: latency-svc-4n44k
  Jun 10 13:37:28.577: INFO: Got endpoints: latency-svc-48mbn [157.327064ms]
  Jun 10 13:37:28.585: INFO: Got endpoints: latency-svc-k5kmj [206.726939ms]
  Jun 10 13:37:28.587: INFO: Created: latency-svc-57hpf
  Jun 10 13:37:28.595: INFO: Created: latency-svc-7nmqs
  Jun 10 13:37:28.601: INFO: Created: latency-svc-k6r5x
  Jun 10 13:37:28.606: INFO: Created: latency-svc-tvvsp
  Jun 10 13:37:28.613: INFO: Created: latency-svc-vvsxs
  Jun 10 13:37:28.619: INFO: Created: latency-svc-mwp7s
  Jun 10 13:37:28.627: INFO: Created: latency-svc-zw6sh
  Jun 10 13:37:28.639: INFO: Got endpoints: latency-svc-67w2m [224.22692ms]
  Jun 10 13:37:28.643: INFO: Created: latency-svc-b6h2s
  Jun 10 13:37:28.647: INFO: Created: latency-svc-rzpwf
  Jun 10 13:37:28.653: INFO: Created: latency-svc-j9ws7
  Jun 10 13:37:28.677: INFO: Got endpoints: latency-svc-2j5vl [324.265173ms]
  Jun 10 13:37:28.688: INFO: Created: latency-svc-gb82g
  Jun 10 13:37:28.729: INFO: Got endpoints: latency-svc-tvxm6 [300.768201ms]
  Jun 10 13:37:28.742: INFO: Created: latency-svc-pjnl2
  Jun 10 13:37:28.778: INFO: Got endpoints: latency-svc-f5txs [349.593841ms]
  Jun 10 13:37:28.790: INFO: Created: latency-svc-7tlx8
  Jun 10 13:37:28.828: INFO: Got endpoints: latency-svc-p9l9q [449.850015ms]
  Jun 10 13:37:28.842: INFO: Created: latency-svc-qrr5r
  Jun 10 13:37:28.877: INFO: Got endpoints: latency-svc-4n44k [336.732495ms]
  Jun 10 13:37:28.889: INFO: Created: latency-svc-jz2nr
  Jun 10 13:37:28.927: INFO: Got endpoints: latency-svc-57hpf [386.850006ms]
  Jun 10 13:37:28.940: INFO: Created: latency-svc-fcgz5
  Jun 10 13:37:28.977: INFO: Got endpoints: latency-svc-7nmqs [426.85017ms]
  Jun 10 13:37:28.990: INFO: Created: latency-svc-8lbpq
  Jun 10 13:37:29.029: INFO: Got endpoints: latency-svc-k6r5x [478.659517ms]
  Jun 10 13:37:29.042: INFO: Created: latency-svc-kmcwg
  Jun 10 13:37:29.077: INFO: Got endpoints: latency-svc-tvvsp [510.302208ms]
  Jun 10 13:37:29.095: INFO: Created: latency-svc-v5279
  Jun 10 13:37:29.127: INFO: Got endpoints: latency-svc-vvsxs [555.024448ms]
  Jun 10 13:37:29.139: INFO: Created: latency-svc-zrdnw
  Jun 10 13:37:29.177: INFO: Got endpoints: latency-svc-mwp7s [604.462203ms]
  Jun 10 13:37:29.188: INFO: Created: latency-svc-ww4qq
  Jun 10 13:37:29.231: INFO: Got endpoints: latency-svc-zw6sh [658.612794ms]
  Jun 10 13:37:29.243: INFO: Created: latency-svc-8vf7l
  Jun 10 13:37:29.278: INFO: Got endpoints: latency-svc-b6h2s [700.382634ms]
  Jun 10 13:37:29.289: INFO: Created: latency-svc-8q822
  Jun 10 13:37:29.328: INFO: Got endpoints: latency-svc-rzpwf [741.993742ms]
  Jun 10 13:37:29.342: INFO: Created: latency-svc-dwb8q
  Jun 10 13:37:29.377: INFO: Got endpoints: latency-svc-j9ws7 [738.043423ms]
  Jun 10 13:37:29.392: INFO: Created: latency-svc-fdsbr
  Jun 10 13:37:29.428: INFO: Got endpoints: latency-svc-gb82g [751.272683ms]
  Jun 10 13:37:29.443: INFO: Created: latency-svc-grds2
  Jun 10 13:37:29.478: INFO: Got endpoints: latency-svc-pjnl2 [748.570887ms]
  Jun 10 13:37:29.492: INFO: Created: latency-svc-48lg2
  Jun 10 13:37:29.528: INFO: Got endpoints: latency-svc-7tlx8 [750.279154ms]
  Jun 10 13:37:29.540: INFO: Created: latency-svc-qvlfp
  E0610 13:37:29.558616      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:37:29.578: INFO: Got endpoints: latency-svc-qrr5r [749.557897ms]
  Jun 10 13:37:29.589: INFO: Created: latency-svc-hhzgc
  Jun 10 13:37:29.628: INFO: Got endpoints: latency-svc-jz2nr [751.022671ms]
  Jun 10 13:37:29.641: INFO: Created: latency-svc-25mpb
  Jun 10 13:37:29.679: INFO: Got endpoints: latency-svc-fcgz5 [751.084852ms]
  Jun 10 13:37:29.691: INFO: Created: latency-svc-ldtk7
  Jun 10 13:37:29.728: INFO: Got endpoints: latency-svc-8lbpq [750.658717ms]
  Jun 10 13:37:29.741: INFO: Created: latency-svc-45jrl
  Jun 10 13:37:29.777: INFO: Got endpoints: latency-svc-kmcwg [747.554957ms]
  Jun 10 13:37:29.792: INFO: Created: latency-svc-t6ns8
  Jun 10 13:37:29.828: INFO: Got endpoints: latency-svc-v5279 [750.398665ms]
  Jun 10 13:37:29.840: INFO: Created: latency-svc-26kr5
  Jun 10 13:37:29.877: INFO: Got endpoints: latency-svc-zrdnw [750.526707ms]
  Jun 10 13:37:29.890: INFO: Created: latency-svc-rjlhz
  Jun 10 13:37:29.929: INFO: Got endpoints: latency-svc-ww4qq [752.144732ms]
  Jun 10 13:37:29.943: INFO: Created: latency-svc-65hfx
  Jun 10 13:37:29.979: INFO: Got endpoints: latency-svc-8vf7l [747.993112ms]
  Jun 10 13:37:29.991: INFO: Created: latency-svc-4jqc4
  Jun 10 13:37:30.028: INFO: Got endpoints: latency-svc-8q822 [749.750719ms]
  Jun 10 13:37:30.042: INFO: Created: latency-svc-gs2vx
  Jun 10 13:37:30.076: INFO: Got endpoints: latency-svc-dwb8q [748.483026ms]
  Jun 10 13:37:30.093: INFO: Created: latency-svc-jgnn9
  Jun 10 13:37:30.129: INFO: Got endpoints: latency-svc-fdsbr [751.227283ms]
  Jun 10 13:37:30.141: INFO: Created: latency-svc-nsrsw
  Jun 10 13:37:30.178: INFO: Got endpoints: latency-svc-grds2 [749.474625ms]
  Jun 10 13:37:30.190: INFO: Created: latency-svc-75ghh
  Jun 10 13:37:30.228: INFO: Got endpoints: latency-svc-48lg2 [749.782808ms]
  Jun 10 13:37:30.240: INFO: Created: latency-svc-xdb4f
  Jun 10 13:37:30.278: INFO: Got endpoints: latency-svc-qvlfp [749.182463ms]
  Jun 10 13:37:30.292: INFO: Created: latency-svc-rn59t
  Jun 10 13:37:30.328: INFO: Got endpoints: latency-svc-hhzgc [750.234003ms]
  Jun 10 13:37:30.340: INFO: Created: latency-svc-4khqd
  Jun 10 13:37:30.377: INFO: Got endpoints: latency-svc-25mpb [748.96336ms]
  Jun 10 13:37:30.392: INFO: Created: latency-svc-w8qlq
  Jun 10 13:37:30.429: INFO: Got endpoints: latency-svc-ldtk7 [749.96511ms]
  Jun 10 13:37:30.441: INFO: Created: latency-svc-r8t8k
  Jun 10 13:37:30.481: INFO: Got endpoints: latency-svc-45jrl [752.94848ms]
  Jun 10 13:37:30.493: INFO: Created: latency-svc-98n6r
  Jun 10 13:37:30.528: INFO: Got endpoints: latency-svc-t6ns8 [750.270154ms]
  Jun 10 13:37:30.540: INFO: Created: latency-svc-ngzdq
  E0610 13:37:30.559467      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:37:30.576: INFO: Got endpoints: latency-svc-26kr5 [747.84899ms]
  Jun 10 13:37:30.589: INFO: Created: latency-svc-vksp7
  Jun 10 13:37:30.627: INFO: Got endpoints: latency-svc-rjlhz [749.813269ms]
  Jun 10 13:37:30.650: INFO: Created: latency-svc-b4ml9
  Jun 10 13:37:30.677: INFO: Got endpoints: latency-svc-65hfx [748.458275ms]
  Jun 10 13:37:30.691: INFO: Created: latency-svc-xpxgf
  Jun 10 13:37:30.728: INFO: Got endpoints: latency-svc-4jqc4 [748.88187ms]
  Jun 10 13:37:30.739: INFO: Created: latency-svc-dgqhx
  Jun 10 13:37:30.778: INFO: Got endpoints: latency-svc-gs2vx [749.810169ms]
  Jun 10 13:37:30.789: INFO: Created: latency-svc-j7j9t
  Jun 10 13:37:30.827: INFO: Got endpoints: latency-svc-jgnn9 [749.9207ms]
  Jun 10 13:37:30.839: INFO: Created: latency-svc-27757
  Jun 10 13:37:30.879: INFO: Got endpoints: latency-svc-nsrsw [749.742339ms]
  Jun 10 13:37:30.890: INFO: Created: latency-svc-ngw46
  Jun 10 13:37:30.927: INFO: Got endpoints: latency-svc-75ghh [749.490936ms]
  Jun 10 13:37:30.942: INFO: Created: latency-svc-l7hk4
  Jun 10 13:37:30.978: INFO: Got endpoints: latency-svc-xdb4f [749.743298ms]
  Jun 10 13:37:30.993: INFO: Created: latency-svc-h5qv5
  Jun 10 13:37:31.029: INFO: Got endpoints: latency-svc-rn59t [750.706568ms]
  Jun 10 13:37:31.041: INFO: Created: latency-svc-xtqts
  Jun 10 13:37:31.079: INFO: Got endpoints: latency-svc-4khqd [750.692717ms]
  Jun 10 13:37:31.093: INFO: Created: latency-svc-79snz
  Jun 10 13:37:31.130: INFO: Got endpoints: latency-svc-w8qlq [752.847949ms]
  Jun 10 13:37:31.144: INFO: Created: latency-svc-c4sxd
  Jun 10 13:37:31.179: INFO: Got endpoints: latency-svc-r8t8k [750.205953ms]
  Jun 10 13:37:31.194: INFO: Created: latency-svc-cdvmd
  Jun 10 13:37:31.229: INFO: Got endpoints: latency-svc-98n6r [747.737918ms]
  Jun 10 13:37:31.241: INFO: Created: latency-svc-5qtz6
  Jun 10 13:37:31.277: INFO: Got endpoints: latency-svc-ngzdq [749.685718ms]
  Jun 10 13:37:31.289: INFO: Created: latency-svc-mbnd7
  Jun 10 13:37:31.327: INFO: Got endpoints: latency-svc-vksp7 [750.94106ms]
  Jun 10 13:37:31.341: INFO: Created: latency-svc-2pfg4
  Jun 10 13:37:31.376: INFO: Got endpoints: latency-svc-b4ml9 [748.201983ms]
  Jun 10 13:37:31.387: INFO: Created: latency-svc-pr268
  Jun 10 13:37:31.427: INFO: Got endpoints: latency-svc-xpxgf [748.141693ms]
  Jun 10 13:37:31.449: INFO: Created: latency-svc-vslq7
  Jun 10 13:37:31.477: INFO: Got endpoints: latency-svc-dgqhx [748.701678ms]
  Jun 10 13:37:31.490: INFO: Created: latency-svc-4cr52
  Jun 10 13:37:31.531: INFO: Got endpoints: latency-svc-j7j9t [752.526626ms]
  Jun 10 13:37:31.545: INFO: Created: latency-svc-dn5xv
  E0610 13:37:31.560524      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:37:31.578: INFO: Got endpoints: latency-svc-27757 [750.656277ms]
  Jun 10 13:37:31.589: INFO: Created: latency-svc-x5qm5
  Jun 10 13:37:31.626: INFO: Got endpoints: latency-svc-ngw46 [746.995852ms]
  Jun 10 13:37:31.639: INFO: Created: latency-svc-82rxn
  Jun 10 13:37:31.678: INFO: Got endpoints: latency-svc-l7hk4 [749.98326ms]
  Jun 10 13:37:31.689: INFO: Created: latency-svc-l7gwk
  Jun 10 13:37:31.729: INFO: Got endpoints: latency-svc-h5qv5 [750.86174ms]
  Jun 10 13:37:31.742: INFO: Created: latency-svc-mgbrb
  Jun 10 13:37:31.787: INFO: Got endpoints: latency-svc-xtqts [757.577055ms]
  Jun 10 13:37:31.806: INFO: Created: latency-svc-hrczw
  Jun 10 13:37:31.826: INFO: Got endpoints: latency-svc-79snz [746.131203ms]
  Jun 10 13:37:31.838: INFO: Created: latency-svc-d624j
  Jun 10 13:37:31.877: INFO: Got endpoints: latency-svc-c4sxd [746.355565ms]
  Jun 10 13:37:31.890: INFO: Created: latency-svc-m4hkz
  Jun 10 13:37:31.928: INFO: Got endpoints: latency-svc-cdvmd [749.331295ms]
  Jun 10 13:37:31.943: INFO: Created: latency-svc-xdw6l
  Jun 10 13:37:31.979: INFO: Got endpoints: latency-svc-5qtz6 [749.442065ms]
  Jun 10 13:37:31.992: INFO: Created: latency-svc-4qd4l
  Jun 10 13:37:32.029: INFO: Got endpoints: latency-svc-mbnd7 [752.116702ms]
  Jun 10 13:37:32.042: INFO: Created: latency-svc-lkl5z
  Jun 10 13:37:32.076: INFO: Got endpoints: latency-svc-2pfg4 [748.467656ms]
  Jun 10 13:37:32.091: INFO: Created: latency-svc-c6cz2
  Jun 10 13:37:32.128: INFO: Got endpoints: latency-svc-pr268 [752.235572ms]
  Jun 10 13:37:32.140: INFO: Created: latency-svc-mgq7h
  Jun 10 13:37:32.180: INFO: Got endpoints: latency-svc-vslq7 [753.483945ms]
  Jun 10 13:37:32.193: INFO: Created: latency-svc-vwjfq
  Jun 10 13:37:32.227: INFO: Got endpoints: latency-svc-4cr52 [750.073572ms]
  Jun 10 13:37:32.240: INFO: Created: latency-svc-qjfrr
  Jun 10 13:37:32.279: INFO: Got endpoints: latency-svc-dn5xv [748.542596ms]
  Jun 10 13:37:32.291: INFO: Created: latency-svc-zs8w6
  Jun 10 13:37:32.328: INFO: Got endpoints: latency-svc-x5qm5 [750.348704ms]
  Jun 10 13:37:32.345: INFO: Created: latency-svc-j994q
  Jun 10 13:37:32.378: INFO: Got endpoints: latency-svc-82rxn [751.652047ms]
  Jun 10 13:37:32.393: INFO: Created: latency-svc-5gndr
  Jun 10 13:37:32.428: INFO: Got endpoints: latency-svc-l7gwk [750.448935ms]
  Jun 10 13:37:32.440: INFO: Created: latency-svc-8bncv
  Jun 10 13:37:32.480: INFO: Got endpoints: latency-svc-mgbrb [751.509566ms]
  Jun 10 13:37:32.494: INFO: Created: latency-svc-k9fqk
  Jun 10 13:37:32.528: INFO: Got endpoints: latency-svc-hrczw [740.020783ms]
  Jun 10 13:37:32.541: INFO: Created: latency-svc-2wxqv
  E0610 13:37:32.561502      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:37:32.577: INFO: Got endpoints: latency-svc-d624j [751.715298ms]
  Jun 10 13:37:32.664: INFO: Got endpoints: latency-svc-m4hkz [786.909433ms]
  Jun 10 13:37:32.668: INFO: Created: latency-svc-cljn5
  Jun 10 13:37:32.678: INFO: Got endpoints: latency-svc-xdw6l [749.690538ms]
  Jun 10 13:37:32.684: INFO: Created: latency-svc-8jcd9
  Jun 10 13:37:32.691: INFO: Created: latency-svc-dn75b
  Jun 10 13:37:32.729: INFO: Got endpoints: latency-svc-4qd4l [750.142272ms]
  Jun 10 13:37:32.740: INFO: Created: latency-svc-rkj98
  Jun 10 13:37:32.776: INFO: Got endpoints: latency-svc-lkl5z [746.723289ms]
  Jun 10 13:37:32.790: INFO: Created: latency-svc-6mqx4
  Jun 10 13:37:32.827: INFO: Got endpoints: latency-svc-c6cz2 [750.862309ms]
  Jun 10 13:37:32.842: INFO: Created: latency-svc-h9w8n
  Jun 10 13:37:32.879: INFO: Got endpoints: latency-svc-mgq7h [750.494776ms]
  Jun 10 13:37:32.892: INFO: Created: latency-svc-trpft
  Jun 10 13:37:32.928: INFO: Got endpoints: latency-svc-vwjfq [747.374345ms]
  Jun 10 13:37:32.940: INFO: Created: latency-svc-nc8pw
  Jun 10 13:37:32.976: INFO: Got endpoints: latency-svc-qjfrr [749.035191ms]
  Jun 10 13:37:32.989: INFO: Created: latency-svc-j6cbj
  Jun 10 13:37:33.026: INFO: Got endpoints: latency-svc-zs8w6 [746.94013ms]
  Jun 10 13:37:33.038: INFO: Created: latency-svc-9tk7l
  Jun 10 13:37:33.078: INFO: Got endpoints: latency-svc-j994q [749.761038ms]
  Jun 10 13:37:33.093: INFO: Created: latency-svc-5fsjc
  Jun 10 13:37:33.129: INFO: Got endpoints: latency-svc-5gndr [751.280564ms]
  Jun 10 13:37:33.141: INFO: Created: latency-svc-7fzl9
  Jun 10 13:37:33.179: INFO: Got endpoints: latency-svc-8bncv [750.196454ms]
  Jun 10 13:37:33.189: INFO: Created: latency-svc-mm4tg
  Jun 10 13:37:33.228: INFO: Got endpoints: latency-svc-k9fqk [747.291064ms]
  Jun 10 13:37:33.240: INFO: Created: latency-svc-w9jrr
  Jun 10 13:37:33.276: INFO: Got endpoints: latency-svc-2wxqv [748.355755ms]
  Jun 10 13:37:33.290: INFO: Created: latency-svc-6wcdv
  Jun 10 13:37:33.328: INFO: Got endpoints: latency-svc-cljn5 [750.764169ms]
  Jun 10 13:37:33.341: INFO: Created: latency-svc-nmcj9
  Jun 10 13:37:33.377: INFO: Got endpoints: latency-svc-8jcd9 [712.856826ms]
  Jun 10 13:37:33.388: INFO: Created: latency-svc-fjmj2
  Jun 10 13:37:33.428: INFO: Got endpoints: latency-svc-dn75b [749.569386ms]
  Jun 10 13:37:33.440: INFO: Created: latency-svc-8krmc
  Jun 10 13:37:33.479: INFO: Got endpoints: latency-svc-rkj98 [749.685818ms]
  Jun 10 13:37:33.492: INFO: Created: latency-svc-9l4mk
  Jun 10 13:37:33.529: INFO: Got endpoints: latency-svc-6mqx4 [753.145381ms]
  Jun 10 13:37:33.541: INFO: Created: latency-svc-p98mn
  E0610 13:37:33.561746      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:37:33.577: INFO: Got endpoints: latency-svc-h9w8n [749.507585ms]
  Jun 10 13:37:33.589: INFO: Created: latency-svc-ns8zq
  Jun 10 13:37:33.627: INFO: Got endpoints: latency-svc-trpft [748.536876ms]
  Jun 10 13:37:33.644: INFO: Created: latency-svc-svdjv
  Jun 10 13:37:33.679: INFO: Got endpoints: latency-svc-nc8pw [750.345554ms]
  Jun 10 13:37:33.691: INFO: Created: latency-svc-mzs6f
  Jun 10 13:37:33.728: INFO: Got endpoints: latency-svc-j6cbj [751.155172ms]
  Jun 10 13:37:33.739: INFO: Created: latency-svc-d22pw
  Jun 10 13:37:33.775: INFO: Got endpoints: latency-svc-9tk7l [748.779879ms]
  Jun 10 13:37:33.791: INFO: Created: latency-svc-2lbqp
  Jun 10 13:37:33.826: INFO: Got endpoints: latency-svc-5fsjc [747.737779ms]
  Jun 10 13:37:33.838: INFO: Created: latency-svc-jhvgs
  Jun 10 13:37:33.877: INFO: Got endpoints: latency-svc-7fzl9 [747.241373ms]
  Jun 10 13:37:33.891: INFO: Created: latency-svc-gc4j4
  Jun 10 13:37:33.930: INFO: Got endpoints: latency-svc-mm4tg [751.493695ms]
  Jun 10 13:37:33.942: INFO: Created: latency-svc-hxpmm
  Jun 10 13:37:33.978: INFO: Got endpoints: latency-svc-w9jrr [750.519796ms]
  Jun 10 13:37:33.991: INFO: Created: latency-svc-pg9lh
  Jun 10 13:37:34.027: INFO: Got endpoints: latency-svc-6wcdv [750.420425ms]
  Jun 10 13:37:34.039: INFO: Created: latency-svc-blsz8
  Jun 10 13:37:34.079: INFO: Got endpoints: latency-svc-nmcj9 [750.481956ms]
  Jun 10 13:37:34.093: INFO: Created: latency-svc-498xf
  Jun 10 13:37:34.130: INFO: Got endpoints: latency-svc-fjmj2 [752.798369ms]
  Jun 10 13:37:34.143: INFO: Created: latency-svc-xmtfr
  Jun 10 13:37:34.177: INFO: Got endpoints: latency-svc-8krmc [748.674858ms]
  Jun 10 13:37:34.188: INFO: Created: latency-svc-c5b6k
  Jun 10 13:37:34.229: INFO: Got endpoints: latency-svc-9l4mk [749.738648ms]
  Jun 10 13:37:34.242: INFO: Created: latency-svc-ppqkd
  Jun 10 13:37:34.279: INFO: Got endpoints: latency-svc-p98mn [749.464095ms]
  Jun 10 13:37:34.293: INFO: Created: latency-svc-l4mgs
  Jun 10 13:37:34.329: INFO: Got endpoints: latency-svc-ns8zq [751.443945ms]
  Jun 10 13:37:34.343: INFO: Created: latency-svc-lszdk
  Jun 10 13:37:34.376: INFO: Got endpoints: latency-svc-svdjv [748.043451ms]
  Jun 10 13:37:34.391: INFO: Created: latency-svc-6774j
  Jun 10 13:37:34.428: INFO: Got endpoints: latency-svc-mzs6f [748.90452ms]
  Jun 10 13:37:34.439: INFO: Created: latency-svc-lx48n
  Jun 10 13:37:34.479: INFO: Got endpoints: latency-svc-d22pw [750.732738ms]
  Jun 10 13:37:34.491: INFO: Created: latency-svc-s76z7
  Jun 10 13:37:34.527: INFO: Got endpoints: latency-svc-2lbqp [751.015341ms]
  Jun 10 13:37:34.542: INFO: Created: latency-svc-6r5sk
  E0610 13:37:34.562316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:37:34.577: INFO: Got endpoints: latency-svc-jhvgs [750.91061ms]
  Jun 10 13:37:34.590: INFO: Created: latency-svc-xs6dw
  Jun 10 13:37:34.627: INFO: Got endpoints: latency-svc-gc4j4 [750.403666ms]
  Jun 10 13:37:34.642: INFO: Created: latency-svc-4dv58
  Jun 10 13:37:34.680: INFO: Got endpoints: latency-svc-hxpmm [749.207613ms]
  Jun 10 13:37:34.693: INFO: Created: latency-svc-hwt2m
  Jun 10 13:37:34.729: INFO: Got endpoints: latency-svc-pg9lh [750.155843ms]
  Jun 10 13:37:34.741: INFO: Created: latency-svc-576kj
  Jun 10 13:37:34.777: INFO: Got endpoints: latency-svc-blsz8 [750.114312ms]
  Jun 10 13:37:34.791: INFO: Created: latency-svc-tckm2
  Jun 10 13:37:34.827: INFO: Got endpoints: latency-svc-498xf [747.171444ms]
  Jun 10 13:37:34.839: INFO: Created: latency-svc-wrpx5
  Jun 10 13:37:34.878: INFO: Got endpoints: latency-svc-xmtfr [747.364225ms]
  Jun 10 13:37:34.891: INFO: Created: latency-svc-92hsh
  Jun 10 13:37:34.928: INFO: Got endpoints: latency-svc-c5b6k [751.569556ms]
  Jun 10 13:37:34.945: INFO: Created: latency-svc-qjv6f
  Jun 10 13:37:34.979: INFO: Got endpoints: latency-svc-ppqkd [750.251053ms]
  Jun 10 13:37:34.991: INFO: Created: latency-svc-jbj5d
  Jun 10 13:37:35.027: INFO: Got endpoints: latency-svc-l4mgs [748.037531ms]
  Jun 10 13:37:35.039: INFO: Created: latency-svc-6dpxt
  Jun 10 13:37:35.079: INFO: Got endpoints: latency-svc-lszdk [749.99256ms]
  Jun 10 13:37:35.101: INFO: Created: latency-svc-j6hdt
  Jun 10 13:37:35.129: INFO: Got endpoints: latency-svc-6774j [753.492735ms]
  Jun 10 13:37:35.145: INFO: Created: latency-svc-w2rth
  Jun 10 13:37:35.179: INFO: Got endpoints: latency-svc-lx48n [751.078801ms]
  Jun 10 13:37:35.192: INFO: Created: latency-svc-z6984
  Jun 10 13:37:35.228: INFO: Got endpoints: latency-svc-s76z7 [748.496176ms]
  Jun 10 13:37:35.244: INFO: Created: latency-svc-qj62w
  Jun 10 13:37:35.278: INFO: Got endpoints: latency-svc-6r5sk [751.498735ms]
  Jun 10 13:37:35.291: INFO: Created: latency-svc-7kkvs
  Jun 10 13:37:35.330: INFO: Got endpoints: latency-svc-xs6dw [752.687767ms]
  Jun 10 13:37:35.343: INFO: Created: latency-svc-zjfbv
  Jun 10 13:37:35.377: INFO: Got endpoints: latency-svc-4dv58 [749.007231ms]
  Jun 10 13:37:35.391: INFO: Created: latency-svc-7fj8w
  Jun 10 13:37:35.428: INFO: Got endpoints: latency-svc-hwt2m [747.996001ms]
  Jun 10 13:37:35.440: INFO: Created: latency-svc-9s6fc
  Jun 10 13:37:35.478: INFO: Got endpoints: latency-svc-576kj [748.97822ms]
  Jun 10 13:37:35.489: INFO: Created: latency-svc-p2754
  Jun 10 13:37:35.528: INFO: Got endpoints: latency-svc-tckm2 [751.243613ms]
  Jun 10 13:37:35.541: INFO: Created: latency-svc-j2wjn
  E0610 13:37:35.562698      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:37:35.577: INFO: Got endpoints: latency-svc-wrpx5 [749.891679ms]
  Jun 10 13:37:35.591: INFO: Created: latency-svc-qmcxl
  Jun 10 13:37:35.627: INFO: Got endpoints: latency-svc-92hsh [749.578696ms]
  Jun 10 13:37:35.641: INFO: Created: latency-svc-c9m4k
  Jun 10 13:37:35.676: INFO: Got endpoints: latency-svc-qjv6f [747.938461ms]
  Jun 10 13:37:35.691: INFO: Created: latency-svc-vp7lb
  Jun 10 13:37:35.728: INFO: Got endpoints: latency-svc-jbj5d [748.582286ms]
  Jun 10 13:37:35.740: INFO: Created: latency-svc-w8rjp
  Jun 10 13:37:35.777: INFO: Got endpoints: latency-svc-6dpxt [750.227403ms]
  Jun 10 13:37:35.790: INFO: Created: latency-svc-shgzk
  Jun 10 13:37:35.828: INFO: Got endpoints: latency-svc-j6hdt [748.985241ms]
  Jun 10 13:37:35.840: INFO: Created: latency-svc-nvqhh
  Jun 10 13:37:35.877: INFO: Got endpoints: latency-svc-w2rth [747.679158ms]
  Jun 10 13:37:35.889: INFO: Created: latency-svc-xl9mw
  Jun 10 13:37:35.928: INFO: Got endpoints: latency-svc-z6984 [749.231954ms]
  Jun 10 13:37:35.979: INFO: Got endpoints: latency-svc-qj62w [751.360464ms]
  Jun 10 13:37:36.029: INFO: Got endpoints: latency-svc-7kkvs [750.350114ms]
  Jun 10 13:37:36.080: INFO: Got endpoints: latency-svc-zjfbv [749.470926ms]
  Jun 10 13:37:36.129: INFO: Got endpoints: latency-svc-7fj8w [752.373894ms]
  Jun 10 13:37:36.180: INFO: Got endpoints: latency-svc-9s6fc [752.311674ms]
  Jun 10 13:37:36.228: INFO: Got endpoints: latency-svc-p2754 [749.92432ms]
  Jun 10 13:37:36.277: INFO: Got endpoints: latency-svc-j2wjn [748.83783ms]
  Jun 10 13:37:36.328: INFO: Got endpoints: latency-svc-qmcxl [750.673917ms]
  Jun 10 13:37:36.377: INFO: Got endpoints: latency-svc-c9m4k [749.018561ms]
  Jun 10 13:37:36.430: INFO: Got endpoints: latency-svc-vp7lb [753.837058ms]
  Jun 10 13:37:36.478: INFO: Got endpoints: latency-svc-w8rjp [749.783469ms]
  Jun 10 13:37:36.527: INFO: Got endpoints: latency-svc-shgzk [749.382334ms]
  E0610 13:37:36.563336      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:37:36.578: INFO: Got endpoints: latency-svc-nvqhh [749.189434ms]
  Jun 10 13:37:36.628: INFO: Got endpoints: latency-svc-xl9mw [750.783128ms]
  Jun 10 13:37:36.628: INFO: Latencies: [29.816733ms 45.879921ms 54.365584ms 68.22732ms 83.569091ms 88.365207ms 92.580139ms 97.009663ms 98.077153ms 98.143573ms 103.733628ms 114.523914ms 121.277481ms 121.577013ms 131.635772ms 137.61106ms 141.514249ms 142.456329ms 148.970412ms 157.327064ms 174.102798ms 174.351901ms 180.871085ms 182.29671ms 192.872404ms 196.056014ms 201.621099ms 201.906972ms 206.726939ms 211.602376ms 212.351344ms 220.818717ms 221.535954ms 224.22692ms 246.692612ms 270.503235ms 284.660084ms 290.142448ms 294.259188ms 300.768201ms 324.265173ms 331.626715ms 336.732495ms 349.593841ms 386.850006ms 426.85017ms 449.850015ms 478.659517ms 510.302208ms 555.024448ms 604.462203ms 658.612794ms 700.382634ms 712.856826ms 738.043423ms 740.020783ms 741.993742ms 746.131203ms 746.355565ms 746.723289ms 746.94013ms 746.995852ms 747.171444ms 747.241373ms 747.291064ms 747.364225ms 747.374345ms 747.554957ms 747.679158ms 747.737779ms 747.737918ms 747.84899ms 747.938461ms 747.993112ms 747.996001ms 748.037531ms 748.043451ms 748.141693ms 748.201983ms 748.355755ms 748.458275ms 748.467656ms 748.483026ms 748.496176ms 748.536876ms 748.542596ms 748.570887ms 748.582286ms 748.674858ms 748.701678ms 748.779879ms 748.83783ms 748.88187ms 748.90452ms 748.96336ms 748.97822ms 748.985241ms 749.007231ms 749.018561ms 749.035191ms 749.182463ms 749.189434ms 749.207613ms 749.231954ms 749.331295ms 749.382334ms 749.442065ms 749.464095ms 749.470926ms 749.474625ms 749.490936ms 749.507585ms 749.557897ms 749.569386ms 749.578696ms 749.685718ms 749.685818ms 749.690538ms 749.738648ms 749.742339ms 749.743298ms 749.750719ms 749.761038ms 749.782808ms 749.783469ms 749.810169ms 749.813269ms 749.891679ms 749.9207ms 749.92432ms 749.96511ms 749.98326ms 749.99256ms 750.073572ms 750.114312ms 750.142272ms 750.155843ms 750.196454ms 750.205953ms 750.227403ms 750.234003ms 750.251053ms 750.270154ms 750.279154ms 750.345554ms 750.348704ms 750.350114ms 750.398665ms 750.403666ms 750.420425ms 750.448935ms 750.481956ms 750.494776ms 750.519796ms 750.526707ms 750.656277ms 750.658717ms 750.673917ms 750.692717ms 750.706568ms 750.732738ms 750.764169ms 750.783128ms 750.86174ms 750.862309ms 750.91061ms 750.94106ms 751.015341ms 751.022671ms 751.078801ms 751.084852ms 751.155172ms 751.227283ms 751.243613ms 751.272683ms 751.280564ms 751.360464ms 751.443945ms 751.493695ms 751.498735ms 751.509566ms 751.569556ms 751.652047ms 751.715298ms 752.116702ms 752.144732ms 752.235572ms 752.311674ms 752.373894ms 752.526626ms 752.687767ms 752.798369ms 752.847949ms 752.94848ms 753.145381ms 753.483945ms 753.492735ms 753.837058ms 757.577055ms 786.909433ms]
  Jun 10 13:37:36.629: INFO: 50 %ile: 749.182463ms
  Jun 10 13:37:36.629: INFO: 90 %ile: 751.509566ms
  Jun 10 13:37:36.629: INFO: 99 %ile: 757.577055ms
  Jun 10 13:37:36.629: INFO: Total sample count: 200
  Jun 10 13:37:36.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-939" for this suite. @ 06/10/23 13:37:36.637
• [10.776 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 06/10/23 13:37:36.649
  Jun 10 13:37:36.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename secrets @ 06/10/23 13:37:36.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:37:36.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:37:36.672
  STEP: Creating projection with secret that has name secret-emptykey-test-f166959f-9ade-40e3-ac36-6aed96f147dc @ 06/10/23 13:37:36.676
  Jun 10 13:37:36.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-166" for this suite. @ 06/10/23 13:37:36.684
• [0.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 06/10/23 13:37:36.695
  Jun 10 13:37:36.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename dns @ 06/10/23 13:37:36.697
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:37:36.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:37:36.72
  STEP: Creating a test headless service @ 06/10/23 13:37:36.723
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9107.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9107.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 06/10/23 13:37:36.729
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9107.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9107.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 06/10/23 13:37:36.729
  STEP: creating a pod to probe DNS @ 06/10/23 13:37:36.729
  STEP: submitting the pod to kubernetes @ 06/10/23 13:37:36.729
  E0610 13:37:37.564070      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:38.564411      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/10/23 13:37:38.76
  STEP: looking for the results for each expected name from probers @ 06/10/23 13:37:38.764
  Jun 10 13:37:38.783: INFO: DNS probes using dns-9107/dns-test-a2f01813-6b7b-4e41-ac8d-99e04b874d93 succeeded

  Jun 10 13:37:38.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/10/23 13:37:38.788
  STEP: deleting the test headless service @ 06/10/23 13:37:38.803
  STEP: Destroying namespace "dns-9107" for this suite. @ 06/10/23 13:37:38.819
• [2.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 06/10/23 13:37:38.832
  Jun 10 13:37:38.832: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename replicaset @ 06/10/23 13:37:38.833
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:37:38.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:37:38.856
  Jun 10 13:37:38.875: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0610 13:37:39.564914      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:40.564595      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:41.564682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:42.564779      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:43.564890      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:37:43.881: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/10/23 13:37:43.881
  STEP: Scaling up "test-rs" replicaset  @ 06/10/23 13:37:43.881
  Jun 10 13:37:43.899: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 06/10/23 13:37:43.899
  W0610 13:37:43.914936      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jun 10 13:37:43.917: INFO: observed ReplicaSet test-rs in namespace replicaset-9725 with ReadyReplicas 1, AvailableReplicas 1
  Jun 10 13:37:43.944: INFO: observed ReplicaSet test-rs in namespace replicaset-9725 with ReadyReplicas 1, AvailableReplicas 1
  Jun 10 13:37:43.969: INFO: observed ReplicaSet test-rs in namespace replicaset-9725 with ReadyReplicas 1, AvailableReplicas 1
  Jun 10 13:37:43.979: INFO: observed ReplicaSet test-rs in namespace replicaset-9725 with ReadyReplicas 1, AvailableReplicas 1
  E0610 13:37:44.565512      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:45.565822      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:37:45.657: INFO: observed ReplicaSet test-rs in namespace replicaset-9725 with ReadyReplicas 2, AvailableReplicas 2
  Jun 10 13:37:45.700: INFO: observed Replicaset test-rs in namespace replicaset-9725 with ReadyReplicas 3 found true
  Jun 10 13:37:45.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9725" for this suite. @ 06/10/23 13:37:45.706
• [6.882 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 06/10/23 13:37:45.719
  Jun 10 13:37:45.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename watch @ 06/10/23 13:37:45.72
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:37:45.742
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:37:45.748
  STEP: creating a watch on configmaps with a certain label @ 06/10/23 13:37:45.756
  STEP: creating a new configmap @ 06/10/23 13:37:45.759
  STEP: modifying the configmap once @ 06/10/23 13:37:45.764
  STEP: changing the label value of the configmap @ 06/10/23 13:37:45.78
  STEP: Expecting to observe a delete notification for the watched object @ 06/10/23 13:37:45.792
  Jun 10 13:37:45.792: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2468  d7dddbf7-b08b-4cb9-a8a7-21869dcf4051 38521 0 2023-06-10 13:37:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-10 13:37:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 13:37:45.793: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2468  d7dddbf7-b08b-4cb9-a8a7-21869dcf4051 38524 0 2023-06-10 13:37:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-10 13:37:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 13:37:45.793: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2468  d7dddbf7-b08b-4cb9-a8a7-21869dcf4051 38527 0 2023-06-10 13:37:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-10 13:37:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 06/10/23 13:37:45.793
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 06/10/23 13:37:45.803
  E0610 13:37:46.565972      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:47.566427      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:48.566527      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:49.566648      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:50.566758      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:51.566895      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:52.567146      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:53.567971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:54.568126      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:55.568283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 06/10/23 13:37:55.804
  STEP: modifying the configmap a third time @ 06/10/23 13:37:55.814
  STEP: deleting the configmap @ 06/10/23 13:37:55.824
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 06/10/23 13:37:55.83
  Jun 10 13:37:55.830: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2468  d7dddbf7-b08b-4cb9-a8a7-21869dcf4051 38811 0 2023-06-10 13:37:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-10 13:37:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 13:37:55.831: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2468  d7dddbf7-b08b-4cb9-a8a7-21869dcf4051 38812 0 2023-06-10 13:37:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-10 13:37:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 13:37:55.831: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2468  d7dddbf7-b08b-4cb9-a8a7-21869dcf4051 38813 0 2023-06-10 13:37:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-10 13:37:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 10 13:37:55.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2468" for this suite. @ 06/10/23 13:37:55.837
• [10.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 06/10/23 13:37:55.851
  Jun 10 13:37:55.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename crd-webhook @ 06/10/23 13:37:55.853
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:37:55.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:37:55.876
  STEP: Setting up server cert @ 06/10/23 13:37:55.88
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 06/10/23 13:37:56.186
  STEP: Deploying the custom resource conversion webhook pod @ 06/10/23 13:37:56.195
  STEP: Wait for the deployment to be ready @ 06/10/23 13:37:56.209
  Jun 10 13:37:56.219: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0610 13:37:56.569291      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:37:57.569372      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/10/23 13:37:58.231
  STEP: Verifying the service has paired with the endpoint @ 06/10/23 13:37:58.25
  E0610 13:37:58.569725      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:37:59.250: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jun 10 13:37:59.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  E0610 13:37:59.571943      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:00.572996      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:01.573087      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 06/10/23 13:38:01.845
  STEP: v2 custom resource should be converted @ 06/10/23 13:38:01.85
  Jun 10 13:38:01.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-9575" for this suite. @ 06/10/23 13:38:02.452
• [6.615 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 06/10/23 13:38:02.47
  Jun 10 13:38:02.470: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename subjectreview @ 06/10/23 13:38:02.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:38:02.49
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:38:02.498
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-2918" @ 06/10/23 13:38:02.502
  Jun 10 13:38:02.507: INFO: saUsername: "system:serviceaccount:subjectreview-2918:e2e"
  Jun 10 13:38:02.507: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-2918"}
  Jun 10 13:38:02.507: INFO: saUID: "c6148f25-dd2c-46ba-8767-582dafec7843"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-2918:e2e" @ 06/10/23 13:38:02.507
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-2918:e2e" @ 06/10/23 13:38:02.508
  Jun 10 13:38:02.510: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-2918:e2e" api 'list' configmaps in "subjectreview-2918" namespace @ 06/10/23 13:38:02.51
  Jun 10 13:38:02.512: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-2918:e2e" @ 06/10/23 13:38:02.512
  Jun 10 13:38:02.515: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Jun 10 13:38:02.515: INFO: LocalSubjectAccessReview has been verified
  Jun 10 13:38:02.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-2918" for this suite. @ 06/10/23 13:38:02.524
• [0.064 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 06/10/23 13:38:02.535
  Jun 10 13:38:02.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename container-probe @ 06/10/23 13:38:02.537
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:38:02.554
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:38:02.559
  STEP: Creating pod liveness-826fffe5-2978-4033-8eac-f8b0d7f2937b in namespace container-probe-4843 @ 06/10/23 13:38:02.563
  E0610 13:38:02.573619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:03.573768      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:04.573896      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:38:04.584: INFO: Started pod liveness-826fffe5-2978-4033-8eac-f8b0d7f2937b in namespace container-probe-4843
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/10/23 13:38:04.584
  Jun 10 13:38:04.588: INFO: Initial restart count of pod liveness-826fffe5-2978-4033-8eac-f8b0d7f2937b is 0
  E0610 13:38:05.574034      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:06.574341      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:07.575048      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:08.575152      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:09.575382      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:10.575795      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:11.575893      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:12.576096      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:13.576329      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:14.576454      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:15.576572      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:16.576679      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:17.576828      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:18.576905      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:19.577005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:20.577122      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:21.577281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:22.577346      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:23.578112      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:24.578223      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:38:24.644: INFO: Restart count of pod container-probe-4843/liveness-826fffe5-2978-4033-8eac-f8b0d7f2937b is now 1 (20.056642414s elapsed)
  E0610 13:38:25.578341      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:26.578449      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:27.578553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:28.578994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:29.579118      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:30.579219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:31.579333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:32.579523      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:33.580081      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:34.580194      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:35.580505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:36.581581      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:37.582205      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:38.582660      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:39.582776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:40.583021      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:41.583135      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:42.583257      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:43.583363      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:44.583446      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:38:44.697: INFO: Restart count of pod container-probe-4843/liveness-826fffe5-2978-4033-8eac-f8b0d7f2937b is now 2 (40.109493749s elapsed)
  E0610 13:38:45.584104      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:46.584217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:47.584456      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:48.584911      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:49.585046      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:50.585158      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:51.586223      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:52.586375      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:53.586433      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:54.586959      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:55.587020      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:56.587133      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:57.587236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:58.587318      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:38:59.588117      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:00.588750      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:01.588826      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:02.588923      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:03.589082      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:04.589260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:39:04.746: INFO: Restart count of pod container-probe-4843/liveness-826fffe5-2978-4033-8eac-f8b0d7f2937b is now 3 (1m0.158792579s elapsed)
  E0610 13:39:05.589845      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:06.590274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:07.591029      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:08.591118      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:09.591259      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:10.591746      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:11.592035      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:12.592305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:13.592449      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:14.592562      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:15.592640      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:16.592708      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:17.592850      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:18.592930      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:19.593926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:20.594163      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:21.594236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:22.594336      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:23.594884      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:24.595235      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:39:24.796: INFO: Restart count of pod container-probe-4843/liveness-826fffe5-2978-4033-8eac-f8b0d7f2937b is now 4 (1m20.20815336s elapsed)
  E0610 13:39:25.596136      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:26.596195      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:27.596345      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:28.596402      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:29.596520      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:30.596625      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:31.596742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:32.596854      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:33.597189      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:34.597303      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:35.597409      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:36.597530      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:37.597645      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:38.597746      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:39.597872      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:40.599135      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:41.599200      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:42.599300      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:43.599342      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:44.599445      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:45.600092      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:46.600181      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:47.600506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:48.600885      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:49.601474      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:50.601686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:51.602114      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:52.602433      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:53.603514      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:54.603618      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:55.604125      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:56.604499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:57.604604      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:58.605038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:39:59.606031      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:00.606990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:01.608072      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:02.608251      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:03.608675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:04.608980      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:05.609980      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:06.610086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:07.610580      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:08.611016      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:09.612086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:10.612203      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:11.612892      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:12.613000      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:13.614033      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:14.614359      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:15.614525      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:16.614702      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:17.614980      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:18.615098      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:19.616127      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:20.616237      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:21.616333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:22.616708      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:23.616988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:24.617450      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:25.618443      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:26.619025      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:40:26.964: INFO: Restart count of pod container-probe-4843/liveness-826fffe5-2978-4033-8eac-f8b0d7f2937b is now 5 (2m22.376439001s elapsed)
  Jun 10 13:40:26.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/10/23 13:40:26.969
  STEP: Destroying namespace "container-probe-4843" for this suite. @ 06/10/23 13:40:26.983
• [144.457 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 06/10/23 13:40:26.992
  Jun 10 13:40:26.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename daemonsets @ 06/10/23 13:40:26.993
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:40:27.011
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:40:27.017
  STEP: Creating a simple DaemonSet "daemon-set" @ 06/10/23 13:40:27.043
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/10/23 13:40:27.051
  Jun 10 13:40:27.057: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:40:27.057: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:40:27.063: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:40:27.063: INFO: Node ip-172-31-27-177 is running 0 daemon pod, expected 1
  E0610 13:40:27.619536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:40:28.068: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:40:28.069: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:40:28.072: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 10 13:40:28.072: INFO: Node ip-172-31-27-177 is running 0 daemon pod, expected 1
  E0610 13:40:28.620121      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:40:29.068: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:40:29.069: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:40:29.073: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 10 13:40:29.073: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 06/10/23 13:40:29.077
  Jun 10 13:40:29.095: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:40:29.095: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:40:29.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 10 13:40:29.103: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 06/10/23 13:40:29.104
  STEP: Deleting DaemonSet "daemon-set" @ 06/10/23 13:40:29.113
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2016, will wait for the garbage collector to delete the pods @ 06/10/23 13:40:29.113
  Jun 10 13:40:29.179: INFO: Deleting DaemonSet.extensions daemon-set took: 11.554638ms
  Jun 10 13:40:29.280: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.35462ms
  E0610 13:40:29.620621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:30.620805      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:31.621626      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:40:32.085: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:40:32.086: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 10 13:40:32.089: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"39360"},"items":null}

  Jun 10 13:40:32.094: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"39360"},"items":null}

  Jun 10 13:40:32.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2016" for this suite. @ 06/10/23 13:40:32.116
• [5.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 06/10/23 13:40:32.131
  Jun 10 13:40:32.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename pod-network-test @ 06/10/23 13:40:32.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:40:32.15
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:40:32.16
  STEP: Performing setup for networking test in namespace pod-network-test-9234 @ 06/10/23 13:40:32.165
  STEP: creating a selector @ 06/10/23 13:40:32.165
  STEP: Creating the service pods in kubernetes @ 06/10/23 13:40:32.165
  Jun 10 13:40:32.165: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0610 13:40:32.622495      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:33.622561      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:34.622717      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:35.623038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:36.623427      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:37.623532      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:38.623628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:39.623770      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:40.624868      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:41.625135      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:42.625821      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:43.626315      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:44.626422      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:45.626496      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:46.626604      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:47.627040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:48.627703      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:49.627825      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:50.628709      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:51.628938      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:52.629203      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:53.629733      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 06/10/23 13:40:54.289
  E0610 13:40:54.630657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:55.630811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:40:56.308: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jun 10 13:40:56.308: INFO: Breadth first check of 192.168.109.37 on host 172.31.27.177...
  Jun 10 13:40:56.312: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.109.30:9080/dial?request=hostname&protocol=udp&host=192.168.109.37&port=8081&tries=1'] Namespace:pod-network-test-9234 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 13:40:56.312: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:40:56.312: INFO: ExecWithOptions: Clientset creation
  Jun 10 13:40:56.312: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9234/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.109.30%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.109.37%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 10 13:40:56.401: INFO: Waiting for responses: map[]
  Jun 10 13:40:56.401: INFO: reached 192.168.109.37 after 0/1 tries
  Jun 10 13:40:56.401: INFO: Breadth first check of 192.168.92.15 on host 172.31.46.40...
  Jun 10 13:40:56.406: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.109.30:9080/dial?request=hostname&protocol=udp&host=192.168.92.15&port=8081&tries=1'] Namespace:pod-network-test-9234 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 13:40:56.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:40:56.407: INFO: ExecWithOptions: Clientset creation
  Jun 10 13:40:56.407: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9234/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.109.30%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.92.15%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 10 13:40:56.487: INFO: Waiting for responses: map[]
  Jun 10 13:40:56.487: INFO: reached 192.168.92.15 after 0/1 tries
  Jun 10 13:40:56.488: INFO: Breadth first check of 192.168.149.75 on host 172.31.89.0...
  Jun 10 13:40:56.492: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.109.30:9080/dial?request=hostname&protocol=udp&host=192.168.149.75&port=8081&tries=1'] Namespace:pod-network-test-9234 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 10 13:40:56.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  Jun 10 13:40:56.493: INFO: ExecWithOptions: Clientset creation
  Jun 10 13:40:56.493: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9234/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.109.30%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.149.75%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 10 13:40:56.569: INFO: Waiting for responses: map[]
  Jun 10 13:40:56.569: INFO: reached 192.168.149.75 after 0/1 tries
  Jun 10 13:40:56.569: INFO: Going to retry 0 out of 3 pods....
  Jun 10 13:40:56.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-9234" for this suite. @ 06/10/23 13:40:56.575
• [24.452 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 06/10/23 13:40:56.585
  Jun 10 13:40:56.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename kubectl @ 06/10/23 13:40:56.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:40:56.607
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:40:56.611
  STEP: creating the pod @ 06/10/23 13:40:56.615
  Jun 10 13:40:56.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-1778 create -f -'
  E0610 13:40:56.631522      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:40:57.623: INFO: stderr: ""
  Jun 10 13:40:57.623: INFO: stdout: "pod/pause created\n"
  E0610 13:40:57.632146      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:58.632311      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:40:59.632550      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 06/10/23 13:40:59.634
  Jun 10 13:40:59.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-1778 label pods pause testing-label=testing-label-value'
  Jun 10 13:40:59.731: INFO: stderr: ""
  Jun 10 13:40:59.731: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 06/10/23 13:40:59.731
  Jun 10 13:40:59.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-1778 get pod pause -L testing-label'
  Jun 10 13:40:59.814: INFO: stderr: ""
  Jun 10 13:40:59.814: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 06/10/23 13:40:59.814
  Jun 10 13:40:59.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-1778 label pods pause testing-label-'
  Jun 10 13:40:59.904: INFO: stderr: ""
  Jun 10 13:40:59.904: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 06/10/23 13:40:59.904
  Jun 10 13:40:59.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-1778 get pod pause -L testing-label'
  Jun 10 13:40:59.987: INFO: stderr: ""
  Jun 10 13:40:59.987: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 06/10/23 13:40:59.987
  Jun 10 13:40:59.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-1778 delete --grace-period=0 --force -f -'
  Jun 10 13:41:00.076: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 10 13:41:00.077: INFO: stdout: "pod \"pause\" force deleted\n"
  Jun 10 13:41:00.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-1778 get rc,svc -l name=pause --no-headers'
  Jun 10 13:41:00.165: INFO: stderr: "No resources found in kubectl-1778 namespace.\n"
  Jun 10 13:41:00.165: INFO: stdout: ""
  Jun 10 13:41:00.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=kubectl-1778 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jun 10 13:41:00.245: INFO: stderr: ""
  Jun 10 13:41:00.245: INFO: stdout: ""
  Jun 10 13:41:00.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1778" for this suite. @ 06/10/23 13:41:00.251
• [3.675 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 06/10/23 13:41:00.261
  Jun 10 13:41:00.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 13:41:00.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:41:00.277
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:41:00.281
  STEP: Creating configMap with name projected-configmap-test-volume-4d9c12d4-9693-4ca5-a266-6e54a1f2c6d9 @ 06/10/23 13:41:00.285
  STEP: Creating a pod to test consume configMaps @ 06/10/23 13:41:00.292
  E0610 13:41:00.633612      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:01.633685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:02.634254      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:03.634284      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:41:04.322
  Jun 10 13:41:04.326: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-projected-configmaps-2a7064d8-207a-4864-9365-bbadfd07cb7d container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 06/10/23 13:41:04.352
  Jun 10 13:41:04.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4736" for this suite. @ 06/10/23 13:41:04.377
• [4.123 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 06/10/23 13:41:04.386
  Jun 10 13:41:04.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename statefulset @ 06/10/23 13:41:04.387
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:41:04.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:41:04.409
  STEP: Creating service test in namespace statefulset-4257 @ 06/10/23 13:41:04.413
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 06/10/23 13:41:04.419
  STEP: Creating stateful set ss in namespace statefulset-4257 @ 06/10/23 13:41:04.425
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4257 @ 06/10/23 13:41:04.434
  Jun 10 13:41:04.438: INFO: Found 0 stateful pods, waiting for 1
  E0610 13:41:04.634347      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:05.634618      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:06.634727      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:07.634923      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:08.635015      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:09.635133      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:10.635235      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:11.635345      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:12.635464      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:13.635563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:14.445: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 06/10/23 13:41:14.445
  Jun 10 13:41:14.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4257 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 10 13:41:14.612: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 10 13:41:14.612: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 10 13:41:14.612: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 10 13:41:14.617: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0610 13:41:14.636575      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:15.636701      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:16.636825      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:17.636934      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:18.637043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:19.637158      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:20.637295      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:21.637400      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:22.637468      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:23.637572      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:24.622: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jun 10 13:41:24.622: INFO: Waiting for statefulset status.replicas updated to 0
  E0610 13:41:24.637886      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:24.641: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999998s
  E0610 13:41:25.638582      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:25.646: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994796392s
  E0610 13:41:26.639544      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:26.651: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989608514s
  E0610 13:41:27.640196      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:27.656: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984194044s
  E0610 13:41:28.640957      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:28.661: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.97940047s
  E0610 13:41:29.641689      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:29.667: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974568935s
  E0610 13:41:30.641784      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:30.672: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.968570679s
  E0610 13:41:31.641903      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:31.678: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.963440991s
  E0610 13:41:32.642251      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:32.683: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.958036271s
  E0610 13:41:33.643012      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:33.689: INFO: Verifying statefulset ss doesn't scale past 1 for another 952.314587ms
  E0610 13:41:34.644132      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4257 @ 06/10/23 13:41:34.689
  Jun 10 13:41:34.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4257 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 10 13:41:34.847: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 10 13:41:34.847: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 10 13:41:34.847: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 10 13:41:34.852: INFO: Found 1 stateful pods, waiting for 3
  E0610 13:41:35.644192      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:36.644348      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:37.645208      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:38.645529      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:39.645635      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:40.645738      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:41.645882      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:42.645966      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:43.646050      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:44.646165      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:44.863: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 10 13:41:44.863: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 10 13:41:44.864: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 06/10/23 13:41:44.864
  STEP: Scale down will halt with unhealthy stateful pod @ 06/10/23 13:41:44.864
  Jun 10 13:41:44.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4257 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 10 13:41:45.029: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 10 13:41:45.029: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 10 13:41:45.029: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 10 13:41:45.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4257 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 10 13:41:45.194: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 10 13:41:45.194: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 10 13:41:45.194: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 10 13:41:45.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4257 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 10 13:41:45.448: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 10 13:41:45.448: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 10 13:41:45.448: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 10 13:41:45.448: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 10 13:41:45.452: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0610 13:41:45.647263      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:46.648117      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:47.648314      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:48.648706      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:49.648790      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:50.648935      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:51.649366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:52.649566      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:53.650266      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:41:54.651257      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:55.465: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jun 10 13:41:55.465: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jun 10 13:41:55.465: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jun 10 13:41:55.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999998s
  E0610 13:41:55.651497      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:56.485: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994941473s
  E0610 13:41:56.651584      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:57.490: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990585013s
  E0610 13:41:57.652087      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:58.496: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984814647s
  E0610 13:41:58.652315      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:41:59.500: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979780718s
  E0610 13:41:59.653110      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:00.506: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975179753s
  E0610 13:42:00.653661      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:01.511: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969561037s
  E0610 13:42:01.654395      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:02.516: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964574318s
  E0610 13:42:02.654476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:03.521: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.959223496s
  E0610 13:42:03.654704      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:04.526: INFO: Verifying statefulset ss doesn't scale past 3 for another 953.899944ms
  E0610 13:42:04.655052      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4257 @ 06/10/23 13:42:05.526
  Jun 10 13:42:05.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4257 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0610 13:42:05.655188      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:05.687: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 10 13:42:05.687: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 10 13:42:05.687: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 10 13:42:05.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4257 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 10 13:42:05.840: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 10 13:42:05.840: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 10 13:42:05.840: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 10 13:42:05.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=statefulset-4257 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 10 13:42:05.992: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 10 13:42:05.992: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 10 13:42:05.992: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 10 13:42:05.992: INFO: Scaling statefulset ss to 0
  E0610 13:42:06.656254      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:07.657238      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:08.657753      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:09.657858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:10.658235      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:11.658333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:12.658555      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:13.659008      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:14.660148      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:15.660208      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 06/10/23 13:42:16.013
  Jun 10 13:42:16.013: INFO: Deleting all statefulset in ns statefulset-4257
  Jun 10 13:42:16.017: INFO: Scaling statefulset ss to 0
  Jun 10 13:42:16.029: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 10 13:42:16.033: INFO: Deleting statefulset ss
  Jun 10 13:42:16.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4257" for this suite. @ 06/10/23 13:42:16.055
• [71.676 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 06/10/23 13:42:16.064
  Jun 10 13:42:16.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename subpath @ 06/10/23 13:42:16.065
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:42:16.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:42:16.087
  STEP: Setting up data @ 06/10/23 13:42:16.091
  STEP: Creating pod pod-subpath-test-projected-fggb @ 06/10/23 13:42:16.103
  STEP: Creating a pod to test atomic-volume-subpath @ 06/10/23 13:42:16.103
  E0610 13:42:16.660425      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:17.660531      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:18.661273      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:19.661777      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:20.662338      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:21.662610      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:22.663714      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:23.664328      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:24.665123      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:25.665215      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:26.665948      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:27.666002      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:28.666399      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:29.666513      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:30.667121      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:31.667244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:32.668124      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:33.668790      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:34.669679      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:35.670061      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:36.671074      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:37.671190      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:38.671314      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:39.672161      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:42:40.184
  Jun 10 13:42:40.188: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-subpath-test-projected-fggb container test-container-subpath-projected-fggb: <nil>
  STEP: delete the pod @ 06/10/23 13:42:40.21
  STEP: Deleting pod pod-subpath-test-projected-fggb @ 06/10/23 13:42:40.229
  Jun 10 13:42:40.229: INFO: Deleting pod "pod-subpath-test-projected-fggb" in namespace "subpath-1661"
  Jun 10 13:42:40.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1661" for this suite. @ 06/10/23 13:42:40.238
• [24.182 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 06/10/23 13:42:40.253
  Jun 10 13:42:40.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename pods @ 06/10/23 13:42:40.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:42:40.275
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:42:40.279
  STEP: creating pod @ 06/10/23 13:42:40.283
  E0610 13:42:40.672349      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:41.672437      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:42.310: INFO: Pod pod-hostip-1e3fb0c4-1c79-4808-b4cd-b015c3077e32 has hostIP: 172.31.27.177
  Jun 10 13:42:42.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1258" for this suite. @ 06/10/23 13:42:42.316
• [2.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 06/10/23 13:42:42.328
  Jun 10 13:42:42.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename pods @ 06/10/23 13:42:42.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:42:42.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:42:42.356
  STEP: creating the pod @ 06/10/23 13:42:42.36
  STEP: submitting the pod to kubernetes @ 06/10/23 13:42:42.36
  W0610 13:42:42.370731      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0610 13:42:42.672749      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:43.673722      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 06/10/23 13:42:44.383
  STEP: updating the pod @ 06/10/23 13:42:44.387
  E0610 13:42:44.673757      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:44.902: INFO: Successfully updated pod "pod-update-activedeadlineseconds-4993bcfd-767b-4856-a60c-3f8e22b870e1"
  E0610 13:42:45.673910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:46.674274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:47.674806      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:42:48.675044      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:48.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9294" for this suite. @ 06/10/23 13:42:48.921
• [6.601 seconds]
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 06/10/23 13:42:48.93
  Jun 10 13:42:48.930: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename daemonsets @ 06/10/23 13:42:48.931
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:42:48.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:42:48.954
  Jun 10 13:42:48.982: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/10/23 13:42:48.989
  Jun 10 13:42:48.995: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:48.995: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:48.999: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:42:48.999: INFO: Node ip-172-31-27-177 is running 0 daemon pod, expected 1
  E0610 13:42:49.675061      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:50.006: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:50.006: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:50.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:42:50.017: INFO: Node ip-172-31-27-177 is running 0 daemon pod, expected 1
  E0610 13:42:50.675279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:51.005: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:51.005: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:51.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 10 13:42:51.010: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 06/10/23 13:42:51.026
  STEP: Check that daemon pods images are updated. @ 06/10/23 13:42:51.045
  Jun 10 13:42:51.050: INFO: Wrong image for pod: daemon-set-2xd5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 10 13:42:51.050: INFO: Wrong image for pod: daemon-set-hdv29. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 10 13:42:51.050: INFO: Wrong image for pod: daemon-set-kn4js. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 10 13:42:51.056: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:51.057: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0610 13:42:51.676233      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:52.062: INFO: Wrong image for pod: daemon-set-2xd5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 10 13:42:52.062: INFO: Wrong image for pod: daemon-set-hdv29. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 10 13:42:52.066: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:52.066: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0610 13:42:52.676903      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:53.063: INFO: Wrong image for pod: daemon-set-2xd5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 10 13:42:53.064: INFO: Wrong image for pod: daemon-set-hdv29. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 10 13:42:53.064: INFO: Pod daemon-set-k5747 is not available
  Jun 10 13:42:53.069: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:53.069: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0610 13:42:53.677245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:54.064: INFO: Wrong image for pod: daemon-set-hdv29. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 10 13:42:54.067: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:54.068: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0610 13:42:54.677339      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:55.065: INFO: Pod daemon-set-64rct is not available
  Jun 10 13:42:55.065: INFO: Wrong image for pod: daemon-set-hdv29. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 10 13:42:55.070: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:55.070: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0610 13:42:55.678133      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:56.068: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:56.068: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0610 13:42:56.678786      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:57.061: INFO: Pod daemon-set-7xdff is not available
  Jun 10 13:42:57.067: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:57.067: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 06/10/23 13:42:57.067
  Jun 10 13:42:57.072: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:57.072: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:57.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 10 13:42:57.077: INFO: Node ip-172-31-46-40 is running 0 daemon pod, expected 1
  E0610 13:42:57.679913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:58.083: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:58.083: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:58.087: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 10 13:42:58.087: INFO: Node ip-172-31-46-40 is running 0 daemon pod, expected 1
  E0610 13:42:58.679940      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:42:59.082: INFO: DaemonSet pods can't tolerate node ip-172-31-13-130 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:59.082: INFO: DaemonSet pods can't tolerate node ip-172-31-22-107 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 10 13:42:59.087: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 10 13:42:59.087: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 06/10/23 13:42:59.107
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-518, will wait for the garbage collector to delete the pods @ 06/10/23 13:42:59.107
  Jun 10 13:42:59.174: INFO: Deleting DaemonSet.extensions daemon-set took: 8.003722ms
  Jun 10 13:42:59.274: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.498218ms
  E0610 13:42:59.680869      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:43:00.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:43:00.380: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 10 13:43:00.384: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40387"},"items":null}

  Jun 10 13:43:00.388: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40387"},"items":null}

  Jun 10 13:43:00.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-518" for this suite. @ 06/10/23 13:43:00.408
• [11.486 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 06/10/23 13:43:00.416
  Jun 10 13:43:00.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename sysctl @ 06/10/23 13:43:00.417
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:43:00.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:43:00.442
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 06/10/23 13:43:00.447
  STEP: Watching for error events or started pod @ 06/10/23 13:43:00.458
  E0610 13:43:00.681245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:01.682325      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 06/10/23 13:43:02.465
  E0610 13:43:02.683003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:03.683135      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 06/10/23 13:43:04.478
  STEP: Getting logs from the pod @ 06/10/23 13:43:04.478
  STEP: Checking that the sysctl is actually updated @ 06/10/23 13:43:04.486
  Jun 10 13:43:04.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-32" for this suite. @ 06/10/23 13:43:04.491
• [4.083 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 06/10/23 13:43:04.5
  Jun 10 13:43:04.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename configmap @ 06/10/23 13:43:04.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:43:04.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:43:04.519
  STEP: Creating configMap with name configmap-test-volume-836ec946-11bf-4d8a-a738-bd56b2e55926 @ 06/10/23 13:43:04.524
  STEP: Creating a pod to test consume configMaps @ 06/10/23 13:43:04.53
  E0610 13:43:04.683233      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:05.683310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:06.683451      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:07.683514      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:43:08.555
  Jun 10 13:43:08.559: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-configmaps-d36344d4-61a9-48c5-8b23-fcee91329813 container agnhost-container: <nil>
  STEP: delete the pod @ 06/10/23 13:43:08.568
  Jun 10 13:43:08.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9147" for this suite. @ 06/10/23 13:43:08.589
• [4.097 seconds]
------------------------------
SS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 06/10/23 13:43:08.597
  Jun 10 13:43:08.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename taint-multiple-pods @ 06/10/23 13:43:08.598
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:43:08.612
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:43:08.616
  Jun 10 13:43:08.626: INFO: Waiting up to 1m0s for all nodes to be ready
  E0610 13:43:08.684524      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:09.684663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:10.685657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:11.685854      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:12.685971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:13.686126      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:14.687191      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:15.688200      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:16.688221      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:17.688342      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:18.688842      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:19.688970      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:20.689943      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:21.690377      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:22.691303      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:23.691368      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:24.692233      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:25.692448      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:26.693271      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:27.693355      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:28.694217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:29.694322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:30.695244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:31.695372      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:32.696283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:33.696379      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:34.696563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:35.696919      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:36.696966      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:37.697115      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:38.697997      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:39.698163      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:40.698746      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:41.698541      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:42.698926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:43.699055      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:44.699293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:45.699411      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:46.699497      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:47.700181      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:48.700904      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:49.701130      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:50.701993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:51.702716      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:52.703200      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:53.703388      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:54.703440      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:55.703590      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:56.703604      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:57.703757      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:58.704722      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:43:59.704838      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:00.705277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:01.705591      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:02.705957      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:03.706019      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:04.706804      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:05.707196      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:06.708164      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:07.708437      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:44:08.643: INFO: Waiting for terminating namespaces to be deleted...
  Jun 10 13:44:08.648: INFO: Starting informer...
  STEP: Starting pods... @ 06/10/23 13:44:08.648
  E0610 13:44:08.709369      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:44:08.870: INFO: Pod1 is running on ip-172-31-27-177. Tainting Node
  E0610 13:44:09.709465      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:10.709539      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:44:11.095: INFO: Pod2 is running on ip-172-31-27-177. Tainting Node
  STEP: Trying to apply a taint on the Node @ 06/10/23 13:44:11.095
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 06/10/23 13:44:11.108
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 06/10/23 13:44:11.115
  E0610 13:44:11.709671      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:12.710266      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:13.710360      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:14.711255      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:15.711333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:16.712355      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:44:16.980: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0610 13:44:17.713307      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:18.714341      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:19.714646      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:20.715022      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:21.715086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:22.716159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:23.716270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:24.716370      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:25.716470      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:26.717294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:27.717380      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:28.717472      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:29.717588      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:30.717675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:31.717853      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:32.718607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:33.718964      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:34.719050      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:35.719121      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:36.719586      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:44:37.027: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Jun 10 13:44:37.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 06/10/23 13:44:37.046
  STEP: Destroying namespace "taint-multiple-pods-4869" for this suite. @ 06/10/23 13:44:37.051
• [88.469 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 06/10/23 13:44:37.068
  Jun 10 13:44:37.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename configmap @ 06/10/23 13:44:37.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:44:37.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:44:37.101
  STEP: Creating configMap with name configmap-test-volume-map-1fc8544a-8796-4d76-a2d4-3f59a848309b @ 06/10/23 13:44:37.105
  STEP: Creating a pod to test consume configMaps @ 06/10/23 13:44:37.112
  E0610 13:44:37.720622      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:38.720741      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:39.721649      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:40.721745      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:44:41.137
  Jun 10 13:44:41.140: INFO: Trying to get logs from node ip-172-31-27-177 pod pod-configmaps-7a0a6c22-0ac2-4499-b0e0-ef08af7410f1 container agnhost-container: <nil>
  STEP: delete the pod @ 06/10/23 13:44:41.161
  Jun 10 13:44:41.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3650" for this suite. @ 06/10/23 13:44:41.184
• [4.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 06/10/23 13:44:41.193
  Jun 10 13:44:41.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename services @ 06/10/23 13:44:41.194
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:44:41.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:44:41.215
  STEP: creating service in namespace services-7219 @ 06/10/23 13:44:41.22
  STEP: creating service affinity-clusterip-transition in namespace services-7219 @ 06/10/23 13:44:41.22
  STEP: creating replication controller affinity-clusterip-transition in namespace services-7219 @ 06/10/23 13:44:41.232
  I0610 13:44:41.247841      18 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-7219, replica count: 3
  E0610 13:44:41.723180      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:42.723846      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:43.723927      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0610 13:44:44.299723      18 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 10 13:44:44.308: INFO: Creating new exec pod
  E0610 13:44:44.724556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:45.725292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:46.725906      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:44:47.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-7219 exec execpod-affinity4mqzg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Jun 10 13:44:47.494: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Jun 10 13:44:47.494: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 13:44:47.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-7219 exec execpod-affinity4mqzg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.141 80'
  Jun 10 13:44:47.648: INFO: stderr: "+ nc -v -t -w 2 10.152.183.141 80\nConnection to 10.152.183.141 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun 10 13:44:47.648: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 10 13:44:47.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-7219 exec execpod-affinity4mqzg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.141:80/ ; done'
  E0610 13:44:47.726230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:44:47.913: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n"
  Jun 10 13:44:47.913: INFO: stdout: "\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-vfsnr\naffinity-clusterip-transition-jwzff\naffinity-clusterip-transition-jwzff\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-vfsnr\naffinity-clusterip-transition-jwzff\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-vfsnr\naffinity-clusterip-transition-vfsnr\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-jwzff\naffinity-clusterip-transition-jwzff\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s"
  Jun 10 13:44:47.913: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:47.913: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:47.913: INFO: Received response from host: affinity-clusterip-transition-vfsnr
  Jun 10 13:44:47.913: INFO: Received response from host: affinity-clusterip-transition-jwzff
  Jun 10 13:44:47.913: INFO: Received response from host: affinity-clusterip-transition-jwzff
  Jun 10 13:44:47.913: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:47.913: INFO: Received response from host: affinity-clusterip-transition-vfsnr
  Jun 10 13:44:47.913: INFO: Received response from host: affinity-clusterip-transition-jwzff
  Jun 10 13:44:47.913: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:47.913: INFO: Received response from host: affinity-clusterip-transition-vfsnr
  Jun 10 13:44:47.913: INFO: Received response from host: affinity-clusterip-transition-vfsnr
  Jun 10 13:44:47.913: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:47.913: INFO: Received response from host: affinity-clusterip-transition-jwzff
  Jun 10 13:44:47.913: INFO: Received response from host: affinity-clusterip-transition-jwzff
  Jun 10 13:44:47.913: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:47.913: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:47.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1208248023 --namespace=services-7219 exec execpod-affinity4mqzg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.141:80/ ; done'
  Jun 10 13:44:48.196: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.141:80/\n"
  Jun 10 13:44:48.196: INFO: stdout: "\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s\naffinity-clusterip-transition-6gd8s"
  Jun 10 13:44:48.196: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:48.196: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:48.196: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:48.196: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:48.196: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:48.196: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:48.196: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:48.196: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:48.196: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:48.196: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:48.196: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:48.196: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:48.196: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:48.196: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:48.196: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:48.196: INFO: Received response from host: affinity-clusterip-transition-6gd8s
  Jun 10 13:44:48.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 10 13:44:48.201: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7219, will wait for the garbage collector to delete the pods @ 06/10/23 13:44:48.216
  Jun 10 13:44:48.280: INFO: Deleting ReplicationController affinity-clusterip-transition took: 8.425567ms
  Jun 10 13:44:48.381: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.989934ms
  E0610 13:44:48.726675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:44:49.727162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-7219" for this suite. @ 06/10/23 13:44:50.707
• [9.526 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 06/10/23 13:44:50.724
  Jun 10 13:44:50.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename daemonsets @ 06/10/23 13:44:50.725
  E0610 13:44:50.727850      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:44:50.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:44:50.758
  Jun 10 13:44:50.788: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 06/10/23 13:44:50.796
  Jun 10 13:44:50.801: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:44:50.801: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 06/10/23 13:44:50.801
  Jun 10 13:44:50.838: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:44:50.838: INFO: Node ip-172-31-89-0 is running 0 daemon pod, expected 1
  E0610 13:44:51.730286      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:44:51.844: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:44:51.844: INFO: Node ip-172-31-89-0 is running 0 daemon pod, expected 1
  E0610 13:44:52.730321      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:44:52.845: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:44:52.846: INFO: Node ip-172-31-89-0 is running 0 daemon pod, expected 1
  E0610 13:44:53.731392      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:44:53.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 10 13:44:53.843: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 06/10/23 13:44:53.847
  Jun 10 13:44:53.879: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 10 13:44:53.879: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0610 13:44:54.731562      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:44:54.886: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:44:54.887: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 06/10/23 13:44:54.887
  Jun 10 13:44:54.902: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:44:54.903: INFO: Node ip-172-31-89-0 is running 0 daemon pod, expected 1
  E0610 13:44:55.731559      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:44:55.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:44:55.909: INFO: Node ip-172-31-89-0 is running 0 daemon pod, expected 1
  E0610 13:44:56.731590      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:44:56.908: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 10 13:44:56.908: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 06/10/23 13:44:56.916
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1245, will wait for the garbage collector to delete the pods @ 06/10/23 13:44:56.916
  Jun 10 13:44:56.980: INFO: Deleting DaemonSet.extensions daemon-set took: 8.238055ms
  Jun 10 13:44:57.081: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.061892ms
  E0610 13:44:57.732569      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:44:58.688: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 10 13:44:58.688: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 10 13:44:58.691: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41147"},"items":null}

  Jun 10 13:44:58.695: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41147"},"items":null}

  Jun 10 13:44:58.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1245" for this suite. @ 06/10/23 13:44:58.732
  E0610 13:44:58.733475      18 retrywatcher.go:130] "Watch failed" err="context canceled"
• [8.019 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 06/10/23 13:44:58.745
  Jun 10 13:44:58.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename resourcequota @ 06/10/23 13:44:58.746
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:44:58.769
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:44:58.773
  STEP: Counting existing ResourceQuota @ 06/10/23 13:44:58.779
  E0610 13:44:59.734369      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:00.735456      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:01.735560      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:02.736425      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:03.737379      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 06/10/23 13:45:03.784
  STEP: Ensuring resource quota status is calculated @ 06/10/23 13:45:03.793
  E0610 13:45:04.738227      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:05.738432      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 06/10/23 13:45:05.797
  STEP: Creating a NodePort Service @ 06/10/23 13:45:05.816
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 06/10/23 13:45:05.844
  STEP: Ensuring resource quota status captures service creation @ 06/10/23 13:45:05.867
  E0610 13:45:06.738506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:07.738995      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 06/10/23 13:45:07.872
  STEP: Ensuring resource quota status released usage @ 06/10/23 13:45:07.948
  E0610 13:45:08.739379      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:09.740197      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:45:09.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6484" for this suite. @ 06/10/23 13:45:09.958
• [11.222 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 06/10/23 13:45:09.975
  Jun 10 13:45:09.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename projected @ 06/10/23 13:45:09.976
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:45:09.995
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:45:09.999
  STEP: Creating a pod to test downward API volume plugin @ 06/10/23 13:45:10.004
  E0610 13:45:10.740349      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:11.740417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:12.741406      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:13.741592      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/10/23 13:45:14.033
  Jun 10 13:45:14.037: INFO: Trying to get logs from node ip-172-31-27-177 pod downwardapi-volume-7a298c80-4a35-4530-9d56-1f6d705849b7 container client-container: <nil>
  STEP: delete the pod @ 06/10/23 13:45:14.046
  Jun 10 13:45:14.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3351" for this suite. @ 06/10/23 13:45:14.069
• [4.104 seconds]
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 06/10/23 13:45:14.079
  Jun 10 13:45:14.079: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename gc @ 06/10/23 13:45:14.08
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:45:14.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:45:14.099
  STEP: create the rc1 @ 06/10/23 13:45:14.107
  STEP: create the rc2 @ 06/10/23 13:45:14.113
  E0610 13:45:14.742632      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:15.743352      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:16.744127      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:17.744332      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:18.744886      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:19.745409      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 06/10/23 13:45:20.127
  STEP: delete the rc simpletest-rc-to-be-deleted @ 06/10/23 13:45:20.742
  E0610 13:45:20.746254      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for the rc to be deleted @ 06/10/23 13:45:20.751
  E0610 13:45:21.746720      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:22.747551      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:23.747701      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:24.748245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:25.751628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 10 13:45:25.774: INFO: 71 pods remaining
  Jun 10 13:45:25.774: INFO: 71 pods has nil DeletionTimestamp
  Jun 10 13:45:25.774: INFO: 
  E0610 13:45:26.752379      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:27.753422      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:28.753885      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:29.754926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:30.755737      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 06/10/23 13:45:30.765
  W0610 13:45:30.770149      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 10 13:45:30.770: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 10 13:45:30.772: INFO: Deleting pod "simpletest-rc-to-be-deleted-28r95" in namespace "gc-5079"
  Jun 10 13:45:30.787: INFO: Deleting pod "simpletest-rc-to-be-deleted-2pbgc" in namespace "gc-5079"
  Jun 10 13:45:30.800: INFO: Deleting pod "simpletest-rc-to-be-deleted-2pz8z" in namespace "gc-5079"
  Jun 10 13:45:30.821: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tpsd" in namespace "gc-5079"
  Jun 10 13:45:30.837: INFO: Deleting pod "simpletest-rc-to-be-deleted-45jc2" in namespace "gc-5079"
  Jun 10 13:45:30.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-47tcs" in namespace "gc-5079"
  Jun 10 13:45:30.868: INFO: Deleting pod "simpletest-rc-to-be-deleted-49bxz" in namespace "gc-5079"
  Jun 10 13:45:30.886: INFO: Deleting pod "simpletest-rc-to-be-deleted-4q7js" in namespace "gc-5079"
  Jun 10 13:45:30.901: INFO: Deleting pod "simpletest-rc-to-be-deleted-4w46g" in namespace "gc-5079"
  Jun 10 13:45:30.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-584hl" in namespace "gc-5079"
  Jun 10 13:45:30.936: INFO: Deleting pod "simpletest-rc-to-be-deleted-5btmh" in namespace "gc-5079"
  Jun 10 13:45:30.951: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dqtn" in namespace "gc-5079"
  Jun 10 13:45:30.965: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mpfw" in namespace "gc-5079"
  Jun 10 13:45:30.983: INFO: Deleting pod "simpletest-rc-to-be-deleted-5sgqw" in namespace "gc-5079"
  Jun 10 13:45:30.997: INFO: Deleting pod "simpletest-rc-to-be-deleted-5tltf" in namespace "gc-5079"
  Jun 10 13:45:31.013: INFO: Deleting pod "simpletest-rc-to-be-deleted-62fcb" in namespace "gc-5079"
  Jun 10 13:45:31.027: INFO: Deleting pod "simpletest-rc-to-be-deleted-62z5d" in namespace "gc-5079"
  Jun 10 13:45:31.042: INFO: Deleting pod "simpletest-rc-to-be-deleted-646ck" in namespace "gc-5079"
  Jun 10 13:45:31.059: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gr5b" in namespace "gc-5079"
  Jun 10 13:45:31.077: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qz49" in namespace "gc-5079"
  Jun 10 13:45:31.091: INFO: Deleting pod "simpletest-rc-to-be-deleted-6slqr" in namespace "gc-5079"
  Jun 10 13:45:31.109: INFO: Deleting pod "simpletest-rc-to-be-deleted-7b92n" in namespace "gc-5079"
  Jun 10 13:45:31.125: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fssh" in namespace "gc-5079"
  Jun 10 13:45:31.139: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hgxk" in namespace "gc-5079"
  Jun 10 13:45:31.150: INFO: Deleting pod "simpletest-rc-to-be-deleted-7j8mn" in namespace "gc-5079"
  Jun 10 13:45:31.167: INFO: Deleting pod "simpletest-rc-to-be-deleted-7l6kj" in namespace "gc-5079"
  Jun 10 13:45:31.181: INFO: Deleting pod "simpletest-rc-to-be-deleted-7nfxx" in namespace "gc-5079"
  Jun 10 13:45:31.199: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vnr9" in namespace "gc-5079"
  Jun 10 13:45:31.229: INFO: Deleting pod "simpletest-rc-to-be-deleted-88l5l" in namespace "gc-5079"
  Jun 10 13:45:31.250: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tp6q" in namespace "gc-5079"
  Jun 10 13:45:31.273: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tbn6" in namespace "gc-5079"
  Jun 10 13:45:31.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ztss" in namespace "gc-5079"
  Jun 10 13:45:31.318: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2jb4" in namespace "gc-5079"
  Jun 10 13:45:31.337: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbbcp" in namespace "gc-5079"
  Jun 10 13:45:31.357: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcl5q" in namespace "gc-5079"
  Jun 10 13:45:31.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-bh4jw" in namespace "gc-5079"
  Jun 10 13:45:31.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckkfv" in namespace "gc-5079"
  Jun 10 13:45:31.420: INFO: Deleting pod "simpletest-rc-to-be-deleted-ct9sz" in namespace "gc-5079"
  Jun 10 13:45:31.437: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4h4n" in namespace "gc-5079"
  Jun 10 13:45:31.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddsjx" in namespace "gc-5079"
  Jun 10 13:45:31.463: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfcwp" in namespace "gc-5079"
  Jun 10 13:45:31.480: INFO: Deleting pod "simpletest-rc-to-be-deleted-dq9sn" in namespace "gc-5079"
  Jun 10 13:45:31.497: INFO: Deleting pod "simpletest-rc-to-be-deleted-dql67" in namespace "gc-5079"
  Jun 10 13:45:31.518: INFO: Deleting pod "simpletest-rc-to-be-deleted-drssz" in namespace "gc-5079"
  Jun 10 13:45:31.534: INFO: Deleting pod "simpletest-rc-to-be-deleted-dw8tm" in namespace "gc-5079"
  Jun 10 13:45:31.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-dx76g" in namespace "gc-5079"
  Jun 10 13:45:31.566: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4g8f" in namespace "gc-5079"
  Jun 10 13:45:31.580: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8r5t" in namespace "gc-5079"
  Jun 10 13:45:31.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-flgx9" in namespace "gc-5079"
  Jun 10 13:45:31.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-gft7z" in namespace "gc-5079"
  Jun 10 13:45:31.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5079" for this suite. @ 06/10/23 13:45:31.638
• [17.569 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 06/10/23 13:45:31.648
  Jun 10 13:45:31.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1208248023
  STEP: Building a namespace api object, basename disruption @ 06/10/23 13:45:31.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/10/23 13:45:31.673
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/10/23 13:45:31.685
  STEP: creating the pdb @ 06/10/23 13:45:31.689
  STEP: Waiting for the pdb to be processed @ 06/10/23 13:45:31.697
  STEP: updating the pdb @ 06/10/23 13:45:31.706
  STEP: Waiting for the pdb to be processed @ 06/10/23 13:45:31.716
  E0610 13:45:31.756183      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:32.757011      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 06/10/23 13:45:33.73
  STEP: Waiting for the pdb to be processed @ 06/10/23 13:45:33.747
  E0610 13:45:33.757782      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:34.757888      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0610 13:45:35.758338      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 06/10/23 13:45:35.766
  Jun 10 13:45:35.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4955" for this suite. @ 06/10/23 13:45:35.78
• [4.141 seconds]
------------------------------
SSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Jun 10 13:45:35.792: INFO: Running AfterSuite actions on node 1
  Jun 10 13:45:35.792: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.192 seconds]
------------------------------

Ran 378 of 7207 Specs in 5902.767 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h38m23.464039301s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

