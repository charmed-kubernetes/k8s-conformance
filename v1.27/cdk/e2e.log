  I0527 12:15:57.131851      18 e2e.go:117] Starting e2e run "7bfe1a40-97ab-42a2-b1e3-e2a98f36b04a" on Ginkgo node 1
  May 27 12:15:57.169: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1685189757 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.002 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  May 27 12:15:57.423: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:15:57.425: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  May 27 12:15:57.477: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  May 27 12:15:57.482: INFO: e2e test version: v1.27.2
  May 27 12:15:57.484: INFO: kube-apiserver version: v1.27.2
  May 27 12:15:57.485: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:15:57.492: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.069 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 05/27/23 12:15:57.878
  May 27 12:15:57.878: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl @ 05/27/23 12:15:57.879
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:15:57.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:15:57.906
  STEP: starting the proxy server @ 05/27/23 12:15:57.911
  May 27 12:15:57.911: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-5920 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 05/27/23 12:15:57.974
  May 27 12:15:57.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5920" for this suite. @ 05/27/23 12:15:57.993
• [0.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 05/27/23 12:15:58.004
  May 27 12:15:58.004: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl @ 05/27/23 12:15:58.006
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:15:58.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:15:58.03
  STEP: Starting the proxy @ 05/27/23 12:15:58.035
  May 27 12:15:58.035: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-4445 proxy --unix-socket=/tmp/kubectl-proxy-unix1628294670/test'
  STEP: retrieving proxy /api/ output @ 05/27/23 12:15:58.097
  May 27 12:15:58.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4445" for this suite. @ 05/27/23 12:15:58.104
• [0.114 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 05/27/23 12:15:58.119
  May 27 12:15:58.119: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubelet-test @ 05/27/23 12:15:58.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:15:58.142
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:15:58.147
  May 27 12:16:00.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7555" for this suite. @ 05/27/23 12:16:00.232
• [2.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 05/27/23 12:16:00.247
  May 27 12:16:00.247: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename init-container @ 05/27/23 12:16:00.248
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:16:00.285
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:16:00.294
  STEP: creating the pod @ 05/27/23 12:16:00.301
  May 27 12:16:00.301: INFO: PodSpec: initContainers in spec.initContainers
  May 27 12:16:05.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-9747" for this suite. @ 05/27/23 12:16:05.433
• [5.199 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 05/27/23 12:16:05.458
  May 27 12:16:05.459: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/27/23 12:16:05.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:16:05.489
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:16:05.499
  STEP: create the container to handle the HTTPGet hook request. @ 05/27/23 12:16:05.521
  STEP: create the pod with lifecycle hook @ 05/27/23 12:16:11.574
  STEP: check poststart hook @ 05/27/23 12:16:15.602
  STEP: delete the pod with lifecycle hook @ 05/27/23 12:16:15.628
  May 27 12:16:19.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-6530" for this suite. @ 05/27/23 12:16:19.661
• [14.213 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 05/27/23 12:16:19.673
  May 27 12:16:19.673: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename dns @ 05/27/23 12:16:19.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:16:19.695
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:16:19.699
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 05/27/23 12:16:19.704
  May 27 12:16:19.716: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8571  97cf6066-1cff-49b1-a6f9-206e870a471f 3771 0 2023-05-27 12:16:19 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-27 12:16:19 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9pgzj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9pgzj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 05/27/23 12:16:21.731
  May 27 12:16:21.731: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8571 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 12:16:21.731: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:16:21.731: INFO: ExecWithOptions: Clientset creation
  May 27 12:16:21.732: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-8571/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 05/27/23 12:16:21.824
  May 27 12:16:21.824: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8571 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 12:16:21.824: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:16:21.825: INFO: ExecWithOptions: Clientset creation
  May 27 12:16:21.825: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-8571/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 27 12:16:21.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 27 12:16:21.934: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-8571" for this suite. @ 05/27/23 12:16:21.951
• [2.285 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 05/27/23 12:16:21.963
  May 27 12:16:21.963: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename containers @ 05/27/23 12:16:21.964
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:16:21.99
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:16:21.996
  May 27 12:16:24.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8240" for this suite. @ 05/27/23 12:16:24.036
• [2.084 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 05/27/23 12:16:24.048
  May 27 12:16:24.048: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 12:16:24.049
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:16:24.07
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:16:24.075
  STEP: Creating configMap with name projected-configmap-test-volume-c6445aad-481b-4657-afc8-8e52846c46f1 @ 05/27/23 12:16:24.08
  STEP: Creating a pod to test consume configMaps @ 05/27/23 12:16:24.086
  STEP: Saw pod success @ 05/27/23 12:16:32.128
  May 27 12:16:32.132: INFO: Trying to get logs from node ip-172-31-10-136 pod pod-projected-configmaps-75810699-9193-493e-8e72-a1c7700852a7 container agnhost-container: <nil>
  STEP: delete the pod @ 05/27/23 12:16:32.157
  May 27 12:16:32.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9266" for this suite. @ 05/27/23 12:16:32.187
• [8.148 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 05/27/23 12:16:32.201
  May 27 12:16:32.201: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 12:16:32.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:16:32.224
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:16:32.23
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 12:16:32.234
  STEP: Saw pod success @ 05/27/23 12:16:36.264
  May 27 12:16:36.269: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-c4962521-37e2-43b6-8c80-504269c56e3c container client-container: <nil>
  STEP: delete the pod @ 05/27/23 12:16:36.276
  May 27 12:16:36.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6655" for this suite. @ 05/27/23 12:16:36.303
• [4.110 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 05/27/23 12:16:36.311
  May 27 12:16:36.311: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename replication-controller @ 05/27/23 12:16:36.312
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:16:36.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:16:36.336
  STEP: Creating replication controller my-hostname-basic-2d127b03-b476-4a28-974c-5eff748af81f @ 05/27/23 12:16:36.341
  May 27 12:16:36.356: INFO: Pod name my-hostname-basic-2d127b03-b476-4a28-974c-5eff748af81f: Found 0 pods out of 1
  May 27 12:16:41.366: INFO: Pod name my-hostname-basic-2d127b03-b476-4a28-974c-5eff748af81f: Found 1 pods out of 1
  May 27 12:16:41.366: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-2d127b03-b476-4a28-974c-5eff748af81f" are running
  May 27 12:16:41.373: INFO: Pod "my-hostname-basic-2d127b03-b476-4a28-974c-5eff748af81f-g99jv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-27 12:16:36 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-27 12:16:37 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-27 12:16:37 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-27 12:16:36 +0000 UTC Reason: Message:}])
  May 27 12:16:41.373: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/27/23 12:16:41.374
  May 27 12:16:41.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3800" for this suite. @ 05/27/23 12:16:41.397
• [5.096 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 05/27/23 12:16:41.409
  May 27 12:16:41.410: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/27/23 12:16:41.411
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:16:41.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:16:41.443
  May 27 12:16:41.450: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:16:44.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-3108" for this suite. @ 05/27/23 12:16:44.62
• [3.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 05/27/23 12:16:44.634
  May 27 12:16:44.634: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename job @ 05/27/23 12:16:44.635
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:16:44.654
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:16:44.662
  STEP: Creating a job @ 05/27/23 12:16:44.667
  STEP: Ensuring active pods == parallelism @ 05/27/23 12:16:44.679
  STEP: delete a job @ 05/27/23 12:16:46.691
  STEP: deleting Job.batch foo in namespace job-3161, will wait for the garbage collector to delete the pods @ 05/27/23 12:16:46.691
  May 27 12:16:46.766: INFO: Deleting Job.batch foo took: 20.111882ms
  May 27 12:16:46.867: INFO: Terminating Job.batch foo pods took: 101.038842ms
  STEP: Ensuring job was deleted @ 05/27/23 12:17:18.668
  May 27 12:17:18.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3161" for this suite. @ 05/27/23 12:17:18.681
• [34.055 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 05/27/23 12:17:18.689
  May 27 12:17:18.689: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename init-container @ 05/27/23 12:17:18.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:17:18.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:17:18.721
  STEP: creating the pod @ 05/27/23 12:17:18.725
  May 27 12:17:18.726: INFO: PodSpec: initContainers in spec.initContainers
  May 27 12:17:23.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-2873" for this suite. @ 05/27/23 12:17:23.027
• [4.346 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 05/27/23 12:17:23.037
  May 27 12:17:23.037: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename secrets @ 05/27/23 12:17:23.038
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:17:23.06
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:17:23.065
  STEP: Creating secret with name secret-test-dd339623-1ff6-42f4-9edb-05c3856a8f5f @ 05/27/23 12:17:23.07
  STEP: Creating a pod to test consume secrets @ 05/27/23 12:17:23.076
  STEP: Saw pod success @ 05/27/23 12:17:27.107
  May 27 12:17:27.110: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-secrets-675f4243-3fed-47f7-9c80-4d3cbae0fed4 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/27/23 12:17:27.119
  May 27 12:17:27.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3065" for this suite. @ 05/27/23 12:17:27.141
• [4.113 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 05/27/23 12:17:27.15
  May 27 12:17:27.150: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 12:17:27.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:17:27.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:17:27.177
  STEP: Creating configMap with name cm-test-opt-del-8832ad4d-b45b-41cd-87db-6fc570433f02 @ 05/27/23 12:17:27.187
  STEP: Creating configMap with name cm-test-opt-upd-4e3488b2-08b8-4bc4-8d6a-40f880328d44 @ 05/27/23 12:17:27.193
  STEP: Creating the pod @ 05/27/23 12:17:27.201
  STEP: Deleting configmap cm-test-opt-del-8832ad4d-b45b-41cd-87db-6fc570433f02 @ 05/27/23 12:17:29.256
  STEP: Updating configmap cm-test-opt-upd-4e3488b2-08b8-4bc4-8d6a-40f880328d44 @ 05/27/23 12:17:29.263
  STEP: Creating configMap with name cm-test-opt-create-0e8c3376-6222-4f98-8fc1-378e7c5f1803 @ 05/27/23 12:17:29.269
  STEP: waiting to observe update in volume @ 05/27/23 12:17:29.275
  May 27 12:18:59.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2694" for this suite. @ 05/27/23 12:18:59.788
• [92.646 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 05/27/23 12:18:59.797
  May 27 12:18:59.797: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir @ 05/27/23 12:18:59.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:18:59.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:18:59.824
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/27/23 12:18:59.829
  STEP: Saw pod success @ 05/27/23 12:19:03.858
  May 27 12:19:03.862: INFO: Trying to get logs from node ip-172-31-10-136 pod pod-60451e45-80dd-44d0-9a95-b8228ce04560 container test-container: <nil>
  STEP: delete the pod @ 05/27/23 12:19:03.895
  May 27 12:19:03.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3201" for this suite. @ 05/27/23 12:19:03.929
• [4.141 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 05/27/23 12:19:03.941
  May 27 12:19:03.941: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename proxy @ 05/27/23 12:19:03.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:19:03.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:19:03.989
  May 27 12:19:03.994: INFO: Creating pod...
  May 27 12:19:06.025: INFO: Creating service...
  May 27 12:19:06.039: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1769/pods/agnhost/proxy/some/path/with/DELETE
  May 27 12:19:06.052: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 27 12:19:06.052: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1769/pods/agnhost/proxy/some/path/with/GET
  May 27 12:19:06.059: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May 27 12:19:06.059: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1769/pods/agnhost/proxy/some/path/with/HEAD
  May 27 12:19:06.065: INFO: http.Client request:HEAD | StatusCode:200
  May 27 12:19:06.065: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1769/pods/agnhost/proxy/some/path/with/OPTIONS
  May 27 12:19:06.070: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 27 12:19:06.070: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1769/pods/agnhost/proxy/some/path/with/PATCH
  May 27 12:19:06.075: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 27 12:19:06.075: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1769/pods/agnhost/proxy/some/path/with/POST
  May 27 12:19:06.082: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 27 12:19:06.082: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1769/pods/agnhost/proxy/some/path/with/PUT
  May 27 12:19:06.088: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 27 12:19:06.088: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1769/services/test-service/proxy/some/path/with/DELETE
  May 27 12:19:06.095: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 27 12:19:06.095: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1769/services/test-service/proxy/some/path/with/GET
  May 27 12:19:06.104: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May 27 12:19:06.104: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1769/services/test-service/proxy/some/path/with/HEAD
  May 27 12:19:06.111: INFO: http.Client request:HEAD | StatusCode:200
  May 27 12:19:06.111: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1769/services/test-service/proxy/some/path/with/OPTIONS
  May 27 12:19:06.117: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 27 12:19:06.117: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1769/services/test-service/proxy/some/path/with/PATCH
  May 27 12:19:06.126: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 27 12:19:06.126: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1769/services/test-service/proxy/some/path/with/POST
  May 27 12:19:06.133: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 27 12:19:06.133: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1769/services/test-service/proxy/some/path/with/PUT
  May 27 12:19:06.140: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 27 12:19:06.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-1769" for this suite. @ 05/27/23 12:19:06.146
• [2.215 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 05/27/23 12:19:06.156
  May 27 12:19:06.156: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 12:19:06.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:19:06.181
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:19:06.188
  STEP: Setting up server cert @ 05/27/23 12:19:06.219
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 12:19:06.958
  STEP: Deploying the webhook pod @ 05/27/23 12:19:06.971
  STEP: Wait for the deployment to be ready @ 05/27/23 12:19:06.989
  May 27 12:19:06.997: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/27/23 12:19:09.015
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 12:19:09.027
  May 27 12:19:10.028: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 27 12:19:10.032: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9275-crds.webhook.example.com via the AdmissionRegistration API @ 05/27/23 12:19:10.547
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/27/23 12:19:10.569
  May 27 12:19:12.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5709" for this suite. @ 05/27/23 12:19:13.176
  STEP: Destroying namespace "webhook-markers-1025" for this suite. @ 05/27/23 12:19:13.186
• [7.036 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 05/27/23 12:19:13.192
  May 27 12:19:13.192: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 12:19:13.193
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:19:13.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:19:13.217
  STEP: Setting up server cert @ 05/27/23 12:19:13.248
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 12:19:13.53
  STEP: Deploying the webhook pod @ 05/27/23 12:19:13.538
  STEP: Wait for the deployment to be ready @ 05/27/23 12:19:13.554
  May 27 12:19:13.569: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/27/23 12:19:15.582
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 12:19:15.592
  May 27 12:19:16.593: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 05/27/23 12:19:16.597
  STEP: create a configmap that should be updated by the webhook @ 05/27/23 12:19:16.621
  May 27 12:19:16.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6246" for this suite. @ 05/27/23 12:19:16.714
  STEP: Destroying namespace "webhook-markers-5421" for this suite. @ 05/27/23 12:19:16.722
• [3.539 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 05/27/23 12:19:16.733
  May 27 12:19:16.733: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 12:19:16.734
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:19:16.754
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:19:16.76
  STEP: Creating configMap with name projected-configmap-test-volume-e64fc380-78de-4c1a-9808-55d4d08a9e01 @ 05/27/23 12:19:16.766
  STEP: Creating a pod to test consume configMaps @ 05/27/23 12:19:16.775
  STEP: Saw pod success @ 05/27/23 12:19:20.813
  May 27 12:19:20.819: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-projected-configmaps-fb2fc828-b3d1-41d2-86a4-e7552b47e47f container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 05/27/23 12:19:20.827
  May 27 12:19:20.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-953" for this suite. @ 05/27/23 12:19:20.855
• [4.131 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 05/27/23 12:19:20.865
  May 27 12:19:20.866: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 12:19:20.867
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:19:20.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:19:20.898
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-5a175b5f-81e0-4edd-8744-80cf170b6e72 @ 05/27/23 12:19:20.908
  STEP: Creating the pod @ 05/27/23 12:19:20.915
  STEP: Updating configmap projected-configmap-test-upd-5a175b5f-81e0-4edd-8744-80cf170b6e72 @ 05/27/23 12:19:22.951
  STEP: waiting to observe update in volume @ 05/27/23 12:19:22.96
  May 27 12:20:49.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9477" for this suite. @ 05/27/23 12:20:49.429
• [88.575 seconds]
------------------------------
S
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 05/27/23 12:20:49.441
  May 27 12:20:49.441: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename csistoragecapacity @ 05/27/23 12:20:49.442
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:20:49.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:20:49.467
  STEP: getting /apis @ 05/27/23 12:20:49.471
  STEP: getting /apis/storage.k8s.io @ 05/27/23 12:20:49.476
  STEP: getting /apis/storage.k8s.io/v1 @ 05/27/23 12:20:49.477
  STEP: creating @ 05/27/23 12:20:49.479
  STEP: watching @ 05/27/23 12:20:49.501
  May 27 12:20:49.501: INFO: starting watch
  STEP: getting @ 05/27/23 12:20:49.511
  STEP: listing in namespace @ 05/27/23 12:20:49.514
  STEP: listing across namespaces @ 05/27/23 12:20:49.519
  STEP: patching @ 05/27/23 12:20:49.524
  STEP: updating @ 05/27/23 12:20:49.531
  May 27 12:20:49.538: INFO: waiting for watch events with expected annotations in namespace
  May 27 12:20:49.538: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 05/27/23 12:20:49.539
  STEP: deleting a collection @ 05/27/23 12:20:49.555
  May 27 12:20:49.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-5926" for this suite. @ 05/27/23 12:20:49.583
• [0.153 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 05/27/23 12:20:49.595
  May 27 12:20:49.595: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir @ 05/27/23 12:20:49.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:20:49.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:20:49.619
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/27/23 12:20:49.623
  STEP: Saw pod success @ 05/27/23 12:20:53.658
  May 27 12:20:53.661: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-22419200-7452-4bec-bf13-b321ebf302f2 container test-container: <nil>
  STEP: delete the pod @ 05/27/23 12:20:53.669
  May 27 12:20:53.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7791" for this suite. @ 05/27/23 12:20:53.697
• [4.114 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 05/27/23 12:20:53.711
  May 27 12:20:53.711: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename var-expansion @ 05/27/23 12:20:53.712
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:20:53.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:20:53.737
  STEP: Creating a pod to test substitution in container's args @ 05/27/23 12:20:53.742
  STEP: Saw pod success @ 05/27/23 12:20:57.774
  May 27 12:20:57.778: INFO: Trying to get logs from node ip-172-31-68-172 pod var-expansion-60aa391a-f74b-4093-9151-c7bb595fd775 container dapi-container: <nil>
  STEP: delete the pod @ 05/27/23 12:20:57.787
  May 27 12:20:57.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6423" for this suite. @ 05/27/23 12:20:57.811
• [4.110 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 05/27/23 12:20:57.822
  May 27 12:20:57.822: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename subpath @ 05/27/23 12:20:57.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:20:57.843
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:20:57.849
  STEP: Setting up data @ 05/27/23 12:20:57.853
  STEP: Creating pod pod-subpath-test-secret-s2w5 @ 05/27/23 12:20:57.865
  STEP: Creating a pod to test atomic-volume-subpath @ 05/27/23 12:20:57.865
  STEP: Saw pod success @ 05/27/23 12:21:21.951
  May 27 12:21:21.956: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-subpath-test-secret-s2w5 container test-container-subpath-secret-s2w5: <nil>
  STEP: delete the pod @ 05/27/23 12:21:21.968
  STEP: Deleting pod pod-subpath-test-secret-s2w5 @ 05/27/23 12:21:21.987
  May 27 12:21:21.987: INFO: Deleting pod "pod-subpath-test-secret-s2w5" in namespace "subpath-2749"
  May 27 12:21:21.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2749" for this suite. @ 05/27/23 12:21:21.996
• [24.183 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 05/27/23 12:21:22.007
  May 27 12:21:22.007: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl-logs @ 05/27/23 12:21:22.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:21:22.025
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:21:22.032
  STEP: creating an pod @ 05/27/23 12:21:22.036
  May 27 12:21:22.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-logs-4133 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  May 27 12:21:22.139: INFO: stderr: ""
  May 27 12:21:22.139: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 05/27/23 12:21:22.139
  May 27 12:21:22.139: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  May 27 12:21:24.150: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 05/27/23 12:21:24.15
  May 27 12:21:24.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-logs-4133 logs logs-generator logs-generator'
  May 27 12:21:24.256: INFO: stderr: ""
  May 27 12:21:24.256: INFO: stdout: "I0527 12:21:22.920399       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/vcwq 509\nI0527 12:21:23.120657       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/458d 582\nI0527 12:21:23.321306       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/c75 386\nI0527 12:21:23.520502       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/wzj 433\nI0527 12:21:23.720835       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/ss6x 417\nI0527 12:21:23.921146       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/mzzg 452\nI0527 12:21:24.120677       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/975 327\n"
  STEP: limiting log lines @ 05/27/23 12:21:24.256
  May 27 12:21:24.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-logs-4133 logs logs-generator logs-generator --tail=1'
  May 27 12:21:24.347: INFO: stderr: ""
  May 27 12:21:24.347: INFO: stdout: "I0527 12:21:24.320992       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/wss 401\n"
  May 27 12:21:24.347: INFO: got output "I0527 12:21:24.320992       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/wss 401\n"
  STEP: limiting log bytes @ 05/27/23 12:21:24.347
  May 27 12:21:24.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-logs-4133 logs logs-generator logs-generator --limit-bytes=1'
  May 27 12:21:24.438: INFO: stderr: ""
  May 27 12:21:24.438: INFO: stdout: "I"
  May 27 12:21:24.438: INFO: got output "I"
  STEP: exposing timestamps @ 05/27/23 12:21:24.438
  May 27 12:21:24.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-logs-4133 logs logs-generator logs-generator --tail=1 --timestamps'
  May 27 12:21:24.534: INFO: stderr: ""
  May 27 12:21:24.534: INFO: stdout: "2023-05-27T12:21:24.521427317Z I0527 12:21:24.521317       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/pcc 365\n"
  May 27 12:21:24.534: INFO: got output "2023-05-27T12:21:24.521427317Z I0527 12:21:24.521317       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/pcc 365\n"
  STEP: restricting to a time range @ 05/27/23 12:21:24.534
  May 27 12:21:27.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-logs-4133 logs logs-generator logs-generator --since=1s'
  May 27 12:21:27.125: INFO: stderr: ""
  May 27 12:21:27.125: INFO: stdout: "I0527 12:21:26.320698       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/5dqh 556\nI0527 12:21:26.521031       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/f9sq 396\nI0527 12:21:26.721252       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/5n7 542\nI0527 12:21:26.920721       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/vcwf 508\nI0527 12:21:27.121080       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/rjbg 406\n"
  May 27 12:21:27.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-logs-4133 logs logs-generator logs-generator --since=24h'
  May 27 12:21:27.215: INFO: stderr: ""
  May 27 12:21:27.215: INFO: stdout: "I0527 12:21:22.920399       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/vcwq 509\nI0527 12:21:23.120657       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/458d 582\nI0527 12:21:23.321306       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/c75 386\nI0527 12:21:23.520502       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/wzj 433\nI0527 12:21:23.720835       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/ss6x 417\nI0527 12:21:23.921146       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/mzzg 452\nI0527 12:21:24.120677       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/975 327\nI0527 12:21:24.320992       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/wss 401\nI0527 12:21:24.521317       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/pcc 365\nI0527 12:21:24.720721       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/bnp 469\nI0527 12:21:24.921073       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/2w4q 429\nI0527 12:21:25.121447       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/tvr2 598\nI0527 12:21:25.320742       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/wlfb 383\nI0527 12:21:25.521090       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/k6d 438\nI0527 12:21:25.721469       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/msr 207\nI0527 12:21:25.920746       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/kswq 248\nI0527 12:21:26.121099       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/q7wc 271\nI0527 12:21:26.320698       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/5dqh 556\nI0527 12:21:26.521031       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/f9sq 396\nI0527 12:21:26.721252       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/5n7 542\nI0527 12:21:26.920721       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/vcwf 508\nI0527 12:21:27.121080       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/rjbg 406\n"
  May 27 12:21:27.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-logs-4133 delete pod logs-generator'
  May 27 12:21:28.230: INFO: stderr: ""
  May 27 12:21:28.230: INFO: stdout: "pod \"logs-generator\" deleted\n"
  May 27 12:21:28.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-4133" for this suite. @ 05/27/23 12:21:28.236
• [6.237 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 05/27/23 12:21:28.244
  May 27 12:21:28.244: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename var-expansion @ 05/27/23 12:21:28.245
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:21:28.266
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:21:28.271
  STEP: Creating a pod to test env composition @ 05/27/23 12:21:28.276
  STEP: Saw pod success @ 05/27/23 12:21:32.308
  May 27 12:21:32.312: INFO: Trying to get logs from node ip-172-31-68-172 pod var-expansion-9d565881-1eb1-40b4-90e3-d5017f12e521 container dapi-container: <nil>
  STEP: delete the pod @ 05/27/23 12:21:32.322
  May 27 12:21:32.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1472" for this suite. @ 05/27/23 12:21:32.348
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 05/27/23 12:21:32.359
  May 27 12:21:32.359: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename conformance-tests @ 05/27/23 12:21:32.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:21:32.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:21:32.382
  STEP: Getting node addresses @ 05/27/23 12:21:32.387
  May 27 12:21:32.387: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  May 27 12:21:32.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-9711" for this suite. @ 05/27/23 12:21:32.399
• [0.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 05/27/23 12:21:32.411
  May 27 12:21:32.411: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename taint-multiple-pods @ 05/27/23 12:21:32.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:21:32.43
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:21:32.435
  May 27 12:21:32.440: INFO: Waiting up to 1m0s for all nodes to be ready
  May 27 12:22:32.458: INFO: Waiting for terminating namespaces to be deleted...
  May 27 12:22:32.463: INFO: Starting informer...
  STEP: Starting pods... @ 05/27/23 12:22:32.463
  May 27 12:22:32.686: INFO: Pod1 is running on ip-172-31-68-172. Tainting Node
  May 27 12:22:34.912: INFO: Pod2 is running on ip-172-31-68-172. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/27/23 12:22:34.912
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/27/23 12:22:34.931
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 05/27/23 12:22:34.943
  May 27 12:22:40.723: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  May 27 12:23:00.760: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  May 27 12:23:00.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/27/23 12:23:00.78
  STEP: Destroying namespace "taint-multiple-pods-5199" for this suite. @ 05/27/23 12:23:00.785
• [88.388 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 05/27/23 12:23:00.8
  May 27 12:23:00.800: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/27/23 12:23:00.801
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:23:00.828
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:23:00.833
  STEP: set up a multi version CRD @ 05/27/23 12:23:00.837
  May 27 12:23:00.838: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: rename a version @ 05/27/23 12:23:04.593
  STEP: check the new version name is served @ 05/27/23 12:23:04.613
  STEP: check the old version name is removed @ 05/27/23 12:23:06.114
  STEP: check the other version is not changed @ 05/27/23 12:23:06.858
  May 27 12:23:09.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1966" for this suite. @ 05/27/23 12:23:09.784
• [8.991 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 05/27/23 12:23:09.792
  May 27 12:23:09.792: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename deployment @ 05/27/23 12:23:09.793
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:23:09.81
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:23:09.814
  May 27 12:23:09.818: INFO: Creating simple deployment test-new-deployment
  May 27 12:23:09.835: INFO: deployment "test-new-deployment" doesn't have the required revision set
  May 27 12:23:11.846: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 27, 12, 23, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 23, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 12, 23, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 23, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-67bd4bf6dc\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 12:23:13.853: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 27, 12, 23, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 23, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 12, 23, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 23, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-67bd4bf6dc\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: getting scale subresource @ 05/27/23 12:23:15.855
  STEP: updating a scale subresource @ 05/27/23 12:23:15.86
  STEP: verifying the deployment Spec.Replicas was modified @ 05/27/23 12:23:15.868
  STEP: Patch a scale subresource @ 05/27/23 12:23:15.871
  May 27 12:23:15.888: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-8799  83c05aed-4c0a-4d4b-a012-f53a996d7d9c 5762 3 2023-05-27 12:23:09 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-27 12:23:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 12:23:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0020a2fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-27 12:23:14 +0000 UTC,LastTransitionTime:2023-05-27 12:23:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-05-27 12:23:14 +0000 UTC,LastTransitionTime:2023-05-27 12:23:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 27 12:23:15.895: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-8799  97ab45fa-a568-4de4-82fa-2e4b80f1d529 5761 2 2023-05-27 12:23:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 83c05aed-4c0a-4d4b-a012-f53a996d7d9c 0xc0020a3400 0xc0020a3401}] [] [{kube-controller-manager Update apps/v1 2023-05-27 12:23:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-27 12:23:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83c05aed-4c0a-4d4b-a012-f53a996d7d9c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0020a3488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 27 12:23:15.900: INFO: Pod "test-new-deployment-67bd4bf6dc-h7h6d" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-h7h6d test-new-deployment-67bd4bf6dc- deployment-8799  57f26fb0-3e93-4109-bed3-4e3d7b7e5090 5749 0 2023-05-27 12:23:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 97ab45fa-a568-4de4-82fa-2e4b80f1d529 0xc0034fb590 0xc0034fb591}] [] [{kube-controller-manager Update v1 2023-05-27 12:23:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"97ab45fa-a568-4de4-82fa-2e4b80f1d529\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:23:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.19.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t7jbc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t7jbc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:23:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:23:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:23:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:23:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.172,PodIP:192.168.19.88,StartTime:2023-05-27 12:23:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-27 12:23:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ce42f9530086000eaf28ef3b4521db990dad6ca461b86127caa0c525b095d996,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.19.88,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:23:15.901: INFO: Pod "test-new-deployment-67bd4bf6dc-pctxk" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-pctxk test-new-deployment-67bd4bf6dc- deployment-8799  a4712b49-c3fb-44a4-bba4-717b9ee05259 5766 0 2023-05-27 12:23:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 97ab45fa-a568-4de4-82fa-2e4b80f1d529 0xc0034fb777 0xc0034fb778}] [] [{kube-controller-manager Update v1 2023-05-27 12:23:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"97ab45fa-a568-4de4-82fa-2e4b80f1d529\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rqw5t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rqw5t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:23:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:23:15.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8799" for this suite. @ 05/27/23 12:23:15.906
• [6.122 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 05/27/23 12:23:15.915
  May 27 12:23:15.915: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename gc @ 05/27/23 12:23:15.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:23:15.945
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:23:15.948
  STEP: create the rc @ 05/27/23 12:23:15.952
  W0527 12:23:15.958880      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 05/27/23 12:23:20.964
  STEP: wait for all pods to be garbage collected @ 05/27/23 12:23:20.973
  STEP: Gathering metrics @ 05/27/23 12:23:25.984
  W0527 12:23:25.989113      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May 27 12:23:25.989: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 27 12:23:25.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-9928" for this suite. @ 05/27/23 12:23:25.994
• [10.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 05/27/23 12:23:26.01
  May 27 12:23:26.010: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename configmap @ 05/27/23 12:23:26.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:23:26.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:23:26.035
  STEP: creating a ConfigMap @ 05/27/23 12:23:26.04
  STEP: fetching the ConfigMap @ 05/27/23 12:23:26.046
  STEP: patching the ConfigMap @ 05/27/23 12:23:26.051
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 05/27/23 12:23:26.056
  STEP: deleting the ConfigMap by collection with a label selector @ 05/27/23 12:23:26.062
  STEP: listing all ConfigMaps in test namespace @ 05/27/23 12:23:26.071
  May 27 12:23:26.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2113" for this suite. @ 05/27/23 12:23:26.08
• [0.078 seconds]
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 05/27/23 12:23:26.089
  May 27 12:23:26.089: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename configmap @ 05/27/23 12:23:26.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:23:26.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:23:26.116
  STEP: Creating configMap with name configmap-test-upd-28c4bdf1-1770-4f1b-8fb5-6a638d1686fd @ 05/27/23 12:23:26.125
  STEP: Creating the pod @ 05/27/23 12:23:26.13
  STEP: Updating configmap configmap-test-upd-28c4bdf1-1770-4f1b-8fb5-6a638d1686fd @ 05/27/23 12:23:28.18
  STEP: waiting to observe update in volume @ 05/27/23 12:23:28.189
  May 27 12:24:46.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5915" for this suite. @ 05/27/23 12:24:46.595
• [80.514 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 05/27/23 12:24:46.628
  May 27 12:24:46.628: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 12:24:46.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:24:46.653
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:24:46.656
  STEP: Creating configMap with name projected-configmap-test-volume-map-003ee0c6-5d64-4bfe-9bac-734241efbb82 @ 05/27/23 12:24:46.661
  STEP: Creating a pod to test consume configMaps @ 05/27/23 12:24:46.673
  STEP: Saw pod success @ 05/27/23 12:24:50.703
  May 27 12:24:50.707: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-projected-configmaps-d802fae7-1242-48ae-af5d-3446a33d1918 container agnhost-container: <nil>
  STEP: delete the pod @ 05/27/23 12:24:50.717
  May 27 12:24:50.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4821" for this suite. @ 05/27/23 12:24:50.743
• [4.125 seconds]
------------------------------
SSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 05/27/23 12:24:50.754
  May 27 12:24:50.754: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename containers @ 05/27/23 12:24:50.755
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:24:50.776
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:24:50.78
  STEP: Creating a pod to test override arguments @ 05/27/23 12:24:50.783
  STEP: Saw pod success @ 05/27/23 12:24:54.808
  May 27 12:24:54.812: INFO: Trying to get logs from node ip-172-31-68-172 pod client-containers-3bc66b0a-d1cf-4000-92e4-6ff1a873e096 container agnhost-container: <nil>
  STEP: delete the pod @ 05/27/23 12:24:54.819
  May 27 12:24:54.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-4602" for this suite. @ 05/27/23 12:24:54.844
• [4.098 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 05/27/23 12:24:54.852
  May 27 12:24:54.852: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename cronjob @ 05/27/23 12:24:54.853
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:24:54.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:24:54.884
  STEP: Creating a ForbidConcurrent cronjob @ 05/27/23 12:24:54.888
  STEP: Ensuring a job is scheduled @ 05/27/23 12:24:54.896
  STEP: Ensuring exactly one is scheduled @ 05/27/23 12:25:00.902
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/27/23 12:25:00.906
  STEP: Ensuring no more jobs are scheduled @ 05/27/23 12:25:00.91
  STEP: Removing cronjob @ 05/27/23 12:30:00.918
  May 27 12:30:00.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1624" for this suite. @ 05/27/23 12:30:00.93
• [306.087 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 05/27/23 12:30:00.94
  May 27 12:30:00.940: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename disruption @ 05/27/23 12:30:00.941
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:30:00.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:30:00.974
  STEP: Creating a pdb that targets all three pods in a test replica set @ 05/27/23 12:30:00.977
  STEP: Waiting for the pdb to be processed @ 05/27/23 12:30:00.984
  STEP: First trying to evict a pod which shouldn't be evictable @ 05/27/23 12:30:03
  STEP: Waiting for all pods to be running @ 05/27/23 12:30:03
  May 27 12:30:03.004: INFO: pods: 0 < 3
  May 27 12:30:05.011: INFO: running pods: 2 < 3
  STEP: locating a running pod @ 05/27/23 12:30:07.009
  STEP: Updating the pdb to allow a pod to be evicted @ 05/27/23 12:30:07.02
  STEP: Waiting for the pdb to be processed @ 05/27/23 12:30:07.032
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/27/23 12:30:09.04
  STEP: Waiting for all pods to be running @ 05/27/23 12:30:09.041
  STEP: Waiting for the pdb to observed all healthy pods @ 05/27/23 12:30:09.045
  STEP: Patching the pdb to disallow a pod to be evicted @ 05/27/23 12:30:09.073
  STEP: Waiting for the pdb to be processed @ 05/27/23 12:30:09.092
  STEP: Waiting for all pods to be running @ 05/27/23 12:30:11.101
  STEP: locating a running pod @ 05/27/23 12:30:11.107
  STEP: Deleting the pdb to allow a pod to be evicted @ 05/27/23 12:30:11.117
  STEP: Waiting for the pdb to be deleted @ 05/27/23 12:30:11.124
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/27/23 12:30:11.127
  STEP: Waiting for all pods to be running @ 05/27/23 12:30:11.127
  May 27 12:30:11.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-6104" for this suite. @ 05/27/23 12:30:11.161
• [10.234 seconds]
------------------------------
S
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 05/27/23 12:30:11.174
  May 27 12:30:11.175: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename services @ 05/27/23 12:30:11.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:30:11.198
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:30:11.202
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-744 @ 05/27/23 12:30:11.208
  STEP: changing the ExternalName service to type=ClusterIP @ 05/27/23 12:30:11.216
  STEP: creating replication controller externalname-service in namespace services-744 @ 05/27/23 12:30:11.235
  I0527 12:30:11.243438      18 runners.go:194] Created replication controller with name: externalname-service, namespace: services-744, replica count: 2
  I0527 12:30:14.294219      18 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 27 12:30:14.294: INFO: Creating new exec pod
  May 27 12:30:17.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-744 exec execpodtcmlw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 27 12:30:17.495: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 27 12:30:17.495: INFO: stdout: "externalname-service-bt46d"
  May 27 12:30:17.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-744 exec execpodtcmlw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.33 80'
  May 27 12:30:17.649: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.33 80\nConnection to 10.152.183.33 80 port [tcp/http] succeeded!\n"
  May 27 12:30:17.649: INFO: stdout: "externalname-service-c2j4l"
  May 27 12:30:17.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 27 12:30:17.653: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-744" for this suite. @ 05/27/23 12:30:17.673
• [6.507 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 05/27/23 12:30:17.682
  May 27 12:30:17.682: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename events @ 05/27/23 12:30:17.683
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:30:17.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:30:17.719
  STEP: Create set of events @ 05/27/23 12:30:17.723
  May 27 12:30:17.733: INFO: created test-event-1
  May 27 12:30:17.741: INFO: created test-event-2
  May 27 12:30:17.746: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 05/27/23 12:30:17.746
  STEP: delete collection of events @ 05/27/23 12:30:17.75
  May 27 12:30:17.750: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/27/23 12:30:17.777
  May 27 12:30:17.777: INFO: requesting list of events to confirm quantity
  May 27 12:30:17.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-2158" for this suite. @ 05/27/23 12:30:17.786
• [0.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 05/27/23 12:30:17.798
  May 27 12:30:17.798: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 12:30:17.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:30:17.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:30:17.825
  STEP: Creating configMap with name projected-configmap-test-volume-map-07498bd1-c049-4179-8e84-40c4be40eef2 @ 05/27/23 12:30:17.83
  STEP: Creating a pod to test consume configMaps @ 05/27/23 12:30:17.836
  STEP: Saw pod success @ 05/27/23 12:30:21.864
  May 27 12:30:21.868: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-projected-configmaps-3be8e6b2-6274-4da8-9e74-8eece38d447b container agnhost-container: <nil>
  STEP: delete the pod @ 05/27/23 12:30:21.893
  May 27 12:30:21.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5878" for this suite. @ 05/27/23 12:30:21.912
• [4.123 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 05/27/23 12:30:21.921
  May 27 12:30:21.921: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename downward-api @ 05/27/23 12:30:21.922
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:30:21.945
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:30:21.949
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 12:30:21.953
  STEP: Saw pod success @ 05/27/23 12:30:25.977
  May 27 12:30:25.983: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-f56dae46-468f-4da8-bd47-0ff86d952410 container client-container: <nil>
  STEP: delete the pod @ 05/27/23 12:30:25.992
  May 27 12:30:26.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-528" for this suite. @ 05/27/23 12:30:26.018
• [4.105 seconds]
------------------------------
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 05/27/23 12:30:26.026
  May 27 12:30:26.026: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename certificates @ 05/27/23 12:30:26.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:30:26.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:30:26.052
  STEP: getting /apis @ 05/27/23 12:30:26.531
  STEP: getting /apis/certificates.k8s.io @ 05/27/23 12:30:26.536
  STEP: getting /apis/certificates.k8s.io/v1 @ 05/27/23 12:30:26.537
  STEP: creating @ 05/27/23 12:30:26.539
  STEP: getting @ 05/27/23 12:30:26.56
  STEP: listing @ 05/27/23 12:30:26.563
  STEP: watching @ 05/27/23 12:30:26.568
  May 27 12:30:26.568: INFO: starting watch
  STEP: patching @ 05/27/23 12:30:26.569
  STEP: updating @ 05/27/23 12:30:26.577
  May 27 12:30:26.583: INFO: waiting for watch events with expected annotations
  May 27 12:30:26.583: INFO: saw patched and updated annotations
  STEP: getting /approval @ 05/27/23 12:30:26.583
  STEP: patching /approval @ 05/27/23 12:30:26.588
  STEP: updating /approval @ 05/27/23 12:30:26.596
  STEP: getting /status @ 05/27/23 12:30:26.603
  STEP: patching /status @ 05/27/23 12:30:26.608
  STEP: updating /status @ 05/27/23 12:30:26.617
  STEP: deleting @ 05/27/23 12:30:26.625
  STEP: deleting a collection @ 05/27/23 12:30:26.641
  May 27 12:30:26.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-3279" for this suite. @ 05/27/23 12:30:26.663
• [0.650 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 05/27/23 12:30:26.683
  May 27 12:30:26.683: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename disruption @ 05/27/23 12:30:26.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:30:26.706
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:30:26.71
  STEP: Creating a kubernetes client @ 05/27/23 12:30:26.714
  May 27 12:30:26.714: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename disruption-2 @ 05/27/23 12:30:26.715
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:30:26.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:30:26.739
  STEP: Waiting for the pdb to be processed @ 05/27/23 12:30:26.748
  STEP: Waiting for the pdb to be processed @ 05/27/23 12:30:28.763
  STEP: Waiting for the pdb to be processed @ 05/27/23 12:30:30.779
  STEP: listing a collection of PDBs across all namespaces @ 05/27/23 12:30:32.788
  STEP: listing a collection of PDBs in namespace disruption-208 @ 05/27/23 12:30:32.793
  STEP: deleting a collection of PDBs @ 05/27/23 12:30:32.796
  STEP: Waiting for the PDB collection to be deleted @ 05/27/23 12:30:32.811
  May 27 12:30:32.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 27 12:30:32.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-4885" for this suite. @ 05/27/23 12:30:32.825
  STEP: Destroying namespace "disruption-208" for this suite. @ 05/27/23 12:30:32.833
• [6.157 seconds]
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 05/27/23 12:30:32.841
  May 27 12:30:32.841: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/27/23 12:30:32.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:30:32.862
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:30:32.867
  STEP: create the container to handle the HTTPGet hook request. @ 05/27/23 12:30:32.879
  STEP: create the pod with lifecycle hook @ 05/27/23 12:30:34.904
  STEP: delete the pod with lifecycle hook @ 05/27/23 12:30:36.925
  STEP: check prestop hook @ 05/27/23 12:30:38.941
  May 27 12:30:38.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1863" for this suite. @ 05/27/23 12:30:38.952
• [6.117 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 05/27/23 12:30:38.958
  May 27 12:30:38.958: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename resourcequota @ 05/27/23 12:30:38.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:30:38.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:30:38.991
  STEP: Creating resourceQuota "e2e-rq-status-mj7dp" @ 05/27/23 12:30:38.999
  May 27 12:30:39.009: INFO: Resource quota "e2e-rq-status-mj7dp" reports spec: hard cpu limit of 500m
  May 27 12:30:39.010: INFO: Resource quota "e2e-rq-status-mj7dp" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-mj7dp" /status @ 05/27/23 12:30:39.01
  STEP: Confirm /status for "e2e-rq-status-mj7dp" resourceQuota via watch @ 05/27/23 12:30:39.02
  May 27 12:30:39.022: INFO: observed resourceQuota "e2e-rq-status-mj7dp" in namespace "resourcequota-5609" with hard status: v1.ResourceList(nil)
  May 27 12:30:39.023: INFO: Found resourceQuota "e2e-rq-status-mj7dp" in namespace "resourcequota-5609" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May 27 12:30:39.023: INFO: ResourceQuota "e2e-rq-status-mj7dp" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 05/27/23 12:30:39.026
  May 27 12:30:39.033: INFO: Resource quota "e2e-rq-status-mj7dp" reports spec: hard cpu limit of 1
  May 27 12:30:39.033: INFO: Resource quota "e2e-rq-status-mj7dp" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-mj7dp" /status @ 05/27/23 12:30:39.033
  STEP: Confirm /status for "e2e-rq-status-mj7dp" resourceQuota via watch @ 05/27/23 12:30:39.042
  May 27 12:30:39.044: INFO: observed resourceQuota "e2e-rq-status-mj7dp" in namespace "resourcequota-5609" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May 27 12:30:39.044: INFO: Found resourceQuota "e2e-rq-status-mj7dp" in namespace "resourcequota-5609" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  May 27 12:30:39.044: INFO: ResourceQuota "e2e-rq-status-mj7dp" /status was patched
  STEP: Get "e2e-rq-status-mj7dp" /status @ 05/27/23 12:30:39.044
  May 27 12:30:39.048: INFO: Resourcequota "e2e-rq-status-mj7dp" reports status: hard cpu of 1
  May 27 12:30:39.048: INFO: Resourcequota "e2e-rq-status-mj7dp" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-mj7dp" /status before checking Spec is unchanged @ 05/27/23 12:30:39.052
  May 27 12:30:39.058: INFO: Resourcequota "e2e-rq-status-mj7dp" reports status: hard cpu of 2
  May 27 12:30:39.058: INFO: Resourcequota "e2e-rq-status-mj7dp" reports status: hard memory of 2Gi
  May 27 12:30:39.060: INFO: Found resourceQuota "e2e-rq-status-mj7dp" in namespace "resourcequota-5609" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  May 27 12:32:14.072: INFO: ResourceQuota "e2e-rq-status-mj7dp" Spec was unchanged and /status reset
  May 27 12:32:14.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5609" for this suite. @ 05/27/23 12:32:14.079
• [95.131 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 05/27/23 12:32:14.092
  May 27 12:32:14.092: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename replication-controller @ 05/27/23 12:32:14.093
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:32:14.119
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:32:14.126
  STEP: Creating ReplicationController "e2e-rc-rbn4f" @ 05/27/23 12:32:14.132
  May 27 12:32:14.138: INFO: Get Replication Controller "e2e-rc-rbn4f" to confirm replicas
  May 27 12:32:15.147: INFO: Get Replication Controller "e2e-rc-rbn4f" to confirm replicas
  May 27 12:32:15.153: INFO: Found 1 replicas for "e2e-rc-rbn4f" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-rbn4f" @ 05/27/23 12:32:15.154
  STEP: Updating a scale subresource @ 05/27/23 12:32:15.161
  STEP: Verifying replicas where modified for replication controller "e2e-rc-rbn4f" @ 05/27/23 12:32:15.172
  May 27 12:32:15.172: INFO: Get Replication Controller "e2e-rc-rbn4f" to confirm replicas
  May 27 12:32:16.178: INFO: Get Replication Controller "e2e-rc-rbn4f" to confirm replicas
  May 27 12:32:16.183: INFO: Found 2 replicas for "e2e-rc-rbn4f" replication controller
  May 27 12:32:16.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5774" for this suite. @ 05/27/23 12:32:16.192
• [2.111 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 05/27/23 12:32:16.204
  May 27 12:32:16.204: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/27/23 12:32:16.205
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:32:16.224
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:32:16.229
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 05/27/23 12:32:16.235
  May 27 12:32:16.236: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:32:17.962: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:32:24.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6401" for this suite. @ 05/27/23 12:32:24.288
• [8.093 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 05/27/23 12:32:24.298
  May 27 12:32:24.298: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename downward-api @ 05/27/23 12:32:24.299
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:32:24.318
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:32:24.324
  STEP: Creating a pod to test downward api env vars @ 05/27/23 12:32:24.33
  STEP: Saw pod success @ 05/27/23 12:32:28.355
  May 27 12:32:28.361: INFO: Trying to get logs from node ip-172-31-68-172 pod downward-api-60a1a0d0-fc5a-4708-8a11-f7d13194067a container dapi-container: <nil>
  STEP: delete the pod @ 05/27/23 12:32:28.38
  May 27 12:32:28.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7073" for this suite. @ 05/27/23 12:32:28.403
• [4.115 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 05/27/23 12:32:28.414
  May 27 12:32:28.414: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename dns @ 05/27/23 12:32:28.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:32:28.432
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:32:28.441
  STEP: Creating a test headless service @ 05/27/23 12:32:28.446
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6899.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6899.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6899.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6899.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6899.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6899.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6899.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6899.svc.cluster.local;sleep 1; done
   @ 05/27/23 12:32:28.455
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6899.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6899.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6899.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6899.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6899.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6899.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6899.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6899.svc.cluster.local;sleep 1; done
   @ 05/27/23 12:32:28.455
  STEP: creating a pod to probe DNS @ 05/27/23 12:32:28.455
  STEP: submitting the pod to kubernetes @ 05/27/23 12:32:28.455
  STEP: retrieving the pod @ 05/27/23 12:32:36.502
  STEP: looking for the results for each expected name from probers @ 05/27/23 12:32:36.507
  May 27 12:32:36.512: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6899.svc.cluster.local from pod dns-6899/dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19: the server could not find the requested resource (get pods dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19)
  May 27 12:32:36.518: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6899.svc.cluster.local from pod dns-6899/dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19: the server could not find the requested resource (get pods dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19)
  May 27 12:32:36.523: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6899.svc.cluster.local from pod dns-6899/dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19: the server could not find the requested resource (get pods dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19)
  May 27 12:32:36.527: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6899.svc.cluster.local from pod dns-6899/dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19: the server could not find the requested resource (get pods dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19)
  May 27 12:32:36.533: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6899.svc.cluster.local from pod dns-6899/dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19: the server could not find the requested resource (get pods dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19)
  May 27 12:32:36.538: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6899.svc.cluster.local from pod dns-6899/dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19: the server could not find the requested resource (get pods dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19)
  May 27 12:32:36.542: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6899.svc.cluster.local from pod dns-6899/dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19: the server could not find the requested resource (get pods dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19)
  May 27 12:32:36.548: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6899.svc.cluster.local from pod dns-6899/dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19: the server could not find the requested resource (get pods dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19)
  May 27 12:32:36.548: INFO: Lookups using dns-6899/dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6899.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6899.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6899.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6899.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6899.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6899.svc.cluster.local jessie_udp@dns-test-service-2.dns-6899.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6899.svc.cluster.local]

  May 27 12:32:41.592: INFO: DNS probes using dns-6899/dns-test-fc018c65-3978-4dfc-bdeb-bf4b32e3fc19 succeeded

  May 27 12:32:41.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/27/23 12:32:41.599
  STEP: deleting the test headless service @ 05/27/23 12:32:41.622
  STEP: Destroying namespace "dns-6899" for this suite. @ 05/27/23 12:32:41.645
• [13.243 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 05/27/23 12:32:41.66
  May 27 12:32:41.660: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename resourcequota @ 05/27/23 12:32:41.661
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:32:41.681
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:32:41.687
  STEP: Counting existing ResourceQuota @ 05/27/23 12:32:41.695
  STEP: Creating a ResourceQuota @ 05/27/23 12:32:46.699
  STEP: Ensuring resource quota status is calculated @ 05/27/23 12:32:46.71
  STEP: Creating a ReplicationController @ 05/27/23 12:32:48.715
  STEP: Ensuring resource quota status captures replication controller creation @ 05/27/23 12:32:48.729
  STEP: Deleting a ReplicationController @ 05/27/23 12:32:50.736
  STEP: Ensuring resource quota status released usage @ 05/27/23 12:32:50.743
  May 27 12:32:52.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6153" for this suite. @ 05/27/23 12:32:52.755
• [11.103 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 05/27/23 12:32:52.764
  May 27 12:32:52.764: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 12:32:52.765
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:32:52.787
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:32:52.792
  STEP: Setting up server cert @ 05/27/23 12:32:52.822
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 12:32:53.526
  STEP: Deploying the webhook pod @ 05/27/23 12:32:53.535
  STEP: Wait for the deployment to be ready @ 05/27/23 12:32:53.551
  May 27 12:32:53.564: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/27/23 12:32:55.577
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 12:32:55.596
  May 27 12:32:56.596: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 27 12:32:56.601: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4811-crds.webhook.example.com via the AdmissionRegistration API @ 05/27/23 12:32:57.119
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/27/23 12:32:57.14
  May 27 12:32:59.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5033" for this suite. @ 05/27/23 12:32:59.883
  STEP: Destroying namespace "webhook-markers-1320" for this suite. @ 05/27/23 12:32:59.893
• [7.140 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 05/27/23 12:32:59.904
  May 27 12:32:59.904: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 12:32:59.905
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:32:59.923
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:32:59.933
  STEP: Setting up server cert @ 05/27/23 12:32:59.965
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 12:33:00.405
  STEP: Deploying the webhook pod @ 05/27/23 12:33:00.413
  STEP: Wait for the deployment to be ready @ 05/27/23 12:33:00.429
  May 27 12:33:00.443: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/27/23 12:33:02.456
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 12:33:02.471
  May 27 12:33:03.472: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 05/27/23 12:33:03.477
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/27/23 12:33:03.477
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 05/27/23 12:33:03.496
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 05/27/23 12:33:04.512
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/27/23 12:33:04.512
  STEP: Having no error when timeout is longer than webhook latency @ 05/27/23 12:33:05.548
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/27/23 12:33:05.548
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 05/27/23 12:33:10.603
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/27/23 12:33:10.603
  May 27 12:33:15.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3781" for this suite. @ 05/27/23 12:33:15.735
  STEP: Destroying namespace "webhook-markers-2044" for this suite. @ 05/27/23 12:33:15.747
• [15.852 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 05/27/23 12:33:15.756
  May 27 12:33:15.756: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename runtimeclass @ 05/27/23 12:33:15.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:33:15.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:33:15.782
  STEP: Deleting RuntimeClass runtimeclass-4667-delete-me @ 05/27/23 12:33:15.793
  STEP: Waiting for the RuntimeClass to disappear @ 05/27/23 12:33:15.801
  May 27 12:33:15.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4667" for this suite. @ 05/27/23 12:33:15.82
• [0.071 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 05/27/23 12:33:15.83
  May 27 12:33:15.830: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 12:33:15.831
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:33:15.851
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:33:15.857
  STEP: Creating projection with secret that has name projected-secret-test-map-fe2a5737-b30f-4b03-bd58-181f4d3be1c8 @ 05/27/23 12:33:15.863
  STEP: Creating a pod to test consume secrets @ 05/27/23 12:33:15.87
  STEP: Saw pod success @ 05/27/23 12:33:19.899
  May 27 12:33:19.904: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-projected-secrets-f9fb92be-69e6-4b99-8042-163dc24571d2 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/27/23 12:33:19.913
  May 27 12:33:19.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7468" for this suite. @ 05/27/23 12:33:19.939
• [4.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 05/27/23 12:33:19.949
  May 27 12:33:19.949: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename job @ 05/27/23 12:33:19.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:33:19.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:33:19.975
  STEP: Creating a job @ 05/27/23 12:33:19.983
  STEP: Ensure pods equal to parallelism count is attached to the job @ 05/27/23 12:33:19.995
  STEP: patching /status @ 05/27/23 12:33:22.001
  STEP: updating /status @ 05/27/23 12:33:22.01
  STEP: get /status @ 05/27/23 12:33:22.048
  May 27 12:33:22.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1373" for this suite. @ 05/27/23 12:33:22.057
• [2.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 05/27/23 12:33:22.07
  May 27 12:33:22.070: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename svcaccounts @ 05/27/23 12:33:22.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:33:22.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:33:22.098
  May 27 12:33:22.108: INFO: Got root ca configmap in namespace "svcaccounts-7047"
  May 27 12:33:22.116: INFO: Deleted root ca configmap in namespace "svcaccounts-7047"
  STEP: waiting for a new root ca configmap created @ 05/27/23 12:33:22.617
  May 27 12:33:22.622: INFO: Recreated root ca configmap in namespace "svcaccounts-7047"
  May 27 12:33:22.628: INFO: Updated root ca configmap in namespace "svcaccounts-7047"
  STEP: waiting for the root ca configmap reconciled @ 05/27/23 12:33:23.129
  May 27 12:33:23.133: INFO: Reconciled root ca configmap in namespace "svcaccounts-7047"
  May 27 12:33:23.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7047" for this suite. @ 05/27/23 12:33:23.139
• [1.079 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 05/27/23 12:33:23.149
  May 27 12:33:23.149: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename replicaset @ 05/27/23 12:33:23.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:33:23.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:33:23.175
  STEP: Create a ReplicaSet @ 05/27/23 12:33:23.179
  STEP: Verify that the required pods have come up @ 05/27/23 12:33:23.187
  May 27 12:33:23.192: INFO: Pod name sample-pod: Found 0 pods out of 3
  May 27 12:33:28.204: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 05/27/23 12:33:28.204
  May 27 12:33:30.218: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 05/27/23 12:33:30.219
  STEP: DeleteCollection of the ReplicaSets @ 05/27/23 12:33:30.224
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 05/27/23 12:33:30.235
  May 27 12:33:30.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1405" for this suite. @ 05/27/23 12:33:30.246
• [7.116 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 05/27/23 12:33:30.27
  May 27 12:33:30.271: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename downward-api @ 05/27/23 12:33:30.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:33:30.301
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:33:30.306
  STEP: Creating a pod to test downward api env vars @ 05/27/23 12:33:30.311
  STEP: Saw pod success @ 05/27/23 12:33:34.345
  May 27 12:33:34.349: INFO: Trying to get logs from node ip-172-31-68-172 pod downward-api-562f9dfb-758d-46b6-b6c5-cf6efc28f8e2 container dapi-container: <nil>
  STEP: delete the pod @ 05/27/23 12:33:34.359
  May 27 12:33:34.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-36" for this suite. @ 05/27/23 12:33:34.385
• [4.124 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 05/27/23 12:33:34.396
  May 27 12:33:34.396: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 12:33:34.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:33:34.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:33:34.42
  STEP: Setting up server cert @ 05/27/23 12:33:34.454
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 12:33:35.272
  STEP: Deploying the webhook pod @ 05/27/23 12:33:35.283
  STEP: Wait for the deployment to be ready @ 05/27/23 12:33:35.298
  May 27 12:33:35.313: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/27/23 12:33:37.329
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 12:33:37.351
  May 27 12:33:38.352: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 27 12:33:38.356: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 05/27/23 12:33:38.868
  STEP: Creating a custom resource that should be denied by the webhook @ 05/27/23 12:33:38.886
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 05/27/23 12:33:40.921
  STEP: Updating the custom resource with disallowed data should be denied @ 05/27/23 12:33:40.929
  STEP: Deleting the custom resource should be denied @ 05/27/23 12:33:40.941
  STEP: Remove the offending key and value from the custom resource data @ 05/27/23 12:33:40.949
  STEP: Deleting the updated custom resource should be successful @ 05/27/23 12:33:40.962
  May 27 12:33:40.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7967" for this suite. @ 05/27/23 12:33:41.573
  STEP: Destroying namespace "webhook-markers-5101" for this suite. @ 05/27/23 12:33:41.582
• [7.196 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 05/27/23 12:33:41.592
  May 27 12:33:41.592: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename sched-pred @ 05/27/23 12:33:41.593
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:33:41.612
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:33:41.616
  May 27 12:33:41.621: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 27 12:33:41.631: INFO: Waiting for terminating namespaces to be deleted...
  May 27 12:33:41.635: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-10-136 before test
  May 27 12:33:41.642: INFO: default-http-backend-kubernetes-worker-65fc475d49-8hlsq from ingress-nginx-kubernetes-worker started at 2023-05-27 12:22:35 +0000 UTC (1 container statuses recorded)
  May 27 12:33:41.642: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  May 27 12:33:41.642: INFO: nginx-ingress-controller-kubernetes-worker-jkk42 from ingress-nginx-kubernetes-worker started at 2023-05-27 12:11:29 +0000 UTC (1 container statuses recorded)
  May 27 12:33:41.642: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  May 27 12:33:41.642: INFO: calico-kube-controllers-79678b7759-2xtvf from kube-system started at 2023-05-27 12:22:35 +0000 UTC (1 container statuses recorded)
  May 27 12:33:41.642: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 27 12:33:41.642: INFO: sonobuoy from sonobuoy started at 2023-05-27 12:15:40 +0000 UTC (1 container statuses recorded)
  May 27 12:33:41.642: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 27 12:33:41.642: INFO: sonobuoy-e2e-job-0cf94b30a28f4573 from sonobuoy started at 2023-05-27 12:15:43 +0000 UTC (2 container statuses recorded)
  May 27 12:33:41.642: INFO: 	Container e2e ready: true, restart count 0
  May 27 12:33:41.642: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 27 12:33:41.642: INFO: sonobuoy-systemd-logs-daemon-set-f9890aca51654f74-zr2qt from sonobuoy started at 2023-05-27 12:15:43 +0000 UTC (2 container statuses recorded)
  May 27 12:33:41.642: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 27 12:33:41.642: INFO: 	Container systemd-logs ready: true, restart count 0
  May 27 12:33:41.642: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-22-3 before test
  May 27 12:33:41.651: INFO: nginx-ingress-controller-kubernetes-worker-2hlwz from ingress-nginx-kubernetes-worker started at 2023-05-27 11:59:07 +0000 UTC (1 container statuses recorded)
  May 27 12:33:41.651: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  May 27 12:33:41.651: INFO: coredns-5c7f76ccb8-b4zh4 from kube-system started at 2023-05-27 11:59:00 +0000 UTC (1 container statuses recorded)
  May 27 12:33:41.651: INFO: 	Container coredns ready: true, restart count 0
  May 27 12:33:41.651: INFO: kube-state-metrics-5b95b4459c-8rvst from kube-system started at 2023-05-27 11:59:00 +0000 UTC (1 container statuses recorded)
  May 27 12:33:41.651: INFO: 	Container kube-state-metrics ready: true, restart count 0
  May 27 12:33:41.651: INFO: metrics-server-v0.5.2-6cf8c8b69c-t499d from kube-system started at 2023-05-27 11:59:00 +0000 UTC (2 container statuses recorded)
  May 27 12:33:41.651: INFO: 	Container metrics-server ready: true, restart count 0
  May 27 12:33:41.651: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  May 27 12:33:41.651: INFO: dashboard-metrics-scraper-6b8586b5c9-7kfbs from kubernetes-dashboard started at 2023-05-27 11:59:00 +0000 UTC (1 container statuses recorded)
  May 27 12:33:41.651: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  May 27 12:33:41.651: INFO: kubernetes-dashboard-6869f4cd5f-js8mr from kubernetes-dashboard started at 2023-05-27 11:59:00 +0000 UTC (1 container statuses recorded)
  May 27 12:33:41.651: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  May 27 12:33:41.651: INFO: sonobuoy-systemd-logs-daemon-set-f9890aca51654f74-c4nxx from sonobuoy started at 2023-05-27 12:15:43 +0000 UTC (2 container statuses recorded)
  May 27 12:33:41.651: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 27 12:33:41.651: INFO: 	Container systemd-logs ready: true, restart count 0
  May 27 12:33:41.651: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-68-172 before test
  May 27 12:33:41.657: INFO: nginx-ingress-controller-kubernetes-worker-gtdv9 from ingress-nginx-kubernetes-worker started at 2023-05-27 12:23:00 +0000 UTC (1 container statuses recorded)
  May 27 12:33:41.657: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  May 27 12:33:41.657: INFO: suspend-false-to-true-25h6j from job-1373 started at 2023-05-27 12:33:20 +0000 UTC (1 container statuses recorded)
  May 27 12:33:41.657: INFO: 	Container c ready: true, restart count 0
  May 27 12:33:41.657: INFO: suspend-false-to-true-d8tfh from job-1373 started at 2023-05-27 12:33:20 +0000 UTC (1 container statuses recorded)
  May 27 12:33:41.657: INFO: 	Container c ready: true, restart count 0
  May 27 12:33:41.657: INFO: sonobuoy-systemd-logs-daemon-set-f9890aca51654f74-mmhtp from sonobuoy started at 2023-05-27 12:15:43 +0000 UTC (2 container statuses recorded)
  May 27 12:33:41.657: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 27 12:33:41.657: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/27/23 12:33:41.658
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/27/23 12:33:43.686
  STEP: Trying to apply a random label on the found node. @ 05/27/23 12:33:43.7
  STEP: verifying the node has the label kubernetes.io/e2e-3fd1c90e-b889-4113-aaa6-9a1b67e813aa 42 @ 05/27/23 12:33:43.711
  STEP: Trying to relaunch the pod, now with labels. @ 05/27/23 12:33:43.716
  STEP: removing the label kubernetes.io/e2e-3fd1c90e-b889-4113-aaa6-9a1b67e813aa off the node ip-172-31-68-172 @ 05/27/23 12:33:45.745
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-3fd1c90e-b889-4113-aaa6-9a1b67e813aa @ 05/27/23 12:33:45.762
  May 27 12:33:45.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-822" for this suite. @ 05/27/23 12:33:45.773
• [4.191 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 05/27/23 12:33:45.784
  May 27 12:33:45.784: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename endpointslice @ 05/27/23 12:33:45.785
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:33:45.807
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:33:45.812
  STEP: getting /apis @ 05/27/23 12:33:45.817
  STEP: getting /apis/discovery.k8s.io @ 05/27/23 12:33:45.822
  STEP: getting /apis/discovery.k8s.iov1 @ 05/27/23 12:33:45.824
  STEP: creating @ 05/27/23 12:33:45.826
  STEP: getting @ 05/27/23 12:33:45.848
  STEP: listing @ 05/27/23 12:33:45.852
  STEP: watching @ 05/27/23 12:33:45.857
  May 27 12:33:45.857: INFO: starting watch
  STEP: cluster-wide listing @ 05/27/23 12:33:45.859
  STEP: cluster-wide watching @ 05/27/23 12:33:45.866
  May 27 12:33:45.866: INFO: starting watch
  STEP: patching @ 05/27/23 12:33:45.868
  STEP: updating @ 05/27/23 12:33:45.875
  May 27 12:33:45.891: INFO: waiting for watch events with expected annotations
  May 27 12:33:45.892: INFO: saw patched and updated annotations
  STEP: deleting @ 05/27/23 12:33:45.892
  STEP: deleting a collection @ 05/27/23 12:33:45.909
  May 27 12:33:45.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8689" for this suite. @ 05/27/23 12:33:45.937
• [0.163 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 05/27/23 12:33:45.948
  May 27 12:33:45.948: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename crd-watch @ 05/27/23 12:33:45.949
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:33:45.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:33:45.974
  May 27 12:33:45.979: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Creating first CR  @ 05/27/23 12:33:48.538
  May 27 12:33:48.547: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-27T12:33:48Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-27T12:33:48Z]] name:name1 resourceVersion:8547 uid:48c368aa-85a8-4c1a-b719-c38ff5bf78a6] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 05/27/23 12:33:58.548
  May 27 12:33:58.559: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-27T12:33:58Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-27T12:33:58Z]] name:name2 resourceVersion:8609 uid:8827acbe-e6d4-406e-921d-db6988796f61] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 05/27/23 12:34:08.56
  May 27 12:34:08.570: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-27T12:33:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-27T12:34:08Z]] name:name1 resourceVersion:8635 uid:48c368aa-85a8-4c1a-b719-c38ff5bf78a6] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 05/27/23 12:34:18.571
  May 27 12:34:18.581: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-27T12:33:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-27T12:34:18Z]] name:name2 resourceVersion:8655 uid:8827acbe-e6d4-406e-921d-db6988796f61] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 05/27/23 12:34:28.581
  May 27 12:34:28.591: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-27T12:33:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-27T12:34:08Z]] name:name1 resourceVersion:8675 uid:48c368aa-85a8-4c1a-b719-c38ff5bf78a6] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 05/27/23 12:34:38.591
  May 27 12:34:38.602: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-27T12:33:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-27T12:34:18Z]] name:name2 resourceVersion:8694 uid:8827acbe-e6d4-406e-921d-db6988796f61] num:map[num1:9223372036854775807 num2:1000000]]}
  May 27 12:34:49.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-7909" for this suite. @ 05/27/23 12:34:49.13
• [63.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 05/27/23 12:34:49.142
  May 27 12:34:49.142: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 12:34:49.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:34:49.164
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:34:49.168
  STEP: Creating projection with secret that has name projected-secret-test-e54b1018-e2a6-4851-b85d-72804aa0cf78 @ 05/27/23 12:34:49.173
  STEP: Creating a pod to test consume secrets @ 05/27/23 12:34:49.18
  STEP: Saw pod success @ 05/27/23 12:34:53.213
  May 27 12:34:53.219: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-projected-secrets-4354feea-7132-498d-b71c-ee816e8499b9 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/27/23 12:34:53.229
  May 27 12:34:53.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1990" for this suite. @ 05/27/23 12:34:53.258
• [4.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 05/27/23 12:34:53.271
  May 27 12:34:53.271: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/27/23 12:34:53.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:34:53.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:34:53.297
  STEP: creating @ 05/27/23 12:34:53.302
  STEP: getting @ 05/27/23 12:34:53.329
  STEP: listing in namespace @ 05/27/23 12:34:53.335
  STEP: patching @ 05/27/23 12:34:53.341
  STEP: deleting @ 05/27/23 12:34:53.363
  May 27 12:34:53.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-180" for this suite. @ 05/27/23 12:34:53.388
• [0.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 05/27/23 12:34:53.402
  May 27 12:34:53.402: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/27/23 12:34:53.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:34:53.421
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:34:53.426
  STEP: fetching the /apis discovery document @ 05/27/23 12:34:53.432
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 05/27/23 12:34:53.434
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 05/27/23 12:34:53.434
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 05/27/23 12:34:53.434
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 05/27/23 12:34:53.436
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 05/27/23 12:34:53.436
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 05/27/23 12:34:53.438
  May 27 12:34:53.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6505" for this suite. @ 05/27/23 12:34:53.443
• [0.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 05/27/23 12:34:53.459
  May 27 12:34:53.459: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename secrets @ 05/27/23 12:34:53.46
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:34:53.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:34:53.486
  STEP: Creating secret with name secret-test-de5846d5-f4cc-4012-84f1-0c6fa066c7fa @ 05/27/23 12:34:53.518
  STEP: Creating a pod to test consume secrets @ 05/27/23 12:34:53.527
  STEP: Saw pod success @ 05/27/23 12:34:57.557
  May 27 12:34:57.561: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-secrets-325fe88a-f56a-4f10-a74f-aa3eec34b947 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/27/23 12:34:57.571
  May 27 12:34:57.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9153" for this suite. @ 05/27/23 12:34:57.6
  STEP: Destroying namespace "secret-namespace-342" for this suite. @ 05/27/23 12:34:57.609
• [4.160 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 05/27/23 12:34:57.624
  May 27 12:34:57.624: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename podtemplate @ 05/27/23 12:34:57.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:34:57.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:34:57.649
  May 27 12:34:57.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-7193" for this suite. @ 05/27/23 12:34:57.701
• [0.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 05/27/23 12:34:57.715
  May 27 12:34:57.715: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename pods @ 05/27/23 12:34:57.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:34:57.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:34:57.74
  STEP: Create set of pods @ 05/27/23 12:34:57.745
  May 27 12:34:57.758: INFO: created test-pod-1
  May 27 12:34:57.770: INFO: created test-pod-2
  May 27 12:34:57.783: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 05/27/23 12:34:57.783
  STEP: waiting for all pods to be deleted @ 05/27/23 12:35:01.86
  May 27 12:35:01.864: INFO: Pod quantity 3 is different from expected quantity 0
  May 27 12:35:02.871: INFO: Pod quantity 3 is different from expected quantity 0
  May 27 12:35:03.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9750" for this suite. @ 05/27/23 12:35:03.876
• [6.171 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 05/27/23 12:35:03.89
  May 27 12:35:03.890: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename runtimeclass @ 05/27/23 12:35:03.892
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:35:03.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:35:03.92
  May 27 12:35:05.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2845" for this suite. @ 05/27/23 12:35:05.975
• [2.093 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 05/27/23 12:35:05.984
  May 27 12:35:05.984: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename daemonsets @ 05/27/23 12:35:05.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:35:06.006
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:35:06.015
  STEP: Creating simple DaemonSet "daemon-set" @ 05/27/23 12:35:06.047
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/27/23 12:35:06.057
  May 27 12:35:06.068: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:06.068: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:06.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 12:35:06.074: INFO: Node ip-172-31-10-136 is running 0 daemon pod, expected 1
  May 27 12:35:07.080: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:07.080: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:07.085: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 27 12:35:07.085: INFO: Node ip-172-31-22-3 is running 0 daemon pod, expected 1
  May 27 12:35:08.079: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:08.079: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:08.084: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 27 12:35:08.084: INFO: Node ip-172-31-22-3 is running 0 daemon pod, expected 1
  May 27 12:35:09.080: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:09.080: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:09.087: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 27 12:35:09.087: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 05/27/23 12:35:09.091
  May 27 12:35:09.113: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:09.113: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:09.117: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 27 12:35:09.117: INFO: Node ip-172-31-22-3 is running 0 daemon pod, expected 1
  May 27 12:35:10.124: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:10.124: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:10.128: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 27 12:35:10.128: INFO: Node ip-172-31-22-3 is running 0 daemon pod, expected 1
  May 27 12:35:11.124: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:11.124: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:11.136: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 27 12:35:11.136: INFO: Node ip-172-31-22-3 is running 0 daemon pod, expected 1
  May 27 12:35:12.122: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:12.122: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:12.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 27 12:35:12.127: INFO: Node ip-172-31-22-3 is running 0 daemon pod, expected 1
  May 27 12:35:13.124: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:13.124: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:35:13.128: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 27 12:35:13.128: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/27/23 12:35:13.132
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3101, will wait for the garbage collector to delete the pods @ 05/27/23 12:35:13.132
  May 27 12:35:13.195: INFO: Deleting DaemonSet.extensions daemon-set took: 7.382752ms
  May 27 12:35:13.295: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.918288ms
  May 27 12:35:15.902: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 12:35:15.903: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 27 12:35:15.910: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9116"},"items":null}

  May 27 12:35:15.915: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9116"},"items":null}

  May 27 12:35:15.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3101" for this suite. @ 05/27/23 12:35:15.936
• [9.963 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 05/27/23 12:35:15.947
  May 27 12:35:15.947: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename security-context @ 05/27/23 12:35:15.949
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:35:15.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:35:15.973
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/27/23 12:35:15.979
  STEP: Saw pod success @ 05/27/23 12:35:20.012
  May 27 12:35:20.017: INFO: Trying to get logs from node ip-172-31-68-172 pod security-context-76755352-096d-40aa-9799-c485724823ae container test-container: <nil>
  STEP: delete the pod @ 05/27/23 12:35:20.026
  May 27 12:35:20.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-7273" for this suite. @ 05/27/23 12:35:20.054
• [4.116 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 05/27/23 12:35:20.065
  May 27 12:35:20.065: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename cronjob @ 05/27/23 12:35:20.066
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:35:20.089
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:35:20.094
  STEP: Creating a cronjob @ 05/27/23 12:35:20.099
  STEP: Ensuring more than one job is running at a time @ 05/27/23 12:35:20.11
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 05/27/23 12:37:02.116
  STEP: Removing cronjob @ 05/27/23 12:37:02.12
  May 27 12:37:02.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4793" for this suite. @ 05/27/23 12:37:02.136
• [102.083 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 05/27/23 12:37:02.15
  May 27 12:37:02.150: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename aggregator @ 05/27/23 12:37:02.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:37:02.185
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:37:02.19
  May 27 12:37:02.196: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Registering the sample API server. @ 05/27/23 12:37:02.198
  May 27 12:37:02.595: INFO: Found ClusterRoles; assuming RBAC is enabled.
  May 27 12:37:02.638: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  May 27 12:37:04.717: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 12:37:06.724: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 12:37:08.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 12:37:10.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 12:37:12.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 12:37:14.724: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 12:37:16.724: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 12:37:18.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 12:37:20.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 12:37:22.724: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 12:37:24.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 12:37:26.851: INFO: Waited 117.759973ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 05/27/23 12:37:26.903
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 05/27/23 12:37:26.907
  STEP: List APIServices @ 05/27/23 12:37:26.917
  May 27 12:37:26.931: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 05/27/23 12:37:26.931
  May 27 12:37:26.947: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 05/27/23 12:37:26.947
  May 27 12:37:26.962: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.May, 27, 12, 37, 26, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 05/27/23 12:37:26.963
  May 27 12:37:26.967: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-05-27 12:37:26 +0000 UTC Passed all checks passed}
  May 27 12:37:26.967: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 27 12:37:26.967: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 05/27/23 12:37:26.968
  May 27 12:37:26.982: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1117804111" @ 05/27/23 12:37:26.982
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 05/27/23 12:37:26.996
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 05/27/23 12:37:27.004
  STEP: Patch APIService Status @ 05/27/23 12:37:27.008
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 05/27/23 12:37:27.019
  May 27 12:37:27.023: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-05-27 12:37:26 +0000 UTC Passed all checks passed}
  May 27 12:37:27.023: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 27 12:37:27.023: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  May 27 12:37:27.023: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 05/27/23 12:37:27.024
  STEP: Confirm that the generated APIService has been deleted @ 05/27/23 12:37:27.03
  May 27 12:37:27.030: INFO: Requesting list of APIServices to confirm quantity
  May 27 12:37:27.037: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  May 27 12:37:27.037: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  May 27 12:37:27.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-5018" for this suite. @ 05/27/23 12:37:27.206
• [25.064 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 05/27/23 12:37:27.214
  May 27 12:37:27.214: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename server-version @ 05/27/23 12:37:27.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:37:27.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:37:27.241
  STEP: Request ServerVersion @ 05/27/23 12:37:27.246
  STEP: Confirm major version @ 05/27/23 12:37:27.247
  May 27 12:37:27.248: INFO: Major version: 1
  STEP: Confirm minor version @ 05/27/23 12:37:27.248
  May 27 12:37:27.248: INFO: cleanMinorVersion: 27
  May 27 12:37:27.248: INFO: Minor version: 27
  May 27 12:37:27.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-2741" for this suite. @ 05/27/23 12:37:27.253
• [0.046 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 05/27/23 12:37:27.264
  May 27 12:37:27.264: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename secrets @ 05/27/23 12:37:27.265
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:37:27.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:37:27.289
  STEP: Creating projection with secret that has name secret-emptykey-test-837c4031-0cab-46a8-8750-503ca2976064 @ 05/27/23 12:37:27.293
  May 27 12:37:27.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6412" for this suite. @ 05/27/23 12:37:27.304
• [0.054 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 05/27/23 12:37:27.319
  May 27 12:37:27.319: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename configmap @ 05/27/23 12:37:27.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:37:27.34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:37:27.345
  STEP: Creating configMap with name configmap-test-volume-map-5c32604b-81bc-46b5-814f-345d600ad4cb @ 05/27/23 12:37:27.35
  STEP: Creating a pod to test consume configMaps @ 05/27/23 12:37:27.356
  STEP: Saw pod success @ 05/27/23 12:37:31.389
  May 27 12:37:31.392: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-configmaps-34af605f-f377-48a5-8f49-515f0056ef94 container agnhost-container: <nil>
  STEP: delete the pod @ 05/27/23 12:37:31.417
  May 27 12:37:31.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1086" for this suite. @ 05/27/23 12:37:31.443
• [4.133 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 05/27/23 12:37:31.455
  May 27 12:37:31.456: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/27/23 12:37:31.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:37:31.482
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:37:31.488
  May 27 12:37:33.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 05/27/23 12:37:33.539
  STEP: Cleaning up the configmap @ 05/27/23 12:37:33.548
  STEP: Cleaning up the pod @ 05/27/23 12:37:33.557
  STEP: Destroying namespace "emptydir-wrapper-8774" for this suite. @ 05/27/23 12:37:33.572
• [2.126 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 05/27/23 12:37:33.583
  May 27 12:37:33.583: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename downward-api @ 05/27/23 12:37:33.585
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:37:33.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:37:33.61
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 12:37:33.615
  STEP: Saw pod success @ 05/27/23 12:37:37.64
  May 27 12:37:37.646: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-245b3b7c-8adc-4f33-a9a6-d3e5b46e34e0 container client-container: <nil>
  STEP: delete the pod @ 05/27/23 12:37:37.654
  May 27 12:37:37.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1994" for this suite. @ 05/27/23 12:37:37.675
• [4.102 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 05/27/23 12:37:37.686
  May 27 12:37:37.686: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 12:37:37.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:37:37.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:37:37.711
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 12:37:37.716
  STEP: Saw pod success @ 05/27/23 12:37:41.75
  May 27 12:37:41.754: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-87257adc-5d6a-4849-9b6e-b70d0a729a8c container client-container: <nil>
  STEP: delete the pod @ 05/27/23 12:37:41.762
  May 27 12:37:41.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1321" for this suite. @ 05/27/23 12:37:41.785
• [4.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 05/27/23 12:37:41.796
  May 27 12:37:41.796: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl @ 05/27/23 12:37:41.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:37:41.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:37:41.824
  May 27 12:37:41.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-215 create -f -'
  May 27 12:37:42.614: INFO: stderr: ""
  May 27 12:37:42.614: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  May 27 12:37:42.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-215 create -f -'
  May 27 12:37:43.185: INFO: stderr: ""
  May 27 12:37:43.185: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/27/23 12:37:43.185
  May 27 12:37:44.189: INFO: Selector matched 1 pods for map[app:agnhost]
  May 27 12:37:44.189: INFO: Found 1 / 1
  May 27 12:37:44.189: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May 27 12:37:44.195: INFO: Selector matched 1 pods for map[app:agnhost]
  May 27 12:37:44.195: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 27 12:37:44.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-215 describe pod agnhost-primary-b98w4'
  May 27 12:37:44.295: INFO: stderr: ""
  May 27 12:37:44.295: INFO: stdout: "Name:             agnhost-primary-b98w4\nNamespace:        kubectl-215\nPriority:         0\nService Account:  default\nNode:             ip-172-31-68-172/172.31.68.172\nStart Time:       Sat, 27 May 2023 12:37:42 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.19.67\nIPs:\n  IP:           192.168.19.67\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://4fc5cbabf0596d62a995bbe3a9b69c7769ae8c221c84aac167c18a99b1b7eaf4\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 27 May 2023 12:37:43 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-88wzr (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-88wzr:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-215/agnhost-primary-b98w4 to ip-172-31-68-172\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  May 27 12:37:44.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-215 describe rc agnhost-primary'
  May 27 12:37:44.393: INFO: stderr: ""
  May 27 12:37:44.393: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-215\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-b98w4\n"
  May 27 12:37:44.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-215 describe service agnhost-primary'
  May 27 12:37:44.490: INFO: stderr: ""
  May 27 12:37:44.490: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-215\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.113\nIPs:               10.152.183.113\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.19.67:6379\nSession Affinity:  None\nEvents:            <none>\n"
  May 27 12:37:44.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-215 describe node ip-172-31-10-136'
  May 27 12:37:44.625: INFO: stderr: ""
  May 27 12:37:44.625: INFO: stdout: "Name:               ip-172-31-10-136\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-worker\n                    juju-charm=kubernetes-worker\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-10-136\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 27 May 2023 12:11:28 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-10-136\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 27 May 2023 12:37:37 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 27 May 2023 12:37:37 +0000   Sat, 27 May 2023 12:11:28 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 27 May 2023 12:37:37 +0000   Sat, 27 May 2023 12:11:28 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 27 May 2023 12:37:37 +0000   Sat, 27 May 2023 12:11:28 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 27 May 2023 12:37:37 +0000   Sat, 27 May 2023 12:11:28 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.10.136\n  Hostname:    ip-172-31-10-136\nCapacity:\n  cpu:                2\n  ephemeral-storage:  16069568Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7882292Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  14809713845\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7779892Ki\n  pods:               110\nSystem Info:\n  Machine ID:                      ec291888bfe41be04ee52911f4e1937e\n  System UUID:                     ec291888-bfe4-1be0-4ee5-2911f4e1937e\n  Boot ID:                         6791d961-8157-42b7-8e87-6f79e4f19878\n  Kernel Version:                  5.19.0-1025-aws\n  OS Image:                        Ubuntu 22.04.2 LTS\n  Operating System:                linux\n  Architecture:                    amd64\n  Container Runtime Version:       containerd://1.6.8\n  Kubelet Version:                 v1.27.2\n  Kube-Proxy Version:              v1.27.2\nNon-terminated Pods:               (6 in total)\n  Namespace                        Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                        ----                                                       ------------  ----------  ---------------  -------------  ---\n  ingress-nginx-kubernetes-worker  default-http-backend-kubernetes-worker-65fc475d49-8hlsq    10m (0%)      10m (0%)    20Mi (0%)        20Mi (0%)      15m\n  ingress-nginx-kubernetes-worker  nginx-ingress-controller-kubernetes-worker-jkk42           0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                      calico-kube-controllers-79678b7759-2xtvf                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\n  sonobuoy                         sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m\n  sonobuoy                         sonobuoy-e2e-job-0cf94b30a28f4573                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m\n  sonobuoy                         sonobuoy-systemd-logs-daemon-set-f9890aca51654f74-zr2qt    0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                10m (0%)   10m (0%)\n  memory             20Mi (0%)  20Mi (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\n  hugepages-1Gi      0 (0%)     0 (0%)\n  hugepages-2Mi      0 (0%)     0 (0%)\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 26m                kube-proxy       \n  Normal   Starting                 26m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      26m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  26m (x2 over 26m)  kubelet          Node ip-172-31-10-136 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    26m (x2 over 26m)  kubelet          Node ip-172-31-10-136 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     26m (x2 over 26m)  kubelet          Node ip-172-31-10-136 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  26m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeReady                26m                kubelet          Node ip-172-31-10-136 status is now: NodeReady\n  Normal   RegisteredNode           26m                node-controller  Node ip-172-31-10-136 event: Registered Node ip-172-31-10-136 in Controller\n"
  May 27 12:37:44.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-215 describe namespace kubectl-215'
  May 27 12:37:44.729: INFO: stderr: ""
  May 27 12:37:44.729: INFO: stdout: "Name:         kubectl-215\nLabels:       e2e-framework=kubectl\n              e2e-run=7bfe1a40-97ab-42a2-b1e3-e2a98f36b04a\n              kubernetes.io/metadata.name=kubectl-215\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  May 27 12:37:44.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-215" for this suite. @ 05/27/23 12:37:44.736
• [2.947 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 05/27/23 12:37:44.744
  May 27 12:37:44.744: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 12:37:44.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:37:44.771
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:37:44.776
  STEP: Setting up server cert @ 05/27/23 12:37:44.807
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 12:37:45.254
  STEP: Deploying the webhook pod @ 05/27/23 12:37:45.266
  STEP: Wait for the deployment to be ready @ 05/27/23 12:37:45.285
  May 27 12:37:45.311: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/27/23 12:37:47.326
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 12:37:47.341
  May 27 12:37:48.341: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/27/23 12:37:48.345
  STEP: create a pod @ 05/27/23 12:37:48.364
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 05/27/23 12:37:50.385
  May 27 12:37:50.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=webhook-1498 attach --namespace=webhook-1498 to-be-attached-pod -i -c=container1'
  May 27 12:37:50.483: INFO: rc: 1
  May 27 12:37:50.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1498" for this suite. @ 05/27/23 12:37:50.563
  STEP: Destroying namespace "webhook-markers-7210" for this suite. @ 05/27/23 12:37:50.574
• [5.840 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 05/27/23 12:37:50.584
  May 27 12:37:50.584: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename runtimeclass @ 05/27/23 12:37:50.585
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:37:50.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:37:50.61
  STEP: getting /apis @ 05/27/23 12:37:50.615
  STEP: getting /apis/node.k8s.io @ 05/27/23 12:37:50.621
  STEP: getting /apis/node.k8s.io/v1 @ 05/27/23 12:37:50.623
  STEP: creating @ 05/27/23 12:37:50.624
  STEP: watching @ 05/27/23 12:37:50.651
  May 27 12:37:50.651: INFO: starting watch
  STEP: getting @ 05/27/23 12:37:50.658
  STEP: listing @ 05/27/23 12:37:50.663
  STEP: patching @ 05/27/23 12:37:50.669
  STEP: updating @ 05/27/23 12:37:50.675
  May 27 12:37:50.684: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 05/27/23 12:37:50.685
  STEP: deleting a collection @ 05/27/23 12:37:50.703
  May 27 12:37:50.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6126" for this suite. @ 05/27/23 12:37:50.729
• [0.154 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 05/27/23 12:37:50.746
  May 27 12:37:50.746: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename downward-api @ 05/27/23 12:37:50.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:37:50.764
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:37:50.771
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 12:37:50.775
  STEP: Saw pod success @ 05/27/23 12:37:54.802
  May 27 12:37:54.808: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-61cc5c83-6da6-42cb-b782-1210b6735d41 container client-container: <nil>
  STEP: delete the pod @ 05/27/23 12:37:54.815
  May 27 12:37:54.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3801" for this suite. @ 05/27/23 12:37:54.839
• [4.102 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 05/27/23 12:37:54.85
  May 27 12:37:54.850: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename secrets @ 05/27/23 12:37:54.851
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:37:54.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:37:54.876
  May 27 12:37:54.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2820" for this suite. @ 05/27/23 12:37:54.937
• [0.097 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 05/27/23 12:37:54.948
  May 27 12:37:54.948: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename gc @ 05/27/23 12:37:54.949
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:37:54.969
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:37:54.975
  STEP: create the deployment @ 05/27/23 12:37:54.983
  W0527 12:37:54.990970      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/27/23 12:37:54.991
  STEP: delete the deployment @ 05/27/23 12:37:55.511
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 05/27/23 12:37:55.521
  STEP: Gathering metrics @ 05/27/23 12:37:56.05
  W0527 12:37:56.056007      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May 27 12:37:56.056: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 27 12:37:56.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7140" for this suite. @ 05/27/23 12:37:56.063
• [1.126 seconds]
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 05/27/23 12:37:56.075
  May 27 12:37:56.075: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename dns @ 05/27/23 12:37:56.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:37:56.096
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:37:56.101
  STEP: Creating a test externalName service @ 05/27/23 12:37:56.105
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5991.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5991.svc.cluster.local; sleep 1; done
   @ 05/27/23 12:37:56.114
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5991.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5991.svc.cluster.local; sleep 1; done
   @ 05/27/23 12:37:56.114
  STEP: creating a pod to probe DNS @ 05/27/23 12:37:56.115
  STEP: submitting the pod to kubernetes @ 05/27/23 12:37:56.115
  STEP: retrieving the pod @ 05/27/23 12:37:58.142
  STEP: looking for the results for each expected name from probers @ 05/27/23 12:37:58.147
  May 27 12:37:58.159: INFO: DNS probes using dns-test-35b6ef2b-1c65-40b9-a889-efe2380794e9 succeeded

  STEP: changing the externalName to bar.example.com @ 05/27/23 12:37:58.159
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5991.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5991.svc.cluster.local; sleep 1; done
   @ 05/27/23 12:37:58.173
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5991.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5991.svc.cluster.local; sleep 1; done
   @ 05/27/23 12:37:58.173
  STEP: creating a second pod to probe DNS @ 05/27/23 12:37:58.173
  STEP: submitting the pod to kubernetes @ 05/27/23 12:37:58.173
  STEP: retrieving the pod @ 05/27/23 12:38:00.192
  STEP: looking for the results for each expected name from probers @ 05/27/23 12:38:00.198
  May 27 12:38:00.206: INFO: File wheezy_udp@dns-test-service-3.dns-5991.svc.cluster.local from pod  dns-5991/dns-test-a492e7f0-f863-42ef-b15c-e535fa1b1871 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 27 12:38:00.214: INFO: File jessie_udp@dns-test-service-3.dns-5991.svc.cluster.local from pod  dns-5991/dns-test-a492e7f0-f863-42ef-b15c-e535fa1b1871 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 27 12:38:00.214: INFO: Lookups using dns-5991/dns-test-a492e7f0-f863-42ef-b15c-e535fa1b1871 failed for: [wheezy_udp@dns-test-service-3.dns-5991.svc.cluster.local jessie_udp@dns-test-service-3.dns-5991.svc.cluster.local]

  May 27 12:38:05.225: INFO: DNS probes using dns-test-a492e7f0-f863-42ef-b15c-e535fa1b1871 succeeded

  STEP: changing the service to type=ClusterIP @ 05/27/23 12:38:05.225
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5991.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5991.svc.cluster.local; sleep 1; done
   @ 05/27/23 12:38:05.245
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5991.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5991.svc.cluster.local; sleep 1; done
   @ 05/27/23 12:38:05.246
  STEP: creating a third pod to probe DNS @ 05/27/23 12:38:05.246
  STEP: submitting the pod to kubernetes @ 05/27/23 12:38:05.251
  STEP: retrieving the pod @ 05/27/23 12:38:13.296
  STEP: looking for the results for each expected name from probers @ 05/27/23 12:38:13.3
  May 27 12:38:13.313: INFO: DNS probes using dns-test-8ffe15b2-1611-400f-a2f6-d7b3b2bae710 succeeded

  May 27 12:38:13.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/27/23 12:38:13.318
  STEP: deleting the pod @ 05/27/23 12:38:13.334
  STEP: deleting the pod @ 05/27/23 12:38:13.349
  STEP: deleting the test externalName service @ 05/27/23 12:38:13.377
  STEP: Destroying namespace "dns-5991" for this suite. @ 05/27/23 12:38:13.398
• [17.333 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 05/27/23 12:38:13.408
  May 27 12:38:13.408: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename resourcequota @ 05/27/23 12:38:13.41
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:38:13.431
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:38:13.437
  STEP: Creating a ResourceQuota @ 05/27/23 12:38:13.442
  STEP: Getting a ResourceQuota @ 05/27/23 12:38:13.448
  STEP: Updating a ResourceQuota @ 05/27/23 12:38:13.454
  STEP: Verifying a ResourceQuota was modified @ 05/27/23 12:38:13.46
  STEP: Deleting a ResourceQuota @ 05/27/23 12:38:13.468
  STEP: Verifying the deleted ResourceQuota @ 05/27/23 12:38:13.476
  May 27 12:38:13.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5592" for this suite. @ 05/27/23 12:38:13.485
• [0.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 05/27/23 12:38:13.496
  May 27 12:38:13.496: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename sysctl @ 05/27/23 12:38:13.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:38:13.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:38:13.521
  STEP: Creating a pod with one valid and two invalid sysctls @ 05/27/23 12:38:13.526
  May 27 12:38:13.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-656" for this suite. @ 05/27/23 12:38:13.539
• [0.051 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 05/27/23 12:38:13.548
  May 27 12:38:13.549: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename var-expansion @ 05/27/23 12:38:13.55
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:38:13.571
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:38:13.577
  STEP: creating the pod with failed condition @ 05/27/23 12:38:13.582
  STEP: updating the pod @ 05/27/23 12:40:13.595
  May 27 12:40:14.111: INFO: Successfully updated pod "var-expansion-8729f957-7439-4437-a411-c4459a133e9b"
  STEP: waiting for pod running @ 05/27/23 12:40:14.111
  STEP: deleting the pod gracefully @ 05/27/23 12:40:16.122
  May 27 12:40:16.122: INFO: Deleting pod "var-expansion-8729f957-7439-4437-a411-c4459a133e9b" in namespace "var-expansion-6934"
  May 27 12:40:16.133: INFO: Wait up to 5m0s for pod "var-expansion-8729f957-7439-4437-a411-c4459a133e9b" to be fully deleted
  May 27 12:40:48.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6934" for this suite. @ 05/27/23 12:40:48.231
• [154.691 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 05/27/23 12:40:48.242
  May 27 12:40:48.242: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename tables @ 05/27/23 12:40:48.243
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:40:48.26
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:40:48.267
  May 27 12:40:48.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-2191" for this suite. @ 05/27/23 12:40:48.28
• [0.048 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 05/27/23 12:40:48.29
  May 27 12:40:48.290: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename sysctl @ 05/27/23 12:40:48.291
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:40:48.312
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:40:48.316
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 05/27/23 12:40:48.32
  STEP: Watching for error events or started pod @ 05/27/23 12:40:48.336
  STEP: Waiting for pod completion @ 05/27/23 12:40:50.342
  STEP: Checking that the pod succeeded @ 05/27/23 12:40:52.357
  STEP: Getting logs from the pod @ 05/27/23 12:40:52.357
  STEP: Checking that the sysctl is actually updated @ 05/27/23 12:40:52.376
  May 27 12:40:52.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-1549" for this suite. @ 05/27/23 12:40:52.382
• [4.101 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 05/27/23 12:40:52.393
  May 27 12:40:52.393: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename resourcequota @ 05/27/23 12:40:52.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:40:52.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:40:52.415
  STEP: Counting existing ResourceQuota @ 05/27/23 12:40:52.42
  STEP: Creating a ResourceQuota @ 05/27/23 12:40:57.427
  STEP: Ensuring resource quota status is calculated @ 05/27/23 12:40:57.436
  May 27 12:40:59.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-355" for this suite. @ 05/27/23 12:40:59.446
• [7.063 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 05/27/23 12:40:59.46
  May 27 12:40:59.460: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename init-container @ 05/27/23 12:40:59.461
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:40:59.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:40:59.486
  STEP: creating the pod @ 05/27/23 12:40:59.491
  May 27 12:40:59.491: INFO: PodSpec: initContainers in spec.initContainers
  May 27 12:41:03.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1983" for this suite. @ 05/27/23 12:41:03.375
• [3.927 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 05/27/23 12:41:03.388
  May 27 12:41:03.388: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename downward-api @ 05/27/23 12:41:03.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:41:03.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:41:03.415
  STEP: Creating a pod to test downward api env vars @ 05/27/23 12:41:03.424
  STEP: Saw pod success @ 05/27/23 12:41:07.452
  May 27 12:41:07.457: INFO: Trying to get logs from node ip-172-31-68-172 pod downward-api-e28ea590-a881-4bb8-a53b-990c01384d60 container dapi-container: <nil>
  STEP: delete the pod @ 05/27/23 12:41:07.466
  May 27 12:41:07.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4157" for this suite. @ 05/27/23 12:41:07.491
• [4.112 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 05/27/23 12:41:07.501
  May 27 12:41:07.501: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename disruption @ 05/27/23 12:41:07.502
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:41:07.523
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:41:07.532
  STEP: creating the pdb @ 05/27/23 12:41:07.535
  STEP: Waiting for the pdb to be processed @ 05/27/23 12:41:07.541
  STEP: updating the pdb @ 05/27/23 12:41:09.552
  STEP: Waiting for the pdb to be processed @ 05/27/23 12:41:09.566
  STEP: patching the pdb @ 05/27/23 12:41:09.571
  STEP: Waiting for the pdb to be processed @ 05/27/23 12:41:09.585
  STEP: Waiting for the pdb to be deleted @ 05/27/23 12:41:11.603
  May 27 12:41:11.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9383" for this suite. @ 05/27/23 12:41:11.613
• [4.119 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 05/27/23 12:41:11.621
  May 27 12:41:11.621: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-probe @ 05/27/23 12:41:11.622
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:41:11.646
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:41:11.651
  STEP: Creating pod liveness-dbe0576d-c416-4bfe-9c47-02a098d57f57 in namespace container-probe-9754 @ 05/27/23 12:41:11.656
  May 27 12:41:13.678: INFO: Started pod liveness-dbe0576d-c416-4bfe-9c47-02a098d57f57 in namespace container-probe-9754
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/27/23 12:41:13.678
  May 27 12:41:13.682: INFO: Initial restart count of pod liveness-dbe0576d-c416-4bfe-9c47-02a098d57f57 is 0
  May 27 12:41:33.753: INFO: Restart count of pod container-probe-9754/liveness-dbe0576d-c416-4bfe-9c47-02a098d57f57 is now 1 (20.07027007s elapsed)
  May 27 12:41:33.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/27/23 12:41:33.758
  STEP: Destroying namespace "container-probe-9754" for this suite. @ 05/27/23 12:41:33.77
• [22.160 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 05/27/23 12:41:33.782
  May 27 12:41:33.783: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir @ 05/27/23 12:41:33.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:41:33.8
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:41:33.806
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/27/23 12:41:33.811
  STEP: Saw pod success @ 05/27/23 12:41:37.844
  May 27 12:41:37.848: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-93818d14-710e-439f-bf20-45e0e782a1d6 container test-container: <nil>
  STEP: delete the pod @ 05/27/23 12:41:37.857
  May 27 12:41:37.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2652" for this suite. @ 05/27/23 12:41:37.881
• [4.106 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 05/27/23 12:41:37.889
  May 27 12:41:37.889: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename services @ 05/27/23 12:41:37.89
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:41:37.907
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:41:37.914
  STEP: creating service in namespace services-196 @ 05/27/23 12:41:37.918
  STEP: creating service affinity-nodeport in namespace services-196 @ 05/27/23 12:41:37.918
  STEP: creating replication controller affinity-nodeport in namespace services-196 @ 05/27/23 12:41:37.935
  I0527 12:41:37.955166      18 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-196, replica count: 3
  I0527 12:41:41.007204      18 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 27 12:41:41.022: INFO: Creating new exec pod
  May 27 12:41:44.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-196 exec execpod-affinitycnwwq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  May 27 12:41:44.215: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  May 27 12:41:44.215: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 12:41:44.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-196 exec execpod-affinitycnwwq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.169 80'
  May 27 12:41:44.377: INFO: stderr: "+ nc -v -t -w 2 10.152.183.169 80\n+ echo hostName\nConnection to 10.152.183.169 80 port [tcp/http] succeeded!\n"
  May 27 12:41:44.377: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 12:41:44.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-196 exec execpod-affinitycnwwq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.22.3 30064'
  May 27 12:41:44.526: INFO: stderr: "+ nc -v -t -w 2 172.31.22.3 30064\n+ echo hostName\nConnection to 172.31.22.3 30064 port [tcp/*] succeeded!\n"
  May 27 12:41:44.526: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 12:41:44.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-196 exec execpod-affinitycnwwq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.68.172 30064'
  May 27 12:41:44.677: INFO: stderr: "+ nc -v -t -w 2 172.31.68.172 30064\n+ echo hostName\nConnection to 172.31.68.172 30064 port [tcp/*] succeeded!\n"
  May 27 12:41:44.677: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 12:41:44.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-196 exec execpod-affinitycnwwq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.10.136:30064/ ; done'
  May 27 12:41:44.955: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:30064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:30064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:30064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:30064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:30064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:30064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:30064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:30064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:30064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:30064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:30064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:30064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:30064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:30064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:30064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:30064/\n"
  May 27 12:41:44.955: INFO: stdout: "\naffinity-nodeport-g4dsd\naffinity-nodeport-g4dsd\naffinity-nodeport-g4dsd\naffinity-nodeport-g4dsd\naffinity-nodeport-g4dsd\naffinity-nodeport-g4dsd\naffinity-nodeport-g4dsd\naffinity-nodeport-g4dsd\naffinity-nodeport-g4dsd\naffinity-nodeport-g4dsd\naffinity-nodeport-g4dsd\naffinity-nodeport-g4dsd\naffinity-nodeport-g4dsd\naffinity-nodeport-g4dsd\naffinity-nodeport-g4dsd\naffinity-nodeport-g4dsd"
  May 27 12:41:44.955: INFO: Received response from host: affinity-nodeport-g4dsd
  May 27 12:41:44.955: INFO: Received response from host: affinity-nodeport-g4dsd
  May 27 12:41:44.955: INFO: Received response from host: affinity-nodeport-g4dsd
  May 27 12:41:44.955: INFO: Received response from host: affinity-nodeport-g4dsd
  May 27 12:41:44.955: INFO: Received response from host: affinity-nodeport-g4dsd
  May 27 12:41:44.955: INFO: Received response from host: affinity-nodeport-g4dsd
  May 27 12:41:44.955: INFO: Received response from host: affinity-nodeport-g4dsd
  May 27 12:41:44.955: INFO: Received response from host: affinity-nodeport-g4dsd
  May 27 12:41:44.955: INFO: Received response from host: affinity-nodeport-g4dsd
  May 27 12:41:44.955: INFO: Received response from host: affinity-nodeport-g4dsd
  May 27 12:41:44.955: INFO: Received response from host: affinity-nodeport-g4dsd
  May 27 12:41:44.955: INFO: Received response from host: affinity-nodeport-g4dsd
  May 27 12:41:44.955: INFO: Received response from host: affinity-nodeport-g4dsd
  May 27 12:41:44.955: INFO: Received response from host: affinity-nodeport-g4dsd
  May 27 12:41:44.955: INFO: Received response from host: affinity-nodeport-g4dsd
  May 27 12:41:44.955: INFO: Received response from host: affinity-nodeport-g4dsd
  May 27 12:41:44.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 27 12:41:44.960: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-196, will wait for the garbage collector to delete the pods @ 05/27/23 12:41:44.976
  May 27 12:41:45.042: INFO: Deleting ReplicationController affinity-nodeport took: 10.923556ms
  May 27 12:41:45.143: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.910682ms
  STEP: Destroying namespace "services-196" for this suite. @ 05/27/23 12:41:47.672
• [9.792 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 05/27/23 12:41:47.686
  May 27 12:41:47.686: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename sched-preemption @ 05/27/23 12:41:47.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:41:47.718
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:41:47.723
  May 27 12:41:47.749: INFO: Waiting up to 1m0s for all nodes to be ready
  May 27 12:42:47.773: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/27/23 12:42:47.779
  May 27 12:42:47.779: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/27/23 12:42:47.78
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:42:47.805
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:42:47.809
  May 27 12:42:47.833: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  May 27 12:42:47.838: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  May 27 12:42:47.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 27 12:42:47.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-5194" for this suite. @ 05/27/23 12:42:47.939
  STEP: Destroying namespace "sched-preemption-5590" for this suite. @ 05/27/23 12:42:47.947
• [60.270 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 05/27/23 12:42:47.959
  May 27 12:42:47.959: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename deployment @ 05/27/23 12:42:47.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:42:47.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:42:47.982
  May 27 12:42:47.986: INFO: Creating deployment "webserver-deployment"
  May 27 12:42:47.993: INFO: Waiting for observed generation 1
  May 27 12:42:50.007: INFO: Waiting for all required pods to come up
  May 27 12:42:50.018: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 05/27/23 12:42:50.018
  May 27 12:42:52.032: INFO: Waiting for deployment "webserver-deployment" to complete
  May 27 12:42:52.042: INFO: Updating deployment "webserver-deployment" with a non-existent image
  May 27 12:42:52.054: INFO: Updating deployment webserver-deployment
  May 27 12:42:52.054: INFO: Waiting for observed generation 2
  May 27 12:42:54.064: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  May 27 12:42:54.069: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  May 27 12:42:54.073: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May 27 12:42:54.087: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  May 27 12:42:54.087: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  May 27 12:42:54.091: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May 27 12:42:54.100: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  May 27 12:42:54.100: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  May 27 12:42:54.115: INFO: Updating deployment webserver-deployment
  May 27 12:42:54.116: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  May 27 12:42:54.137: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  May 27 12:42:54.152: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  May 27 12:42:54.186: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-5817  619e00f6-824d-42fe-b503-ac6e0156fc0f 11606 3 2023-05-27 12:42:47 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028cc7e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-05-27 12:42:52 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-27 12:42:54 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  May 27 12:42:54.213: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-5817  f10181c9-9d65-49ba-b52f-462e7eb0aa2e 11597 3 2023-05-27 12:42:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 619e00f6-824d-42fe-b503-ac6e0156fc0f 0xc0032af397 0xc0032af398}] [] [{kube-controller-manager Update apps/v1 2023-05-27 12:42:52 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"619e00f6-824d-42fe-b503-ac6e0156fc0f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032af438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 27 12:42:54.214: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  May 27 12:42:54.214: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-5817  f60ed1de-2f47-45e2-ac59-a767a864ac76 11594 3 2023-05-27 12:42:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 619e00f6-824d-42fe-b503-ac6e0156fc0f 0xc0032af2a7 0xc0032af2a8}] [] [{kube-controller-manager Update apps/v1 2023-05-27 12:42:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"619e00f6-824d-42fe-b503-ac6e0156fc0f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032af338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  May 27 12:42:54.238: INFO: Pod "webserver-deployment-67bd4bf6dc-4g4gb" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4g4gb webserver-deployment-67bd4bf6dc- deployment-5817  1673795c-ac5c-4aa2-aaae-3d12dce879a0 11450 0 2023-05-27 12:42:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f60ed1de-2f47-45e2-ac59-a767a864ac76 0xc0028ccc07 0xc0028ccc08}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f60ed1de-2f47-45e2-ac59-a767a864ac76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:42:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mstxs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mstxs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.22.3,PodIP:192.168.7.78,StartTime:2023-05-27 12:42:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-27 12:42:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3967136105774d2ad0d09f1bcd0b9fe2a6a71282e0b1ee66b981f58554e5c34d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.78,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.238: INFO: Pod "webserver-deployment-67bd4bf6dc-58bdm" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-58bdm webserver-deployment-67bd4bf6dc- deployment-5817  922dca19-ce93-4bb6-be76-612768894e9e 11623 0 2023-05-27 12:42:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f60ed1de-2f47-45e2-ac59-a767a864ac76 0xc0028ccdf7 0xc0028ccdf8}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f60ed1de-2f47-45e2-ac59-a767a864ac76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tjzxx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tjzxx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.239: INFO: Pod "webserver-deployment-67bd4bf6dc-725qr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-725qr webserver-deployment-67bd4bf6dc- deployment-5817  f8388501-5c7d-413c-b432-b329ad9f26f5 11609 0 2023-05-27 12:42:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f60ed1de-2f47-45e2-ac59-a767a864ac76 0xc0028ccfa0 0xc0028ccfa1}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f60ed1de-2f47-45e2-ac59-a767a864ac76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pd99p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pd99p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.172,PodIP:,StartTime:2023-05-27 12:42:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.239: INFO: Pod "webserver-deployment-67bd4bf6dc-7dsjq" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7dsjq webserver-deployment-67bd4bf6dc- deployment-5817  c1cbfef6-0a2f-4bee-a568-586867849f7f 11461 0 2023-05-27 12:42:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f60ed1de-2f47-45e2-ac59-a767a864ac76 0xc0028cd167 0xc0028cd168}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f60ed1de-2f47-45e2-ac59-a767a864ac76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:42:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fmqqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fmqqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.136,PodIP:192.168.0.18,StartTime:2023-05-27 12:42:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-27 12:42:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://722b0a8bb8690956d8c4a53121d464442d10abec906005b5775c00865fe3b768,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.18,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.239: INFO: Pod "webserver-deployment-67bd4bf6dc-7smj7" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7smj7 webserver-deployment-67bd4bf6dc- deployment-5817  9e5491f2-add1-42e3-8859-4f038df8f778 11464 0 2023-05-27 12:42:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f60ed1de-2f47-45e2-ac59-a767a864ac76 0xc0028cd357 0xc0028cd358}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f60ed1de-2f47-45e2-ac59-a767a864ac76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:42:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hbdk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hbdk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.136,PodIP:192.168.0.20,StartTime:2023-05-27 12:42:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-27 12:42:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c6abcecd0c254e99f171461949916fdd938d35adffd694f9200a3ff21e938e49,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.20,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.239: INFO: Pod "webserver-deployment-67bd4bf6dc-bjtrl" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-bjtrl webserver-deployment-67bd4bf6dc- deployment-5817  54b785c1-1d59-4469-b8be-41b616af1d62 11622 0 2023-05-27 12:42:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f60ed1de-2f47-45e2-ac59-a767a864ac76 0xc0028cd547 0xc0028cd548}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f60ed1de-2f47-45e2-ac59-a767a864ac76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-glphc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-glphc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.240: INFO: Pod "webserver-deployment-67bd4bf6dc-cd458" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-cd458 webserver-deployment-67bd4bf6dc- deployment-5817  4b149ba0-a5d8-4163-a30f-1ae455b41b5d 11607 0 2023-05-27 12:42:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f60ed1de-2f47-45e2-ac59-a767a864ac76 0xc0028cd6b0 0xc0028cd6b1}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f60ed1de-2f47-45e2-ac59-a767a864ac76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nqsl2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nqsl2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.240: INFO: Pod "webserver-deployment-67bd4bf6dc-crxvf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-crxvf webserver-deployment-67bd4bf6dc- deployment-5817  17c3ab30-f9ab-4123-913b-990f64a402c7 11624 0 2023-05-27 12:42:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f60ed1de-2f47-45e2-ac59-a767a864ac76 0xc0028cd810 0xc0028cd811}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f60ed1de-2f47-45e2-ac59-a767a864ac76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6kmq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6kmq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.240: INFO: Pod "webserver-deployment-67bd4bf6dc-hg8cz" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hg8cz webserver-deployment-67bd4bf6dc- deployment-5817  58fc1227-3cfb-454f-a528-b17165dff010 11618 0 2023-05-27 12:42:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f60ed1de-2f47-45e2-ac59-a767a864ac76 0xc0028cd970 0xc0028cd971}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f60ed1de-2f47-45e2-ac59-a767a864ac76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4f7ft,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4f7ft,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.22.3,PodIP:,StartTime:2023-05-27 12:42:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.240: INFO: Pod "webserver-deployment-67bd4bf6dc-l8llh" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-l8llh webserver-deployment-67bd4bf6dc- deployment-5817  e86d794a-a815-44a6-94d4-d7c8fc63ec63 11476 0 2023-05-27 12:42:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f60ed1de-2f47-45e2-ac59-a767a864ac76 0xc0028cdb37 0xc0028cdb38}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f60ed1de-2f47-45e2-ac59-a767a864ac76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:42:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.19.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d2xmn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d2xmn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.172,PodIP:192.168.19.80,StartTime:2023-05-27 12:42:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-27 12:42:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7df2c8d1e67b82414099fd7996dd67b2f3096cf10514f2e67e420ca8f856d687,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.19.80,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.241: INFO: Pod "webserver-deployment-67bd4bf6dc-lh5g9" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-lh5g9 webserver-deployment-67bd4bf6dc- deployment-5817  7f3b68fa-058f-4b8c-8db5-63f828062ff1 11453 0 2023-05-27 12:42:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f60ed1de-2f47-45e2-ac59-a767a864ac76 0xc0028cdd27 0xc0028cdd28}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f60ed1de-2f47-45e2-ac59-a767a864ac76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:42:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4crl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4crl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.22.3,PodIP:192.168.7.77,StartTime:2023-05-27 12:42:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-27 12:42:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c9a434a72a47c2de206493894a1be6198c79f04df8b3265e302cbb5d69e29a39,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.77,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.241: INFO: Pod "webserver-deployment-67bd4bf6dc-nrbfd" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nrbfd webserver-deployment-67bd4bf6dc- deployment-5817  4fe2eaa1-3ca8-4a9e-97d3-4e63b6caf2e8 11467 0 2023-05-27 12:42:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f60ed1de-2f47-45e2-ac59-a767a864ac76 0xc001d4dc77 0xc001d4dc78}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f60ed1de-2f47-45e2-ac59-a767a864ac76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:42:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.19.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jc7jp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jc7jp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.172,PodIP:192.168.19.84,StartTime:2023-05-27 12:42:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-27 12:42:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f5f205eae6ed1bddaac4e56bd5d92a3941d5de226d28485e57190f536d731374,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.19.84,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.241: INFO: Pod "webserver-deployment-67bd4bf6dc-qscvt" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-qscvt webserver-deployment-67bd4bf6dc- deployment-5817  9e2c1f01-7c66-4e53-9b1a-6a1b9c4af7a8 11625 0 2023-05-27 12:42:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f60ed1de-2f47-45e2-ac59-a767a864ac76 0xc001d4de77 0xc001d4de78}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f60ed1de-2f47-45e2-ac59-a767a864ac76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mqvdt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mqvdt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.136,PodIP:,StartTime:2023-05-27 12:42:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.241: INFO: Pod "webserver-deployment-67bd4bf6dc-tkd2g" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tkd2g webserver-deployment-67bd4bf6dc- deployment-5817  7b2074f1-0163-4376-9ba0-ecf1636ad804 11446 0 2023-05-27 12:42:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f60ed1de-2f47-45e2-ac59-a767a864ac76 0xc002578077 0xc002578078}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f60ed1de-2f47-45e2-ac59-a767a864ac76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:42:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ldd5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ldd5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.22.3,PodIP:192.168.7.79,StartTime:2023-05-27 12:42:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-27 12:42:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9cf02ab806e6cdd8d8cf885192af2bdcbefb8e37cc797a6ca46b60337b6ad7fc,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.79,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.242: INFO: Pod "webserver-deployment-67bd4bf6dc-w5mrw" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-w5mrw webserver-deployment-67bd4bf6dc- deployment-5817  dac5ca43-2990-4cbe-b008-0f26145b152e 11458 0 2023-05-27 12:42:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f60ed1de-2f47-45e2-ac59-a767a864ac76 0xc002578277 0xc002578278}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f60ed1de-2f47-45e2-ac59-a767a864ac76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:42:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tkfhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tkfhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.136,PodIP:192.168.0.19,StartTime:2023-05-27 12:42:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-27 12:42:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9fc414cf09bae8f14068cb3d0250c0fade805c84f67fd163556e257ab53fd59a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.19,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.243: INFO: Pod "webserver-deployment-7b75d79cf5-4dptz" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-4dptz webserver-deployment-7b75d79cf5- deployment-5817  81600999-839e-4c99-a9e5-e7f0f613cb35 11588 0 2023-05-27 12:42:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f10181c9-9d65-49ba-b52f-462e7eb0aa2e 0xc002578467 0xc002578468}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f10181c9-9d65-49ba-b52f-462e7eb0aa2e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:42:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cx4bg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cx4bg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.136,PodIP:192.168.0.21,StartTime:2023-05-27 12:42:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.21,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.245: INFO: Pod "webserver-deployment-7b75d79cf5-c8x8t" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-c8x8t webserver-deployment-7b75d79cf5- deployment-5817  b7c8cf45-6a6b-4fae-a27d-05f3fbfc37c9 11494 0 2023-05-27 12:42:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f10181c9-9d65-49ba-b52f-462e7eb0aa2e 0xc002578687 0xc002578688}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f10181c9-9d65-49ba-b52f-462e7eb0aa2e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:42:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8792l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8792l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.172,PodIP:,StartTime:2023-05-27 12:42:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.246: INFO: Pod "webserver-deployment-7b75d79cf5-cd2m6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-cd2m6 webserver-deployment-7b75d79cf5- deployment-5817  1b3bc840-c960-432b-af64-b2d8b863abc0 11592 0 2023-05-27 12:42:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f10181c9-9d65-49ba-b52f-462e7eb0aa2e 0xc002578887 0xc002578888}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f10181c9-9d65-49ba-b52f-462e7eb0aa2e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tx7kj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tx7kj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.136,PodIP:192.168.0.22,StartTime:2023-05-27 12:42:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.22,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.246: INFO: Pod "webserver-deployment-7b75d79cf5-gnb9d" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-gnb9d webserver-deployment-7b75d79cf5- deployment-5817  b258090d-0313-44b5-891f-8988337a3e1d 11530 0 2023-05-27 12:42:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f10181c9-9d65-49ba-b52f-462e7eb0aa2e 0xc002578ab7 0xc002578ab8}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f10181c9-9d65-49ba-b52f-462e7eb0aa2e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:42:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wx4z7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wx4z7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.172,PodIP:,StartTime:2023-05-27 12:42:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.247: INFO: Pod "webserver-deployment-7b75d79cf5-llxmk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-llxmk webserver-deployment-7b75d79cf5- deployment-5817  1ac4699d-3598-4bc5-bbca-bfbc31fea5c0 11583 0 2023-05-27 12:42:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f10181c9-9d65-49ba-b52f-462e7eb0aa2e 0xc002578cb7 0xc002578cb8}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f10181c9-9d65-49ba-b52f-462e7eb0aa2e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:42:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6fjnh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6fjnh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.22.3,PodIP:192.168.7.80,StartTime:2023-05-27 12:42:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.80,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.247: INFO: Pod "webserver-deployment-7b75d79cf5-shnmf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-shnmf webserver-deployment-7b75d79cf5- deployment-5817  66617d2e-e202-4dfd-a8ef-cd6e29525871 11616 0 2023-05-27 12:42:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f10181c9-9d65-49ba-b52f-462e7eb0aa2e 0xc002578f00 0xc002578f01}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f10181c9-9d65-49ba-b52f-462e7eb0aa2e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gbhvk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gbhvk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.248: INFO: Pod "webserver-deployment-7b75d79cf5-t77zk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-t77zk webserver-deployment-7b75d79cf5- deployment-5817  6791a3b6-a50b-4a2b-b99c-a5db5927d18e 11610 0 2023-05-27 12:42:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f10181c9-9d65-49ba-b52f-462e7eb0aa2e 0xc0025790c7 0xc0025790c8}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f10181c9-9d65-49ba-b52f-462e7eb0aa2e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-96gkw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-96gkw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.248: INFO: Pod "webserver-deployment-7b75d79cf5-tg2rq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-tg2rq webserver-deployment-7b75d79cf5- deployment-5817  ee2c5423-f2fc-4201-aca4-009de2a3bdd0 11621 0 2023-05-27 12:42:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f10181c9-9d65-49ba-b52f-462e7eb0aa2e 0xc002579240 0xc002579241}] [] [{kube-controller-manager Update v1 2023-05-27 12:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f10181c9-9d65-49ba-b52f-462e7eb0aa2e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7xkfs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7xkfs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:42:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:42:54.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5817" for this suite. @ 05/27/23 12:42:54.297
• [6.389 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 05/27/23 12:42:54.347
  May 27 12:42:54.347: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 12:42:54.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:42:54.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:42:54.42
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 12:42:54.427
  STEP: Saw pod success @ 05/27/23 12:42:58.472
  May 27 12:42:58.477: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-333a6656-037d-43bd-9878-b51f40af320c container client-container: <nil>
  STEP: delete the pod @ 05/27/23 12:42:58.495
  May 27 12:42:58.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7360" for this suite. @ 05/27/23 12:42:58.521
• [4.183 seconds]
------------------------------
S
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 05/27/23 12:42:58.531
  May 27 12:42:58.531: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename security-context-test @ 05/27/23 12:42:58.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:42:58.553
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:42:58.558
  May 27 12:43:04.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-33" for this suite. @ 05/27/23 12:43:04.613
• [6.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 05/27/23 12:43:04.624
  May 27 12:43:04.624: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename replication-controller @ 05/27/23 12:43:04.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:43:04.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:43:04.652
  May 27 12:43:04.657: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 05/27/23 12:43:05.673
  STEP: Checking rc "condition-test" has the desired failure condition set @ 05/27/23 12:43:05.68
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 05/27/23 12:43:06.69
  May 27 12:43:06.703: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 05/27/23 12:43:06.703
  May 27 12:43:07.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7344" for this suite. @ 05/27/23 12:43:07.717
• [3.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 05/27/23 12:43:07.727
  May 27 12:43:07.728: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename events @ 05/27/23 12:43:07.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:43:07.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:43:07.752
  STEP: Create set of events @ 05/27/23 12:43:07.757
  STEP: get a list of Events with a label in the current namespace @ 05/27/23 12:43:07.781
  STEP: delete a list of events @ 05/27/23 12:43:07.785
  May 27 12:43:07.785: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/27/23 12:43:07.82
  May 27 12:43:07.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4454" for this suite. @ 05/27/23 12:43:07.833
• [0.115 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 05/27/23 12:43:07.844
  May 27 12:43:07.844: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename pods @ 05/27/23 12:43:07.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:43:07.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:43:07.87
  STEP: creating pod @ 05/27/23 12:43:07.875
  May 27 12:43:09.916: INFO: Pod pod-hostip-57988c87-b5f0-41d2-b7c5-f8529ac38366 has hostIP: 172.31.68.172
  May 27 12:43:09.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4355" for this suite. @ 05/27/23 12:43:09.931
• [2.104 seconds]
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 05/27/23 12:43:09.947
  May 27 12:43:09.948: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename secrets @ 05/27/23 12:43:09.949
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:43:10
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:43:10.012
  STEP: Creating secret with name s-test-opt-del-c2faa912-d1ad-4183-929f-a727def0f7b8 @ 05/27/23 12:43:10.025
  STEP: Creating secret with name s-test-opt-upd-afba2c2f-c3f4-449a-89ae-09a3c00556dc @ 05/27/23 12:43:10.034
  STEP: Creating the pod @ 05/27/23 12:43:10.04
  STEP: Deleting secret s-test-opt-del-c2faa912-d1ad-4183-929f-a727def0f7b8 @ 05/27/23 12:43:12.111
  STEP: Updating secret s-test-opt-upd-afba2c2f-c3f4-449a-89ae-09a3c00556dc @ 05/27/23 12:43:12.122
  STEP: Creating secret with name s-test-opt-create-0e793047-b33a-406c-8b0a-e03ed48024d6 @ 05/27/23 12:43:12.129
  STEP: waiting to observe update in volume @ 05/27/23 12:43:12.135
  May 27 12:44:30.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2221" for this suite. @ 05/27/23 12:44:30.588
• [80.653 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 05/27/23 12:44:30.603
  May 27 12:44:30.603: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/27/23 12:44:30.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:44:30.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:44:30.64
  May 27 12:44:30.645: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:44:31.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4692" for this suite. @ 05/27/23 12:44:31.23
• [0.636 seconds]
------------------------------
S
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 05/27/23 12:44:31.239
  May 27 12:44:31.239: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename endpointslice @ 05/27/23 12:44:31.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:44:31.259
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:44:31.271
  STEP: referencing a single matching pod @ 05/27/23 12:44:36.376
  STEP: referencing matching pods with named port @ 05/27/23 12:44:41.39
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 05/27/23 12:44:46.4
  STEP: recreating EndpointSlices after they've been deleted @ 05/27/23 12:44:51.413
  May 27 12:44:51.442: INFO: EndpointSlice for Service endpointslice-4436/example-named-port not found
  May 27 12:45:01.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4436" for this suite. @ 05/27/23 12:45:01.467
• [30.236 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 05/27/23 12:45:01.476
  May 27 12:45:01.476: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename containers @ 05/27/23 12:45:01.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:45:01.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:45:01.503
  STEP: Creating a pod to test override all @ 05/27/23 12:45:01.511
  STEP: Saw pod success @ 05/27/23 12:45:05.543
  May 27 12:45:05.548: INFO: Trying to get logs from node ip-172-31-68-172 pod client-containers-2a43c767-dc95-4a06-84b1-88d68bc02347 container agnhost-container: <nil>
  STEP: delete the pod @ 05/27/23 12:45:05.557
  May 27 12:45:05.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8498" for this suite. @ 05/27/23 12:45:05.582
• [4.117 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 05/27/23 12:45:05.594
  May 27 12:45:05.594: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename configmap @ 05/27/23 12:45:05.595
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:45:05.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:45:05.63
  STEP: Creating configMap with name configmap-test-volume-map-354b4fcc-cdbb-467c-8347-2642330c2f26 @ 05/27/23 12:45:05.636
  STEP: Creating a pod to test consume configMaps @ 05/27/23 12:45:05.644
  STEP: Saw pod success @ 05/27/23 12:45:09.689
  May 27 12:45:09.693: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-configmaps-4fa40303-d22f-4ada-9afd-f9dc640c6be6 container agnhost-container: <nil>
  STEP: delete the pod @ 05/27/23 12:45:09.703
  May 27 12:45:09.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5861" for this suite. @ 05/27/23 12:45:09.732
• [4.152 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 05/27/23 12:45:09.751
  May 27 12:45:09.751: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename daemonsets @ 05/27/23 12:45:09.752
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:45:09.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:45:09.787
  STEP: Creating simple DaemonSet "daemon-set" @ 05/27/23 12:45:09.831
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/27/23 12:45:09.841
  May 27 12:45:09.849: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:45:09.849: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:45:09.859: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 12:45:09.859: INFO: Node ip-172-31-10-136 is running 0 daemon pod, expected 1
  May 27 12:45:10.868: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:45:10.868: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:45:10.877: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 27 12:45:10.877: INFO: Node ip-172-31-10-136 is running 0 daemon pod, expected 1
  May 27 12:45:11.868: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:45:11.869: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 12:45:11.876: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 27 12:45:11.876: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 05/27/23 12:45:11.881
  May 27 12:45:11.886: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 05/27/23 12:45:11.887
  May 27 12:45:11.905: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 05/27/23 12:45:11.905
  May 27 12:45:11.909: INFO: Observed &DaemonSet event: ADDED
  May 27 12:45:11.909: INFO: Observed &DaemonSet event: MODIFIED
  May 27 12:45:11.910: INFO: Observed &DaemonSet event: MODIFIED
  May 27 12:45:11.910: INFO: Observed &DaemonSet event: MODIFIED
  May 27 12:45:11.910: INFO: Observed &DaemonSet event: MODIFIED
  May 27 12:45:11.910: INFO: Found daemon set daemon-set in namespace daemonsets-5149 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 27 12:45:11.911: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 05/27/23 12:45:11.911
  STEP: watching for the daemon set status to be patched @ 05/27/23 12:45:11.922
  May 27 12:45:11.929: INFO: Observed &DaemonSet event: ADDED
  May 27 12:45:11.929: INFO: Observed &DaemonSet event: MODIFIED
  May 27 12:45:11.929: INFO: Observed &DaemonSet event: MODIFIED
  May 27 12:45:11.930: INFO: Observed &DaemonSet event: MODIFIED
  May 27 12:45:11.930: INFO: Observed &DaemonSet event: MODIFIED
  May 27 12:45:11.930: INFO: Observed daemon set daemon-set in namespace daemonsets-5149 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 27 12:45:11.931: INFO: Observed &DaemonSet event: MODIFIED
  May 27 12:45:11.931: INFO: Found daemon set daemon-set in namespace daemonsets-5149 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  May 27 12:45:11.931: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 05/27/23 12:45:11.94
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5149, will wait for the garbage collector to delete the pods @ 05/27/23 12:45:11.94
  May 27 12:45:12.010: INFO: Deleting DaemonSet.extensions daemon-set took: 14.153509ms
  May 27 12:45:12.111: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.707116ms
  May 27 12:45:13.315: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 12:45:13.315: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 27 12:45:13.320: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12691"},"items":null}

  May 27 12:45:13.326: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12691"},"items":null}

  May 27 12:45:13.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5149" for this suite. @ 05/27/23 12:45:13.351
• [3.611 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 05/27/23 12:45:13.364
  May 27 12:45:13.365: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-probe @ 05/27/23 12:45:13.366
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:45:13.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:45:13.419
  STEP: Creating pod busybox-cf9fc047-d19d-4499-aa6b-c50f367786cc in namespace container-probe-8833 @ 05/27/23 12:45:13.425
  May 27 12:45:15.464: INFO: Started pod busybox-cf9fc047-d19d-4499-aa6b-c50f367786cc in namespace container-probe-8833
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/27/23 12:45:15.464
  May 27 12:45:15.471: INFO: Initial restart count of pod busybox-cf9fc047-d19d-4499-aa6b-c50f367786cc is 0
  May 27 12:49:16.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/27/23 12:49:16.158
  STEP: Destroying namespace "container-probe-8833" for this suite. @ 05/27/23 12:49:16.174
• [242.819 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 05/27/23 12:49:16.186
  May 27 12:49:16.186: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 12:49:16.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:49:16.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:49:16.226
  STEP: Creating configMap with name projected-configmap-test-volume-map-c2a35558-439e-489a-8c8e-a3a69c29a854 @ 05/27/23 12:49:16.232
  STEP: Creating a pod to test consume configMaps @ 05/27/23 12:49:16.239
  STEP: Saw pod success @ 05/27/23 12:49:20.273
  May 27 12:49:20.278: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-projected-configmaps-6d76914a-ea96-40c9-b295-817cf31ba85c container agnhost-container: <nil>
  STEP: delete the pod @ 05/27/23 12:49:20.303
  May 27 12:49:20.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1314" for this suite. @ 05/27/23 12:49:20.328
• [4.151 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 05/27/23 12:49:20.34
  May 27 12:49:20.340: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename svcaccounts @ 05/27/23 12:49:20.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:49:20.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:49:20.365
  STEP: Creating a pod to test service account token:  @ 05/27/23 12:49:20.37
  STEP: Saw pod success @ 05/27/23 12:49:24.402
  May 27 12:49:24.407: INFO: Trying to get logs from node ip-172-31-68-172 pod test-pod-aeb22e68-6587-4052-adcb-f66582a440e3 container agnhost-container: <nil>
  STEP: delete the pod @ 05/27/23 12:49:24.415
  May 27 12:49:24.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9341" for this suite. @ 05/27/23 12:49:24.438
• [4.107 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 05/27/23 12:49:24.449
  May 27 12:49:24.449: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename sched-pred @ 05/27/23 12:49:24.45
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:49:24.468
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:49:24.473
  May 27 12:49:24.517: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 27 12:49:24.528: INFO: Waiting for terminating namespaces to be deleted...
  May 27 12:49:24.532: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-10-136 before test
  May 27 12:49:24.538: INFO: default-http-backend-kubernetes-worker-65fc475d49-8hlsq from ingress-nginx-kubernetes-worker started at 2023-05-27 12:22:35 +0000 UTC (1 container statuses recorded)
  May 27 12:49:24.539: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  May 27 12:49:24.539: INFO: nginx-ingress-controller-kubernetes-worker-jkk42 from ingress-nginx-kubernetes-worker started at 2023-05-27 12:11:29 +0000 UTC (1 container statuses recorded)
  May 27 12:49:24.539: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  May 27 12:49:24.539: INFO: calico-kube-controllers-79678b7759-2xtvf from kube-system started at 2023-05-27 12:22:35 +0000 UTC (1 container statuses recorded)
  May 27 12:49:24.539: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 27 12:49:24.539: INFO: sonobuoy from sonobuoy started at 2023-05-27 12:15:40 +0000 UTC (1 container statuses recorded)
  May 27 12:49:24.539: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 27 12:49:24.539: INFO: sonobuoy-e2e-job-0cf94b30a28f4573 from sonobuoy started at 2023-05-27 12:15:43 +0000 UTC (2 container statuses recorded)
  May 27 12:49:24.539: INFO: 	Container e2e ready: true, restart count 0
  May 27 12:49:24.539: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 27 12:49:24.539: INFO: sonobuoy-systemd-logs-daemon-set-f9890aca51654f74-zr2qt from sonobuoy started at 2023-05-27 12:15:43 +0000 UTC (2 container statuses recorded)
  May 27 12:49:24.539: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 27 12:49:24.539: INFO: 	Container systemd-logs ready: true, restart count 0
  May 27 12:49:24.539: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-22-3 before test
  May 27 12:49:24.546: INFO: nginx-ingress-controller-kubernetes-worker-2hlwz from ingress-nginx-kubernetes-worker started at 2023-05-27 11:59:07 +0000 UTC (1 container statuses recorded)
  May 27 12:49:24.546: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  May 27 12:49:24.546: INFO: coredns-5c7f76ccb8-b4zh4 from kube-system started at 2023-05-27 11:59:00 +0000 UTC (1 container statuses recorded)
  May 27 12:49:24.546: INFO: 	Container coredns ready: true, restart count 0
  May 27 12:49:24.546: INFO: kube-state-metrics-5b95b4459c-8rvst from kube-system started at 2023-05-27 11:59:00 +0000 UTC (1 container statuses recorded)
  May 27 12:49:24.546: INFO: 	Container kube-state-metrics ready: true, restart count 0
  May 27 12:49:24.546: INFO: metrics-server-v0.5.2-6cf8c8b69c-t499d from kube-system started at 2023-05-27 11:59:00 +0000 UTC (2 container statuses recorded)
  May 27 12:49:24.546: INFO: 	Container metrics-server ready: true, restart count 0
  May 27 12:49:24.546: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  May 27 12:49:24.546: INFO: dashboard-metrics-scraper-6b8586b5c9-7kfbs from kubernetes-dashboard started at 2023-05-27 11:59:00 +0000 UTC (1 container statuses recorded)
  May 27 12:49:24.546: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  May 27 12:49:24.546: INFO: kubernetes-dashboard-6869f4cd5f-js8mr from kubernetes-dashboard started at 2023-05-27 11:59:00 +0000 UTC (1 container statuses recorded)
  May 27 12:49:24.546: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  May 27 12:49:24.546: INFO: sonobuoy-systemd-logs-daemon-set-f9890aca51654f74-c4nxx from sonobuoy started at 2023-05-27 12:15:43 +0000 UTC (2 container statuses recorded)
  May 27 12:49:24.546: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 27 12:49:24.546: INFO: 	Container systemd-logs ready: true, restart count 0
  May 27 12:49:24.546: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-68-172 before test
  May 27 12:49:24.553: INFO: nginx-ingress-controller-kubernetes-worker-gtdv9 from ingress-nginx-kubernetes-worker started at 2023-05-27 12:23:00 +0000 UTC (1 container statuses recorded)
  May 27 12:49:24.553: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  May 27 12:49:24.553: INFO: sonobuoy-systemd-logs-daemon-set-f9890aca51654f74-mmhtp from sonobuoy started at 2023-05-27 12:15:43 +0000 UTC (2 container statuses recorded)
  May 27 12:49:24.553: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 27 12:49:24.553: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/27/23 12:49:24.554
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/27/23 12:49:26.582
  STEP: Trying to apply a random label on the found node. @ 05/27/23 12:49:26.602
  STEP: verifying the node has the label kubernetes.io/e2e-49edb47f-f988-42a1-ba18-c8364e73a719 95 @ 05/27/23 12:49:26.613
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 05/27/23 12:49:26.618
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.68.172 on the node which pod4 resides and expect not scheduled @ 05/27/23 12:49:28.638
  STEP: removing the label kubernetes.io/e2e-49edb47f-f988-42a1-ba18-c8364e73a719 off the node ip-172-31-68-172 @ 05/27/23 12:54:28.647
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-49edb47f-f988-42a1-ba18-c8364e73a719 @ 05/27/23 12:54:28.664
  May 27 12:54:28.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9139" for this suite. @ 05/27/23 12:54:28.677
• [304.237 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 05/27/23 12:54:28.688
  May 27 12:54:28.688: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename configmap @ 05/27/23 12:54:28.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:54:28.765
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:54:28.775
  STEP: Creating configMap with name configmap-test-volume-e1ab50b1-5705-4615-9bcb-509b4e74e21d @ 05/27/23 12:54:28.783
  STEP: Creating a pod to test consume configMaps @ 05/27/23 12:54:28.789
  STEP: Saw pod success @ 05/27/23 12:54:32.825
  May 27 12:54:32.830: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-configmaps-4d98b6df-74c4-473c-ac21-3ccb0137a3ec container agnhost-container: <nil>
  STEP: delete the pod @ 05/27/23 12:54:32.855
  May 27 12:54:32.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2913" for this suite. @ 05/27/23 12:54:32.878
• [4.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 05/27/23 12:54:32.893
  May 27 12:54:32.893: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename gc @ 05/27/23 12:54:32.895
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:54:32.911
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:54:32.917
  STEP: create the deployment @ 05/27/23 12:54:32.922
  W0527 12:54:32.928237      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/27/23 12:54:32.928
  STEP: delete the deployment @ 05/27/23 12:54:33.439
  STEP: wait for all rs to be garbage collected @ 05/27/23 12:54:33.449
  STEP: expected 0 rs, got 1 rs @ 05/27/23 12:54:33.457
  STEP: expected 0 pods, got 2 pods @ 05/27/23 12:54:33.462
  STEP: Gathering metrics @ 05/27/23 12:54:33.977
  W0527 12:54:33.982089      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May 27 12:54:33.982: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 27 12:54:33.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-742" for this suite. @ 05/27/23 12:54:33.993
• [1.110 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 05/27/23 12:54:34.004
  May 27 12:54:34.004: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename resourcequota @ 05/27/23 12:54:34.005
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:54:34.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:54:34.035
  STEP: Counting existing ResourceQuota @ 05/27/23 12:54:34.039
  STEP: Creating a ResourceQuota @ 05/27/23 12:54:39.044
  STEP: Ensuring resource quota status is calculated @ 05/27/23 12:54:39.054
  STEP: Creating a Service @ 05/27/23 12:54:41.06
  STEP: Creating a NodePort Service @ 05/27/23 12:54:41.084
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 05/27/23 12:54:41.141
  STEP: Ensuring resource quota status captures service creation @ 05/27/23 12:54:41.172
  STEP: Deleting Services @ 05/27/23 12:54:43.178
  STEP: Ensuring resource quota status released usage @ 05/27/23 12:54:43.253
  May 27 12:54:45.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9400" for this suite. @ 05/27/23 12:54:45.266
• [11.278 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 05/27/23 12:54:45.289
  May 27 12:54:45.289: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/27/23 12:54:45.291
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:54:45.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:54:45.324
  May 27 12:54:45.329: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:54:51.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-896" for this suite. @ 05/27/23 12:54:51.713
• [6.439 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 05/27/23 12:54:51.731
  May 27 12:54:51.731: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename pods @ 05/27/23 12:54:51.732
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:54:51.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:54:51.776
  STEP: creating the pod @ 05/27/23 12:54:51.782
  STEP: submitting the pod to kubernetes @ 05/27/23 12:54:51.782
  W0527 12:54:51.799404      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 05/27/23 12:54:53.818
  STEP: updating the pod @ 05/27/23 12:54:53.822
  May 27 12:54:54.338: INFO: Successfully updated pod "pod-update-activedeadlineseconds-f1fc9e55-2904-47e2-8410-834e15b9f845"
  May 27 12:54:58.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6221" for this suite. @ 05/27/23 12:54:58.356
• [6.633 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 05/27/23 12:54:58.366
  May 27 12:54:58.366: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename services @ 05/27/23 12:54:58.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:54:58.385
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:54:58.39
  STEP: creating service nodeport-test with type=NodePort in namespace services-3563 @ 05/27/23 12:54:58.395
  STEP: creating replication controller nodeport-test in namespace services-3563 @ 05/27/23 12:54:58.414
  I0527 12:54:58.437297      18 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-3563, replica count: 2
  I0527 12:55:01.488429      18 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 27 12:55:01.488: INFO: Creating new exec pod
  May 27 12:55:04.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-3563 exec execpodm4vn6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  May 27 12:55:04.707: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  May 27 12:55:04.707: INFO: stdout: "nodeport-test-s8rrx"
  May 27 12:55:04.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-3563 exec execpodm4vn6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.61 80'
  May 27 12:55:04.869: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.61 80\nConnection to 10.152.183.61 80 port [tcp/http] succeeded!\n"
  May 27 12:55:04.869: INFO: stdout: "nodeport-test-jnwzp"
  May 27 12:55:04.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-3563 exec execpodm4vn6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.22.3 32385'
  May 27 12:55:05.029: INFO: stderr: "+ nc -v -t -w 2 172.31.22.3 32385\n+ echo hostName\nConnection to 172.31.22.3 32385 port [tcp/*] succeeded!\n"
  May 27 12:55:05.029: INFO: stdout: "nodeport-test-jnwzp"
  May 27 12:55:05.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-3563 exec execpodm4vn6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.68.172 32385'
  May 27 12:55:05.181: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.68.172 32385\nConnection to 172.31.68.172 32385 port [tcp/*] succeeded!\n"
  May 27 12:55:05.181: INFO: stdout: "nodeport-test-jnwzp"
  May 27 12:55:05.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3563" for this suite. @ 05/27/23 12:55:05.188
• [6.831 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 05/27/23 12:55:05.198
  May 27 12:55:05.198: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename resourcequota @ 05/27/23 12:55:05.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:55:05.216
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:55:05.223
  STEP: Counting existing ResourceQuota @ 05/27/23 12:55:22.234
  STEP: Creating a ResourceQuota @ 05/27/23 12:55:27.239
  STEP: Ensuring resource quota status is calculated @ 05/27/23 12:55:27.249
  STEP: Creating a ConfigMap @ 05/27/23 12:55:29.254
  STEP: Ensuring resource quota status captures configMap creation @ 05/27/23 12:55:29.269
  STEP: Deleting a ConfigMap @ 05/27/23 12:55:31.276
  STEP: Ensuring resource quota status released usage @ 05/27/23 12:55:31.287
  May 27 12:55:33.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9596" for this suite. @ 05/27/23 12:55:33.298
• [28.109 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 05/27/23 12:55:33.308
  May 27 12:55:33.308: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 12:55:33.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:55:33.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:55:33.336
  STEP: Setting up server cert @ 05/27/23 12:55:33.376
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 12:55:33.736
  STEP: Deploying the webhook pod @ 05/27/23 12:55:33.747
  STEP: Wait for the deployment to be ready @ 05/27/23 12:55:33.765
  May 27 12:55:33.773: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/27/23 12:55:35.789
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 12:55:35.811
  May 27 12:55:36.812: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/27/23 12:55:36.897
  STEP: Creating a configMap that should be mutated @ 05/27/23 12:55:36.914
  STEP: Deleting the collection of validation webhooks @ 05/27/23 12:55:36.951
  STEP: Creating a configMap that should not be mutated @ 05/27/23 12:55:37.016
  May 27 12:55:37.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2124" for this suite. @ 05/27/23 12:55:37.11
  STEP: Destroying namespace "webhook-markers-7177" for this suite. @ 05/27/23 12:55:37.123
• [3.829 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 05/27/23 12:55:37.138
  May 27 12:55:37.138: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir @ 05/27/23 12:55:37.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:55:37.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:55:37.164
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/27/23 12:55:37.17
  STEP: Saw pod success @ 05/27/23 12:55:41.2
  May 27 12:55:41.204: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-cd0b1169-c2f5-432f-81eb-96b0fdb44ade container test-container: <nil>
  STEP: delete the pod @ 05/27/23 12:55:41.214
  May 27 12:55:41.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9477" for this suite. @ 05/27/23 12:55:41.239
• [4.110 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 05/27/23 12:55:41.249
  May 27 12:55:41.249: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-runtime @ 05/27/23 12:55:41.25
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:55:41.269
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:55:41.274
  STEP: create the container @ 05/27/23 12:55:41.279
  W0527 12:55:41.292775      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/27/23 12:55:41.293
  STEP: get the container status @ 05/27/23 12:55:45.32
  STEP: the container should be terminated @ 05/27/23 12:55:45.325
  STEP: the termination message should be set @ 05/27/23 12:55:45.325
  May 27 12:55:45.326: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 05/27/23 12:55:45.326
  May 27 12:55:45.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5130" for this suite. @ 05/27/23 12:55:45.352
• [4.115 seconds]
------------------------------
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 05/27/23 12:55:45.364
  May 27 12:55:45.364: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 05/27/23 12:55:45.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:55:45.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:55:45.394
  STEP: Setting up the test @ 05/27/23 12:55:45.404
  STEP: Creating hostNetwork=false pod @ 05/27/23 12:55:45.404
  STEP: Creating hostNetwork=true pod @ 05/27/23 12:55:47.435
  STEP: Running the test @ 05/27/23 12:55:49.46
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 05/27/23 12:55:49.46
  May 27 12:55:49.460: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2501 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 12:55:49.460: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:55:49.461: INFO: ExecWithOptions: Clientset creation
  May 27 12:55:49.461: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2501/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 27 12:55:49.538: INFO: Exec stderr: ""
  May 27 12:55:49.538: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2501 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 12:55:49.538: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:55:49.539: INFO: ExecWithOptions: Clientset creation
  May 27 12:55:49.539: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2501/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 27 12:55:49.610: INFO: Exec stderr: ""
  May 27 12:55:49.611: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2501 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 12:55:49.611: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:55:49.612: INFO: ExecWithOptions: Clientset creation
  May 27 12:55:49.612: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2501/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 27 12:55:49.701: INFO: Exec stderr: ""
  May 27 12:55:49.702: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2501 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 12:55:49.702: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:55:49.702: INFO: ExecWithOptions: Clientset creation
  May 27 12:55:49.703: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2501/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 27 12:55:49.774: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 05/27/23 12:55:49.774
  May 27 12:55:49.774: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2501 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 12:55:49.774: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:55:49.775: INFO: ExecWithOptions: Clientset creation
  May 27 12:55:49.775: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2501/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May 27 12:55:49.842: INFO: Exec stderr: ""
  May 27 12:55:49.842: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2501 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 12:55:49.843: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:55:49.843: INFO: ExecWithOptions: Clientset creation
  May 27 12:55:49.844: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2501/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May 27 12:55:49.918: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 05/27/23 12:55:49.918
  May 27 12:55:49.918: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2501 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 12:55:49.918: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:55:49.919: INFO: ExecWithOptions: Clientset creation
  May 27 12:55:49.919: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2501/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 27 12:55:49.990: INFO: Exec stderr: ""
  May 27 12:55:49.990: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2501 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 12:55:49.990: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:55:49.991: INFO: ExecWithOptions: Clientset creation
  May 27 12:55:49.991: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2501/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 27 12:55:50.068: INFO: Exec stderr: ""
  May 27 12:55:50.068: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2501 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 12:55:50.068: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:55:50.069: INFO: ExecWithOptions: Clientset creation
  May 27 12:55:50.069: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2501/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 27 12:55:50.140: INFO: Exec stderr: ""
  May 27 12:55:50.141: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2501 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 12:55:50.141: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:55:50.142: INFO: ExecWithOptions: Clientset creation
  May 27 12:55:50.142: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2501/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 27 12:55:50.218: INFO: Exec stderr: ""
  May 27 12:55:50.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-2501" for this suite. @ 05/27/23 12:55:50.224
• [4.868 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 05/27/23 12:55:50.236
  May 27 12:55:50.236: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename gc @ 05/27/23 12:55:50.237
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:55:50.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:55:50.262
  STEP: create the rc @ 05/27/23 12:55:50.277
  W0527 12:55:50.286928      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 05/27/23 12:55:56.294
  STEP: wait for the rc to be deleted @ 05/27/23 12:55:56.304
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 05/27/23 12:56:01.319
  STEP: Gathering metrics @ 05/27/23 12:56:31.336
  W0527 12:56:31.342818      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May 27 12:56:31.342: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 27 12:56:31.343: INFO: Deleting pod "simpletest.rc-26grj" in namespace "gc-8561"
  May 27 12:56:31.360: INFO: Deleting pod "simpletest.rc-2bthf" in namespace "gc-8561"
  May 27 12:56:31.376: INFO: Deleting pod "simpletest.rc-2gtss" in namespace "gc-8561"
  May 27 12:56:31.391: INFO: Deleting pod "simpletest.rc-2mhp2" in namespace "gc-8561"
  May 27 12:56:31.408: INFO: Deleting pod "simpletest.rc-2q28b" in namespace "gc-8561"
  May 27 12:56:31.423: INFO: Deleting pod "simpletest.rc-4jn6x" in namespace "gc-8561"
  May 27 12:56:31.438: INFO: Deleting pod "simpletest.rc-4tfbr" in namespace "gc-8561"
  May 27 12:56:31.457: INFO: Deleting pod "simpletest.rc-566sk" in namespace "gc-8561"
  May 27 12:56:31.474: INFO: Deleting pod "simpletest.rc-58h79" in namespace "gc-8561"
  May 27 12:56:31.490: INFO: Deleting pod "simpletest.rc-67mwb" in namespace "gc-8561"
  May 27 12:56:31.509: INFO: Deleting pod "simpletest.rc-6q2t7" in namespace "gc-8561"
  May 27 12:56:31.522: INFO: Deleting pod "simpletest.rc-6qbjr" in namespace "gc-8561"
  May 27 12:56:31.543: INFO: Deleting pod "simpletest.rc-7bw49" in namespace "gc-8561"
  May 27 12:56:31.558: INFO: Deleting pod "simpletest.rc-7f9cp" in namespace "gc-8561"
  May 27 12:56:31.574: INFO: Deleting pod "simpletest.rc-7gxxv" in namespace "gc-8561"
  May 27 12:56:31.590: INFO: Deleting pod "simpletest.rc-7k5jr" in namespace "gc-8561"
  May 27 12:56:31.606: INFO: Deleting pod "simpletest.rc-7qzxk" in namespace "gc-8561"
  May 27 12:56:31.619: INFO: Deleting pod "simpletest.rc-7x9hb" in namespace "gc-8561"
  May 27 12:56:31.639: INFO: Deleting pod "simpletest.rc-82dbp" in namespace "gc-8561"
  May 27 12:56:31.657: INFO: Deleting pod "simpletest.rc-8gxc9" in namespace "gc-8561"
  May 27 12:56:31.673: INFO: Deleting pod "simpletest.rc-8hxz7" in namespace "gc-8561"
  May 27 12:56:31.685: INFO: Deleting pod "simpletest.rc-8tjc9" in namespace "gc-8561"
  May 27 12:56:31.701: INFO: Deleting pod "simpletest.rc-9g27w" in namespace "gc-8561"
  May 27 12:56:31.724: INFO: Deleting pod "simpletest.rc-9jd8t" in namespace "gc-8561"
  May 27 12:56:31.747: INFO: Deleting pod "simpletest.rc-9nk5r" in namespace "gc-8561"
  May 27 12:56:31.762: INFO: Deleting pod "simpletest.rc-b5ww6" in namespace "gc-8561"
  May 27 12:56:31.781: INFO: Deleting pod "simpletest.rc-b7wd8" in namespace "gc-8561"
  May 27 12:56:31.800: INFO: Deleting pod "simpletest.rc-bj6bm" in namespace "gc-8561"
  May 27 12:56:31.814: INFO: Deleting pod "simpletest.rc-blcxm" in namespace "gc-8561"
  May 27 12:56:31.831: INFO: Deleting pod "simpletest.rc-bmbnq" in namespace "gc-8561"
  May 27 12:56:31.850: INFO: Deleting pod "simpletest.rc-btzf7" in namespace "gc-8561"
  May 27 12:56:31.868: INFO: Deleting pod "simpletest.rc-bzlnp" in namespace "gc-8561"
  May 27 12:56:31.887: INFO: Deleting pod "simpletest.rc-c7877" in namespace "gc-8561"
  May 27 12:56:31.906: INFO: Deleting pod "simpletest.rc-czcmr" in namespace "gc-8561"
  May 27 12:56:31.924: INFO: Deleting pod "simpletest.rc-dm8zc" in namespace "gc-8561"
  May 27 12:56:31.944: INFO: Deleting pod "simpletest.rc-dr944" in namespace "gc-8561"
  May 27 12:56:31.960: INFO: Deleting pod "simpletest.rc-dsxgm" in namespace "gc-8561"
  May 27 12:56:31.976: INFO: Deleting pod "simpletest.rc-dxfm6" in namespace "gc-8561"
  May 27 12:56:31.996: INFO: Deleting pod "simpletest.rc-fhrzk" in namespace "gc-8561"
  May 27 12:56:32.016: INFO: Deleting pod "simpletest.rc-frxqf" in namespace "gc-8561"
  May 27 12:56:32.029: INFO: Deleting pod "simpletest.rc-gbjp6" in namespace "gc-8561"
  May 27 12:56:32.050: INFO: Deleting pod "simpletest.rc-gj2cg" in namespace "gc-8561"
  May 27 12:56:32.064: INFO: Deleting pod "simpletest.rc-grrjp" in namespace "gc-8561"
  May 27 12:56:32.083: INFO: Deleting pod "simpletest.rc-gsqlk" in namespace "gc-8561"
  May 27 12:56:32.103: INFO: Deleting pod "simpletest.rc-hbprr" in namespace "gc-8561"
  May 27 12:56:32.125: INFO: Deleting pod "simpletest.rc-hcpkc" in namespace "gc-8561"
  May 27 12:56:32.141: INFO: Deleting pod "simpletest.rc-hg89h" in namespace "gc-8561"
  May 27 12:56:32.158: INFO: Deleting pod "simpletest.rc-hghmp" in namespace "gc-8561"
  May 27 12:56:32.176: INFO: Deleting pod "simpletest.rc-hl9h6" in namespace "gc-8561"
  May 27 12:56:32.195: INFO: Deleting pod "simpletest.rc-hrvt4" in namespace "gc-8561"
  May 27 12:56:32.215: INFO: Deleting pod "simpletest.rc-hx97d" in namespace "gc-8561"
  May 27 12:56:32.232: INFO: Deleting pod "simpletest.rc-j27wn" in namespace "gc-8561"
  May 27 12:56:32.250: INFO: Deleting pod "simpletest.rc-j92zt" in namespace "gc-8561"
  May 27 12:56:32.269: INFO: Deleting pod "simpletest.rc-jcm2j" in namespace "gc-8561"
  May 27 12:56:32.286: INFO: Deleting pod "simpletest.rc-jt77h" in namespace "gc-8561"
  May 27 12:56:32.304: INFO: Deleting pod "simpletest.rc-k54pk" in namespace "gc-8561"
  May 27 12:56:32.321: INFO: Deleting pod "simpletest.rc-k85q2" in namespace "gc-8561"
  May 27 12:56:32.338: INFO: Deleting pod "simpletest.rc-k9fmv" in namespace "gc-8561"
  May 27 12:56:32.359: INFO: Deleting pod "simpletest.rc-kcb7h" in namespace "gc-8561"
  May 27 12:56:32.375: INFO: Deleting pod "simpletest.rc-lr7c4" in namespace "gc-8561"
  May 27 12:56:32.391: INFO: Deleting pod "simpletest.rc-m2kqf" in namespace "gc-8561"
  May 27 12:56:32.408: INFO: Deleting pod "simpletest.rc-mqwrc" in namespace "gc-8561"
  May 27 12:56:32.424: INFO: Deleting pod "simpletest.rc-mth8k" in namespace "gc-8561"
  May 27 12:56:32.441: INFO: Deleting pod "simpletest.rc-nbrnw" in namespace "gc-8561"
  May 27 12:56:32.458: INFO: Deleting pod "simpletest.rc-npblb" in namespace "gc-8561"
  May 27 12:56:32.475: INFO: Deleting pod "simpletest.rc-nsx7m" in namespace "gc-8561"
  May 27 12:56:32.490: INFO: Deleting pod "simpletest.rc-p4sks" in namespace "gc-8561"
  May 27 12:56:32.507: INFO: Deleting pod "simpletest.rc-pm5wg" in namespace "gc-8561"
  May 27 12:56:32.525: INFO: Deleting pod "simpletest.rc-pmsrt" in namespace "gc-8561"
  May 27 12:56:32.541: INFO: Deleting pod "simpletest.rc-pnb4p" in namespace "gc-8561"
  May 27 12:56:32.554: INFO: Deleting pod "simpletest.rc-ptqxb" in namespace "gc-8561"
  May 27 12:56:32.571: INFO: Deleting pod "simpletest.rc-q6tr4" in namespace "gc-8561"
  May 27 12:56:32.589: INFO: Deleting pod "simpletest.rc-qd989" in namespace "gc-8561"
  May 27 12:56:32.607: INFO: Deleting pod "simpletest.rc-qv67b" in namespace "gc-8561"
  May 27 12:56:32.639: INFO: Deleting pod "simpletest.rc-qzv6c" in namespace "gc-8561"
  May 27 12:56:32.688: INFO: Deleting pod "simpletest.rc-r2ldf" in namespace "gc-8561"
  May 27 12:56:32.736: INFO: Deleting pod "simpletest.rc-rkkvm" in namespace "gc-8561"
  May 27 12:56:32.787: INFO: Deleting pod "simpletest.rc-s2ct5" in namespace "gc-8561"
  May 27 12:56:32.836: INFO: Deleting pod "simpletest.rc-s9j7d" in namespace "gc-8561"
  May 27 12:56:32.890: INFO: Deleting pod "simpletest.rc-sdl6g" in namespace "gc-8561"
  May 27 12:56:32.938: INFO: Deleting pod "simpletest.rc-smkk2" in namespace "gc-8561"
  May 27 12:56:32.988: INFO: Deleting pod "simpletest.rc-ss7ff" in namespace "gc-8561"
  May 27 12:56:33.042: INFO: Deleting pod "simpletest.rc-t9mbc" in namespace "gc-8561"
  May 27 12:56:33.094: INFO: Deleting pod "simpletest.rc-tkhhr" in namespace "gc-8561"
  May 27 12:56:33.141: INFO: Deleting pod "simpletest.rc-tktfr" in namespace "gc-8561"
  May 27 12:56:33.194: INFO: Deleting pod "simpletest.rc-tq7t2" in namespace "gc-8561"
  May 27 12:56:33.238: INFO: Deleting pod "simpletest.rc-v72kh" in namespace "gc-8561"
  May 27 12:56:33.292: INFO: Deleting pod "simpletest.rc-vhd5z" in namespace "gc-8561"
  May 27 12:56:33.339: INFO: Deleting pod "simpletest.rc-vnfrs" in namespace "gc-8561"
  May 27 12:56:33.389: INFO: Deleting pod "simpletest.rc-vt45f" in namespace "gc-8561"
  May 27 12:56:33.440: INFO: Deleting pod "simpletest.rc-w74gb" in namespace "gc-8561"
  May 27 12:56:33.490: INFO: Deleting pod "simpletest.rc-wc7v2" in namespace "gc-8561"
  May 27 12:56:33.536: INFO: Deleting pod "simpletest.rc-wm6kr" in namespace "gc-8561"
  May 27 12:56:33.586: INFO: Deleting pod "simpletest.rc-wrw29" in namespace "gc-8561"
  May 27 12:56:33.641: INFO: Deleting pod "simpletest.rc-z6fj2" in namespace "gc-8561"
  May 27 12:56:33.688: INFO: Deleting pod "simpletest.rc-zf5s4" in namespace "gc-8561"
  May 27 12:56:33.745: INFO: Deleting pod "simpletest.rc-zjxwg" in namespace "gc-8561"
  May 27 12:56:33.788: INFO: Deleting pod "simpletest.rc-zprw5" in namespace "gc-8561"
  May 27 12:56:33.841: INFO: Deleting pod "simpletest.rc-zr5ht" in namespace "gc-8561"
  May 27 12:56:33.889: INFO: Deleting pod "simpletest.rc-zwq8t" in namespace "gc-8561"
  May 27 12:56:33.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8561" for this suite. @ 05/27/23 12:56:33.979
• [43.797 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 05/27/23 12:56:34.034
  May 27 12:56:34.034: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename pods @ 05/27/23 12:56:34.036
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:56:34.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:56:34.061
  STEP: creating a Pod with a static label @ 05/27/23 12:56:34.074
  STEP: watching for Pod to be ready @ 05/27/23 12:56:34.085
  May 27 12:56:34.088: INFO: observed Pod pod-test in namespace pods-6621 in phase Pending with labels: map[test-pod-static:true] & conditions []
  May 27 12:56:34.091: INFO: observed Pod pod-test in namespace pods-6621 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 12:56:34 +0000 UTC  }]
  May 27 12:56:34.127: INFO: observed Pod pod-test in namespace pods-6621 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 12:56:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-27 12:56:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-27 12:56:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 12:56:34 +0000 UTC  }]
  May 27 12:56:44.044: INFO: Found Pod pod-test in namespace pods-6621 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 12:56:34 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 12:56:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 12:56:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 12:56:34 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 05/27/23 12:56:44.051
  STEP: getting the Pod and ensuring that it's patched @ 05/27/23 12:56:44.066
  STEP: replacing the Pod's status Ready condition to False @ 05/27/23 12:56:44.071
  STEP: check the Pod again to ensure its Ready conditions are False @ 05/27/23 12:56:44.089
  STEP: deleting the Pod via a Collection with a LabelSelector @ 05/27/23 12:56:44.089
  STEP: watching for the Pod to be deleted @ 05/27/23 12:56:44.105
  May 27 12:56:44.108: INFO: observed event type MODIFIED
  May 27 12:56:45.873: INFO: observed event type MODIFIED
  May 27 12:56:46.383: INFO: observed event type MODIFIED
  May 27 12:56:46.876: INFO: observed event type MODIFIED
  May 27 12:56:46.889: INFO: observed event type MODIFIED
  May 27 12:56:46.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6621" for this suite. @ 05/27/23 12:56:46.908
• [12.882 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 05/27/23 12:56:46.917
  May 27 12:56:46.917: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename disruption @ 05/27/23 12:56:46.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:56:46.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:56:46.945
  STEP: Waiting for the pdb to be processed @ 05/27/23 12:56:46.957
  STEP: Updating PodDisruptionBudget status @ 05/27/23 12:56:48.968
  STEP: Waiting for all pods to be running @ 05/27/23 12:56:48.979
  May 27 12:56:48.991: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 05/27/23 12:56:50.996
  STEP: Waiting for the pdb to be processed @ 05/27/23 12:56:51.012
  STEP: Patching PodDisruptionBudget status @ 05/27/23 12:56:51.022
  STEP: Waiting for the pdb to be processed @ 05/27/23 12:56:51.037
  May 27 12:56:51.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2711" for this suite. @ 05/27/23 12:56:51.048
• [4.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 05/27/23 12:56:51.062
  May 27 12:56:51.062: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename deployment @ 05/27/23 12:56:51.063
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:56:51.078
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:56:51.085
  STEP: creating a Deployment @ 05/27/23 12:56:51.095
  May 27 12:56:51.095: INFO: Creating simple deployment test-deployment-zmfvh
  May 27 12:56:51.122: INFO: deployment "test-deployment-zmfvh" doesn't have the required revision set
  STEP: Getting /status @ 05/27/23 12:56:53.142
  May 27 12:56:53.148: INFO: Deployment test-deployment-zmfvh has Conditions: [{Available True 2023-05-27 12:56:52 +0000 UTC 2023-05-27 12:56:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-27 12:56:52 +0000 UTC 2023-05-27 12:56:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zmfvh-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 05/27/23 12:56:53.149
  May 27 12:56:53.162: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 12, 56, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 56, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 12, 56, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 12, 56, 51, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-zmfvh-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 05/27/23 12:56:53.162
  May 27 12:56:53.165: INFO: Observed &Deployment event: ADDED
  May 27 12:56:53.165: INFO: Observed Deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-27 12:56:51 +0000 UTC 2023-05-27 12:56:51 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zmfvh-5994cf9475"}
  May 27 12:56:53.165: INFO: Observed &Deployment event: MODIFIED
  May 27 12:56:53.165: INFO: Observed Deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-27 12:56:51 +0000 UTC 2023-05-27 12:56:51 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zmfvh-5994cf9475"}
  May 27 12:56:53.166: INFO: Observed Deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-27 12:56:51 +0000 UTC 2023-05-27 12:56:51 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 27 12:56:53.166: INFO: Observed &Deployment event: MODIFIED
  May 27 12:56:53.166: INFO: Observed Deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-27 12:56:51 +0000 UTC 2023-05-27 12:56:51 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 27 12:56:53.166: INFO: Observed Deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-27 12:56:51 +0000 UTC 2023-05-27 12:56:51 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-zmfvh-5994cf9475" is progressing.}
  May 27 12:56:53.167: INFO: Observed &Deployment event: MODIFIED
  May 27 12:56:53.167: INFO: Observed Deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-27 12:56:52 +0000 UTC 2023-05-27 12:56:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 27 12:56:53.167: INFO: Observed Deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-27 12:56:52 +0000 UTC 2023-05-27 12:56:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zmfvh-5994cf9475" has successfully progressed.}
  May 27 12:56:53.167: INFO: Observed &Deployment event: MODIFIED
  May 27 12:56:53.167: INFO: Observed Deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-27 12:56:52 +0000 UTC 2023-05-27 12:56:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 27 12:56:53.167: INFO: Observed Deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-27 12:56:52 +0000 UTC 2023-05-27 12:56:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zmfvh-5994cf9475" has successfully progressed.}
  May 27 12:56:53.167: INFO: Found Deployment test-deployment-zmfvh in namespace deployment-5902 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 27 12:56:53.167: INFO: Deployment test-deployment-zmfvh has an updated status
  STEP: patching the Statefulset Status @ 05/27/23 12:56:53.168
  May 27 12:56:53.168: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 27 12:56:53.179: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 05/27/23 12:56:53.179
  May 27 12:56:53.182: INFO: Observed &Deployment event: ADDED
  May 27 12:56:53.182: INFO: Observed deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-27 12:56:51 +0000 UTC 2023-05-27 12:56:51 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zmfvh-5994cf9475"}
  May 27 12:56:53.183: INFO: Observed &Deployment event: MODIFIED
  May 27 12:56:53.183: INFO: Observed deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-27 12:56:51 +0000 UTC 2023-05-27 12:56:51 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zmfvh-5994cf9475"}
  May 27 12:56:53.183: INFO: Observed deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-27 12:56:51 +0000 UTC 2023-05-27 12:56:51 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 27 12:56:53.183: INFO: Observed &Deployment event: MODIFIED
  May 27 12:56:53.183: INFO: Observed deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-27 12:56:51 +0000 UTC 2023-05-27 12:56:51 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 27 12:56:53.184: INFO: Observed deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-27 12:56:51 +0000 UTC 2023-05-27 12:56:51 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-zmfvh-5994cf9475" is progressing.}
  May 27 12:56:53.184: INFO: Observed &Deployment event: MODIFIED
  May 27 12:56:53.184: INFO: Observed deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-27 12:56:52 +0000 UTC 2023-05-27 12:56:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 27 12:56:53.185: INFO: Observed deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-27 12:56:52 +0000 UTC 2023-05-27 12:56:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zmfvh-5994cf9475" has successfully progressed.}
  May 27 12:56:53.185: INFO: Observed &Deployment event: MODIFIED
  May 27 12:56:53.185: INFO: Observed deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-27 12:56:52 +0000 UTC 2023-05-27 12:56:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 27 12:56:53.185: INFO: Observed deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-27 12:56:52 +0000 UTC 2023-05-27 12:56:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zmfvh-5994cf9475" has successfully progressed.}
  May 27 12:56:53.185: INFO: Observed deployment test-deployment-zmfvh in namespace deployment-5902 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 27 12:56:53.186: INFO: Observed &Deployment event: MODIFIED
  May 27 12:56:53.186: INFO: Found deployment test-deployment-zmfvh in namespace deployment-5902 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  May 27 12:56:53.186: INFO: Deployment test-deployment-zmfvh has a patched status
  May 27 12:56:53.193: INFO: Deployment "test-deployment-zmfvh":
  &Deployment{ObjectMeta:{test-deployment-zmfvh  deployment-5902  780f99d6-9485-4415-aa77-0821561fa33b 17446 1 2023-05-27 12:56:51 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-27 12:56:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-27 12:56:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-27 12:56:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004abf2d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-zmfvh-5994cf9475",LastUpdateTime:2023-05-27 12:56:53 +0000 UTC,LastTransitionTime:2023-05-27 12:56:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 27 12:56:53.199: INFO: New ReplicaSet "test-deployment-zmfvh-5994cf9475" of Deployment "test-deployment-zmfvh":
  &ReplicaSet{ObjectMeta:{test-deployment-zmfvh-5994cf9475  deployment-5902  c7dd622e-a3a3-4b92-bd7a-62e3614f621e 17442 1 2023-05-27 12:56:51 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-zmfvh 780f99d6-9485-4415-aa77-0821561fa33b 0xc004abf6c0 0xc004abf6c1}] [] [{kube-controller-manager Update apps/v1 2023-05-27 12:56:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"780f99d6-9485-4415-aa77-0821561fa33b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 12:56:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004abf778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 27 12:56:53.206: INFO: Pod "test-deployment-zmfvh-5994cf9475-85mhg" is available:
  &Pod{ObjectMeta:{test-deployment-zmfvh-5994cf9475-85mhg test-deployment-zmfvh-5994cf9475- deployment-5902  6cebbba6-d9d6-43f7-a623-6e08023dcc97 17441 0 2023-05-27 12:56:51 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-zmfvh-5994cf9475 c7dd622e-a3a3-4b92-bd7a-62e3614f621e 0xc003c4dff0 0xc003c4dff1}] [] [{kube-controller-manager Update v1 2023-05-27 12:56:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c7dd622e-a3a3-4b92-bd7a-62e3614f621e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 12:56:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.19.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qfrv2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qfrv2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:56:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:56:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:56:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 12:56:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.172,PodIP:192.168.19.80,StartTime:2023-05-27 12:56:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-27 12:56:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c770051252fba78c1e00fe5b604589118ef7e14743c86c3959a3c50bb675ae5c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.19.80,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 12:56:53.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5902" for this suite. @ 05/27/23 12:56:53.212
• [2.157 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 05/27/23 12:56:53.22
  May 27 12:56:53.220: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename configmap @ 05/27/23 12:56:53.221
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:56:53.238
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:56:53.244
  STEP: Creating configMap with name configmap-test-volume-map-27a44176-c758-4d57-a5b7-d8219310915a @ 05/27/23 12:56:53.249
  STEP: Creating a pod to test consume configMaps @ 05/27/23 12:56:53.255
  STEP: Saw pod success @ 05/27/23 12:56:57.293
  May 27 12:56:57.298: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-configmaps-1b147e32-32d9-4100-9fcf-5409a956e0fb container agnhost-container: <nil>
  STEP: delete the pod @ 05/27/23 12:56:57.308
  May 27 12:56:57.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3496" for this suite. @ 05/27/23 12:56:57.331
• [4.121 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 05/27/23 12:56:57.342
  May 27 12:56:57.342: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl @ 05/27/23 12:56:57.343
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:56:57.365
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:56:57.37
  May 27 12:56:57.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-4835 version'
  May 27 12:56:57.447: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  May 27 12:56:57.447: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.2\", GitCommit:\"7f6f68fdabc4df88cfea2dcf9a19b2b830f1e647\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:20:07Z\", GoVersion:\"go1.20.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.2\", GitCommit:\"7f6f68fdabc4df88cfea2dcf9a19b2b830f1e647\", GitTreeState:\"clean\", BuildDate:\"2023-05-18T02:06:41Z\", GoVersion:\"go1.20.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  May 27 12:56:57.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4835" for this suite. @ 05/27/23 12:56:57.452
• [0.118 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 05/27/23 12:56:57.467
  May 27 12:56:57.467: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename watch @ 05/27/23 12:56:57.467
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:56:57.485
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:56:57.492
  STEP: creating a watch on configmaps @ 05/27/23 12:56:57.496
  STEP: creating a new configmap @ 05/27/23 12:56:57.498
  STEP: modifying the configmap once @ 05/27/23 12:56:57.503
  STEP: closing the watch once it receives two notifications @ 05/27/23 12:56:57.514
  May 27 12:56:57.514: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6646  2f9ebd54-1371-4b3b-b73c-97cc2cc97ed4 17509 0 2023-05-27 12:56:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-27 12:56:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 27 12:56:57.515: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6646  2f9ebd54-1371-4b3b-b73c-97cc2cc97ed4 17510 0 2023-05-27 12:56:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-27 12:56:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 05/27/23 12:56:57.515
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 05/27/23 12:56:57.526
  STEP: deleting the configmap @ 05/27/23 12:56:57.528
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 05/27/23 12:56:57.536
  May 27 12:56:57.536: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6646  2f9ebd54-1371-4b3b-b73c-97cc2cc97ed4 17511 0 2023-05-27 12:56:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-27 12:56:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 27 12:56:57.537: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6646  2f9ebd54-1371-4b3b-b73c-97cc2cc97ed4 17512 0 2023-05-27 12:56:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-27 12:56:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 27 12:56:57.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-6646" for this suite. @ 05/27/23 12:56:57.543
• [0.085 seconds]
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 05/27/23 12:56:57.553
  May 27 12:56:57.553: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename pod-network-test @ 05/27/23 12:56:57.554
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:56:57.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:56:57.589
  STEP: Performing setup for networking test in namespace pod-network-test-2814 @ 05/27/23 12:56:57.593
  STEP: creating a selector @ 05/27/23 12:56:57.593
  STEP: Creating the service pods in kubernetes @ 05/27/23 12:56:57.593
  May 27 12:56:57.593: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/27/23 12:57:19.732
  May 27 12:57:21.779: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May 27 12:57:21.779: INFO: Going to poll 192.168.0.60 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  May 27 12:57:21.783: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.0.60 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2814 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 12:57:21.783: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:57:21.783: INFO: ExecWithOptions: Clientset creation
  May 27 12:57:21.783: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2814/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.0.60+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 27 12:57:22.881: INFO: Found all 1 expected endpoints: [netserver-0]
  May 27 12:57:22.881: INFO: Going to poll 192.168.7.115 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  May 27 12:57:22.886: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.7.115 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2814 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 12:57:22.886: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:57:22.887: INFO: ExecWithOptions: Clientset creation
  May 27 12:57:22.887: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2814/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.7.115+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 27 12:57:23.963: INFO: Found all 1 expected endpoints: [netserver-1]
  May 27 12:57:23.963: INFO: Going to poll 192.168.19.65 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  May 27 12:57:23.968: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.19.65 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2814 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 12:57:23.968: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 12:57:23.969: INFO: ExecWithOptions: Clientset creation
  May 27 12:57:23.969: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2814/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.19.65+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 27 12:57:25.043: INFO: Found all 1 expected endpoints: [netserver-2]
  May 27 12:57:25.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-2814" for this suite. @ 05/27/23 12:57:25.048
• [27.507 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 05/27/23 12:57:25.063
  May 27 12:57:25.063: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename subjectreview @ 05/27/23 12:57:25.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:57:25.087
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:57:25.092
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-6596" @ 05/27/23 12:57:25.098
  May 27 12:57:25.104: INFO: saUsername: "system:serviceaccount:subjectreview-6596:e2e"
  May 27 12:57:25.104: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-6596"}
  May 27 12:57:25.104: INFO: saUID: "66d49a00-44f8-4c02-8614-e0554f63119a"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-6596:e2e" @ 05/27/23 12:57:25.104
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-6596:e2e" @ 05/27/23 12:57:25.104
  May 27 12:57:25.107: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-6596:e2e" api 'list' configmaps in "subjectreview-6596" namespace @ 05/27/23 12:57:25.107
  May 27 12:57:25.111: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-6596:e2e" @ 05/27/23 12:57:25.111
  May 27 12:57:25.115: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  May 27 12:57:25.115: INFO: LocalSubjectAccessReview has been verified
  May 27 12:57:25.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-6596" for this suite. @ 05/27/23 12:57:25.122
• [0.070 seconds]
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 05/27/23 12:57:25.134
  May 27 12:57:25.134: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename statefulset @ 05/27/23 12:57:25.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:57:25.162
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:57:25.172
  STEP: Creating service test in namespace statefulset-9019 @ 05/27/23 12:57:25.18
  STEP: Creating a new StatefulSet @ 05/27/23 12:57:25.186
  May 27 12:57:25.205: INFO: Found 0 stateful pods, waiting for 3
  May 27 12:57:35.213: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 27 12:57:35.213: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 27 12:57:35.213: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/27/23 12:57:35.228
  May 27 12:57:35.250: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/27/23 12:57:35.25
  STEP: Not applying an update when the partition is greater than the number of replicas @ 05/27/23 12:57:45.279
  STEP: Performing a canary update @ 05/27/23 12:57:45.279
  May 27 12:57:45.305: INFO: Updating stateful set ss2
  May 27 12:57:45.320: INFO: Waiting for Pod statefulset-9019/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 05/27/23 12:57:55.333
  May 27 12:57:55.393: INFO: Found 1 stateful pods, waiting for 3
  May 27 12:58:05.400: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 27 12:58:05.400: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 27 12:58:05.400: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 05/27/23 12:58:05.41
  May 27 12:58:05.433: INFO: Updating stateful set ss2
  May 27 12:58:05.447: INFO: Waiting for Pod statefulset-9019/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  May 27 12:58:15.484: INFO: Updating stateful set ss2
  May 27 12:58:15.497: INFO: Waiting for StatefulSet statefulset-9019/ss2 to complete update
  May 27 12:58:15.497: INFO: Waiting for Pod statefulset-9019/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  May 27 12:58:25.511: INFO: Deleting all statefulset in ns statefulset-9019
  May 27 12:58:25.515: INFO: Scaling statefulset ss2 to 0
  May 27 12:58:35.538: INFO: Waiting for statefulset status.replicas updated to 0
  May 27 12:58:35.542: INFO: Deleting statefulset ss2
  May 27 12:58:35.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9019" for this suite. @ 05/27/23 12:58:35.567
• [70.441 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 05/27/23 12:58:35.576
  May 27 12:58:35.576: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename endpointslicemirroring @ 05/27/23 12:58:35.577
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:58:35.605
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:58:35.61
  STEP: mirroring a new custom Endpoint @ 05/27/23 12:58:35.628
  May 27 12:58:35.642: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 05/27/23 12:58:37.65
  May 27 12:58:37.660: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  STEP: mirroring deletion of a custom Endpoint @ 05/27/23 12:58:39.665
  May 27 12:58:39.679: INFO: Waiting for 0 EndpointSlices to exist, got 1
  May 27 12:58:41.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-5137" for this suite. @ 05/27/23 12:58:41.689
• [6.123 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 05/27/23 12:58:41.702
  May 27 12:58:41.702: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-probe @ 05/27/23 12:58:41.703
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:58:41.723
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:58:41.729
  STEP: Creating pod busybox-8a99287f-ea15-4c82-9e4b-e0f9a157ff24 in namespace container-probe-5194 @ 05/27/23 12:58:41.734
  May 27 12:58:43.764: INFO: Started pod busybox-8a99287f-ea15-4c82-9e4b-e0f9a157ff24 in namespace container-probe-5194
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/27/23 12:58:43.764
  May 27 12:58:43.768: INFO: Initial restart count of pod busybox-8a99287f-ea15-4c82-9e4b-e0f9a157ff24 is 0
  May 27 12:59:33.912: INFO: Restart count of pod container-probe-5194/busybox-8a99287f-ea15-4c82-9e4b-e0f9a157ff24 is now 1 (50.144269335s elapsed)
  May 27 12:59:33.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/27/23 12:59:33.918
  STEP: Destroying namespace "container-probe-5194" for this suite. @ 05/27/23 12:59:33.933
• [52.239 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 05/27/23 12:59:33.943
  May 27 12:59:33.943: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename secrets @ 05/27/23 12:59:33.944
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:59:33.963
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:59:33.968
  STEP: creating a secret @ 05/27/23 12:59:33.974
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 05/27/23 12:59:33.98
  STEP: patching the secret @ 05/27/23 12:59:33.985
  STEP: deleting the secret using a LabelSelector @ 05/27/23 12:59:33.999
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 05/27/23 12:59:34.01
  May 27 12:59:34.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8810" for this suite. @ 05/27/23 12:59:34.02
• [0.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 05/27/23 12:59:34.032
  May 27 12:59:34.032: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl @ 05/27/23 12:59:34.033
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:59:34.051
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:59:34.06
  STEP: creating all guestbook components @ 05/27/23 12:59:34.065
  May 27 12:59:34.066: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  May 27 12:59:34.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-8430 create -f -'
  May 27 12:59:34.382: INFO: stderr: ""
  May 27 12:59:34.382: INFO: stdout: "service/agnhost-replica created\n"
  May 27 12:59:34.382: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  May 27 12:59:34.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-8430 create -f -'
  May 27 12:59:34.695: INFO: stderr: ""
  May 27 12:59:34.695: INFO: stdout: "service/agnhost-primary created\n"
  May 27 12:59:34.695: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  May 27 12:59:34.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-8430 create -f -'
  May 27 12:59:34.989: INFO: stderr: ""
  May 27 12:59:34.989: INFO: stdout: "service/frontend created\n"
  May 27 12:59:34.989: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  May 27 12:59:34.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-8430 create -f -'
  May 27 12:59:35.271: INFO: stderr: ""
  May 27 12:59:35.271: INFO: stdout: "deployment.apps/frontend created\n"
  May 27 12:59:35.271: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May 27 12:59:35.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-8430 create -f -'
  May 27 12:59:35.748: INFO: stderr: ""
  May 27 12:59:35.748: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  May 27 12:59:35.748: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May 27 12:59:35.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-8430 create -f -'
  May 27 12:59:36.032: INFO: stderr: ""
  May 27 12:59:36.033: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 05/27/23 12:59:36.033
  May 27 12:59:36.033: INFO: Waiting for all frontend pods to be Running.
  May 27 12:59:41.085: INFO: Waiting for frontend to serve content.
  May 27 12:59:41.098: INFO: Trying to add a new entry to the guestbook.
  May 27 12:59:41.113: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 05/27/23 12:59:41.125
  May 27 12:59:41.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-8430 delete --grace-period=0 --force -f -'
  May 27 12:59:41.224: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 27 12:59:41.224: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 05/27/23 12:59:41.224
  May 27 12:59:41.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-8430 delete --grace-period=0 --force -f -'
  May 27 12:59:41.333: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 27 12:59:41.333: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/27/23 12:59:41.333
  May 27 12:59:41.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-8430 delete --grace-period=0 --force -f -'
  May 27 12:59:41.427: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 27 12:59:41.427: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/27/23 12:59:41.427
  May 27 12:59:41.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-8430 delete --grace-period=0 --force -f -'
  May 27 12:59:41.524: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 27 12:59:41.524: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/27/23 12:59:41.524
  May 27 12:59:41.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-8430 delete --grace-period=0 --force -f -'
  May 27 12:59:41.634: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 27 12:59:41.634: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/27/23 12:59:41.634
  May 27 12:59:41.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-8430 delete --grace-period=0 --force -f -'
  May 27 12:59:41.764: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 27 12:59:41.764: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  May 27 12:59:41.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8430" for this suite. @ 05/27/23 12:59:41.774
• [7.756 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 05/27/23 12:59:41.788
  May 27 12:59:41.788: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename services @ 05/27/23 12:59:41.789
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:59:41.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:59:41.867
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-561 @ 05/27/23 12:59:41.873
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/27/23 12:59:41.888
  STEP: creating service externalsvc in namespace services-561 @ 05/27/23 12:59:41.888
  STEP: creating replication controller externalsvc in namespace services-561 @ 05/27/23 12:59:41.904
  I0527 12:59:41.914051      18 runners.go:194] Created replication controller with name: externalsvc, namespace: services-561, replica count: 2
  I0527 12:59:44.964872      18 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0527 12:59:47.965082      18 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 05/27/23 12:59:47.97
  May 27 12:59:47.990: INFO: Creating new exec pod
  May 27 12:59:50.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-561 exec execpodnk5mm -- /bin/sh -x -c nslookup clusterip-service.services-561.svc.cluster.local'
  May 27 12:59:50.223: INFO: stderr: "+ nslookup clusterip-service.services-561.svc.cluster.local\n"
  May 27 12:59:50.223: INFO: stdout: "Server:\t\t10.152.183.112\nAddress:\t10.152.183.112#53\n\nclusterip-service.services-561.svc.cluster.local\tcanonical name = externalsvc.services-561.svc.cluster.local.\nName:\texternalsvc.services-561.svc.cluster.local\nAddress: 10.152.183.143\n\n"
  May 27 12:59:50.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-561, will wait for the garbage collector to delete the pods @ 05/27/23 12:59:50.228
  May 27 12:59:50.294: INFO: Deleting ReplicationController externalsvc took: 10.94897ms
  May 27 12:59:50.394: INFO: Terminating ReplicationController externalsvc pods took: 100.581234ms
  May 27 12:59:52.626: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-561" for this suite. @ 05/27/23 12:59:52.641
• [10.866 seconds]
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 05/27/23 12:59:52.654
  May 27 12:59:52.655: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-probe @ 05/27/23 12:59:52.656
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 12:59:52.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 12:59:52.681
  STEP: Creating pod test-grpc-99aa752c-93b3-4377-9b11-f70748a80cd0 in namespace container-probe-1010 @ 05/27/23 12:59:52.689
  May 27 12:59:54.712: INFO: Started pod test-grpc-99aa752c-93b3-4377-9b11-f70748a80cd0 in namespace container-probe-1010
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/27/23 12:59:54.712
  May 27 12:59:54.718: INFO: Initial restart count of pod test-grpc-99aa752c-93b3-4377-9b11-f70748a80cd0 is 0
  May 27 13:03:55.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/27/23 13:03:55.42
  STEP: Destroying namespace "container-probe-1010" for this suite. @ 05/27/23 13:03:55.437
• [242.792 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 05/27/23 13:03:55.449
  May 27 13:03:55.449: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/27/23 13:03:55.45
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:03:55.472
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:03:55.477
  STEP: create the container to handle the HTTPGet hook request. @ 05/27/23 13:03:55.486
  STEP: create the pod with lifecycle hook @ 05/27/23 13:03:57.517
  STEP: delete the pod with lifecycle hook @ 05/27/23 13:03:59.541
  STEP: check prestop hook @ 05/27/23 13:04:01.562
  May 27 13:04:01.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8605" for this suite. @ 05/27/23 13:04:01.593
• [6.153 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 05/27/23 13:04:01.602
  May 27 13:04:01.602: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubelet-test @ 05/27/23 13:04:01.603
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:04:01.632
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:04:01.637
  STEP: Waiting for pod completion @ 05/27/23 13:04:01.655
  May 27 13:04:05.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6886" for this suite. @ 05/27/23 13:04:05.702
• [4.109 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 05/27/23 13:04:05.712
  May 27 13:04:05.712: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-probe @ 05/27/23 13:04:05.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:04:05.736
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:04:05.74
  May 27 13:05:05.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-9643" for this suite. @ 05/27/23 13:05:05.766
• [60.063 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 05/27/23 13:05:05.778
  May 27 13:05:05.778: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 13:05:05.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:05:05.802
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:05:05.809
  STEP: Setting up server cert @ 05/27/23 13:05:05.841
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 13:05:06.3
  STEP: Deploying the webhook pod @ 05/27/23 13:05:06.315
  STEP: Wait for the deployment to be ready @ 05/27/23 13:05:06.331
  May 27 13:05:06.357: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/27/23 13:05:08.374
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 13:05:08.388
  May 27 13:05:09.388: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/27/23 13:05:09.475
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/27/23 13:05:09.529
  STEP: Deleting the collection of validation webhooks @ 05/27/23 13:05:09.574
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/27/23 13:05:09.639
  May 27 13:05:09.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3687" for this suite. @ 05/27/23 13:05:09.716
  STEP: Destroying namespace "webhook-markers-2587" for this suite. @ 05/27/23 13:05:09.725
• [3.962 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 05/27/23 13:05:09.741
  May 27 13:05:09.741: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename pod-network-test @ 05/27/23 13:05:09.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:05:09.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:05:09.766
  STEP: Performing setup for networking test in namespace pod-network-test-6979 @ 05/27/23 13:05:09.771
  STEP: creating a selector @ 05/27/23 13:05:09.771
  STEP: Creating the service pods in kubernetes @ 05/27/23 13:05:09.771
  May 27 13:05:09.771: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/27/23 13:05:21.883
  May 27 13:05:23.908: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May 27 13:05:23.908: INFO: Breadth first check of 192.168.0.4 on host 172.31.10.136...
  May 27 13:05:23.913: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.19.102:9080/dial?request=hostname&protocol=http&host=192.168.0.4&port=8083&tries=1'] Namespace:pod-network-test-6979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 13:05:23.913: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:05:23.914: INFO: ExecWithOptions: Clientset creation
  May 27 13:05:23.914: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6979/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.19.102%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.0.4%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 27 13:05:24.007: INFO: Waiting for responses: map[]
  May 27 13:05:24.007: INFO: reached 192.168.0.4 after 0/1 tries
  May 27 13:05:24.007: INFO: Breadth first check of 192.168.7.122 on host 172.31.22.3...
  May 27 13:05:24.012: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.19.102:9080/dial?request=hostname&protocol=http&host=192.168.7.122&port=8083&tries=1'] Namespace:pod-network-test-6979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 13:05:24.012: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:05:24.013: INFO: ExecWithOptions: Clientset creation
  May 27 13:05:24.013: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6979/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.19.102%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.7.122%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 27 13:05:24.110: INFO: Waiting for responses: map[]
  May 27 13:05:24.110: INFO: reached 192.168.7.122 after 0/1 tries
  May 27 13:05:24.111: INFO: Breadth first check of 192.168.19.99 on host 172.31.68.172...
  May 27 13:05:24.115: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.19.102:9080/dial?request=hostname&protocol=http&host=192.168.19.99&port=8083&tries=1'] Namespace:pod-network-test-6979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 13:05:24.115: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:05:24.116: INFO: ExecWithOptions: Clientset creation
  May 27 13:05:24.116: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6979/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.19.102%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.19.99%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 27 13:05:24.206: INFO: Waiting for responses: map[]
  May 27 13:05:24.206: INFO: reached 192.168.19.99 after 0/1 tries
  May 27 13:05:24.206: INFO: Going to retry 0 out of 3 pods....
  May 27 13:05:24.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6979" for this suite. @ 05/27/23 13:05:24.211
• [14.480 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 05/27/23 13:05:24.221
  May 27 13:05:24.221: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 13:05:24.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:05:24.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:05:24.246
  STEP: Creating secret with name projected-secret-test-1ebe166f-d35a-4de1-b5da-a117cf616a0e @ 05/27/23 13:05:24.25
  STEP: Creating a pod to test consume secrets @ 05/27/23 13:05:24.257
  STEP: Saw pod success @ 05/27/23 13:05:28.284
  May 27 13:05:28.289: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-projected-secrets-1f485986-75c2-46c8-ba88-8ef8a1ab03fa container secret-volume-test: <nil>
  STEP: delete the pod @ 05/27/23 13:05:28.298
  May 27 13:05:28.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6218" for this suite. @ 05/27/23 13:05:28.32
• [4.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 05/27/23 13:05:28.332
  May 27 13:05:28.332: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename podtemplate @ 05/27/23 13:05:28.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:05:28.352
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:05:28.358
  STEP: Create set of pod templates @ 05/27/23 13:05:28.362
  May 27 13:05:28.369: INFO: created test-podtemplate-1
  May 27 13:05:28.375: INFO: created test-podtemplate-2
  May 27 13:05:28.383: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 05/27/23 13:05:28.383
  STEP: delete collection of pod templates @ 05/27/23 13:05:28.388
  May 27 13:05:28.388: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 05/27/23 13:05:28.41
  May 27 13:05:28.410: INFO: requesting list of pod templates to confirm quantity
  May 27 13:05:28.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-3754" for this suite. @ 05/27/23 13:05:28.422
• [0.098 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 05/27/23 13:05:28.432
  May 27 13:05:28.432: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename daemonsets @ 05/27/23 13:05:28.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:05:28.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:05:28.46
  May 27 13:05:28.496: INFO: Create a RollingUpdate DaemonSet
  May 27 13:05:28.506: INFO: Check that daemon pods launch on every node of the cluster
  May 27 13:05:28.512: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:05:28.512: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:05:28.517: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 13:05:28.517: INFO: Node ip-172-31-10-136 is running 0 daemon pod, expected 1
  May 27 13:05:29.524: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:05:29.524: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:05:29.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 13:05:29.531: INFO: Node ip-172-31-10-136 is running 0 daemon pod, expected 1
  May 27 13:05:30.523: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:05:30.523: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:05:30.528: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 27 13:05:30.528: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  May 27 13:05:30.528: INFO: Update the DaemonSet to trigger a rollout
  May 27 13:05:30.540: INFO: Updating DaemonSet daemon-set
  May 27 13:05:32.563: INFO: Roll back the DaemonSet before rollout is complete
  May 27 13:05:32.577: INFO: Updating DaemonSet daemon-set
  May 27 13:05:32.577: INFO: Make sure DaemonSet rollback is complete
  May 27 13:05:32.582: INFO: Wrong image for pod: daemon-set-lpzfw. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  May 27 13:05:32.583: INFO: Pod daemon-set-lpzfw is not available
  May 27 13:05:32.588: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:05:32.588: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:05:33.601: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:05:33.602: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:05:34.595: INFO: Pod daemon-set-nhjvw is not available
  May 27 13:05:34.600: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:05:34.600: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 05/27/23 13:05:34.609
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5790, will wait for the garbage collector to delete the pods @ 05/27/23 13:05:34.609
  May 27 13:05:34.675: INFO: Deleting DaemonSet.extensions daemon-set took: 9.119692ms
  May 27 13:05:34.776: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.20251ms
  May 27 13:05:36.282: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 13:05:36.282: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 27 13:05:36.287: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20078"},"items":null}

  May 27 13:05:36.291: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20078"},"items":null}

  May 27 13:05:36.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5790" for this suite. @ 05/27/23 13:05:36.316
• [7.893 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 05/27/23 13:05:36.326
  May 27 13:05:36.326: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename configmap @ 05/27/23 13:05:36.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:05:36.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:05:36.353
  STEP: Creating configMap with name configmap-test-volume-18fbf29a-7be1-4b1b-ae9f-a3d4063bc438 @ 05/27/23 13:05:36.358
  STEP: Creating a pod to test consume configMaps @ 05/27/23 13:05:36.365
  STEP: Saw pod success @ 05/27/23 13:05:40.399
  May 27 13:05:40.404: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-configmaps-0201bc4b-db56-48f2-aec7-580f6dcc053d container configmap-volume-test: <nil>
  STEP: delete the pod @ 05/27/23 13:05:40.418
  May 27 13:05:40.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-128" for this suite. @ 05/27/23 13:05:40.44
• [4.124 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 05/27/23 13:05:40.452
  May 27 13:05:40.452: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename pods @ 05/27/23 13:05:40.453
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:05:40.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:05:40.473
  STEP: creating the pod @ 05/27/23 13:05:40.478
  STEP: submitting the pod to kubernetes @ 05/27/23 13:05:40.479
  STEP: verifying the pod is in kubernetes @ 05/27/23 13:05:42.509
  STEP: updating the pod @ 05/27/23 13:05:42.513
  May 27 13:05:43.029: INFO: Successfully updated pod "pod-update-b78bf9fc-bfd1-40da-9d8f-525518eb6d69"
  STEP: verifying the updated pod is in kubernetes @ 05/27/23 13:05:43.033
  May 27 13:05:43.039: INFO: Pod update OK
  May 27 13:05:43.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-228" for this suite. @ 05/27/23 13:05:43.044
• [2.600 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 05/27/23 13:05:43.052
  May 27 13:05:43.052: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir @ 05/27/23 13:05:43.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:05:43.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:05:43.078
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/27/23 13:05:43.083
  STEP: Saw pod success @ 05/27/23 13:05:47.116
  May 27 13:05:47.120: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-1f0607b5-7333-4ce5-8e44-fd0108d3940b container test-container: <nil>
  STEP: delete the pod @ 05/27/23 13:05:47.13
  May 27 13:05:47.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3408" for this suite. @ 05/27/23 13:05:47.153
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 05/27/23 13:05:47.165
  May 27 13:05:47.165: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename deployment @ 05/27/23 13:05:47.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:05:47.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:05:47.194
  STEP: creating a Deployment @ 05/27/23 13:05:47.206
  STEP: waiting for Deployment to be created @ 05/27/23 13:05:47.213
  STEP: waiting for all Replicas to be Ready @ 05/27/23 13:05:47.216
  May 27 13:05:47.218: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 27 13:05:47.218: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 27 13:05:47.228: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 27 13:05:47.228: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 27 13:05:47.253: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 27 13:05:47.253: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 27 13:05:47.282: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 27 13:05:47.282: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 27 13:05:48.345: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May 27 13:05:48.345: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May 27 13:05:49.047: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 05/27/23 13:05:49.047
  W0527 13:05:49.060486      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 27 13:05:49.063: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 05/27/23 13:05:49.063
  May 27 13:05:49.066: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 0
  May 27 13:05:49.066: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 0
  May 27 13:05:49.066: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 0
  May 27 13:05:49.066: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 0
  May 27 13:05:49.066: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 0
  May 27 13:05:49.066: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 0
  May 27 13:05:49.066: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 0
  May 27 13:05:49.067: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 0
  May 27 13:05:49.067: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1
  May 27 13:05:49.067: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1
  May 27 13:05:49.067: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2
  May 27 13:05:49.067: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2
  May 27 13:05:49.067: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2
  May 27 13:05:49.067: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2
  May 27 13:05:49.078: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2
  May 27 13:05:49.079: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2
  May 27 13:05:49.112: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2
  May 27 13:05:49.112: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2
  May 27 13:05:49.124: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1
  May 27 13:05:49.124: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1
  May 27 13:05:49.137: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1
  May 27 13:05:49.137: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1
  May 27 13:05:50.357: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2
  May 27 13:05:50.357: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2
  May 27 13:05:50.386: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1
  STEP: listing Deployments @ 05/27/23 13:05:50.386
  May 27 13:05:50.391: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 05/27/23 13:05:50.391
  May 27 13:05:50.409: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 05/27/23 13:05:50.409
  May 27 13:05:50.421: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 27 13:05:50.427: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 27 13:05:50.473: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 27 13:05:50.493: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 27 13:05:51.405: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 27 13:05:51.442: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 27 13:05:51.454: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 27 13:05:51.470: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 27 13:05:51.479: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 27 13:05:53.220: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 05/27/23 13:05:53.254
  STEP: fetching the DeploymentStatus @ 05/27/23 13:05:53.263
  May 27 13:05:53.271: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1
  May 27 13:05:53.271: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1
  May 27 13:05:53.271: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1
  May 27 13:05:53.271: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 1
  May 27 13:05:53.272: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2
  May 27 13:05:53.272: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2
  May 27 13:05:53.272: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2
  May 27 13:05:53.273: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2
  May 27 13:05:53.273: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 2
  May 27 13:05:53.273: INFO: observed Deployment test-deployment in namespace deployment-4862 with ReadyReplicas 3
  STEP: deleting the Deployment @ 05/27/23 13:05:53.273
  May 27 13:05:53.288: INFO: observed event type MODIFIED
  May 27 13:05:53.289: INFO: observed event type MODIFIED
  May 27 13:05:53.289: INFO: observed event type MODIFIED
  May 27 13:05:53.289: INFO: observed event type MODIFIED
  May 27 13:05:53.289: INFO: observed event type MODIFIED
  May 27 13:05:53.290: INFO: observed event type MODIFIED
  May 27 13:05:53.290: INFO: observed event type MODIFIED
  May 27 13:05:53.290: INFO: observed event type MODIFIED
  May 27 13:05:53.290: INFO: observed event type MODIFIED
  May 27 13:05:53.291: INFO: observed event type MODIFIED
  May 27 13:05:53.291: INFO: observed event type MODIFIED
  May 27 13:05:53.298: INFO: Log out all the ReplicaSets if there is no deployment created
  May 27 13:05:53.306: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-4862  6d83d05d-e8a4-475f-bce0-da4b680eac54 20315 3 2023-05-27 13:05:47 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e985da1d-0f73-49d5-bc35-767865adebbe 0xc004a271e7 0xc004a271e8}] [] [{kube-controller-manager Update apps/v1 2023-05-27 13:05:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e985da1d-0f73-49d5-bc35-767865adebbe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 13:05:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a27270 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  May 27 13:05:53.311: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-4862  d9eaf090-fb5d-4e96-bfa9-034f319b905f 20427 4 2023-05-27 13:05:49 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e985da1d-0f73-49d5-bc35-767865adebbe 0xc004a272d7 0xc004a272d8}] [] [{kube-controller-manager Update apps/v1 2023-05-27 13:05:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e985da1d-0f73-49d5-bc35-767865adebbe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 13:05:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a27360 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  May 27 13:05:53.319: INFO: pod: "test-deployment-5b5dcbcd95-gzdfs":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-gzdfs test-deployment-5b5dcbcd95- deployment-4862  063f0b84-f347-48a6-a144-c46f27ea9c49 20422 0 2023-05-27 13:05:49 +0000 UTC 2023-05-27 13:05:54 +0000 UTC 0xc004694228 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 d9eaf090-fb5d-4e96-bfa9-034f319b905f 0xc004694257 0xc004694258}] [] [{kube-controller-manager Update v1 2023-05-27 13:05:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9eaf090-fb5d-4e96-bfa9-034f319b905f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 13:05:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.19.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-slrtf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-slrtf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:05:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:05:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.172,PodIP:192.168.19.111,StartTime:2023-05-27 13:05:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-27 13:05:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://9b4f4f82856376d8a1b6309320375e70f0124e6bf4be42f73270a8e27186fe43,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.19.111,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 27 13:05:53.320: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-4862  e9722ac9-b5ef-4f4a-83a9-a3b468a0cfd0 20419 2 2023-05-27 13:05:50 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e985da1d-0f73-49d5-bc35-767865adebbe 0xc004a273c7 0xc004a273c8}] [] [{kube-controller-manager Update apps/v1 2023-05-27 13:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e985da1d-0f73-49d5-bc35-767865adebbe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 13:05:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a27450 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  May 27 13:05:53.325: INFO: pod: "test-deployment-6fc78d85c6-44fds":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-44fds test-deployment-6fc78d85c6- deployment-4862  ec8d03e5-e74b-4c04-899d-7bdf22987511 20418 0 2023-05-27 13:05:51 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 e9722ac9-b5ef-4f4a-83a9-a3b468a0cfd0 0xc004a27947 0xc004a27948}] [] [{kube-controller-manager Update v1 2023-05-27 13:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9722ac9-b5ef-4f4a-83a9-a3b468a0cfd0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 13:05:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bt459,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bt459,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.22.3,PodIP:192.168.7.126,StartTime:2023-05-27 13:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-27 13:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://754220a410b1f2e19295380087c0b2e9c1653cfe1a5c187ddee4864c6240b783,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.126,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 27 13:05:53.326: INFO: pod: "test-deployment-6fc78d85c6-kn8hg":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-kn8hg test-deployment-6fc78d85c6- deployment-4862  769922fd-4c8c-4abd-8445-e270f36cca44 20359 0 2023-05-27 13:05:50 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 e9722ac9-b5ef-4f4a-83a9-a3b468a0cfd0 0xc004a27b37 0xc004a27b38}] [] [{kube-controller-manager Update v1 2023-05-27 13:05:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9722ac9-b5ef-4f4a-83a9-a3b468a0cfd0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 13:05:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.19.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wpcdg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wpcdg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:05:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.172,PodIP:192.168.19.120,StartTime:2023-05-27 13:05:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-27 13:05:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c3ac56c34509e9952b97031ae0541f4e0a98f3d84eb759182e48f72d89e845a0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.19.120,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 27 13:05:53.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4862" for this suite. @ 05/27/23 13:05:53.334
• [6.182 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 05/27/23 13:05:53.348
  May 27 13:05:53.349: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename deployment @ 05/27/23 13:05:53.35
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:05:53.375
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:05:53.38
  May 27 13:05:53.390: INFO: Creating deployment "test-recreate-deployment"
  May 27 13:05:53.398: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  May 27 13:05:53.411: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  May 27 13:05:55.421: INFO: Waiting deployment "test-recreate-deployment" to complete
  May 27 13:05:55.424: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  May 27 13:05:55.439: INFO: Updating deployment test-recreate-deployment
  May 27 13:05:55.440: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  May 27 13:05:55.542: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-7875  cd609f0a-7b81-4118-9bd2-1d71d2899871 20511 2 2023-05-27 13:05:53 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-27 13:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 13:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047fabc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-27 13:05:55 +0000 UTC,LastTransitionTime:2023-05-27 13:05:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-05-27 13:05:55 +0000 UTC,LastTransitionTime:2023-05-27 13:05:53 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  May 27 13:05:55.547: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-7875  5d985fef-1af7-4969-aade-a3fd95469771 20509 1 2023-05-27 13:05:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment cd609f0a-7b81-4118-9bd2-1d71d2899871 0xc00481d7f7 0xc00481d7f8}] [] [{kube-controller-manager Update apps/v1 2023-05-27 13:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd609f0a-7b81-4118-9bd2-1d71d2899871\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 13:05:55 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00481d898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 27 13:05:55.548: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  May 27 13:05:55.548: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-7875  de6e6d5a-6669-455a-9cd8-26f8bf58a2d2 20499 2 2023-05-27 13:05:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment cd609f0a-7b81-4118-9bd2-1d71d2899871 0xc00481d907 0xc00481d908}] [] [{kube-controller-manager Update apps/v1 2023-05-27 13:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd609f0a-7b81-4118-9bd2-1d71d2899871\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 13:05:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00481d9b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 27 13:05:55.553: INFO: Pod "test-recreate-deployment-54757ffd6c-lxv9z" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-lxv9z test-recreate-deployment-54757ffd6c- deployment-7875  ebed4d3d-62bb-40b0-b163-9efe0b9f12b5 20508 0 2023-05-27 13:05:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 5d985fef-1af7-4969-aade-a3fd95469771 0xc0047faf47 0xc0047faf48}] [] [{kube-controller-manager Update v1 2023-05-27 13:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d985fef-1af7-4969-aade-a3fd95469771\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 13:05:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xpv2r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xpv2r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:05:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:05:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:05:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:05:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.172,PodIP:,StartTime:2023-05-27 13:05:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 13:05:55.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7875" for this suite. @ 05/27/23 13:05:55.559
• [2.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 05/27/23 13:05:55.574
  May 27 13:05:55.574: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename pods @ 05/27/23 13:05:55.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:05:55.596
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:05:55.602
  STEP: Create a pod @ 05/27/23 13:05:55.608
  STEP: patching /status @ 05/27/23 13:05:57.634
  May 27 13:05:57.645: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  May 27 13:05:57.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4020" for this suite. @ 05/27/23 13:05:57.654
• [2.088 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 05/27/23 13:05:57.663
  May 27 13:05:57.663: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename lease-test @ 05/27/23 13:05:57.664
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:05:57.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:05:57.692
  May 27 13:05:57.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-5311" for this suite. @ 05/27/23 13:05:57.783
• [0.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 05/27/23 13:05:57.796
  May 27 13:05:57.796: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename namespaces @ 05/27/23 13:05:57.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:05:57.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:05:57.826
  STEP: Read namespace status @ 05/27/23 13:05:57.831
  May 27 13:05:57.836: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 05/27/23 13:05:57.836
  May 27 13:05:57.844: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 05/27/23 13:05:57.845
  May 27 13:05:57.858: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  May 27 13:05:57.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8029" for this suite. @ 05/27/23 13:05:57.865
• [0.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 05/27/23 13:05:57.881
  May 27 13:05:57.881: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename taint-single-pod @ 05/27/23 13:05:57.882
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:05:57.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:05:57.91
  May 27 13:05:57.915: INFO: Waiting up to 1m0s for all nodes to be ready
  May 27 13:06:57.935: INFO: Waiting for terminating namespaces to be deleted...
  May 27 13:06:57.940: INFO: Starting informer...
  STEP: Starting pod... @ 05/27/23 13:06:57.94
  May 27 13:06:58.161: INFO: Pod is running on ip-172-31-68-172. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/27/23 13:06:58.161
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/27/23 13:06:58.173
  STEP: Waiting short time to make sure Pod is queued for deletion @ 05/27/23 13:06:58.179
  May 27 13:06:58.179: INFO: Pod wasn't evicted. Proceeding
  May 27 13:06:58.179: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/27/23 13:06:58.204
  STEP: Waiting some time to make sure that toleration time passed. @ 05/27/23 13:06:58.213
  May 27 13:08:13.214: INFO: Pod wasn't evicted. Test successful
  May 27 13:08:13.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-1591" for this suite. @ 05/27/23 13:08:13.221
• [135.349 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 05/27/23 13:08:13.233
  May 27 13:08:13.233: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename configmap @ 05/27/23 13:08:13.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:08:13.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:08:13.261
  STEP: Creating configMap with name configmap-test-volume-159911ff-ee4e-4fb1-94be-ce79bcf2794d @ 05/27/23 13:08:13.266
  STEP: Creating a pod to test consume configMaps @ 05/27/23 13:08:13.272
  STEP: Saw pod success @ 05/27/23 13:08:17.299
  May 27 13:08:17.303: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-configmaps-d96d0837-9903-4810-b739-649ee3660df7 container agnhost-container: <nil>
  STEP: delete the pod @ 05/27/23 13:08:17.327
  May 27 13:08:17.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-300" for this suite. @ 05/27/23 13:08:17.352
• [4.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 05/27/23 13:08:17.365
  May 27 13:08:17.365: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl @ 05/27/23 13:08:17.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:08:17.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:08:17.391
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/27/23 13:08:17.396
  May 27 13:08:17.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-9467 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May 27 13:08:17.486: INFO: stderr: ""
  May 27 13:08:17.486: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 05/27/23 13:08:17.486
  May 27 13:08:17.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-9467 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  May 27 13:08:17.574: INFO: stderr: ""
  May 27 13:08:17.574: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/27/23 13:08:17.574
  May 27 13:08:17.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-9467 delete pods e2e-test-httpd-pod'
  May 27 13:08:19.766: INFO: stderr: ""
  May 27 13:08:19.766: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 27 13:08:19.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9467" for this suite. @ 05/27/23 13:08:19.772
• [2.415 seconds]
------------------------------
S
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 05/27/23 13:08:19.78
  May 27 13:08:19.780: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename job @ 05/27/23 13:08:19.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:08:19.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:08:19.808
  STEP: Creating Indexed job @ 05/27/23 13:08:19.814
  STEP: Ensuring job reaches completions @ 05/27/23 13:08:19.822
  STEP: Ensuring pods with index for job exist @ 05/27/23 13:08:29.827
  May 27 13:08:29.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-253" for this suite. @ 05/27/23 13:08:29.837
• [10.066 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 05/27/23 13:08:29.846
  May 27 13:08:29.847: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename namespaces @ 05/27/23 13:08:29.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:08:29.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:08:29.872
  STEP: Creating a test namespace @ 05/27/23 13:08:29.877
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:08:29.892
  STEP: Creating a pod in the namespace @ 05/27/23 13:08:29.898
  STEP: Waiting for the pod to have running status @ 05/27/23 13:08:29.908
  STEP: Deleting the namespace @ 05/27/23 13:08:31.926
  STEP: Waiting for the namespace to be removed. @ 05/27/23 13:08:31.935
  STEP: Recreating the namespace @ 05/27/23 13:08:42.941
  STEP: Verifying there are no pods in the namespace @ 05/27/23 13:08:42.961
  May 27 13:08:42.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5653" for this suite. @ 05/27/23 13:08:42.974
  STEP: Destroying namespace "nsdeletetest-2172" for this suite. @ 05/27/23 13:08:42.983
  May 27 13:08:42.988: INFO: Namespace nsdeletetest-2172 was already deleted
  STEP: Destroying namespace "nsdeletetest-7850" for this suite. @ 05/27/23 13:08:42.988
• [13.152 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 05/27/23 13:08:43
  May 27 13:08:43.000: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename subpath @ 05/27/23 13:08:43.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:08:43.02
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:08:43.031
  STEP: Setting up data @ 05/27/23 13:08:43.035
  STEP: Creating pod pod-subpath-test-configmap-hv48 @ 05/27/23 13:08:43.048
  STEP: Creating a pod to test atomic-volume-subpath @ 05/27/23 13:08:43.048
  STEP: Saw pod success @ 05/27/23 13:09:07.132
  May 27 13:09:07.137: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-subpath-test-configmap-hv48 container test-container-subpath-configmap-hv48: <nil>
  STEP: delete the pod @ 05/27/23 13:09:07.151
  STEP: Deleting pod pod-subpath-test-configmap-hv48 @ 05/27/23 13:09:07.17
  May 27 13:09:07.170: INFO: Deleting pod "pod-subpath-test-configmap-hv48" in namespace "subpath-5794"
  May 27 13:09:07.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5794" for this suite. @ 05/27/23 13:09:07.18
• [24.189 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 05/27/23 13:09:07.19
  May 27 13:09:07.190: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename pods @ 05/27/23 13:09:07.191
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:09:07.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:09:07.216
  STEP: Saw pod success @ 05/27/23 13:09:13.3
  May 27 13:09:13.304: INFO: Trying to get logs from node ip-172-31-68-172 pod client-envvars-648227cd-0b85-46dc-84af-298551cc9d73 container env3cont: <nil>
  STEP: delete the pod @ 05/27/23 13:09:13.315
  May 27 13:09:13.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6204" for this suite. @ 05/27/23 13:09:13.342
• [6.162 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 05/27/23 13:09:13.352
  May 27 13:09:13.352: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 13:09:13.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:09:13.374
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:09:13.38
  STEP: Creating the pod @ 05/27/23 13:09:13.385
  May 27 13:09:15.939: INFO: Successfully updated pod "annotationupdate22c7f887-12dc-46e5-84fd-fdae1649c0fe"
  May 27 13:09:17.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1894" for this suite. @ 05/27/23 13:09:17.969
• [4.628 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 05/27/23 13:09:17.981
  May 27 13:09:17.981: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename downward-api @ 05/27/23 13:09:17.982
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:09:17.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:09:18.006
  STEP: Creating a pod to test downward api env vars @ 05/27/23 13:09:18.012
  STEP: Saw pod success @ 05/27/23 13:09:22.041
  May 27 13:09:22.045: INFO: Trying to get logs from node ip-172-31-68-172 pod downward-api-a08b8151-94ec-41d9-9bf1-96e4b13cccd2 container dapi-container: <nil>
  STEP: delete the pod @ 05/27/23 13:09:22.055
  May 27 13:09:22.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9088" for this suite. @ 05/27/23 13:09:22.077
• [4.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 05/27/23 13:09:22.089
  May 27 13:09:22.089: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename resourcequota @ 05/27/23 13:09:22.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:09:22.114
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:09:22.12
  STEP: Creating a ResourceQuota @ 05/27/23 13:09:22.124
  STEP: Getting a ResourceQuota @ 05/27/23 13:09:22.131
  STEP: Listing all ResourceQuotas with LabelSelector @ 05/27/23 13:09:22.137
  STEP: Patching the ResourceQuota @ 05/27/23 13:09:22.142
  STEP: Deleting a Collection of ResourceQuotas @ 05/27/23 13:09:22.151
  STEP: Verifying the deleted ResourceQuota @ 05/27/23 13:09:22.162
  May 27 13:09:22.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1844" for this suite. @ 05/27/23 13:09:22.173
• [0.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 05/27/23 13:09:22.186
  May 27 13:09:22.186: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 13:09:22.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:09:22.208
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:09:22.212
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 13:09:22.217
  STEP: Saw pod success @ 05/27/23 13:09:26.248
  May 27 13:09:26.253: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-36eaf6b5-61b5-4442-871d-ebdc791ea6ca container client-container: <nil>
  STEP: delete the pod @ 05/27/23 13:09:26.261
  May 27 13:09:26.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9932" for this suite. @ 05/27/23 13:09:26.285
• [4.113 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 05/27/23 13:09:26.3
  May 27 13:09:26.301: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 13:09:26.302
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:09:26.325
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:09:26.329
  STEP: Creating configMap with name projected-configmap-test-volume-d4f87b74-d12f-4276-9970-56c7a580175c @ 05/27/23 13:09:26.339
  STEP: Creating a pod to test consume configMaps @ 05/27/23 13:09:26.347
  STEP: Saw pod success @ 05/27/23 13:09:30.383
  May 27 13:09:30.389: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-projected-configmaps-69f924a5-0148-4776-b3bd-e195150b3cb2 container agnhost-container: <nil>
  STEP: delete the pod @ 05/27/23 13:09:30.399
  May 27 13:09:30.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1357" for this suite. @ 05/27/23 13:09:30.445
• [4.157 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 05/27/23 13:09:30.457
  May 27 13:09:30.457: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename cronjob @ 05/27/23 13:09:30.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:09:30.489
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:09:30.494
  STEP: Creating a suspended cronjob @ 05/27/23 13:09:30.501
  STEP: Ensuring no jobs are scheduled @ 05/27/23 13:09:30.519
  STEP: Ensuring no job exists by listing jobs explicitly @ 05/27/23 13:14:30.529
  STEP: Removing cronjob @ 05/27/23 13:14:30.532
  May 27 13:14:30.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8145" for this suite. @ 05/27/23 13:14:30.547
• [300.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 05/27/23 13:14:30.561
  May 27 13:14:30.561: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename gc @ 05/27/23 13:14:30.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:14:30.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:14:30.59
  STEP: create the rc1 @ 05/27/23 13:14:30.601
  STEP: create the rc2 @ 05/27/23 13:14:30.609
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 05/27/23 13:14:36.621
  STEP: delete the rc simpletest-rc-to-be-deleted @ 05/27/23 13:14:37.194
  STEP: wait for the rc to be deleted @ 05/27/23 13:14:37.205
  May 27 13:14:42.221: INFO: 72 pods remaining
  May 27 13:14:42.221: INFO: 72 pods has nil DeletionTimestamp
  May 27 13:14:42.221: INFO: 
  STEP: Gathering metrics @ 05/27/23 13:14:47.221
  W0527 13:14:47.226818      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May 27 13:14:47.227: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 27 13:14:47.229: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bl7w" in namespace "gc-5727"
  May 27 13:14:47.243: INFO: Deleting pod "simpletest-rc-to-be-deleted-2drng" in namespace "gc-5727"
  May 27 13:14:47.257: INFO: Deleting pod "simpletest-rc-to-be-deleted-2f547" in namespace "gc-5727"
  May 27 13:14:47.274: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tmhm" in namespace "gc-5727"
  May 27 13:14:47.289: INFO: Deleting pod "simpletest-rc-to-be-deleted-49k4v" in namespace "gc-5727"
  May 27 13:14:47.304: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hw45" in namespace "gc-5727"
  May 27 13:14:47.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wplr" in namespace "gc-5727"
  May 27 13:14:47.341: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wtkb" in namespace "gc-5727"
  May 27 13:14:47.360: INFO: Deleting pod "simpletest-rc-to-be-deleted-56dvv" in namespace "gc-5727"
  May 27 13:14:47.381: INFO: Deleting pod "simpletest-rc-to-be-deleted-59xjt" in namespace "gc-5727"
  May 27 13:14:47.397: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nbzj" in namespace "gc-5727"
  May 27 13:14:47.416: INFO: Deleting pod "simpletest-rc-to-be-deleted-5s4s7" in namespace "gc-5727"
  May 27 13:14:47.434: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wxps" in namespace "gc-5727"
  May 27 13:14:47.459: INFO: Deleting pod "simpletest-rc-to-be-deleted-66d8s" in namespace "gc-5727"
  May 27 13:14:47.479: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bmbd" in namespace "gc-5727"
  May 27 13:14:47.495: INFO: Deleting pod "simpletest-rc-to-be-deleted-74dhb" in namespace "gc-5727"
  May 27 13:14:47.513: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bzlk" in namespace "gc-5727"
  May 27 13:14:47.527: INFO: Deleting pod "simpletest-rc-to-be-deleted-856bq" in namespace "gc-5727"
  May 27 13:14:47.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cjs4" in namespace "gc-5727"
  May 27 13:14:47.562: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mvb5" in namespace "gc-5727"
  May 27 13:14:47.583: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vgmp" in namespace "gc-5727"
  May 27 13:14:47.596: INFO: Deleting pod "simpletest-rc-to-be-deleted-c8946" in namespace "gc-5727"
  May 27 13:14:47.616: INFO: Deleting pod "simpletest-rc-to-be-deleted-c89qn" in namespace "gc-5727"
  May 27 13:14:47.631: INFO: Deleting pod "simpletest-rc-to-be-deleted-cj6sx" in namespace "gc-5727"
  May 27 13:14:47.646: INFO: Deleting pod "simpletest-rc-to-be-deleted-cpzbf" in namespace "gc-5727"
  May 27 13:14:47.666: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctsbc" in namespace "gc-5727"
  May 27 13:14:47.684: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvp76" in namespace "gc-5727"
  May 27 13:14:47.703: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgb7g" in namespace "gc-5727"
  May 27 13:14:47.720: INFO: Deleting pod "simpletest-rc-to-be-deleted-djbk5" in namespace "gc-5727"
  May 27 13:14:47.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdpmn" in namespace "gc-5727"
  May 27 13:14:47.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgm4p" in namespace "gc-5727"
  May 27 13:14:47.776: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkctk" in namespace "gc-5727"
  May 27 13:14:47.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-fktdx" in namespace "gc-5727"
  May 27 13:14:47.809: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzbbq" in namespace "gc-5727"
  May 27 13:14:47.828: INFO: Deleting pod "simpletest-rc-to-be-deleted-fztdr" in namespace "gc-5727"
  May 27 13:14:47.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8vk4" in namespace "gc-5727"
  May 27 13:14:47.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-grqfb" in namespace "gc-5727"
  May 27 13:14:47.880: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwwsh" in namespace "gc-5727"
  May 27 13:14:47.895: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4w48" in namespace "gc-5727"
  May 27 13:14:47.914: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdf8d" in namespace "gc-5727"
  May 27 13:14:47.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-hg4sp" in namespace "gc-5727"
  May 27 13:14:47.943: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgzwx" in namespace "gc-5727"
  May 27 13:14:47.957: INFO: Deleting pod "simpletest-rc-to-be-deleted-hkts8" in namespace "gc-5727"
  May 27 13:14:47.971: INFO: Deleting pod "simpletest-rc-to-be-deleted-hth7r" in namespace "gc-5727"
  May 27 13:14:48.000: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvs6d" in namespace "gc-5727"
  May 27 13:14:48.020: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzgsg" in namespace "gc-5727"
  May 27 13:14:48.036: INFO: Deleting pod "simpletest-rc-to-be-deleted-j4j5p" in namespace "gc-5727"
  May 27 13:14:48.053: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9d75" in namespace "gc-5727"
  May 27 13:14:48.069: INFO: Deleting pod "simpletest-rc-to-be-deleted-jdhz6" in namespace "gc-5727"
  May 27 13:14:48.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-jpnsk" in namespace "gc-5727"
  May 27 13:14:48.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5727" for this suite. @ 05/27/23 13:14:48.112
• [17.573 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 05/27/23 13:14:48.135
  May 27 13:14:48.135: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename gc @ 05/27/23 13:14:48.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:14:48.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:14:48.162
  May 27 13:14:48.217: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"371f4504-909c-4030-b95f-8ce35c3b865b", Controller:(*bool)(0xc00350c8ee), BlockOwnerDeletion:(*bool)(0xc00350c8ef)}}
  May 27 13:14:48.231: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"7c1fab1f-8214-4dbe-967c-ef63a2d6cc89", Controller:(*bool)(0xc00350cb26), BlockOwnerDeletion:(*bool)(0xc00350cb27)}}
  May 27 13:14:48.240: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a0bfdedf-d743-4e0f-b925-4ec99da5c88e", Controller:(*bool)(0xc001e1fdd6), BlockOwnerDeletion:(*bool)(0xc001e1fdd7)}}
  May 27 13:14:53.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7120" for this suite. @ 05/27/23 13:14:53.267
• [5.162 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 05/27/23 13:14:53.299
  May 27 13:14:53.299: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl @ 05/27/23 13:14:53.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:14:53.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:14:53.347
  STEP: validating cluster-info @ 05/27/23 13:14:53.364
  May 27 13:14:53.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-5704 cluster-info'
  May 27 13:14:53.613: INFO: stderr: ""
  May 27 13:14:53.613: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  May 27 13:14:53.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5704" for this suite. @ 05/27/23 13:14:53.62
• [0.335 seconds]
------------------------------
S
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 05/27/23 13:14:53.634
  May 27 13:14:53.634: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename dns @ 05/27/23 13:14:53.635
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:14:53.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:14:53.676
  STEP: Creating a test headless service @ 05/27/23 13:14:53.683
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8194.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8194.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 05/27/23 13:14:53.693
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8194.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8194.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 05/27/23 13:14:53.693
  STEP: creating a pod to probe DNS @ 05/27/23 13:14:53.693
  STEP: submitting the pod to kubernetes @ 05/27/23 13:14:53.693
  STEP: retrieving the pod @ 05/27/23 13:14:57.748
  STEP: looking for the results for each expected name from probers @ 05/27/23 13:14:57.753
  May 27 13:14:57.777: INFO: DNS probes using dns-8194/dns-test-74cbd349-5597-4dd5-abb9-03d30e4bdf41 succeeded

  May 27 13:14:57.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/27/23 13:14:57.785
  STEP: deleting the test headless service @ 05/27/23 13:14:57.828
  STEP: Destroying namespace "dns-8194" for this suite. @ 05/27/23 13:14:57.857
• [4.241 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 05/27/23 13:14:57.877
  May 27 13:14:57.877: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename downward-api @ 05/27/23 13:14:57.879
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:14:57.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:14:57.925
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 13:14:57.932
  STEP: Saw pod success @ 05/27/23 13:15:01.967
  May 27 13:15:01.973: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-fae9a8dd-100d-4904-89be-8fcc63086bc2 container client-container: <nil>
  STEP: delete the pod @ 05/27/23 13:15:02.003
  May 27 13:15:02.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4537" for this suite. @ 05/27/23 13:15:02.034
• [4.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 05/27/23 13:15:02.051
  May 27 13:15:02.051: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 13:15:02.052
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:15:02.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:15:02.089
  STEP: Setting up server cert @ 05/27/23 13:15:02.136
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 13:15:02.574
  STEP: Deploying the webhook pod @ 05/27/23 13:15:02.586
  STEP: Wait for the deployment to be ready @ 05/27/23 13:15:02.612
  May 27 13:15:02.641: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/27/23 13:15:04.663
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 13:15:04.675
  May 27 13:15:05.676: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 05/27/23 13:15:05.68
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/27/23 13:15:05.7
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 05/27/23 13:15:05.71
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/27/23 13:15:05.726
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 05/27/23 13:15:05.745
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/27/23 13:15:05.757
  May 27 13:15:05.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4806" for this suite. @ 05/27/23 13:15:05.852
  STEP: Destroying namespace "webhook-markers-4446" for this suite. @ 05/27/23 13:15:05.869
• [3.829 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 05/27/23 13:15:05.882
  May 27 13:15:05.882: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename field-validation @ 05/27/23 13:15:05.883
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:15:05.954
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:15:05.961
  May 27 13:15:05.968: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  W0527 13:15:08.582274      18 warnings.go:70] unknown field "alpha"
  W0527 13:15:08.582350      18 warnings.go:70] unknown field "beta"
  W0527 13:15:08.582358      18 warnings.go:70] unknown field "delta"
  W0527 13:15:08.582365      18 warnings.go:70] unknown field "epsilon"
  W0527 13:15:08.582372      18 warnings.go:70] unknown field "gamma"
  May 27 13:15:08.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1102" for this suite. @ 05/27/23 13:15:08.639
• [2.769 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 05/27/23 13:15:08.652
  May 27 13:15:08.652: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename configmap @ 05/27/23 13:15:08.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:15:08.674
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:15:08.679
  STEP: Creating configMap configmap-1436/configmap-test-a6f77c24-d7e3-4170-9d68-eba0082b8a84 @ 05/27/23 13:15:08.685
  STEP: Creating a pod to test consume configMaps @ 05/27/23 13:15:08.694
  STEP: Saw pod success @ 05/27/23 13:15:12.734
  May 27 13:15:12.739: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-configmaps-abf081f5-020b-4ad7-8273-7ef574d36b46 container env-test: <nil>
  STEP: delete the pod @ 05/27/23 13:15:12.749
  May 27 13:15:12.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1436" for this suite. @ 05/27/23 13:15:12.777
• [4.134 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 05/27/23 13:15:12.788
  May 27 13:15:12.788: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 13:15:12.789
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:15:12.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:15:12.821
  STEP: Creating projection with secret that has name projected-secret-test-cfed6ffb-a66e-4308-8300-d8d40dc65009 @ 05/27/23 13:15:12.826
  STEP: Creating a pod to test consume secrets @ 05/27/23 13:15:12.831
  STEP: Saw pod success @ 05/27/23 13:15:16.859
  May 27 13:15:16.865: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-projected-secrets-8afb5ddd-0fa7-46f8-bf14-dda19c891007 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/27/23 13:15:16.874
  May 27 13:15:16.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1803" for this suite. @ 05/27/23 13:15:16.899
• [4.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 05/27/23 13:15:16.92
  May 27 13:15:16.920: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename configmap @ 05/27/23 13:15:16.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:15:16.942
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:15:16.948
  STEP: Creating configMap with name configmap-test-upd-55d351c4-5484-4121-975e-bc205ece1377 @ 05/27/23 13:15:16.96
  STEP: Creating the pod @ 05/27/23 13:15:16.966
  STEP: Waiting for pod with text data @ 05/27/23 13:15:18.993
  STEP: Waiting for pod with binary data @ 05/27/23 13:15:19.004
  May 27 13:15:19.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2441" for this suite. @ 05/27/23 13:15:19.018
• [2.106 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 05/27/23 13:15:19.027
  May 27 13:15:19.027: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename disruption @ 05/27/23 13:15:19.028
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:15:19.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:15:19.052
  STEP: Waiting for the pdb to be processed @ 05/27/23 13:15:19.065
  STEP: Waiting for all pods to be running @ 05/27/23 13:15:21.115
  May 27 13:15:21.123: INFO: running pods: 0 < 3
  May 27 13:15:23.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-8391" for this suite. @ 05/27/23 13:15:23.14
• [4.120 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 05/27/23 13:15:23.148
  May 27 13:15:23.148: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/27/23 13:15:23.149
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:15:23.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:15:23.183
  May 27 13:15:23.189: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:15:24.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-246" for this suite. @ 05/27/23 13:15:24.234
• [1.096 seconds]
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 05/27/23 13:15:24.245
  May 27 13:15:24.245: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-runtime @ 05/27/23 13:15:24.246
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:15:24.264
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:15:24.27
  STEP: create the container @ 05/27/23 13:15:24.276
  W0527 13:15:24.290839      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/27/23 13:15:24.291
  STEP: get the container status @ 05/27/23 13:15:27.32
  STEP: the container should be terminated @ 05/27/23 13:15:27.325
  STEP: the termination message should be set @ 05/27/23 13:15:27.325
  May 27 13:15:27.325: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/27/23 13:15:27.325
  May 27 13:15:27.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3059" for this suite. @ 05/27/23 13:15:27.347
• [3.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 05/27/23 13:15:27.356
  May 27 13:15:27.356: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename controllerrevisions @ 05/27/23 13:15:27.357
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:15:27.373
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:15:27.381
  STEP: Creating DaemonSet "e2e-ttdlz-daemon-set" @ 05/27/23 13:15:27.409
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/27/23 13:15:27.418
  May 27 13:15:27.425: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:15:27.425: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:15:27.430: INFO: Number of nodes with available pods controlled by daemonset e2e-ttdlz-daemon-set: 0
  May 27 13:15:27.431: INFO: Node ip-172-31-10-136 is running 0 daemon pod, expected 1
  May 27 13:15:28.437: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:15:28.437: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:15:28.442: INFO: Number of nodes with available pods controlled by daemonset e2e-ttdlz-daemon-set: 0
  May 27 13:15:28.442: INFO: Node ip-172-31-10-136 is running 0 daemon pod, expected 1
  May 27 13:15:29.438: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:15:29.438: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:15:29.444: INFO: Number of nodes with available pods controlled by daemonset e2e-ttdlz-daemon-set: 3
  May 27 13:15:29.444: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-ttdlz-daemon-set
  STEP: Confirm DaemonSet "e2e-ttdlz-daemon-set" successfully created with "daemonset-name=e2e-ttdlz-daemon-set" label @ 05/27/23 13:15:29.448
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-ttdlz-daemon-set" @ 05/27/23 13:15:29.458
  May 27 13:15:29.466: INFO: Located ControllerRevision: "e2e-ttdlz-daemon-set-6bc5446499"
  STEP: Patching ControllerRevision "e2e-ttdlz-daemon-set-6bc5446499" @ 05/27/23 13:15:29.47
  May 27 13:15:29.480: INFO: e2e-ttdlz-daemon-set-6bc5446499 has been patched
  STEP: Create a new ControllerRevision @ 05/27/23 13:15:29.48
  May 27 13:15:29.489: INFO: Created ControllerRevision: e2e-ttdlz-daemon-set-6679c7f95
  STEP: Confirm that there are two ControllerRevisions @ 05/27/23 13:15:29.489
  May 27 13:15:29.489: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 27 13:15:29.494: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-ttdlz-daemon-set-6bc5446499" @ 05/27/23 13:15:29.494
  STEP: Confirm that there is only one ControllerRevision @ 05/27/23 13:15:29.502
  May 27 13:15:29.502: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 27 13:15:29.514: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-ttdlz-daemon-set-6679c7f95" @ 05/27/23 13:15:29.518
  May 27 13:15:29.532: INFO: e2e-ttdlz-daemon-set-6679c7f95 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 05/27/23 13:15:29.533
  W0527 13:15:29.550391      18 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 05/27/23 13:15:29.55
  May 27 13:15:29.550: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 27 13:15:30.558: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 27 13:15:30.563: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-ttdlz-daemon-set-6679c7f95=updated" @ 05/27/23 13:15:30.563
  STEP: Confirm that there is only one ControllerRevision @ 05/27/23 13:15:30.574
  May 27 13:15:30.574: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 27 13:15:30.578: INFO: Found 1 ControllerRevisions
  May 27 13:15:30.583: INFO: ControllerRevision "e2e-ttdlz-daemon-set-7c6684cbb" has revision 3
  STEP: Deleting DaemonSet "e2e-ttdlz-daemon-set" @ 05/27/23 13:15:30.587
  STEP: deleting DaemonSet.extensions e2e-ttdlz-daemon-set in namespace controllerrevisions-6820, will wait for the garbage collector to delete the pods @ 05/27/23 13:15:30.587
  May 27 13:15:30.651: INFO: Deleting DaemonSet.extensions e2e-ttdlz-daemon-set took: 8.595028ms
  May 27 13:15:30.751: INFO: Terminating DaemonSet.extensions e2e-ttdlz-daemon-set pods took: 100.34895ms
  May 27 13:15:32.458: INFO: Number of nodes with available pods controlled by daemonset e2e-ttdlz-daemon-set: 0
  May 27 13:15:32.458: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-ttdlz-daemon-set
  May 27 13:15:32.462: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25384"},"items":null}

  May 27 13:15:32.466: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25384"},"items":null}

  May 27 13:15:32.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-6820" for this suite. @ 05/27/23 13:15:32.494
• [5.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 05/27/23 13:15:32.507
  May 27 13:15:32.507: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 13:15:32.508
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:15:32.531
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:15:32.536
  STEP: Setting up server cert @ 05/27/23 13:15:32.576
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 13:15:33.342
  STEP: Deploying the webhook pod @ 05/27/23 13:15:33.351
  STEP: Wait for the deployment to be ready @ 05/27/23 13:15:33.368
  May 27 13:15:33.388: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/27/23 13:15:35.405
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 13:15:35.425
  May 27 13:15:36.426: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/27/23 13:15:36.431
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/27/23 13:15:36.451
  STEP: Creating a dummy validating-webhook-configuration object @ 05/27/23 13:15:36.473
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 05/27/23 13:15:36.484
  STEP: Creating a dummy mutating-webhook-configuration object @ 05/27/23 13:15:36.491
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 05/27/23 13:15:36.503
  May 27 13:15:36.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8382" for this suite. @ 05/27/23 13:15:36.595
  STEP: Destroying namespace "webhook-markers-2344" for this suite. @ 05/27/23 13:15:36.605
• [4.105 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 05/27/23 13:15:36.614
  May 27 13:15:36.614: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename daemonsets @ 05/27/23 13:15:36.615
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:15:36.636
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:15:36.642
  May 27 13:15:36.672: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 05/27/23 13:15:36.679
  May 27 13:15:36.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 13:15:36.684: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 05/27/23 13:15:36.684
  May 27 13:15:36.714: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 13:15:36.714: INFO: Node ip-172-31-68-172 is running 0 daemon pod, expected 1
  May 27 13:15:37.718: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 13:15:37.718: INFO: Node ip-172-31-68-172 is running 0 daemon pod, expected 1
  May 27 13:15:38.720: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 27 13:15:38.720: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 05/27/23 13:15:38.724
  May 27 13:15:38.743: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 27 13:15:38.743: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  May 27 13:15:39.749: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 13:15:39.749: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 05/27/23 13:15:39.749
  May 27 13:15:39.763: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 13:15:39.763: INFO: Node ip-172-31-68-172 is running 0 daemon pod, expected 1
  May 27 13:15:40.768: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 13:15:40.768: INFO: Node ip-172-31-68-172 is running 0 daemon pod, expected 1
  May 27 13:15:41.769: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 27 13:15:41.769: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/27/23 13:15:41.782
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8577, will wait for the garbage collector to delete the pods @ 05/27/23 13:15:41.782
  May 27 13:15:41.848: INFO: Deleting DaemonSet.extensions daemon-set took: 10.037928ms
  May 27 13:15:41.949: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.456871ms
  May 27 13:15:43.056: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 13:15:43.056: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 27 13:15:43.060: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25593"},"items":null}

  May 27 13:15:43.064: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25593"},"items":null}

  May 27 13:15:43.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8577" for this suite. @ 05/27/23 13:15:43.1
• [6.496 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 05/27/23 13:15:43.111
  May 27 13:15:43.111: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename field-validation @ 05/27/23 13:15:43.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:15:43.136
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:15:43.141
  May 27 13:15:43.146: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  W0527 13:15:43.147651      18 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc0013b85b0 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0527 13:15:45.714420      18 warnings.go:70] unknown field "alpha"
  W0527 13:15:45.714459      18 warnings.go:70] unknown field "beta"
  W0527 13:15:45.714467      18 warnings.go:70] unknown field "delta"
  W0527 13:15:45.714474      18 warnings.go:70] unknown field "epsilon"
  W0527 13:15:45.714481      18 warnings.go:70] unknown field "gamma"
  May 27 13:15:45.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9517" for this suite. @ 05/27/23 13:15:45.76
• [2.657 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 05/27/23 13:15:45.77
  May 27 13:15:45.770: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename job @ 05/27/23 13:15:45.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:15:45.796
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:15:45.8
  STEP: Creating a job @ 05/27/23 13:15:45.804
  STEP: Ensuring active pods == parallelism @ 05/27/23 13:15:45.813
  STEP: Orphaning one of the Job's Pods @ 05/27/23 13:15:47.819
  May 27 13:15:48.339: INFO: Successfully updated pod "adopt-release-bjgdx"
  STEP: Checking that the Job readopts the Pod @ 05/27/23 13:15:48.34
  STEP: Removing the labels from the Job's Pod @ 05/27/23 13:15:50.353
  May 27 13:15:50.869: INFO: Successfully updated pod "adopt-release-bjgdx"
  STEP: Checking that the Job releases the Pod @ 05/27/23 13:15:50.869
  May 27 13:15:52.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-314" for this suite. @ 05/27/23 13:15:52.884
• [7.122 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 05/27/23 13:15:52.894
  May 27 13:15:52.894: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename resourcequota @ 05/27/23 13:15:52.895
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:15:52.916
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:15:52.921
  STEP: Counting existing ResourceQuota @ 05/27/23 13:15:52.926
  STEP: Creating a ResourceQuota @ 05/27/23 13:15:57.931
  STEP: Ensuring resource quota status is calculated @ 05/27/23 13:15:57.939
  STEP: Creating a ReplicaSet @ 05/27/23 13:15:59.944
  STEP: Ensuring resource quota status captures replicaset creation @ 05/27/23 13:15:59.959
  STEP: Deleting a ReplicaSet @ 05/27/23 13:16:01.966
  STEP: Ensuring resource quota status released usage @ 05/27/23 13:16:01.975
  May 27 13:16:03.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9387" for this suite. @ 05/27/23 13:16:03.986
• [11.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 05/27/23 13:16:04.004
  May 27 13:16:04.004: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename services @ 05/27/23 13:16:04.005
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:16:04.025
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:16:04.035
  STEP: creating a Service @ 05/27/23 13:16:04.044
  STEP: watching for the Service to be added @ 05/27/23 13:16:04.059
  May 27 13:16:04.063: INFO: Found Service test-service-xjbls in namespace services-3833 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  May 27 13:16:04.064: INFO: Service test-service-xjbls created
  STEP: Getting /status @ 05/27/23 13:16:04.064
  May 27 13:16:04.072: INFO: Service test-service-xjbls has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 05/27/23 13:16:04.072
  STEP: watching for the Service to be patched @ 05/27/23 13:16:04.082
  May 27 13:16:04.085: INFO: observed Service test-service-xjbls in namespace services-3833 with annotations: map[] & LoadBalancer: {[]}
  May 27 13:16:04.085: INFO: Found Service test-service-xjbls in namespace services-3833 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  May 27 13:16:04.085: INFO: Service test-service-xjbls has service status patched
  STEP: updating the ServiceStatus @ 05/27/23 13:16:04.085
  May 27 13:16:04.102: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 05/27/23 13:16:04.102
  May 27 13:16:04.105: INFO: Observed Service test-service-xjbls in namespace services-3833 with annotations: map[] & Conditions: {[]}
  May 27 13:16:04.105: INFO: Observed event: &Service{ObjectMeta:{test-service-xjbls  services-3833  f2a5f542-90f0-4ad1-b108-5204d8f1d27d 25772 0 2023-05-27 13:16:04 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-27 13:16:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-27 13:16:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.91,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.91],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  May 27 13:16:04.105: INFO: Found Service test-service-xjbls in namespace services-3833 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 27 13:16:04.105: INFO: Service test-service-xjbls has service status updated
  STEP: patching the service @ 05/27/23 13:16:04.105
  STEP: watching for the Service to be patched @ 05/27/23 13:16:04.118
  May 27 13:16:04.121: INFO: observed Service test-service-xjbls in namespace services-3833 with labels: map[test-service-static:true]
  May 27 13:16:04.121: INFO: observed Service test-service-xjbls in namespace services-3833 with labels: map[test-service-static:true]
  May 27 13:16:04.122: INFO: observed Service test-service-xjbls in namespace services-3833 with labels: map[test-service-static:true]
  May 27 13:16:04.122: INFO: Found Service test-service-xjbls in namespace services-3833 with labels: map[test-service:patched test-service-static:true]
  May 27 13:16:04.122: INFO: Service test-service-xjbls patched
  STEP: deleting the service @ 05/27/23 13:16:04.122
  STEP: watching for the Service to be deleted @ 05/27/23 13:16:04.14
  May 27 13:16:04.143: INFO: Observed event: ADDED
  May 27 13:16:04.143: INFO: Observed event: MODIFIED
  May 27 13:16:04.143: INFO: Observed event: MODIFIED
  May 27 13:16:04.143: INFO: Observed event: MODIFIED
  May 27 13:16:04.143: INFO: Found Service test-service-xjbls in namespace services-3833 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  May 27 13:16:04.144: INFO: Service test-service-xjbls deleted
  May 27 13:16:04.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3833" for this suite. @ 05/27/23 13:16:04.15
• [0.155 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 05/27/23 13:16:04.161
  May 27 13:16:04.161: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 13:16:04.162
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:16:04.181
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:16:04.187
  STEP: Creating secret with name s-test-opt-del-056f884b-6914-4cce-8cfb-1a678fe362d4 @ 05/27/23 13:16:04.196
  STEP: Creating secret with name s-test-opt-upd-6fba8b28-cb48-40ae-b786-d155ad221c48 @ 05/27/23 13:16:04.204
  STEP: Creating the pod @ 05/27/23 13:16:04.211
  STEP: Deleting secret s-test-opt-del-056f884b-6914-4cce-8cfb-1a678fe362d4 @ 05/27/23 13:16:06.271
  STEP: Updating secret s-test-opt-upd-6fba8b28-cb48-40ae-b786-d155ad221c48 @ 05/27/23 13:16:06.28
  STEP: Creating secret with name s-test-opt-create-b286eef9-c262-458b-bfb1-c14705db5a9d @ 05/27/23 13:16:06.287
  STEP: waiting to observe update in volume @ 05/27/23 13:16:06.293
  May 27 13:16:10.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-492" for this suite. @ 05/27/23 13:16:10.342
• [6.190 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 05/27/23 13:16:10.357
  May 27 13:16:10.358: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename deployment @ 05/27/23 13:16:10.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:16:10.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:16:10.388
  May 27 13:16:10.409: INFO: Pod name rollover-pod: Found 0 pods out of 1
  May 27 13:16:15.416: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/27/23 13:16:15.416
  May 27 13:16:15.416: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  May 27 13:16:17.422: INFO: Creating deployment "test-rollover-deployment"
  May 27 13:16:17.432: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  May 27 13:16:19.441: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  May 27 13:16:19.452: INFO: Ensure that both replica sets have 1 created replica
  May 27 13:16:19.461: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  May 27 13:16:19.474: INFO: Updating deployment test-rollover-deployment
  May 27 13:16:19.474: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  May 27 13:16:21.486: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  May 27 13:16:21.497: INFO: Make sure deployment "test-rollover-deployment" is complete
  May 27 13:16:21.507: INFO: all replica sets need to contain the pod-template-hash label
  May 27 13:16:21.507: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 13, 16, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 13, 16, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 13, 16, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 13, 16, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 13:16:23.517: INFO: all replica sets need to contain the pod-template-hash label
  May 27 13:16:23.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 13, 16, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 13, 16, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 13, 16, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 13, 16, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 13:16:25.519: INFO: all replica sets need to contain the pod-template-hash label
  May 27 13:16:25.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 13, 16, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 13, 16, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 13, 16, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 13, 16, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 13:16:27.519: INFO: all replica sets need to contain the pod-template-hash label
  May 27 13:16:27.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 13, 16, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 13, 16, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 13, 16, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 13, 16, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 13:16:29.516: INFO: all replica sets need to contain the pod-template-hash label
  May 27 13:16:29.516: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 13, 16, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 13, 16, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 27, 13, 16, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 27, 13, 16, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 27 13:16:31.516: INFO: 
  May 27 13:16:31.516: INFO: Ensure that both old replica sets have no replicas
  May 27 13:16:31.536: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-1522  00b77e89-1a7b-4813-91a6-a8c3e33ae6b5 26010 2 2023-05-27 13:16:17 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-27 13:16:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 13:16:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0020a2308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-27 13:16:17 +0000 UTC,LastTransitionTime:2023-05-27 13:16:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-05-27 13:16:31 +0000 UTC,LastTransitionTime:2023-05-27 13:16:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 27 13:16:31.541: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-1522  8a45496e-a3bb-49bb-92ef-0052ebecce01 26000 2 2023-05-27 13:16:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 00b77e89-1a7b-4813-91a6-a8c3e33ae6b5 0xc003d2e907 0xc003d2e908}] [] [{kube-controller-manager Update apps/v1 2023-05-27 13:16:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00b77e89-1a7b-4813-91a6-a8c3e33ae6b5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 13:16:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d2eca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 27 13:16:31.541: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  May 27 13:16:31.542: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1522  96322119-c3df-4e6d-a7c6-fd9bd3052381 26009 2 2023-05-27 13:16:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 00b77e89-1a7b-4813-91a6-a8c3e33ae6b5 0xc003d2e4d7 0xc003d2e4d8}] [] [{e2e.test Update apps/v1 2023-05-27 13:16:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 13:16:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00b77e89-1a7b-4813-91a6-a8c3e33ae6b5\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-27 13:16:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003d2e798 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 27 13:16:31.542: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-1522  ed900f0a-d345-4c8a-a49b-afeed3345e83 25935 2 2023-05-27 13:16:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 00b77e89-1a7b-4813-91a6-a8c3e33ae6b5 0xc003d2ee27 0xc003d2ee28}] [] [{kube-controller-manager Update apps/v1 2023-05-27 13:16:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00b77e89-1a7b-4813-91a6-a8c3e33ae6b5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 13:16:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d2efb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 27 13:16:31.548: INFO: Pod "test-rollover-deployment-57777854c9-jx78z" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-jx78z test-rollover-deployment-57777854c9- deployment-1522  e0572491-37b2-4f3f-a7b2-49401a1decb8 25953 0 2023-05-27 13:16:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 8a45496e-a3bb-49bb-92ef-0052ebecce01 0xc003aec7a7 0xc003aec7a8}] [] [{kube-controller-manager Update v1 2023-05-27 13:16:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8a45496e-a3bb-49bb-92ef-0052ebecce01\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 13:16:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.19.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttnzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttnzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:16:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:16:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:16:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:16:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.172,PodIP:192.168.19.77,StartTime:2023-05-27 13:16:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-27 13:16:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://1f3bd3784218d10eba4590c58de7de09a7f7c587dd423ff70f132c1761b0d35c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.19.77,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 13:16:31.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1522" for this suite. @ 05/27/23 13:16:31.556
• [21.208 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 05/27/23 13:16:31.566
  May 27 13:16:31.566: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename downward-api @ 05/27/23 13:16:31.568
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:16:31.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:16:31.594
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 13:16:31.602
  STEP: Saw pod success @ 05/27/23 13:16:35.638
  May 27 13:16:35.642: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-b2b1105a-384f-4d34-bfd5-377ded23d6db container client-container: <nil>
  STEP: delete the pod @ 05/27/23 13:16:35.652
  May 27 13:16:35.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8388" for this suite. @ 05/27/23 13:16:35.675
• [4.118 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 05/27/23 13:16:35.685
  May 27 13:16:35.685: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-probe @ 05/27/23 13:16:35.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:16:35.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:16:35.714
  STEP: Creating pod liveness-663b47b5-5bbd-41ce-9e96-c8f5c2d35b59 in namespace container-probe-9760 @ 05/27/23 13:16:35.717
  May 27 13:16:37.743: INFO: Started pod liveness-663b47b5-5bbd-41ce-9e96-c8f5c2d35b59 in namespace container-probe-9760
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/27/23 13:16:37.743
  May 27 13:16:37.747: INFO: Initial restart count of pod liveness-663b47b5-5bbd-41ce-9e96-c8f5c2d35b59 is 0
  May 27 13:16:57.806: INFO: Restart count of pod container-probe-9760/liveness-663b47b5-5bbd-41ce-9e96-c8f5c2d35b59 is now 1 (20.058725937s elapsed)
  May 27 13:17:17.865: INFO: Restart count of pod container-probe-9760/liveness-663b47b5-5bbd-41ce-9e96-c8f5c2d35b59 is now 2 (40.117974283s elapsed)
  May 27 13:17:37.923: INFO: Restart count of pod container-probe-9760/liveness-663b47b5-5bbd-41ce-9e96-c8f5c2d35b59 is now 3 (1m0.176086278s elapsed)
  May 27 13:17:57.976: INFO: Restart count of pod container-probe-9760/liveness-663b47b5-5bbd-41ce-9e96-c8f5c2d35b59 is now 4 (1m20.22824078s elapsed)
  May 27 13:19:08.172: INFO: Restart count of pod container-probe-9760/liveness-663b47b5-5bbd-41ce-9e96-c8f5c2d35b59 is now 5 (2m30.424293314s elapsed)
  May 27 13:19:08.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/27/23 13:19:08.178
  STEP: Destroying namespace "container-probe-9760" for this suite. @ 05/27/23 13:19:08.194
• [152.521 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 05/27/23 13:19:08.207
  May 27 13:19:08.207: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename crd-webhook @ 05/27/23 13:19:08.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:19:08.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:19:08.232
  STEP: Setting up server cert @ 05/27/23 13:19:08.237
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/27/23 13:19:08.959
  STEP: Deploying the custom resource conversion webhook pod @ 05/27/23 13:19:08.97
  STEP: Wait for the deployment to be ready @ 05/27/23 13:19:08.986
  May 27 13:19:08.999: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/27/23 13:19:11.014
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 13:19:11.03
  May 27 13:19:12.031: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May 27 13:19:12.035: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Creating a v1 custom resource @ 05/27/23 13:19:14.63
  STEP: v2 custom resource should be converted @ 05/27/23 13:19:14.637
  May 27 13:19:14.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-9811" for this suite. @ 05/27/23 13:19:15.241
• [7.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 05/27/23 13:19:15.259
  May 27 13:19:15.259: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename watch @ 05/27/23 13:19:15.261
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:19:15.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:19:15.287
  STEP: creating a watch on configmaps with a certain label @ 05/27/23 13:19:15.291
  STEP: creating a new configmap @ 05/27/23 13:19:15.293
  STEP: modifying the configmap once @ 05/27/23 13:19:15.3
  STEP: changing the label value of the configmap @ 05/27/23 13:19:15.311
  STEP: Expecting to observe a delete notification for the watched object @ 05/27/23 13:19:15.322
  May 27 13:19:15.322: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-785  1c222357-9be4-4265-a317-a007d138ff6f 26534 0 2023-05-27 13:19:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-27 13:19:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 27 13:19:15.323: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-785  1c222357-9be4-4265-a317-a007d138ff6f 26535 0 2023-05-27 13:19:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-27 13:19:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 27 13:19:15.323: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-785  1c222357-9be4-4265-a317-a007d138ff6f 26536 0 2023-05-27 13:19:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-27 13:19:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 05/27/23 13:19:15.323
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 05/27/23 13:19:15.337
  STEP: changing the label value of the configmap back @ 05/27/23 13:19:25.34
  STEP: modifying the configmap a third time @ 05/27/23 13:19:25.353
  STEP: deleting the configmap @ 05/27/23 13:19:25.364
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 05/27/23 13:19:25.373
  May 27 13:19:25.373: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-785  1c222357-9be4-4265-a317-a007d138ff6f 26572 0 2023-05-27 13:19:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-27 13:19:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 27 13:19:25.374: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-785  1c222357-9be4-4265-a317-a007d138ff6f 26573 0 2023-05-27 13:19:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-27 13:19:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 27 13:19:25.374: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-785  1c222357-9be4-4265-a317-a007d138ff6f 26574 0 2023-05-27 13:19:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-27 13:19:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 27 13:19:25.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-785" for this suite. @ 05/27/23 13:19:25.381
• [10.131 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 05/27/23 13:19:25.392
  May 27 13:19:25.392: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl @ 05/27/23 13:19:25.393
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:19:25.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:19:25.425
  STEP: creating a replication controller @ 05/27/23 13:19:25.432
  May 27 13:19:25.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 create -f -'
  May 27 13:19:25.963: INFO: stderr: ""
  May 27 13:19:25.963: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/27/23 13:19:25.963
  May 27 13:19:25.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 27 13:19:26.053: INFO: stderr: ""
  May 27 13:19:26.053: INFO: stdout: "update-demo-nautilus-2t86q update-demo-nautilus-4lf62 "
  May 27 13:19:26.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods update-demo-nautilus-2t86q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 27 13:19:26.135: INFO: stderr: ""
  May 27 13:19:26.135: INFO: stdout: ""
  May 27 13:19:26.135: INFO: update-demo-nautilus-2t86q is created but not running
  May 27 13:19:31.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 27 13:19:31.218: INFO: stderr: ""
  May 27 13:19:31.218: INFO: stdout: "update-demo-nautilus-2t86q update-demo-nautilus-4lf62 "
  May 27 13:19:31.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods update-demo-nautilus-2t86q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 27 13:19:31.311: INFO: stderr: ""
  May 27 13:19:31.311: INFO: stdout: "true"
  May 27 13:19:31.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods update-demo-nautilus-2t86q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 27 13:19:31.391: INFO: stderr: ""
  May 27 13:19:31.391: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 27 13:19:31.391: INFO: validating pod update-demo-nautilus-2t86q
  May 27 13:19:31.398: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 27 13:19:31.398: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 27 13:19:31.398: INFO: update-demo-nautilus-2t86q is verified up and running
  May 27 13:19:31.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods update-demo-nautilus-4lf62 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 27 13:19:31.480: INFO: stderr: ""
  May 27 13:19:31.480: INFO: stdout: "true"
  May 27 13:19:31.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods update-demo-nautilus-4lf62 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 27 13:19:31.557: INFO: stderr: ""
  May 27 13:19:31.557: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 27 13:19:31.557: INFO: validating pod update-demo-nautilus-4lf62
  May 27 13:19:31.564: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 27 13:19:31.564: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 27 13:19:31.564: INFO: update-demo-nautilus-4lf62 is verified up and running
  STEP: scaling down the replication controller @ 05/27/23 13:19:31.564
  May 27 13:19:31.566: INFO: scanned /root for discovery docs: <nil>
  May 27 13:19:31.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  May 27 13:19:32.671: INFO: stderr: ""
  May 27 13:19:32.671: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/27/23 13:19:32.671
  May 27 13:19:32.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 27 13:19:32.754: INFO: stderr: ""
  May 27 13:19:32.754: INFO: stdout: "update-demo-nautilus-4lf62 "
  May 27 13:19:32.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods update-demo-nautilus-4lf62 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 27 13:19:32.838: INFO: stderr: ""
  May 27 13:19:32.838: INFO: stdout: "true"
  May 27 13:19:32.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods update-demo-nautilus-4lf62 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 27 13:19:32.919: INFO: stderr: ""
  May 27 13:19:32.919: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 27 13:19:32.919: INFO: validating pod update-demo-nautilus-4lf62
  May 27 13:19:32.924: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 27 13:19:32.924: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 27 13:19:32.924: INFO: update-demo-nautilus-4lf62 is verified up and running
  STEP: scaling up the replication controller @ 05/27/23 13:19:32.924
  May 27 13:19:32.926: INFO: scanned /root for discovery docs: <nil>
  May 27 13:19:32.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  May 27 13:19:34.031: INFO: stderr: ""
  May 27 13:19:34.031: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/27/23 13:19:34.031
  May 27 13:19:34.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 27 13:19:34.132: INFO: stderr: ""
  May 27 13:19:34.132: INFO: stdout: "update-demo-nautilus-4lf62 update-demo-nautilus-jr8bk "
  May 27 13:19:34.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods update-demo-nautilus-4lf62 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 27 13:19:34.212: INFO: stderr: ""
  May 27 13:19:34.212: INFO: stdout: "true"
  May 27 13:19:34.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods update-demo-nautilus-4lf62 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 27 13:19:34.296: INFO: stderr: ""
  May 27 13:19:34.296: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 27 13:19:34.296: INFO: validating pod update-demo-nautilus-4lf62
  May 27 13:19:34.303: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 27 13:19:34.303: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 27 13:19:34.303: INFO: update-demo-nautilus-4lf62 is verified up and running
  May 27 13:19:34.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods update-demo-nautilus-jr8bk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 27 13:19:34.385: INFO: stderr: ""
  May 27 13:19:34.385: INFO: stdout: ""
  May 27 13:19:34.385: INFO: update-demo-nautilus-jr8bk is created but not running
  May 27 13:19:39.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 27 13:19:39.471: INFO: stderr: ""
  May 27 13:19:39.471: INFO: stdout: "update-demo-nautilus-4lf62 update-demo-nautilus-jr8bk "
  May 27 13:19:39.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods update-demo-nautilus-4lf62 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 27 13:19:39.552: INFO: stderr: ""
  May 27 13:19:39.552: INFO: stdout: "true"
  May 27 13:19:39.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods update-demo-nautilus-4lf62 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 27 13:19:39.632: INFO: stderr: ""
  May 27 13:19:39.632: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 27 13:19:39.632: INFO: validating pod update-demo-nautilus-4lf62
  May 27 13:19:39.637: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 27 13:19:39.637: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 27 13:19:39.637: INFO: update-demo-nautilus-4lf62 is verified up and running
  May 27 13:19:39.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods update-demo-nautilus-jr8bk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 27 13:19:39.716: INFO: stderr: ""
  May 27 13:19:39.716: INFO: stdout: "true"
  May 27 13:19:39.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods update-demo-nautilus-jr8bk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 27 13:19:39.798: INFO: stderr: ""
  May 27 13:19:39.798: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 27 13:19:39.798: INFO: validating pod update-demo-nautilus-jr8bk
  May 27 13:19:39.806: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 27 13:19:39.806: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 27 13:19:39.806: INFO: update-demo-nautilus-jr8bk is verified up and running
  STEP: using delete to clean up resources @ 05/27/23 13:19:39.806
  May 27 13:19:39.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 delete --grace-period=0 --force -f -'
  May 27 13:19:39.900: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 27 13:19:39.900: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May 27 13:19:39.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get rc,svc -l name=update-demo --no-headers'
  May 27 13:19:40.017: INFO: stderr: "No resources found in kubectl-6134 namespace.\n"
  May 27 13:19:40.017: INFO: stdout: ""
  May 27 13:19:40.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6134 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 27 13:19:40.112: INFO: stderr: ""
  May 27 13:19:40.112: INFO: stdout: ""
  May 27 13:19:40.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6134" for this suite. @ 05/27/23 13:19:40.119
• [14.738 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 05/27/23 13:19:40.13
  May 27 13:19:40.130: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename endpointslice @ 05/27/23 13:19:40.131
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:19:40.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:19:40.165
  May 27 13:19:40.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6489" for this suite. @ 05/27/23 13:19:40.269
• [0.150 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 05/27/23 13:19:40.282
  May 27 13:19:40.282: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename statefulset @ 05/27/23 13:19:40.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:19:40.313
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:19:40.319
  STEP: Creating service test in namespace statefulset-4613 @ 05/27/23 13:19:40.326
  STEP: Creating statefulset ss in namespace statefulset-4613 @ 05/27/23 13:19:40.353
  May 27 13:19:40.389: INFO: Found 0 stateful pods, waiting for 1
  May 27 13:19:50.397: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 05/27/23 13:19:50.407
  STEP: Getting /status @ 05/27/23 13:19:50.423
  May 27 13:19:50.429: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 05/27/23 13:19:50.429
  May 27 13:19:50.443: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 05/27/23 13:19:50.444
  May 27 13:19:50.447: INFO: Observed &StatefulSet event: ADDED
  May 27 13:19:50.447: INFO: Found Statefulset ss in namespace statefulset-4613 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 27 13:19:50.447: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 05/27/23 13:19:50.447
  May 27 13:19:50.447: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 27 13:19:50.459: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 05/27/23 13:19:50.46
  May 27 13:19:50.462: INFO: Observed &StatefulSet event: ADDED
  May 27 13:19:50.462: INFO: Deleting all statefulset in ns statefulset-4613
  May 27 13:19:50.470: INFO: Scaling statefulset ss to 0
  May 27 13:20:00.505: INFO: Waiting for statefulset status.replicas updated to 0
  May 27 13:20:00.511: INFO: Deleting statefulset ss
  May 27 13:20:00.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4613" for this suite. @ 05/27/23 13:20:00.534
• [20.261 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 05/27/23 13:20:00.548
  May 27 13:20:00.548: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename configmap @ 05/27/23 13:20:00.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:20:00.57
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:20:00.576
  STEP: Creating configMap that has name configmap-test-emptyKey-bb3242b6-422b-4f29-a6f9-cb5f5e1ad3c6 @ 05/27/23 13:20:00.582
  May 27 13:20:00.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8382" for this suite. @ 05/27/23 13:20:00.591
• [0.053 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 05/27/23 13:20:00.602
  May 27 13:20:00.602: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/27/23 13:20:00.603
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:20:00.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:20:00.636
  May 27 13:20:00.641: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/27/23 13:20:02.088
  May 27 13:20:02.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-3847 --namespace=crd-publish-openapi-3847 create -f -'
  May 27 13:20:03.005: INFO: stderr: ""
  May 27 13:20:03.005: INFO: stdout: "e2e-test-crd-publish-openapi-6520-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May 27 13:20:03.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-3847 --namespace=crd-publish-openapi-3847 delete e2e-test-crd-publish-openapi-6520-crds test-cr'
  May 27 13:20:03.098: INFO: stderr: ""
  May 27 13:20:03.098: INFO: stdout: "e2e-test-crd-publish-openapi-6520-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  May 27 13:20:03.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-3847 --namespace=crd-publish-openapi-3847 apply -f -'
  May 27 13:20:03.357: INFO: stderr: ""
  May 27 13:20:03.357: INFO: stdout: "e2e-test-crd-publish-openapi-6520-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May 27 13:20:03.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-3847 --namespace=crd-publish-openapi-3847 delete e2e-test-crd-publish-openapi-6520-crds test-cr'
  May 27 13:20:03.470: INFO: stderr: ""
  May 27 13:20:03.470: INFO: stdout: "e2e-test-crd-publish-openapi-6520-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 05/27/23 13:20:03.47
  May 27 13:20:03.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-3847 explain e2e-test-crd-publish-openapi-6520-crds'
  May 27 13:20:03.717: INFO: stderr: ""
  May 27 13:20:03.717: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-6520-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  May 27 13:20:05.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3847" for this suite. @ 05/27/23 13:20:05.162
• [4.569 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 05/27/23 13:20:05.173
  May 27 13:20:05.173: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir @ 05/27/23 13:20:05.174
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:20:05.194
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:20:05.198
  STEP: Creating a pod to test emptydir volume type on node default medium @ 05/27/23 13:20:05.201
  STEP: Saw pod success @ 05/27/23 13:20:09.225
  May 27 13:20:09.228: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-dcf2b194-abd2-47d1-adc6-16d41936fd02 container test-container: <nil>
  STEP: delete the pod @ 05/27/23 13:20:09.254
  May 27 13:20:09.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4010" for this suite. @ 05/27/23 13:20:09.277
• [4.113 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 05/27/23 13:20:09.286
  May 27 13:20:09.286: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename replication-controller @ 05/27/23 13:20:09.287
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:20:09.305
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:20:09.309
  STEP: creating a ReplicationController @ 05/27/23 13:20:09.316
  STEP: waiting for RC to be added @ 05/27/23 13:20:09.324
  STEP: waiting for available Replicas @ 05/27/23 13:20:09.324
  STEP: patching ReplicationController @ 05/27/23 13:20:10.62
  STEP: waiting for RC to be modified @ 05/27/23 13:20:10.63
  STEP: patching ReplicationController status @ 05/27/23 13:20:10.631
  STEP: waiting for RC to be modified @ 05/27/23 13:20:10.638
  STEP: waiting for available Replicas @ 05/27/23 13:20:10.639
  STEP: fetching ReplicationController status @ 05/27/23 13:20:10.646
  STEP: patching ReplicationController scale @ 05/27/23 13:20:10.651
  STEP: waiting for RC to be modified @ 05/27/23 13:20:10.657
  STEP: waiting for ReplicationController's scale to be the max amount @ 05/27/23 13:20:10.658
  STEP: fetching ReplicationController; ensuring that it's patched @ 05/27/23 13:20:12.382
  STEP: updating ReplicationController status @ 05/27/23 13:20:12.386
  STEP: waiting for RC to be modified @ 05/27/23 13:20:12.394
  STEP: listing all ReplicationControllers @ 05/27/23 13:20:12.394
  STEP: checking that ReplicationController has expected values @ 05/27/23 13:20:12.401
  STEP: deleting ReplicationControllers by collection @ 05/27/23 13:20:12.401
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 05/27/23 13:20:12.411
  May 27 13:20:12.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0527 13:20:12.477854      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-2151" for this suite. @ 05/27/23 13:20:12.483
• [3.204 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 05/27/23 13:20:12.491
  May 27 13:20:12.491: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename field-validation @ 05/27/23 13:20:12.492
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:20:12.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:20:12.519
  May 27 13:20:12.523: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  E0527 13:20:13.478558      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:14.479458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:20:15.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4078" for this suite. @ 05/27/23 13:20:15.135
• [2.651 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 05/27/23 13:20:15.142
  May 27 13:20:15.142: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/27/23 13:20:15.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:20:15.163
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:20:15.167
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 05/27/23 13:20:15.17
  May 27 13:20:15.171: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  E0527 13:20:15.479547      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:16.480585      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:20:16.720: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  E0527 13:20:17.481259      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:18.481341      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:19.481886      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:20.482008      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:21.482997      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:22.484131      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:20:22.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6877" for this suite. @ 05/27/23 13:20:22.925
• [7.792 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 05/27/23 13:20:22.936
  May 27 13:20:22.936: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename dns @ 05/27/23 13:20:22.937
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:20:22.96
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:20:22.972
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8694.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8694.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 05/27/23 13:20:22.977
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8694.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8694.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 05/27/23 13:20:22.977
  STEP: creating a pod to probe /etc/hosts @ 05/27/23 13:20:22.977
  STEP: submitting the pod to kubernetes @ 05/27/23 13:20:22.977
  E0527 13:20:23.484894      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:24.484924      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/27/23 13:20:25.005
  STEP: looking for the results for each expected name from probers @ 05/27/23 13:20:25.009
  May 27 13:20:25.030: INFO: DNS probes using dns-8694/dns-test-4901126a-1eb9-4da4-89ec-f32287949cc3 succeeded

  May 27 13:20:25.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/27/23 13:20:25.035
  STEP: Destroying namespace "dns-8694" for this suite. @ 05/27/23 13:20:25.054
• [2.127 seconds]
------------------------------
S
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 05/27/23 13:20:25.063
  May 27 13:20:25.064: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename services @ 05/27/23 13:20:25.065
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:20:25.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:20:25.088
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-5974 @ 05/27/23 13:20:25.094
  STEP: changing the ExternalName service to type=NodePort @ 05/27/23 13:20:25.102
  STEP: creating replication controller externalname-service in namespace services-5974 @ 05/27/23 13:20:25.12
  I0527 13:20:25.133823      18 runners.go:194] Created replication controller with name: externalname-service, namespace: services-5974, replica count: 2
  E0527 13:20:25.485883      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:26.486943      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:27.487870      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0527 13:20:28.184067      18 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 27 13:20:28.184: INFO: Creating new exec pod
  E0527 13:20:28.488408      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:29.488520      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:30.488748      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:20:31.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-5974 exec execpodtnwkn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 27 13:20:31.379: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 27 13:20:31.379: INFO: stdout: ""
  E0527 13:20:31.489573      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:20:32.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-5974 exec execpodtnwkn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  E0527 13:20:32.489820      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:20:32.546: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 27 13:20:32.546: INFO: stdout: ""
  May 27 13:20:33.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-5974 exec execpodtnwkn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  E0527 13:20:33.490942      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:20:33.533: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 27 13:20:33.533: INFO: stdout: "externalname-service-5stk8"
  May 27 13:20:33.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-5974 exec execpodtnwkn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.46 80'
  May 27 13:20:33.692: INFO: stderr: "+ nc -v -t -w 2 10.152.183.46 80\nConnection to 10.152.183.46 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  May 27 13:20:33.692: INFO: stdout: ""
  E0527 13:20:34.491433      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:20:34.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-5974 exec execpodtnwkn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.46 80'
  May 27 13:20:34.873: INFO: stderr: "+ nc -v -t -w 2 10.152.183.46 80\n+ echo hostName\nConnection to 10.152.183.46 80 port [tcp/http] succeeded!\n"
  May 27 13:20:34.873: INFO: stdout: ""
  E0527 13:20:35.491675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:20:35.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-5974 exec execpodtnwkn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.46 80'
  May 27 13:20:35.851: INFO: stderr: "+ nc -v -t -w 2 10.152.183.46 80\nConnection to 10.152.183.46 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  May 27 13:20:35.851: INFO: stdout: "externalname-service-5stk8"
  May 27 13:20:35.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-5974 exec execpodtnwkn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.68.172 30698'
  May 27 13:20:36.023: INFO: stderr: "+ nc -v -t -w 2 172.31.68.172 30698\n+ echo hostName\nConnection to 172.31.68.172 30698 port [tcp/*] succeeded!\n"
  May 27 13:20:36.024: INFO: stdout: "externalname-service-zt48w"
  May 27 13:20:36.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-5974 exec execpodtnwkn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.22.3 30698'
  May 27 13:20:36.193: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.22.3 30698\nConnection to 172.31.22.3 30698 port [tcp/*] succeeded!\n"
  May 27 13:20:36.193: INFO: stdout: "externalname-service-zt48w"
  May 27 13:20:36.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 27 13:20:36.199: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-5974" for this suite. @ 05/27/23 13:20:36.24
• [11.187 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 05/27/23 13:20:36.251
  May 27 13:20:36.251: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename crd-webhook @ 05/27/23 13:20:36.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:20:36.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:20:36.283
  STEP: Setting up server cert @ 05/27/23 13:20:36.288
  E0527 13:20:36.492524      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/27/23 13:20:37.028
  STEP: Deploying the custom resource conversion webhook pod @ 05/27/23 13:20:37.041
  STEP: Wait for the deployment to be ready @ 05/27/23 13:20:37.059
  May 27 13:20:37.071: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0527 13:20:37.493447      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:38.493680      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/27/23 13:20:39.086
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 13:20:39.097
  E0527 13:20:39.493983      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:20:40.097: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May 27 13:20:40.101: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  E0527 13:20:40.494600      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:41.495272      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:42.495345      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 05/27/23 13:20:42.709
  STEP: Create a v2 custom resource @ 05/27/23 13:20:42.742
  STEP: List CRs in v1 @ 05/27/23 13:20:42.8
  STEP: List CRs in v2 @ 05/27/23 13:20:42.808
  May 27 13:20:42.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-2613" for this suite. @ 05/27/23 13:20:43.425
• [7.187 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 05/27/23 13:20:43.439
  May 27 13:20:43.439: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/27/23 13:20:43.44
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:20:43.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:20:43.474
  STEP: creating @ 05/27/23 13:20:43.48
  E0527 13:20:43.495756      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting @ 05/27/23 13:20:43.511
  STEP: listing @ 05/27/23 13:20:43.524
  STEP: deleting @ 05/27/23 13:20:43.529
  May 27 13:20:43.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-7537" for this suite. @ 05/27/23 13:20:43.561
• [0.132 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 05/27/23 13:20:43.572
  May 27 13:20:43.572: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename services @ 05/27/23 13:20:43.573
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:20:43.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:20:43.611
  STEP: creating service in namespace services-707 @ 05/27/23 13:20:43.618
  STEP: creating service affinity-clusterip in namespace services-707 @ 05/27/23 13:20:43.618
  STEP: creating replication controller affinity-clusterip in namespace services-707 @ 05/27/23 13:20:43.648
  I0527 13:20:43.661394      18 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-707, replica count: 3
  E0527 13:20:44.495910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:45.496467      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:46.496590      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0527 13:20:46.712305      18 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 27 13:20:46.722: INFO: Creating new exec pod
  E0527 13:20:47.497468      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:48.497767      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:49.498802      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:20:49.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-707 exec execpod-affinityfrg8r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  May 27 13:20:49.963: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\n+ echo hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  May 27 13:20:49.963: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 13:20:49.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-707 exec execpod-affinityfrg8r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.208 80'
  May 27 13:20:50.173: INFO: stderr: "+ nc -v -t -w 2 10.152.183.208 80\n+ echo hostName\nConnection to 10.152.183.208 80 port [tcp/http] succeeded!\n"
  May 27 13:20:50.173: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 13:20:50.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-707 exec execpod-affinityfrg8r -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.208:80/ ; done'
  May 27 13:20:50.462: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.208:80/\n"
  May 27 13:20:50.462: INFO: stdout: "\naffinity-clusterip-5rhvf\naffinity-clusterip-5rhvf\naffinity-clusterip-5rhvf\naffinity-clusterip-5rhvf\naffinity-clusterip-5rhvf\naffinity-clusterip-5rhvf\naffinity-clusterip-5rhvf\naffinity-clusterip-5rhvf\naffinity-clusterip-5rhvf\naffinity-clusterip-5rhvf\naffinity-clusterip-5rhvf\naffinity-clusterip-5rhvf\naffinity-clusterip-5rhvf\naffinity-clusterip-5rhvf\naffinity-clusterip-5rhvf\naffinity-clusterip-5rhvf"
  May 27 13:20:50.462: INFO: Received response from host: affinity-clusterip-5rhvf
  May 27 13:20:50.462: INFO: Received response from host: affinity-clusterip-5rhvf
  May 27 13:20:50.462: INFO: Received response from host: affinity-clusterip-5rhvf
  May 27 13:20:50.463: INFO: Received response from host: affinity-clusterip-5rhvf
  May 27 13:20:50.463: INFO: Received response from host: affinity-clusterip-5rhvf
  May 27 13:20:50.463: INFO: Received response from host: affinity-clusterip-5rhvf
  May 27 13:20:50.463: INFO: Received response from host: affinity-clusterip-5rhvf
  May 27 13:20:50.463: INFO: Received response from host: affinity-clusterip-5rhvf
  May 27 13:20:50.463: INFO: Received response from host: affinity-clusterip-5rhvf
  May 27 13:20:50.463: INFO: Received response from host: affinity-clusterip-5rhvf
  May 27 13:20:50.463: INFO: Received response from host: affinity-clusterip-5rhvf
  May 27 13:20:50.463: INFO: Received response from host: affinity-clusterip-5rhvf
  May 27 13:20:50.463: INFO: Received response from host: affinity-clusterip-5rhvf
  May 27 13:20:50.463: INFO: Received response from host: affinity-clusterip-5rhvf
  May 27 13:20:50.463: INFO: Received response from host: affinity-clusterip-5rhvf
  May 27 13:20:50.463: INFO: Received response from host: affinity-clusterip-5rhvf
  May 27 13:20:50.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 27 13:20:50.470: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-707, will wait for the garbage collector to delete the pods @ 05/27/23 13:20:50.487
  E0527 13:20:50.499153      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:20:50.556: INFO: Deleting ReplicationController affinity-clusterip took: 9.703611ms
  May 27 13:20:50.657: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.936558ms
  E0527 13:20:51.499533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:52.500360      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-707" for this suite. @ 05/27/23 13:20:52.885
• [9.323 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 05/27/23 13:20:52.9
  May 27 13:20:52.900: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl @ 05/27/23 13:20:52.901
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:20:52.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:20:52.927
  STEP: creating Agnhost RC @ 05/27/23 13:20:52.933
  May 27 13:20:52.933: INFO: namespace kubectl-329
  May 27 13:20:52.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-329 create -f -'
  E0527 13:20:53.501118      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:20:53.892: INFO: stderr: ""
  May 27 13:20:53.892: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/27/23 13:20:53.893
  E0527 13:20:54.501464      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:20:54.899: INFO: Selector matched 1 pods for map[app:agnhost]
  May 27 13:20:54.899: INFO: Found 1 / 1
  May 27 13:20:54.899: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May 27 13:20:54.902: INFO: Selector matched 1 pods for map[app:agnhost]
  May 27 13:20:54.902: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 27 13:20:54.902: INFO: wait on agnhost-primary startup in kubectl-329 
  May 27 13:20:54.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-329 logs agnhost-primary-wjrpc agnhost-primary'
  May 27 13:20:54.993: INFO: stderr: ""
  May 27 13:20:54.993: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 05/27/23 13:20:54.993
  May 27 13:20:54.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-329 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  May 27 13:20:55.112: INFO: stderr: ""
  May 27 13:20:55.112: INFO: stdout: "service/rm2 exposed\n"
  May 27 13:20:55.117: INFO: Service rm2 in namespace kubectl-329 found.
  E0527 13:20:55.501660      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:56.502003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 05/27/23 13:20:57.126
  May 27 13:20:57.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-329 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  May 27 13:20:57.220: INFO: stderr: ""
  May 27 13:20:57.220: INFO: stdout: "service/rm3 exposed\n"
  May 27 13:20:57.227: INFO: Service rm3 in namespace kubectl-329 found.
  E0527 13:20:57.502035      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:20:58.502150      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:20:59.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-329" for this suite. @ 05/27/23 13:20:59.241
• [6.354 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 05/27/23 13:20:59.254
  May 27 13:20:59.254: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 13:20:59.255
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:20:59.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:20:59.279
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 13:20:59.284
  E0527 13:20:59.502499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:00.503044      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:01.503271      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:02.503771      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:21:03.316
  May 27 13:21:03.321: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-aeb3bb48-04bd-4296-80ab-782151d6e52c container client-container: <nil>
  STEP: delete the pod @ 05/27/23 13:21:03.341
  May 27 13:21:03.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3813" for this suite. @ 05/27/23 13:21:03.365
• [4.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 05/27/23 13:21:03.381
  May 27 13:21:03.381: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename replication-controller @ 05/27/23 13:21:03.382
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:21:03.402
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:21:03.408
  STEP: Given a ReplicationController is created @ 05/27/23 13:21:03.412
  STEP: When the matched label of one of its pods change @ 05/27/23 13:21:03.42
  May 27 13:21:03.427: INFO: Pod name pod-release: Found 0 pods out of 1
  E0527 13:21:03.504625      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:04.504765      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:05.504968      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:06.505232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:07.505799      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:08.431: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/27/23 13:21:08.448
  E0527 13:21:08.506553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:09.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3216" for this suite. @ 05/27/23 13:21:09.471
• [6.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 05/27/23 13:21:09.483
  May 27 13:21:09.484: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl @ 05/27/23 13:21:09.484
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:21:09.505
  E0527 13:21:09.506639      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:21:09.511
  STEP: creating the pod @ 05/27/23 13:21:09.516
  May 27 13:21:09.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-5715 create -f -'
  May 27 13:21:09.855: INFO: stderr: ""
  May 27 13:21:09.855: INFO: stdout: "pod/pause created\n"
  E0527 13:21:10.507308      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:11.507328      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 05/27/23 13:21:11.866
  May 27 13:21:11.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-5715 label pods pause testing-label=testing-label-value'
  May 27 13:21:11.966: INFO: stderr: ""
  May 27 13:21:11.966: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 05/27/23 13:21:11.966
  May 27 13:21:11.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-5715 get pod pause -L testing-label'
  May 27 13:21:12.050: INFO: stderr: ""
  May 27 13:21:12.050: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 05/27/23 13:21:12.05
  May 27 13:21:12.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-5715 label pods pause testing-label-'
  May 27 13:21:12.146: INFO: stderr: ""
  May 27 13:21:12.146: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 05/27/23 13:21:12.146
  May 27 13:21:12.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-5715 get pod pause -L testing-label'
  May 27 13:21:12.233: INFO: stderr: ""
  May 27 13:21:12.233: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 05/27/23 13:21:12.233
  May 27 13:21:12.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-5715 delete --grace-period=0 --force -f -'
  May 27 13:21:12.327: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 27 13:21:12.327: INFO: stdout: "pod \"pause\" force deleted\n"
  May 27 13:21:12.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-5715 get rc,svc -l name=pause --no-headers'
  May 27 13:21:12.418: INFO: stderr: "No resources found in kubectl-5715 namespace.\n"
  May 27 13:21:12.418: INFO: stdout: ""
  May 27 13:21:12.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-5715 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 27 13:21:12.498: INFO: stderr: ""
  May 27 13:21:12.498: INFO: stdout: ""
  May 27 13:21:12.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5715" for this suite. @ 05/27/23 13:21:12.504
  E0527 13:21:12.508322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
• [3.033 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 05/27/23 13:21:12.518
  May 27 13:21:12.518: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 13:21:12.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:21:12.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:21:12.543
  STEP: Creating the pod @ 05/27/23 13:21:12.549
  E0527 13:21:13.508498      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:14.509009      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:15.115: INFO: Successfully updated pod "labelsupdate8e2e9291-c76d-41fe-9cb7-522c9a7b1c8a"
  E0527 13:21:15.509934      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:16.510044      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:17.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2765" for this suite. @ 05/27/23 13:21:17.152
• [4.645 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 05/27/23 13:21:17.168
  May 27 13:21:17.168: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir @ 05/27/23 13:21:17.169
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:21:17.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:21:17.195
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/27/23 13:21:17.2
  E0527 13:21:17.510623      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:18.510705      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:19.510938      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:20.511015      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:21:21.225
  May 27 13:21:21.231: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-4bcf9d21-b3ae-4f45-b7fc-7edc99383040 container test-container: <nil>
  STEP: delete the pod @ 05/27/23 13:21:21.239
  May 27 13:21:21.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7789" for this suite. @ 05/27/23 13:21:21.264
• [4.105 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 05/27/23 13:21:21.273
  May 27 13:21:21.273: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename sched-pred @ 05/27/23 13:21:21.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:21:21.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:21:21.299
  May 27 13:21:21.305: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 27 13:21:21.318: INFO: Waiting for terminating namespaces to be deleted...
  May 27 13:21:21.323: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-10-136 before test
  May 27 13:21:21.331: INFO: default-http-backend-kubernetes-worker-65fc475d49-8hlsq from ingress-nginx-kubernetes-worker started at 2023-05-27 12:22:35 +0000 UTC (1 container statuses recorded)
  May 27 13:21:21.331: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  May 27 13:21:21.331: INFO: nginx-ingress-controller-kubernetes-worker-jkk42 from ingress-nginx-kubernetes-worker started at 2023-05-27 12:11:29 +0000 UTC (1 container statuses recorded)
  May 27 13:21:21.331: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  May 27 13:21:21.331: INFO: calico-kube-controllers-79678b7759-2xtvf from kube-system started at 2023-05-27 12:22:35 +0000 UTC (1 container statuses recorded)
  May 27 13:21:21.331: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 27 13:21:21.331: INFO: sonobuoy from sonobuoy started at 2023-05-27 12:15:40 +0000 UTC (1 container statuses recorded)
  May 27 13:21:21.331: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 27 13:21:21.331: INFO: sonobuoy-e2e-job-0cf94b30a28f4573 from sonobuoy started at 2023-05-27 12:15:43 +0000 UTC (2 container statuses recorded)
  May 27 13:21:21.331: INFO: 	Container e2e ready: true, restart count 0
  May 27 13:21:21.331: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 27 13:21:21.331: INFO: sonobuoy-systemd-logs-daemon-set-f9890aca51654f74-zr2qt from sonobuoy started at 2023-05-27 12:15:43 +0000 UTC (2 container statuses recorded)
  May 27 13:21:21.331: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 27 13:21:21.331: INFO: 	Container systemd-logs ready: true, restart count 0
  May 27 13:21:21.331: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-22-3 before test
  May 27 13:21:21.337: INFO: nginx-ingress-controller-kubernetes-worker-2hlwz from ingress-nginx-kubernetes-worker started at 2023-05-27 11:59:07 +0000 UTC (1 container statuses recorded)
  May 27 13:21:21.337: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  May 27 13:21:21.338: INFO: coredns-5c7f76ccb8-b4zh4 from kube-system started at 2023-05-27 11:59:00 +0000 UTC (1 container statuses recorded)
  May 27 13:21:21.338: INFO: 	Container coredns ready: true, restart count 0
  May 27 13:21:21.338: INFO: kube-state-metrics-5b95b4459c-8rvst from kube-system started at 2023-05-27 11:59:00 +0000 UTC (1 container statuses recorded)
  May 27 13:21:21.338: INFO: 	Container kube-state-metrics ready: true, restart count 0
  May 27 13:21:21.338: INFO: metrics-server-v0.5.2-6cf8c8b69c-t499d from kube-system started at 2023-05-27 11:59:00 +0000 UTC (2 container statuses recorded)
  May 27 13:21:21.338: INFO: 	Container metrics-server ready: true, restart count 0
  May 27 13:21:21.338: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  May 27 13:21:21.338: INFO: dashboard-metrics-scraper-6b8586b5c9-7kfbs from kubernetes-dashboard started at 2023-05-27 11:59:00 +0000 UTC (1 container statuses recorded)
  May 27 13:21:21.338: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  May 27 13:21:21.338: INFO: kubernetes-dashboard-6869f4cd5f-js8mr from kubernetes-dashboard started at 2023-05-27 11:59:00 +0000 UTC (1 container statuses recorded)
  May 27 13:21:21.338: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  May 27 13:21:21.338: INFO: sonobuoy-systemd-logs-daemon-set-f9890aca51654f74-c4nxx from sonobuoy started at 2023-05-27 12:15:43 +0000 UTC (2 container statuses recorded)
  May 27 13:21:21.338: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 27 13:21:21.338: INFO: 	Container systemd-logs ready: true, restart count 0
  May 27 13:21:21.338: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-68-172 before test
  May 27 13:21:21.346: INFO: nginx-ingress-controller-kubernetes-worker-8g7rf from ingress-nginx-kubernetes-worker started at 2023-05-27 13:07:09 +0000 UTC (1 container statuses recorded)
  May 27 13:21:21.346: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  May 27 13:21:21.346: INFO: labelsupdate8e2e9291-c76d-41fe-9cb7-522c9a7b1c8a from projected-2765 started at 2023-05-27 13:21:12 +0000 UTC (1 container statuses recorded)
  May 27 13:21:21.346: INFO: 	Container client-container ready: true, restart count 0
  May 27 13:21:21.346: INFO: sonobuoy-systemd-logs-daemon-set-f9890aca51654f74-mmhtp from sonobuoy started at 2023-05-27 12:15:43 +0000 UTC (2 container statuses recorded)
  May 27 13:21:21.346: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 27 13:21:21.346: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ip-172-31-10-136 @ 05/27/23 13:21:21.366
  STEP: verifying the node has the label node ip-172-31-22-3 @ 05/27/23 13:21:21.393
  STEP: verifying the node has the label node ip-172-31-68-172 @ 05/27/23 13:21:21.42
  May 27 13:21:21.440: INFO: Pod default-http-backend-kubernetes-worker-65fc475d49-8hlsq requesting resource cpu=10m on Node ip-172-31-10-136
  May 27 13:21:21.440: INFO: Pod nginx-ingress-controller-kubernetes-worker-2hlwz requesting resource cpu=0m on Node ip-172-31-22-3
  May 27 13:21:21.440: INFO: Pod nginx-ingress-controller-kubernetes-worker-8g7rf requesting resource cpu=0m on Node ip-172-31-68-172
  May 27 13:21:21.440: INFO: Pod nginx-ingress-controller-kubernetes-worker-jkk42 requesting resource cpu=0m on Node ip-172-31-10-136
  May 27 13:21:21.440: INFO: Pod calico-kube-controllers-79678b7759-2xtvf requesting resource cpu=0m on Node ip-172-31-10-136
  May 27 13:21:21.440: INFO: Pod coredns-5c7f76ccb8-b4zh4 requesting resource cpu=100m on Node ip-172-31-22-3
  May 27 13:21:21.440: INFO: Pod kube-state-metrics-5b95b4459c-8rvst requesting resource cpu=0m on Node ip-172-31-22-3
  May 27 13:21:21.440: INFO: Pod metrics-server-v0.5.2-6cf8c8b69c-t499d requesting resource cpu=5m on Node ip-172-31-22-3
  May 27 13:21:21.440: INFO: Pod dashboard-metrics-scraper-6b8586b5c9-7kfbs requesting resource cpu=0m on Node ip-172-31-22-3
  May 27 13:21:21.440: INFO: Pod kubernetes-dashboard-6869f4cd5f-js8mr requesting resource cpu=0m on Node ip-172-31-22-3
  May 27 13:21:21.440: INFO: Pod labelsupdate8e2e9291-c76d-41fe-9cb7-522c9a7b1c8a requesting resource cpu=0m on Node ip-172-31-68-172
  May 27 13:21:21.440: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-10-136
  May 27 13:21:21.440: INFO: Pod sonobuoy-e2e-job-0cf94b30a28f4573 requesting resource cpu=0m on Node ip-172-31-10-136
  May 27 13:21:21.440: INFO: Pod sonobuoy-systemd-logs-daemon-set-f9890aca51654f74-c4nxx requesting resource cpu=0m on Node ip-172-31-22-3
  May 27 13:21:21.440: INFO: Pod sonobuoy-systemd-logs-daemon-set-f9890aca51654f74-mmhtp requesting resource cpu=0m on Node ip-172-31-68-172
  May 27 13:21:21.440: INFO: Pod sonobuoy-systemd-logs-daemon-set-f9890aca51654f74-zr2qt requesting resource cpu=0m on Node ip-172-31-10-136
  STEP: Starting Pods to consume most of the cluster CPU. @ 05/27/23 13:21:21.44
  May 27 13:21:21.440: INFO: Creating a pod which consumes cpu=1393m on Node ip-172-31-10-136
  May 27 13:21:21.456: INFO: Creating a pod which consumes cpu=1326m on Node ip-172-31-22-3
  May 27 13:21:21.466: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-68-172
  E0527 13:21:21.512119      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:22.512646      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 05/27/23 13:21:23.502
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-81f2b4cd-d4ea-4ef2-9942-583ac32cf3f4.176302b98943bed0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1986/filler-pod-81f2b4cd-d4ea-4ef2-9942-583ac32cf3f4 to ip-172-31-10-136] @ 05/27/23 13:21:23.508
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-81f2b4cd-d4ea-4ef2-9942-583ac32cf3f4.176302b9b426e432], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/27/23 13:21:23.508
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-81f2b4cd-d4ea-4ef2-9942-583ac32cf3f4.176302b9b568044f], Reason = [Created], Message = [Created container filler-pod-81f2b4cd-d4ea-4ef2-9942-583ac32cf3f4] @ 05/27/23 13:21:23.508
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-81f2b4cd-d4ea-4ef2-9942-583ac32cf3f4.176302b9b9f240ac], Reason = [Started], Message = [Started container filler-pod-81f2b4cd-d4ea-4ef2-9942-583ac32cf3f4] @ 05/27/23 13:21:23.508
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a53ab538-89ee-4a95-9e67-8ea8f1c6c649.176302b98a07157f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1986/filler-pod-a53ab538-89ee-4a95-9e67-8ea8f1c6c649 to ip-172-31-22-3] @ 05/27/23 13:21:23.508
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a53ab538-89ee-4a95-9e67-8ea8f1c6c649.176302b9b4afdd54], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/27/23 13:21:23.509
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a53ab538-89ee-4a95-9e67-8ea8f1c6c649.176302b9b5cdf7b2], Reason = [Created], Message = [Created container filler-pod-a53ab538-89ee-4a95-9e67-8ea8f1c6c649] @ 05/27/23 13:21:23.509
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a53ab538-89ee-4a95-9e67-8ea8f1c6c649.176302b9b9ed71ca], Reason = [Started], Message = [Started container filler-pod-a53ab538-89ee-4a95-9e67-8ea8f1c6c649] @ 05/27/23 13:21:23.509
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-babcbe78-91a1-4ecf-a8c1-3c18720e2d69.176302b98ae96c64], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1986/filler-pod-babcbe78-91a1-4ecf-a8c1-3c18720e2d69 to ip-172-31-68-172] @ 05/27/23 13:21:23.509
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-babcbe78-91a1-4ecf-a8c1-3c18720e2d69.176302b9c9a5caaf], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/27/23 13:21:23.509
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-babcbe78-91a1-4ecf-a8c1-3c18720e2d69.176302b9ccfa4af4], Reason = [Created], Message = [Created container filler-pod-babcbe78-91a1-4ecf-a8c1-3c18720e2d69] @ 05/27/23 13:21:23.509
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-babcbe78-91a1-4ecf-a8c1-3c18720e2d69.176302b9d24b6ffd], Reason = [Started], Message = [Started container filler-pod-babcbe78-91a1-4ecf-a8c1-3c18720e2d69] @ 05/27/23 13:21:23.509
  E0527 13:21:23.513491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.176302ba03c47020], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/5 nodes are available: 2 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] @ 05/27/23 13:21:23.524
  E0527 13:21:24.513632      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node ip-172-31-10-136 @ 05/27/23 13:21:24.522
  STEP: verifying the node doesn't have the label node @ 05/27/23 13:21:24.538
  STEP: removing the label node off the node ip-172-31-22-3 @ 05/27/23 13:21:24.543
  STEP: verifying the node doesn't have the label node @ 05/27/23 13:21:24.561
  STEP: removing the label node off the node ip-172-31-68-172 @ 05/27/23 13:21:24.567
  STEP: verifying the node doesn't have the label node @ 05/27/23 13:21:24.583
  May 27 13:21:24.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1986" for this suite. @ 05/27/23 13:21:24.601
• [3.337 seconds]
------------------------------
S
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 05/27/23 13:21:24.611
  May 27 13:21:24.611: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename services @ 05/27/23 13:21:24.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:21:24.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:21:24.635
  STEP: creating service multi-endpoint-test in namespace services-8145 @ 05/27/23 13:21:24.64
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8145 to expose endpoints map[] @ 05/27/23 13:21:24.654
  May 27 13:21:24.663: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  E0527 13:21:25.513702      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:25.673: INFO: successfully validated that service multi-endpoint-test in namespace services-8145 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-8145 @ 05/27/23 13:21:25.673
  E0527 13:21:26.514871      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:27.515846      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8145 to expose endpoints map[pod1:[100]] @ 05/27/23 13:21:27.702
  May 27 13:21:27.720: INFO: successfully validated that service multi-endpoint-test in namespace services-8145 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-8145 @ 05/27/23 13:21:27.72
  E0527 13:21:28.517459      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:29.517448      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8145 to expose endpoints map[pod1:[100] pod2:[101]] @ 05/27/23 13:21:29.759
  May 27 13:21:29.782: INFO: successfully validated that service multi-endpoint-test in namespace services-8145 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 05/27/23 13:21:29.782
  May 27 13:21:29.782: INFO: Creating new exec pod
  E0527 13:21:30.517721      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:31.518443      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:32.518470      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:32.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-8145 exec execpodb2zgc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  May 27 13:21:32.976: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  May 27 13:21:32.976: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 13:21:32.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-8145 exec execpodb2zgc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.104 80'
  May 27 13:21:33.157: INFO: stderr: "+ nc -v -t -w 2 10.152.183.104 80\nConnection to 10.152.183.104 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  May 27 13:21:33.157: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 13:21:33.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-8145 exec execpodb2zgc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  May 27 13:21:33.318: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  May 27 13:21:33.318: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 13:21:33.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-8145 exec execpodb2zgc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.104 81'
  May 27 13:21:33.476: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 10.152.183.104 81\nConnection to 10.152.183.104 81 port [tcp/*] succeeded!\n"
  May 27 13:21:33.476: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-8145 @ 05/27/23 13:21:33.476
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8145 to expose endpoints map[pod2:[101]] @ 05/27/23 13:21:33.502
  E0527 13:21:33.518970      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:33.523: INFO: successfully validated that service multi-endpoint-test in namespace services-8145 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-8145 @ 05/27/23 13:21:33.523
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8145 to expose endpoints map[] @ 05/27/23 13:21:33.553
  May 27 13:21:33.565: INFO: successfully validated that service multi-endpoint-test in namespace services-8145 exposes endpoints map[]
  May 27 13:21:33.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8145" for this suite. @ 05/27/23 13:21:33.595
• [8.993 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 05/27/23 13:21:33.608
  May 27 13:21:33.608: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename secrets @ 05/27/23 13:21:33.609
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:21:33.629
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:21:33.634
  STEP: Creating secret with name secret-test-0fb3a714-56cb-4899-9080-28f96947c0d9 @ 05/27/23 13:21:33.639
  STEP: Creating a pod to test consume secrets @ 05/27/23 13:21:33.649
  E0527 13:21:34.519290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:35.519435      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:36.519826      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:37.520463      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:21:37.682
  May 27 13:21:37.686: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-secrets-3948ee16-9bab-4a97-8ec4-a80d35514e5e container secret-volume-test: <nil>
  STEP: delete the pod @ 05/27/23 13:21:37.695
  May 27 13:21:37.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3393" for this suite. @ 05/27/23 13:21:37.723
• [4.126 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 05/27/23 13:21:37.735
  May 27 13:21:37.735: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename limitrange @ 05/27/23 13:21:37.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:21:37.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:21:37.761
  STEP: Creating a LimitRange @ 05/27/23 13:21:37.766
  STEP: Setting up watch @ 05/27/23 13:21:37.766
  STEP: Submitting a LimitRange @ 05/27/23 13:21:37.871
  STEP: Verifying LimitRange creation was observed @ 05/27/23 13:21:37.877
  STEP: Fetching the LimitRange to ensure it has proper values @ 05/27/23 13:21:37.877
  May 27 13:21:37.883: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May 27 13:21:37.883: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 05/27/23 13:21:37.883
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 05/27/23 13:21:37.892
  May 27 13:21:37.901: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May 27 13:21:37.901: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 05/27/23 13:21:37.901
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 05/27/23 13:21:37.908
  May 27 13:21:37.915: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  May 27 13:21:37.916: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 05/27/23 13:21:37.917
  STEP: Failing to create a Pod with more than max resources @ 05/27/23 13:21:37.922
  STEP: Updating a LimitRange @ 05/27/23 13:21:37.926
  STEP: Verifying LimitRange updating is effective @ 05/27/23 13:21:37.933
  E0527 13:21:38.521451      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:39.521567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 05/27/23 13:21:39.94
  STEP: Failing to create a Pod with more than max resources @ 05/27/23 13:21:39.948
  STEP: Deleting a LimitRange @ 05/27/23 13:21:39.952
  STEP: Verifying the LimitRange was deleted @ 05/27/23 13:21:39.961
  E0527 13:21:40.521698      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:41.521823      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:42.522945      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:43.522990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:44.523162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:44.967: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 05/27/23 13:21:44.967
  May 27 13:21:44.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-5769" for this suite. @ 05/27/23 13:21:44.988
• [7.263 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 05/27/23 13:21:45
  May 27 13:21:45.000: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir @ 05/27/23 13:21:45
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:21:45.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:21:45.028
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/27/23 13:21:45.033
  E0527 13:21:45.523272      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:46.524006      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:47.524827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:21:48.525204      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:21:49.062
  May 27 13:21:49.066: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-287b8916-7672-4f09-be1e-3266af06b4cf container test-container: <nil>
  STEP: delete the pod @ 05/27/23 13:21:49.074
  May 27 13:21:49.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3829" for this suite. @ 05/27/23 13:21:49.101
• [4.111 seconds]
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 05/27/23 13:21:49.111
  May 27 13:21:49.111: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename svc-latency @ 05/27/23 13:21:49.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:21:49.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:21:49.139
  May 27 13:21:49.144: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-3969 @ 05/27/23 13:21:49.145
  I0527 13:21:49.152661      18 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3969, replica count: 1
  E0527 13:21:49.525620      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0527 13:21:50.204016      18 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 27 13:21:50.318: INFO: Created: latency-svc-jz5nd
  May 27 13:21:50.335: INFO: Got endpoints: latency-svc-jz5nd [30.971154ms]
  May 27 13:21:50.362: INFO: Created: latency-svc-r6jtc
  May 27 13:21:50.371: INFO: Got endpoints: latency-svc-r6jtc [35.426677ms]
  May 27 13:21:50.376: INFO: Created: latency-svc-mh82g
  May 27 13:21:50.381: INFO: Created: latency-svc-89w7b
  May 27 13:21:50.387: INFO: Got endpoints: latency-svc-mh82g [51.177705ms]
  May 27 13:21:50.399: INFO: Got endpoints: latency-svc-89w7b [62.374898ms]
  May 27 13:21:50.405: INFO: Created: latency-svc-k2p6x
  May 27 13:21:50.410: INFO: Got endpoints: latency-svc-k2p6x [73.970989ms]
  May 27 13:21:50.419: INFO: Created: latency-svc-m57z2
  May 27 13:21:50.424: INFO: Created: latency-svc-mwhtm
  May 27 13:21:50.426: INFO: Got endpoints: latency-svc-m57z2 [89.39477ms]
  May 27 13:21:50.436: INFO: Got endpoints: latency-svc-mwhtm [99.302787ms]
  May 27 13:21:50.445: INFO: Created: latency-svc-lw8dk
  May 27 13:21:50.452: INFO: Got endpoints: latency-svc-lw8dk [115.576136ms]
  May 27 13:21:50.458: INFO: Created: latency-svc-zs4qv
  May 27 13:21:50.463: INFO: Created: latency-svc-mdz7z
  May 27 13:21:50.466: INFO: Got endpoints: latency-svc-zs4qv [129.193559ms]
  May 27 13:21:50.479: INFO: Got endpoints: latency-svc-mdz7z [142.18585ms]
  May 27 13:21:50.480: INFO: Created: latency-svc-rpdth
  May 27 13:21:50.493: INFO: Got endpoints: latency-svc-rpdth [155.795933ms]
  E0527 13:21:50.526448      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:50.574: INFO: Created: latency-svc-jkr2s
  May 27 13:21:50.580: INFO: Created: latency-svc-5b4tv
  May 27 13:21:50.582: INFO: Created: latency-svc-dbwvb
  May 27 13:21:50.584: INFO: Created: latency-svc-qj6dl
  May 27 13:21:50.584: INFO: Created: latency-svc-ln7h5
  May 27 13:21:50.585: INFO: Created: latency-svc-27nc5
  May 27 13:21:50.588: INFO: Created: latency-svc-l5p74
  May 27 13:21:50.588: INFO: Created: latency-svc-7scf9
  May 27 13:21:50.588: INFO: Created: latency-svc-z9x4j
  May 27 13:21:50.588: INFO: Created: latency-svc-fv86n
  May 27 13:21:50.589: INFO: Got endpoints: latency-svc-jkr2s [163.038163ms]
  May 27 13:21:50.595: INFO: Created: latency-svc-vx8xq
  May 27 13:21:50.596: INFO: Got endpoints: latency-svc-5b4tv [225.092704ms]
  May 27 13:21:50.608: INFO: Created: latency-svc-gctl9
  May 27 13:21:50.608: INFO: Created: latency-svc-l5ckz
  May 27 13:21:50.620: INFO: Created: latency-svc-p8nrb
  May 27 13:21:50.620: INFO: Created: latency-svc-jf6g8
  May 27 13:21:50.625: INFO: Got endpoints: latency-svc-vx8xq [287.634106ms]
  May 27 13:21:50.628: INFO: Got endpoints: latency-svc-gctl9 [290.869553ms]
  May 27 13:21:50.639: INFO: Created: latency-svc-px2j5
  May 27 13:21:50.642: INFO: Got endpoints: latency-svc-p8nrb [304.615449ms]
  May 27 13:21:50.642: INFO: Got endpoints: latency-svc-dbwvb [304.556149ms]
  May 27 13:21:50.650: INFO: Got endpoints: latency-svc-l5ckz [251.059415ms]
  May 27 13:21:50.654: INFO: Got endpoints: latency-svc-7scf9 [217.626339ms]
  May 27 13:21:50.654: INFO: Got endpoints: latency-svc-jf6g8 [243.205121ms]
  May 27 13:21:50.665: INFO: Created: latency-svc-c9zrg
  May 27 13:21:50.677: INFO: Got endpoints: latency-svc-fv86n [183.722633ms]
  May 27 13:21:50.678: INFO: Got endpoints: latency-svc-27nc5 [340.498926ms]
  May 27 13:21:50.678: INFO: Got endpoints: latency-svc-z9x4j [290.6748ms]
  May 27 13:21:50.678: INFO: Got endpoints: latency-svc-qj6dl [198.767447ms]
  May 27 13:21:50.690: INFO: Created: latency-svc-7mj8k
  May 27 13:21:50.691: INFO: Got endpoints: latency-svc-ln7h5 [224.321988ms]
  May 27 13:21:50.691: INFO: Got endpoints: latency-svc-l5p74 [238.187037ms]
  May 27 13:21:50.702: INFO: Got endpoints: latency-svc-px2j5 [113.259818ms]
  May 27 13:21:50.703: INFO: Got endpoints: latency-svc-c9zrg [106.344633ms]
  May 27 13:21:50.707: INFO: Got endpoints: latency-svc-7mj8k [82.049478ms]
  May 27 13:21:50.710: INFO: Created: latency-svc-9mlk4
  May 27 13:21:50.714: INFO: Created: latency-svc-v5pzv
  May 27 13:21:50.724: INFO: Got endpoints: latency-svc-9mlk4 [95.376745ms]
  May 27 13:21:50.724: INFO: Created: latency-svc-6rjnt
  May 27 13:21:50.725: INFO: Got endpoints: latency-svc-v5pzv [83.08557ms]
  May 27 13:21:50.737: INFO: Got endpoints: latency-svc-6rjnt [94.741712ms]
  May 27 13:21:50.740: INFO: Created: latency-svc-kwxv5
  May 27 13:21:50.749: INFO: Created: latency-svc-w6rj5
  May 27 13:21:50.752: INFO: Got endpoints: latency-svc-kwxv5 [102.528614ms]
  May 27 13:21:50.764: INFO: Got endpoints: latency-svc-w6rj5 [110.209634ms]
  May 27 13:21:50.855: INFO: Created: latency-svc-w2629
  May 27 13:21:50.856: INFO: Created: latency-svc-hrf7x
  May 27 13:21:50.862: INFO: Created: latency-svc-l4snq
  May 27 13:21:50.863: INFO: Created: latency-svc-vl8bb
  May 27 13:21:50.865: INFO: Created: latency-svc-d2pnl
  May 27 13:21:50.865: INFO: Created: latency-svc-2hxvq
  May 27 13:21:50.867: INFO: Created: latency-svc-rrqth
  May 27 13:21:50.869: INFO: Created: latency-svc-76w8z
  May 27 13:21:50.870: INFO: Created: latency-svc-r7w5p
  May 27 13:21:50.870: INFO: Created: latency-svc-7nv68
  May 27 13:21:50.870: INFO: Created: latency-svc-ftdxg
  May 27 13:21:50.871: INFO: Created: latency-svc-j28tr
  May 27 13:21:50.871: INFO: Created: latency-svc-859zr
  May 27 13:21:50.872: INFO: Created: latency-svc-vcb75
  May 27 13:21:50.872: INFO: Created: latency-svc-gxmh4
  May 27 13:21:50.876: INFO: Got endpoints: latency-svc-w2629 [111.984781ms]
  May 27 13:21:50.889: INFO: Got endpoints: latency-svc-hrf7x [210.118993ms]
  May 27 13:21:50.912: INFO: Got endpoints: latency-svc-rrqth [258.323376ms]
  May 27 13:21:50.916: INFO: Got endpoints: latency-svc-r7w5p [190.823381ms]
  May 27 13:21:50.930: INFO: Created: latency-svc-rkqpw
  May 27 13:21:50.934: INFO: Got endpoints: latency-svc-ftdxg [210.00684ms]
  May 27 13:21:50.934: INFO: Got endpoints: latency-svc-vl8bb [243.084949ms]
  May 27 13:21:50.936: INFO: Created: latency-svc-c6kfv
  May 27 13:21:50.938: INFO: Got endpoints: latency-svc-7nv68 [260.526222ms]
  May 27 13:21:50.943: INFO: Created: latency-svc-vqrt5
  May 27 13:21:50.945: INFO: Got endpoints: latency-svc-vcb75 [267.258972ms]
  May 27 13:21:50.956: INFO: Created: latency-svc-xv8l7
  May 27 13:21:50.962: INFO: Created: latency-svc-cdtjh
  May 27 13:21:50.967: INFO: Created: latency-svc-94z65
  May 27 13:21:50.978: INFO: Got endpoints: latency-svc-2hxvq [225.881281ms]
  May 27 13:21:50.980: INFO: Created: latency-svc-gnwch
  May 27 13:21:50.989: INFO: Created: latency-svc-h2r5x
  May 27 13:21:50.994: INFO: Created: latency-svc-xgqjc
  May 27 13:21:51.028: INFO: Got endpoints: latency-svc-gxmh4 [324.919702ms]
  May 27 13:21:51.042: INFO: Created: latency-svc-5btpx
  May 27 13:21:51.081: INFO: Got endpoints: latency-svc-d2pnl [342.865535ms]
  May 27 13:21:51.095: INFO: Created: latency-svc-5t6w5
  May 27 13:21:51.129: INFO: Got endpoints: latency-svc-j28tr [421.103994ms]
  May 27 13:21:51.148: INFO: Created: latency-svc-kl5ww
  May 27 13:21:51.181: INFO: Got endpoints: latency-svc-859zr [490.174081ms]
  May 27 13:21:51.197: INFO: Created: latency-svc-kkmck
  May 27 13:21:51.230: INFO: Got endpoints: latency-svc-l4snq [527.786354ms]
  May 27 13:21:51.249: INFO: Created: latency-svc-kzbfs
  May 27 13:21:51.286: INFO: Got endpoints: latency-svc-76w8z [607.021893ms]
  May 27 13:21:51.306: INFO: Created: latency-svc-h6fpp
  May 27 13:21:51.330: INFO: Got endpoints: latency-svc-rkqpw [453.365155ms]
  May 27 13:21:51.346: INFO: Created: latency-svc-sq2wv
  May 27 13:21:51.385: INFO: Got endpoints: latency-svc-c6kfv [496.37482ms]
  May 27 13:21:51.399: INFO: Created: latency-svc-stn8n
  May 27 13:21:51.430: INFO: Got endpoints: latency-svc-vqrt5 [517.619403ms]
  May 27 13:21:51.441: INFO: Created: latency-svc-8qbwj
  May 27 13:21:51.477: INFO: Got endpoints: latency-svc-xv8l7 [560.510825ms]
  May 27 13:21:51.491: INFO: Created: latency-svc-vd5hp
  E0527 13:21:51.526926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:51.527: INFO: Got endpoints: latency-svc-cdtjh [592.840408ms]
  May 27 13:21:51.540: INFO: Created: latency-svc-5b2kc
  May 27 13:21:51.577: INFO: Got endpoints: latency-svc-94z65 [642.995452ms]
  May 27 13:21:51.590: INFO: Created: latency-svc-jz9g8
  May 27 13:21:51.630: INFO: Got endpoints: latency-svc-gnwch [692.045042ms]
  May 27 13:21:51.643: INFO: Created: latency-svc-585gz
  May 27 13:21:51.681: INFO: Got endpoints: latency-svc-h2r5x [735.343093ms]
  May 27 13:21:51.695: INFO: Created: latency-svc-tjpbr
  May 27 13:21:51.729: INFO: Got endpoints: latency-svc-xgqjc [750.408977ms]
  May 27 13:21:51.741: INFO: Created: latency-svc-cq8mm
  May 27 13:21:51.777: INFO: Got endpoints: latency-svc-5btpx [749.183812ms]
  May 27 13:21:51.792: INFO: Created: latency-svc-v2vbl
  May 27 13:21:51.828: INFO: Got endpoints: latency-svc-5t6w5 [747.256332ms]
  May 27 13:21:51.840: INFO: Created: latency-svc-gwnqj
  May 27 13:21:51.878: INFO: Got endpoints: latency-svc-kl5ww [749.07189ms]
  May 27 13:21:51.892: INFO: Created: latency-svc-8jbzm
  May 27 13:21:51.928: INFO: Got endpoints: latency-svc-kkmck [746.813982ms]
  May 27 13:21:51.943: INFO: Created: latency-svc-9gq7g
  May 27 13:21:51.979: INFO: Got endpoints: latency-svc-kzbfs [748.646231ms]
  May 27 13:21:52.004: INFO: Created: latency-svc-7c9j2
  May 27 13:21:52.027: INFO: Got endpoints: latency-svc-h6fpp [740.896699ms]
  May 27 13:21:52.041: INFO: Created: latency-svc-27f57
  May 27 13:21:52.077: INFO: Got endpoints: latency-svc-sq2wv [746.603688ms]
  May 27 13:21:52.092: INFO: Created: latency-svc-hdkmv
  May 27 13:21:52.131: INFO: Got endpoints: latency-svc-stn8n [744.767269ms]
  May 27 13:21:52.143: INFO: Created: latency-svc-v4wj7
  May 27 13:21:52.177: INFO: Got endpoints: latency-svc-8qbwj [747.089708ms]
  May 27 13:21:52.190: INFO: Created: latency-svc-f22jv
  May 27 13:21:52.227: INFO: Got endpoints: latency-svc-vd5hp [749.782435ms]
  May 27 13:21:52.243: INFO: Created: latency-svc-6ctvz
  May 27 13:21:52.277: INFO: Got endpoints: latency-svc-5b2kc [749.926477ms]
  May 27 13:21:52.291: INFO: Created: latency-svc-wrgtp
  May 27 13:21:52.330: INFO: Got endpoints: latency-svc-jz9g8 [752.830188ms]
  May 27 13:21:52.345: INFO: Created: latency-svc-krjh7
  May 27 13:21:52.379: INFO: Got endpoints: latency-svc-585gz [748.361914ms]
  May 27 13:21:52.392: INFO: Created: latency-svc-jv58l
  May 27 13:21:52.431: INFO: Got endpoints: latency-svc-tjpbr [750.216103ms]
  May 27 13:21:52.445: INFO: Created: latency-svc-7wt87
  May 27 13:21:52.478: INFO: Got endpoints: latency-svc-cq8mm [749.190832ms]
  May 27 13:21:52.491: INFO: Created: latency-svc-v58fg
  May 27 13:21:52.526: INFO: Got endpoints: latency-svc-v2vbl [748.785344ms]
  E0527 13:21:52.527602      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:52.542: INFO: Created: latency-svc-lbtpg
  May 27 13:21:52.578: INFO: Got endpoints: latency-svc-gwnqj [749.817385ms]
  May 27 13:21:52.593: INFO: Created: latency-svc-bw8kx
  May 27 13:21:52.628: INFO: Got endpoints: latency-svc-8jbzm [749.299414ms]
  May 27 13:21:52.643: INFO: Created: latency-svc-fvsd7
  May 27 13:21:52.679: INFO: Got endpoints: latency-svc-9gq7g [750.609541ms]
  May 27 13:21:52.692: INFO: Created: latency-svc-vt4vf
  May 27 13:21:52.730: INFO: Got endpoints: latency-svc-7c9j2 [750.497239ms]
  May 27 13:21:52.744: INFO: Created: latency-svc-t4vh9
  May 27 13:21:52.778: INFO: Got endpoints: latency-svc-27f57 [750.260374ms]
  May 27 13:21:52.791: INFO: Created: latency-svc-c5q86
  May 27 13:21:52.827: INFO: Got endpoints: latency-svc-hdkmv [748.813214ms]
  May 27 13:21:52.841: INFO: Created: latency-svc-9ljws
  May 27 13:21:52.878: INFO: Got endpoints: latency-svc-v4wj7 [747.570338ms]
  May 27 13:21:52.892: INFO: Created: latency-svc-q4w7t
  May 27 13:21:52.929: INFO: Got endpoints: latency-svc-f22jv [752.160504ms]
  May 27 13:21:52.942: INFO: Created: latency-svc-n5csz
  May 27 13:21:52.979: INFO: Got endpoints: latency-svc-6ctvz [751.859767ms]
  May 27 13:21:52.993: INFO: Created: latency-svc-jvsld
  May 27 13:21:53.029: INFO: Got endpoints: latency-svc-wrgtp [751.403337ms]
  May 27 13:21:53.042: INFO: Created: latency-svc-8kchp
  May 27 13:21:53.077: INFO: Got endpoints: latency-svc-krjh7 [746.69682ms]
  May 27 13:21:53.092: INFO: Created: latency-svc-vdq2l
  May 27 13:21:53.128: INFO: Got endpoints: latency-svc-jv58l [748.970597ms]
  May 27 13:21:53.143: INFO: Created: latency-svc-mvjvf
  May 27 13:21:53.180: INFO: Got endpoints: latency-svc-7wt87 [748.516198ms]
  May 27 13:21:53.194: INFO: Created: latency-svc-gg8jh
  May 27 13:21:53.229: INFO: Got endpoints: latency-svc-v58fg [751.368167ms]
  May 27 13:21:53.245: INFO: Created: latency-svc-dtfvw
  May 27 13:21:53.278: INFO: Got endpoints: latency-svc-lbtpg [751.639453ms]
  May 27 13:21:53.293: INFO: Created: latency-svc-snhxz
  May 27 13:21:53.329: INFO: Got endpoints: latency-svc-bw8kx [751.429028ms]
  May 27 13:21:53.346: INFO: Created: latency-svc-v9tfc
  May 27 13:21:53.379: INFO: Got endpoints: latency-svc-fvsd7 [751.667813ms]
  May 27 13:21:53.392: INFO: Created: latency-svc-c2q6x
  May 27 13:21:53.433: INFO: Got endpoints: latency-svc-vt4vf [754.4211ms]
  May 27 13:21:53.447: INFO: Created: latency-svc-lg952
  May 27 13:21:53.479: INFO: Got endpoints: latency-svc-t4vh9 [749.58299ms]
  May 27 13:21:53.493: INFO: Created: latency-svc-852x9
  E0527 13:21:53.527341      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:53.527: INFO: Got endpoints: latency-svc-c5q86 [749.375516ms]
  May 27 13:21:53.545: INFO: Created: latency-svc-jlcpq
  May 27 13:21:53.577: INFO: Got endpoints: latency-svc-9ljws [749.637471ms]
  May 27 13:21:53.597: INFO: Created: latency-svc-6vdxf
  May 27 13:21:53.628: INFO: Got endpoints: latency-svc-q4w7t [749.919767ms]
  May 27 13:21:53.643: INFO: Created: latency-svc-xm22z
  May 27 13:21:53.677: INFO: Got endpoints: latency-svc-n5csz [747.386934ms]
  May 27 13:21:53.691: INFO: Created: latency-svc-bz8md
  May 27 13:21:53.728: INFO: Got endpoints: latency-svc-jvsld [748.681511ms]
  May 27 13:21:53.749: INFO: Created: latency-svc-q8qgx
  May 27 13:21:53.779: INFO: Got endpoints: latency-svc-8kchp [750.765865ms]
  May 27 13:21:53.794: INFO: Created: latency-svc-r4ztr
  May 27 13:21:53.827: INFO: Got endpoints: latency-svc-vdq2l [748.742782ms]
  May 27 13:21:53.842: INFO: Created: latency-svc-btrjd
  May 27 13:21:53.878: INFO: Got endpoints: latency-svc-mvjvf [749.59025ms]
  May 27 13:21:53.891: INFO: Created: latency-svc-sg8bl
  May 27 13:21:53.928: INFO: Got endpoints: latency-svc-gg8jh [748.522788ms]
  May 27 13:21:53.945: INFO: Created: latency-svc-n7llf
  May 27 13:21:53.979: INFO: Got endpoints: latency-svc-dtfvw [749.145961ms]
  May 27 13:21:53.993: INFO: Created: latency-svc-n7j92
  May 27 13:21:54.029: INFO: Got endpoints: latency-svc-snhxz [750.131341ms]
  May 27 13:21:54.043: INFO: Created: latency-svc-xthr2
  May 27 13:21:54.078: INFO: Got endpoints: latency-svc-v9tfc [748.187191ms]
  May 27 13:21:54.094: INFO: Created: latency-svc-wpp6b
  May 27 13:21:54.129: INFO: Got endpoints: latency-svc-c2q6x [749.657821ms]
  May 27 13:21:54.145: INFO: Created: latency-svc-pnpl8
  May 27 13:21:54.178: INFO: Got endpoints: latency-svc-lg952 [744.399042ms]
  May 27 13:21:54.193: INFO: Created: latency-svc-d8q6z
  May 27 13:21:54.230: INFO: Got endpoints: latency-svc-852x9 [750.050799ms]
  May 27 13:21:54.250: INFO: Created: latency-svc-xppm9
  May 27 13:21:54.280: INFO: Got endpoints: latency-svc-jlcpq [752.745326ms]
  May 27 13:21:54.297: INFO: Created: latency-svc-x284j
  May 27 13:21:54.327: INFO: Got endpoints: latency-svc-6vdxf [749.656092ms]
  May 27 13:21:54.343: INFO: Created: latency-svc-stn7z
  May 27 13:21:54.380: INFO: Got endpoints: latency-svc-xm22z [751.876648ms]
  May 27 13:21:54.402: INFO: Created: latency-svc-6zvsb
  May 27 13:21:54.429: INFO: Got endpoints: latency-svc-bz8md [752.46198ms]
  May 27 13:21:54.445: INFO: Created: latency-svc-9tzsr
  May 27 13:21:54.479: INFO: Got endpoints: latency-svc-q8qgx [750.498429ms]
  May 27 13:21:54.492: INFO: Created: latency-svc-vxf4t
  E0527 13:21:54.528312      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:54.530: INFO: Got endpoints: latency-svc-r4ztr [750.354416ms]
  May 27 13:21:54.545: INFO: Created: latency-svc-fs4zg
  May 27 13:21:54.577: INFO: Got endpoints: latency-svc-btrjd [750.390557ms]
  May 27 13:21:54.593: INFO: Created: latency-svc-zdcq8
  May 27 13:21:54.628: INFO: Got endpoints: latency-svc-sg8bl [749.854266ms]
  May 27 13:21:54.640: INFO: Created: latency-svc-qqzpb
  May 27 13:21:54.683: INFO: Got endpoints: latency-svc-n7llf [754.315198ms]
  May 27 13:21:54.702: INFO: Created: latency-svc-tvhpk
  May 27 13:21:54.730: INFO: Got endpoints: latency-svc-n7j92 [750.56778ms]
  May 27 13:21:54.744: INFO: Created: latency-svc-28dsm
  May 27 13:21:54.783: INFO: Got endpoints: latency-svc-xthr2 [753.492331ms]
  May 27 13:21:54.800: INFO: Created: latency-svc-q9xp9
  May 27 13:21:54.828: INFO: Got endpoints: latency-svc-wpp6b [750.409527ms]
  May 27 13:21:54.844: INFO: Created: latency-svc-d2wf8
  May 27 13:21:54.878: INFO: Got endpoints: latency-svc-pnpl8 [748.524268ms]
  May 27 13:21:54.892: INFO: Created: latency-svc-zhrb6
  May 27 13:21:54.928: INFO: Got endpoints: latency-svc-d8q6z [749.802145ms]
  May 27 13:21:54.941: INFO: Created: latency-svc-fz5kc
  May 27 13:21:54.980: INFO: Got endpoints: latency-svc-xppm9 [750.300075ms]
  May 27 13:21:54.995: INFO: Created: latency-svc-2265t
  May 27 13:21:55.030: INFO: Got endpoints: latency-svc-x284j [749.159851ms]
  May 27 13:21:55.043: INFO: Created: latency-svc-d9dfk
  May 27 13:21:55.078: INFO: Got endpoints: latency-svc-stn7z [751.422868ms]
  May 27 13:21:55.090: INFO: Created: latency-svc-qmm72
  May 27 13:21:55.128: INFO: Got endpoints: latency-svc-6zvsb [747.707391ms]
  May 27 13:21:55.144: INFO: Created: latency-svc-bccw8
  May 27 13:21:55.179: INFO: Got endpoints: latency-svc-9tzsr [749.470747ms]
  May 27 13:21:55.192: INFO: Created: latency-svc-nfhff
  May 27 13:21:55.228: INFO: Got endpoints: latency-svc-vxf4t [749.787444ms]
  May 27 13:21:55.240: INFO: Created: latency-svc-9lzbd
  May 27 13:21:55.279: INFO: Got endpoints: latency-svc-fs4zg [749.12257ms]
  May 27 13:21:55.293: INFO: Created: latency-svc-n6lls
  May 27 13:21:55.330: INFO: Got endpoints: latency-svc-zdcq8 [753.002181ms]
  May 27 13:21:55.351: INFO: Created: latency-svc-wt2x8
  May 27 13:21:55.379: INFO: Got endpoints: latency-svc-qqzpb [750.410067ms]
  May 27 13:21:55.392: INFO: Created: latency-svc-9vt5z
  May 27 13:21:55.428: INFO: Got endpoints: latency-svc-tvhpk [744.741879ms]
  May 27 13:21:55.443: INFO: Created: latency-svc-hh2zj
  May 27 13:21:55.478: INFO: Got endpoints: latency-svc-28dsm [748.491657ms]
  May 27 13:21:55.491: INFO: Created: latency-svc-6hnl5
  May 27 13:21:55.527: INFO: Got endpoints: latency-svc-q9xp9 [744.171107ms]
  E0527 13:21:55.528330      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:55.540: INFO: Created: latency-svc-485mp
  May 27 13:21:55.579: INFO: Got endpoints: latency-svc-d2wf8 [750.758524ms]
  May 27 13:21:55.595: INFO: Created: latency-svc-cnbdt
  May 27 13:21:55.626: INFO: Got endpoints: latency-svc-zhrb6 [747.366893ms]
  May 27 13:21:55.643: INFO: Created: latency-svc-8chfq
  May 27 13:21:55.678: INFO: Got endpoints: latency-svc-fz5kc [749.519848ms]
  May 27 13:21:55.692: INFO: Created: latency-svc-q72h4
  May 27 13:21:55.730: INFO: Got endpoints: latency-svc-2265t [749.655121ms]
  May 27 13:21:55.746: INFO: Created: latency-svc-qjn5c
  May 27 13:21:55.779: INFO: Got endpoints: latency-svc-d9dfk [749.59799ms]
  May 27 13:21:55.794: INFO: Created: latency-svc-vx9w2
  May 27 13:21:55.828: INFO: Got endpoints: latency-svc-qmm72 [749.664401ms]
  May 27 13:21:55.841: INFO: Created: latency-svc-l25sw
  May 27 13:21:55.880: INFO: Got endpoints: latency-svc-bccw8 [751.621272ms]
  May 27 13:21:55.896: INFO: Created: latency-svc-mbmvw
  May 27 13:21:55.928: INFO: Got endpoints: latency-svc-nfhff [748.310323ms]
  May 27 13:21:55.942: INFO: Created: latency-svc-2sprv
  May 27 13:21:55.978: INFO: Got endpoints: latency-svc-9lzbd [749.60195ms]
  May 27 13:21:55.991: INFO: Created: latency-svc-zkktn
  May 27 13:21:56.033: INFO: Got endpoints: latency-svc-n6lls [753.488921ms]
  May 27 13:21:56.047: INFO: Created: latency-svc-42245
  May 27 13:21:56.079: INFO: Got endpoints: latency-svc-wt2x8 [748.527028ms]
  May 27 13:21:56.092: INFO: Created: latency-svc-dk8n5
  May 27 13:21:56.127: INFO: Got endpoints: latency-svc-9vt5z [747.824693ms]
  May 27 13:21:56.146: INFO: Created: latency-svc-rh5l9
  May 27 13:21:56.179: INFO: Got endpoints: latency-svc-hh2zj [750.255354ms]
  May 27 13:21:56.194: INFO: Created: latency-svc-7tljr
  May 27 13:21:56.228: INFO: Got endpoints: latency-svc-6hnl5 [749.682892ms]
  May 27 13:21:56.246: INFO: Created: latency-svc-5wrmz
  May 27 13:21:56.280: INFO: Got endpoints: latency-svc-485mp [752.201465ms]
  May 27 13:21:56.293: INFO: Created: latency-svc-cz5zz
  May 27 13:21:56.330: INFO: Got endpoints: latency-svc-cnbdt [750.160372ms]
  May 27 13:21:56.349: INFO: Created: latency-svc-9zxll
  May 27 13:21:56.381: INFO: Got endpoints: latency-svc-8chfq [754.85233ms]
  May 27 13:21:56.394: INFO: Created: latency-svc-rncmp
  May 27 13:21:56.428: INFO: Got endpoints: latency-svc-q72h4 [750.166442ms]
  May 27 13:21:56.447: INFO: Created: latency-svc-j5x5g
  May 27 13:21:56.481: INFO: Got endpoints: latency-svc-qjn5c [751.331547ms]
  May 27 13:21:56.496: INFO: Created: latency-svc-fp8vk
  E0527 13:21:56.529406      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:56.529: INFO: Got endpoints: latency-svc-vx9w2 [749.788264ms]
  May 27 13:21:56.542: INFO: Created: latency-svc-v2mlk
  May 27 13:21:56.577: INFO: Got endpoints: latency-svc-l25sw [749.412426ms]
  May 27 13:21:56.590: INFO: Created: latency-svc-c95xl
  May 27 13:21:56.627: INFO: Got endpoints: latency-svc-mbmvw [747.756442ms]
  May 27 13:21:56.641: INFO: Created: latency-svc-s6km6
  May 27 13:21:56.679: INFO: Got endpoints: latency-svc-2sprv [750.406147ms]
  May 27 13:21:56.692: INFO: Created: latency-svc-p7mtz
  May 27 13:21:56.726: INFO: Got endpoints: latency-svc-zkktn [747.376064ms]
  May 27 13:21:56.738: INFO: Created: latency-svc-8wpxh
  May 27 13:21:56.780: INFO: Got endpoints: latency-svc-42245 [747.383364ms]
  May 27 13:21:56.794: INFO: Created: latency-svc-2jt9g
  May 27 13:21:56.827: INFO: Got endpoints: latency-svc-dk8n5 [748.192411ms]
  May 27 13:21:56.843: INFO: Created: latency-svc-vlnnf
  May 27 13:21:56.878: INFO: Got endpoints: latency-svc-rh5l9 [751.622283ms]
  May 27 13:21:56.892: INFO: Created: latency-svc-x4zhc
  May 27 13:21:56.930: INFO: Got endpoints: latency-svc-7tljr [751.142432ms]
  May 27 13:21:56.944: INFO: Created: latency-svc-mpkmc
  May 27 13:21:56.979: INFO: Got endpoints: latency-svc-5wrmz [751.009559ms]
  May 27 13:21:56.993: INFO: Created: latency-svc-msz8r
  May 27 13:21:57.026: INFO: Got endpoints: latency-svc-cz5zz [746.075647ms]
  May 27 13:21:57.040: INFO: Created: latency-svc-c6cjg
  May 27 13:21:57.079: INFO: Got endpoints: latency-svc-9zxll [748.514638ms]
  May 27 13:21:57.094: INFO: Created: latency-svc-2cm6l
  May 27 13:21:57.129: INFO: Got endpoints: latency-svc-rncmp [747.578098ms]
  May 27 13:21:57.143: INFO: Created: latency-svc-jm9xv
  May 27 13:21:57.178: INFO: Got endpoints: latency-svc-j5x5g [749.535649ms]
  May 27 13:21:57.191: INFO: Created: latency-svc-7knzd
  May 27 13:21:57.230: INFO: Got endpoints: latency-svc-fp8vk [748.311693ms]
  May 27 13:21:57.244: INFO: Created: latency-svc-6zjfv
  May 27 13:21:57.280: INFO: Got endpoints: latency-svc-v2mlk [750.665712ms]
  May 27 13:21:57.294: INFO: Created: latency-svc-wz9p6
  May 27 13:21:57.335: INFO: Got endpoints: latency-svc-c95xl [757.425793ms]
  May 27 13:21:57.350: INFO: Created: latency-svc-p6ds5
  May 27 13:21:57.385: INFO: Got endpoints: latency-svc-s6km6 [757.039455ms]
  May 27 13:21:57.402: INFO: Created: latency-svc-wqs97
  May 27 13:21:57.428: INFO: Got endpoints: latency-svc-p7mtz [748.902216ms]
  May 27 13:21:57.445: INFO: Created: latency-svc-xtznn
  May 27 13:21:57.479: INFO: Got endpoints: latency-svc-8wpxh [752.588372ms]
  May 27 13:21:57.492: INFO: Created: latency-svc-tqc6j
  E0527 13:21:57.529697      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:57.530: INFO: Got endpoints: latency-svc-2jt9g [750.285755ms]
  May 27 13:21:57.548: INFO: Created: latency-svc-64xqb
  May 27 13:21:57.578: INFO: Got endpoints: latency-svc-vlnnf [750.05543ms]
  May 27 13:21:57.596: INFO: Created: latency-svc-4xmts
  May 27 13:21:57.629: INFO: Got endpoints: latency-svc-x4zhc [750.560511ms]
  May 27 13:21:57.645: INFO: Created: latency-svc-wn6vc
  May 27 13:21:57.679: INFO: Got endpoints: latency-svc-mpkmc [749.395587ms]
  May 27 13:21:57.696: INFO: Created: latency-svc-2wj6k
  May 27 13:21:57.737: INFO: Got endpoints: latency-svc-msz8r [758.108988ms]
  May 27 13:21:57.752: INFO: Created: latency-svc-5zkpb
  May 27 13:21:57.782: INFO: Got endpoints: latency-svc-c6cjg [755.940772ms]
  May 27 13:21:57.797: INFO: Created: latency-svc-n6mwv
  May 27 13:21:57.830: INFO: Got endpoints: latency-svc-2cm6l [751.172373ms]
  May 27 13:21:57.845: INFO: Created: latency-svc-t6rxf
  May 27 13:21:57.879: INFO: Got endpoints: latency-svc-jm9xv [749.506228ms]
  May 27 13:21:57.897: INFO: Created: latency-svc-xqm6m
  May 27 13:21:57.928: INFO: Got endpoints: latency-svc-7knzd [748.978118ms]
  May 27 13:21:57.940: INFO: Created: latency-svc-sttj9
  May 27 13:21:57.980: INFO: Got endpoints: latency-svc-6zjfv [749.772024ms]
  May 27 13:21:58.006: INFO: Created: latency-svc-hsjsf
  May 27 13:21:58.029: INFO: Got endpoints: latency-svc-wz9p6 [748.367944ms]
  May 27 13:21:58.041: INFO: Created: latency-svc-vck9m
  May 27 13:21:58.080: INFO: Got endpoints: latency-svc-p6ds5 [744.859692ms]
  May 27 13:21:58.092: INFO: Created: latency-svc-kr5t5
  May 27 13:21:58.128: INFO: Got endpoints: latency-svc-wqs97 [743.032983ms]
  May 27 13:21:58.144: INFO: Created: latency-svc-699j4
  May 27 13:21:58.177: INFO: Got endpoints: latency-svc-xtznn [749.425626ms]
  May 27 13:21:58.228: INFO: Got endpoints: latency-svc-tqc6j [748.544738ms]
  May 27 13:21:58.281: INFO: Got endpoints: latency-svc-64xqb [750.997969ms]
  May 27 13:21:58.331: INFO: Got endpoints: latency-svc-4xmts [752.694224ms]
  May 27 13:21:58.385: INFO: Got endpoints: latency-svc-wn6vc [755.985094ms]
  May 27 13:21:58.428: INFO: Got endpoints: latency-svc-2wj6k [748.429896ms]
  May 27 13:21:58.479: INFO: Got endpoints: latency-svc-5zkpb [741.701196ms]
  E0527 13:21:58.530683      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:21:58.531: INFO: Got endpoints: latency-svc-n6mwv [748.112419ms]
  May 27 13:21:58.578: INFO: Got endpoints: latency-svc-t6rxf [747.497576ms]
  May 27 13:21:58.629: INFO: Got endpoints: latency-svc-xqm6m [750.713223ms]
  May 27 13:21:58.679: INFO: Got endpoints: latency-svc-sttj9 [750.368626ms]
  May 27 13:21:58.730: INFO: Got endpoints: latency-svc-hsjsf [750.141961ms]
  May 27 13:21:58.777: INFO: Got endpoints: latency-svc-vck9m [748.595149ms]
  May 27 13:21:58.829: INFO: Got endpoints: latency-svc-kr5t5 [748.830125ms]
  May 27 13:21:58.880: INFO: Got endpoints: latency-svc-699j4 [751.866738ms]
  May 27 13:21:58.880: INFO: Latencies: [35.426677ms 51.177705ms 62.374898ms 73.970989ms 82.049478ms 83.08557ms 89.39477ms 94.741712ms 95.376745ms 99.302787ms 102.528614ms 106.344633ms 110.209634ms 111.984781ms 113.259818ms 115.576136ms 129.193559ms 142.18585ms 155.795933ms 163.038163ms 183.722633ms 190.823381ms 198.767447ms 210.00684ms 210.118993ms 217.626339ms 224.321988ms 225.092704ms 225.881281ms 238.187037ms 243.084949ms 243.205121ms 251.059415ms 258.323376ms 260.526222ms 267.258972ms 287.634106ms 290.6748ms 290.869553ms 304.556149ms 304.615449ms 324.919702ms 340.498926ms 342.865535ms 421.103994ms 453.365155ms 490.174081ms 496.37482ms 517.619403ms 527.786354ms 560.510825ms 592.840408ms 607.021893ms 642.995452ms 692.045042ms 735.343093ms 740.896699ms 741.701196ms 743.032983ms 744.171107ms 744.399042ms 744.741879ms 744.767269ms 744.859692ms 746.075647ms 746.603688ms 746.69682ms 746.813982ms 747.089708ms 747.256332ms 747.366893ms 747.376064ms 747.383364ms 747.386934ms 747.497576ms 747.570338ms 747.578098ms 747.707391ms 747.756442ms 747.824693ms 748.112419ms 748.187191ms 748.192411ms 748.310323ms 748.311693ms 748.361914ms 748.367944ms 748.429896ms 748.491657ms 748.514638ms 748.516198ms 748.522788ms 748.524268ms 748.527028ms 748.544738ms 748.595149ms 748.646231ms 748.681511ms 748.742782ms 748.785344ms 748.813214ms 748.830125ms 748.902216ms 748.970597ms 748.978118ms 749.07189ms 749.12257ms 749.145961ms 749.159851ms 749.183812ms 749.190832ms 749.299414ms 749.375516ms 749.395587ms 749.412426ms 749.425626ms 749.470747ms 749.506228ms 749.519848ms 749.535649ms 749.58299ms 749.59025ms 749.59799ms 749.60195ms 749.637471ms 749.655121ms 749.656092ms 749.657821ms 749.664401ms 749.682892ms 749.772024ms 749.782435ms 749.787444ms 749.788264ms 749.802145ms 749.817385ms 749.854266ms 749.919767ms 749.926477ms 750.050799ms 750.05543ms 750.131341ms 750.141961ms 750.160372ms 750.166442ms 750.216103ms 750.255354ms 750.260374ms 750.285755ms 750.300075ms 750.354416ms 750.368626ms 750.390557ms 750.406147ms 750.408977ms 750.409527ms 750.410067ms 750.497239ms 750.498429ms 750.560511ms 750.56778ms 750.609541ms 750.665712ms 750.713223ms 750.758524ms 750.765865ms 750.997969ms 751.009559ms 751.142432ms 751.172373ms 751.331547ms 751.368167ms 751.403337ms 751.422868ms 751.429028ms 751.621272ms 751.622283ms 751.639453ms 751.667813ms 751.859767ms 751.866738ms 751.876648ms 752.160504ms 752.201465ms 752.46198ms 752.588372ms 752.694224ms 752.745326ms 752.830188ms 753.002181ms 753.488921ms 753.492331ms 754.315198ms 754.4211ms 754.85233ms 755.940772ms 755.985094ms 757.039455ms 757.425793ms 758.108988ms]
  May 27 13:21:58.880: INFO: 50 %ile: 748.813214ms
  May 27 13:21:58.881: INFO: 90 %ile: 751.866738ms
  May 27 13:21:58.881: INFO: 99 %ile: 757.425793ms
  May 27 13:21:58.881: INFO: Total sample count: 200
  May 27 13:21:58.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-3969" for this suite. @ 05/27/23 13:21:58.89
• [9.787 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 05/27/23 13:21:58.903
  May 27 13:21:58.903: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename job @ 05/27/23 13:21:58.904
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:21:58.927
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:21:58.932
  STEP: Creating a suspended job @ 05/27/23 13:21:58.943
  STEP: Patching the Job @ 05/27/23 13:21:58.951
  STEP: Watching for Job to be patched @ 05/27/23 13:21:58.973
  May 27 13:21:58.975: INFO: Event ADDED observed for Job e2e-sk5hn in namespace job-4344 with labels: map[e2e-job-label:e2e-sk5hn] and annotations: map[batch.kubernetes.io/job-tracking:]
  May 27 13:21:58.975: INFO: Event MODIFIED observed for Job e2e-sk5hn in namespace job-4344 with labels: map[e2e-job-label:e2e-sk5hn] and annotations: map[batch.kubernetes.io/job-tracking:]
  May 27 13:21:58.975: INFO: Event MODIFIED found for Job e2e-sk5hn in namespace job-4344 with labels: map[e2e-job-label:e2e-sk5hn e2e-sk5hn:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 05/27/23 13:21:58.976
  STEP: Watching for Job to be updated @ 05/27/23 13:21:58.992
  May 27 13:21:58.998: INFO: Event MODIFIED found for Job e2e-sk5hn in namespace job-4344 with labels: map[e2e-job-label:e2e-sk5hn e2e-sk5hn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 27 13:21:58.998: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 05/27/23 13:21:58.998
  May 27 13:21:59.008: INFO: Job: e2e-sk5hn as labels: map[e2e-job-label:e2e-sk5hn e2e-sk5hn:patched]
  STEP: Waiting for job to complete @ 05/27/23 13:21:59.008
  E0527 13:21:59.531343      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:00.532322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:01.532438      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:02.532558      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:03.533272      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:04.533488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:05.533584      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:06.533641      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:07.533740      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:08.533919      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 05/27/23 13:22:09.014
  STEP: Watching for Job to be deleted @ 05/27/23 13:22:09.027
  May 27 13:22:09.032: INFO: Event MODIFIED observed for Job e2e-sk5hn in namespace job-4344 with labels: map[e2e-job-label:e2e-sk5hn e2e-sk5hn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 27 13:22:09.032: INFO: Event MODIFIED observed for Job e2e-sk5hn in namespace job-4344 with labels: map[e2e-job-label:e2e-sk5hn e2e-sk5hn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 27 13:22:09.032: INFO: Event MODIFIED observed for Job e2e-sk5hn in namespace job-4344 with labels: map[e2e-job-label:e2e-sk5hn e2e-sk5hn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 27 13:22:09.033: INFO: Event MODIFIED observed for Job e2e-sk5hn in namespace job-4344 with labels: map[e2e-job-label:e2e-sk5hn e2e-sk5hn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 27 13:22:09.033: INFO: Event MODIFIED observed for Job e2e-sk5hn in namespace job-4344 with labels: map[e2e-job-label:e2e-sk5hn e2e-sk5hn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 27 13:22:09.033: INFO: Event DELETED found for Job e2e-sk5hn in namespace job-4344 with labels: map[e2e-job-label:e2e-sk5hn e2e-sk5hn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 05/27/23 13:22:09.033
  May 27 13:22:09.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4344" for this suite. @ 05/27/23 13:22:09.051
• [10.163 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 05/27/23 13:22:09.066
  May 27 13:22:09.066: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename services @ 05/27/23 13:22:09.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:22:09.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:22:09.101
  STEP: creating an Endpoint @ 05/27/23 13:22:09.114
  STEP: waiting for available Endpoint @ 05/27/23 13:22:09.121
  STEP: listing all Endpoints @ 05/27/23 13:22:09.123
  STEP: updating the Endpoint @ 05/27/23 13:22:09.128
  STEP: fetching the Endpoint @ 05/27/23 13:22:09.139
  STEP: patching the Endpoint @ 05/27/23 13:22:09.144
  STEP: fetching the Endpoint @ 05/27/23 13:22:09.159
  STEP: deleting the Endpoint by Collection @ 05/27/23 13:22:09.167
  STEP: waiting for Endpoint deletion @ 05/27/23 13:22:09.18
  STEP: fetching the Endpoint @ 05/27/23 13:22:09.185
  May 27 13:22:09.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9130" for this suite. @ 05/27/23 13:22:09.194
• [0.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 05/27/23 13:22:09.207
  May 27 13:22:09.207: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename replicaset @ 05/27/23 13:22:09.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:22:09.232
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:22:09.237
  May 27 13:22:09.242: INFO: Creating ReplicaSet my-hostname-basic-1f65ee50-5164-4ea1-ad9d-238e3b0fe800
  May 27 13:22:09.257: INFO: Pod name my-hostname-basic-1f65ee50-5164-4ea1-ad9d-238e3b0fe800: Found 0 pods out of 1
  E0527 13:22:09.534044      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:10.534074      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:11.534204      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:12.534327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:13.534415      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:22:14.263: INFO: Pod name my-hostname-basic-1f65ee50-5164-4ea1-ad9d-238e3b0fe800: Found 1 pods out of 1
  May 27 13:22:14.263: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-1f65ee50-5164-4ea1-ad9d-238e3b0fe800" is running
  May 27 13:22:14.269: INFO: Pod "my-hostname-basic-1f65ee50-5164-4ea1-ad9d-238e3b0fe800-mftz6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-27 13:22:09 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-27 13:22:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-27 13:22:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-27 13:22:09 +0000 UTC Reason: Message:}])
  May 27 13:22:14.269: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/27/23 13:22:14.269
  May 27 13:22:14.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4164" for this suite. @ 05/27/23 13:22:14.294
• [5.098 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 05/27/23 13:22:14.306
  May 27 13:22:14.306: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 13:22:14.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:22:14.334
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:22:14.34
  STEP: Setting up server cert @ 05/27/23 13:22:14.374
  E0527 13:22:14.535393      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 13:22:14.839
  STEP: Deploying the webhook pod @ 05/27/23 13:22:14.849
  STEP: Wait for the deployment to be ready @ 05/27/23 13:22:14.865
  May 27 13:22:14.876: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0527 13:22:15.536060      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:16.536311      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/27/23 13:22:16.889
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 13:22:16.902
  E0527 13:22:17.536431      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:22:17.903: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  E0527 13:22:18.536533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:22:18.904: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  E0527 13:22:19.536841      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:22:19.903: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 05/27/23 13:22:19.908
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 05/27/23 13:22:19.91
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 05/27/23 13:22:19.91
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 05/27/23 13:22:19.91
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 05/27/23 13:22:19.912
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/27/23 13:22:19.912
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/27/23 13:22:19.913
  May 27 13:22:19.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8459" for this suite. @ 05/27/23 13:22:19.975
  STEP: Destroying namespace "webhook-markers-4601" for this suite. @ 05/27/23 13:22:19.985
• [5.693 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 05/27/23 13:22:20.001
  May 27 13:22:20.001: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename proxy @ 05/27/23 13:22:20.002
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:22:20.023
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:22:20.028
  May 27 13:22:20.033: INFO: Creating pod...
  E0527 13:22:20.537609      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:21.538131      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:22:22.065: INFO: Creating service...
  May 27 13:22:22.080: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6037/pods/agnhost/proxy?method=DELETE
  May 27 13:22:22.096: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 27 13:22:22.096: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6037/pods/agnhost/proxy?method=OPTIONS
  May 27 13:22:22.101: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 27 13:22:22.101: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6037/pods/agnhost/proxy?method=PATCH
  May 27 13:22:22.107: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 27 13:22:22.107: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6037/pods/agnhost/proxy?method=POST
  May 27 13:22:22.113: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 27 13:22:22.114: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6037/pods/agnhost/proxy?method=PUT
  May 27 13:22:22.124: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 27 13:22:22.125: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6037/services/e2e-proxy-test-service/proxy?method=DELETE
  May 27 13:22:22.133: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 27 13:22:22.133: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6037/services/e2e-proxy-test-service/proxy?method=OPTIONS
  May 27 13:22:22.140: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 27 13:22:22.140: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6037/services/e2e-proxy-test-service/proxy?method=PATCH
  May 27 13:22:22.148: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 27 13:22:22.148: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6037/services/e2e-proxy-test-service/proxy?method=POST
  May 27 13:22:22.156: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 27 13:22:22.156: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6037/services/e2e-proxy-test-service/proxy?method=PUT
  May 27 13:22:22.163: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 27 13:22:22.164: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6037/pods/agnhost/proxy?method=GET
  May 27 13:22:22.169: INFO: http.Client request:GET StatusCode:301
  May 27 13:22:22.169: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6037/services/e2e-proxy-test-service/proxy?method=GET
  May 27 13:22:22.177: INFO: http.Client request:GET StatusCode:301
  May 27 13:22:22.177: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6037/pods/agnhost/proxy?method=HEAD
  May 27 13:22:22.181: INFO: http.Client request:HEAD StatusCode:301
  May 27 13:22:22.181: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6037/services/e2e-proxy-test-service/proxy?method=HEAD
  May 27 13:22:22.191: INFO: http.Client request:HEAD StatusCode:301
  May 27 13:22:22.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-6037" for this suite. @ 05/27/23 13:22:22.198
• [2.206 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 05/27/23 13:22:22.21
  May 27 13:22:22.210: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename secrets @ 05/27/23 13:22:22.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:22:22.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:22:22.25
  STEP: Creating secret with name secret-test-d76b7dee-c62d-4d98-8e4f-bab45d159cdd @ 05/27/23 13:22:22.255
  STEP: Creating a pod to test consume secrets @ 05/27/23 13:22:22.264
  E0527 13:22:22.538476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:23.538978      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:24.539530      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:25.539656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:22:26.309
  May 27 13:22:26.313: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-secrets-b8f3550e-bbcc-4461-90ac-5d9234fe5cfc container secret-env-test: <nil>
  STEP: delete the pod @ 05/27/23 13:22:26.322
  May 27 13:22:26.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9093" for this suite. @ 05/27/23 13:22:26.35
• [4.149 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 05/27/23 13:22:26.359
  May 27 13:22:26.359: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename configmap @ 05/27/23 13:22:26.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:22:26.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:22:26.389
  May 27 13:22:26.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8690" for this suite. @ 05/27/23 13:22:26.45
• [0.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 05/27/23 13:22:26.462
  May 27 13:22:26.462: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename configmap @ 05/27/23 13:22:26.463
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:22:26.483
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:22:26.488
  STEP: Creating configMap with name configmap-test-volume-7668594e-08a2-46a1-bdab-9627b4aaaa59 @ 05/27/23 13:22:26.494
  STEP: Creating a pod to test consume configMaps @ 05/27/23 13:22:26.5
  E0527 13:22:26.540479      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:27.540865      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:28.541699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:29.541817      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:22:30.532
  May 27 13:22:30.537: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-configmaps-37a95e12-0001-4d51-953d-1a44491d88f1 container agnhost-container: <nil>
  E0527 13:22:30.542212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 05/27/23 13:22:30.545
  May 27 13:22:30.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9091" for this suite. @ 05/27/23 13:22:30.575
• [4.123 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 05/27/23 13:22:30.585
  May 27 13:22:30.585: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl @ 05/27/23 13:22:30.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:22:30.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:22:30.611
  STEP: validating api versions @ 05/27/23 13:22:30.616
  May 27 13:22:30.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-2392 api-versions'
  May 27 13:22:30.692: INFO: stderr: ""
  May 27 13:22:30.692: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  May 27 13:22:30.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2392" for this suite. @ 05/27/23 13:22:30.698
• [0.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 05/27/23 13:22:30.709
  May 27 13:22:30.709: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-probe @ 05/27/23 13:22:30.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:22:30.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:22:30.736
  STEP: Creating pod test-webserver-58bb6b52-39a8-4e9a-bae5-c0c1ecea944d in namespace container-probe-130 @ 05/27/23 13:22:30.745
  E0527 13:22:31.542379      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:32.542985      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:22:32.770: INFO: Started pod test-webserver-58bb6b52-39a8-4e9a-bae5-c0c1ecea944d in namespace container-probe-130
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/27/23 13:22:32.77
  May 27 13:22:32.776: INFO: Initial restart count of pod test-webserver-58bb6b52-39a8-4e9a-bae5-c0c1ecea944d is 0
  E0527 13:22:33.543239      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:34.543362      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:35.543495      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:36.543615      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:37.543844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:38.543925      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:39.544726      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:40.544875      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:41.544963      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:42.545536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:43.545633      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:44.545735      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:45.545867      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:46.545959      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:47.546663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:48.546791      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:49.546870      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:50.547004      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:51.547209      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:52.547310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:53.547892      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:54.548028      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:55.549137      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:56.549414      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:57.549499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:58.550070      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:22:59.551041      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:00.551208      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:01.552140      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:02.552243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:03.552365      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:04.552658      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:05.552785      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:06.552882      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:07.553703      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:08.553945      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:09.554553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:10.554649      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:11.555206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:12.555317      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:13.555966      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:14.556215      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:15.557017      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:16.557116      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:17.558059      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:18.558171      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:19.559010      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:20.559387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:21.560306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:22.560737      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:23.561545      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:24.561785      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:25.561896      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:26.562180      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:27.563173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:28.563223      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:29.564295      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:30.564396      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:31.565344      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:32.565464      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:33.565557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:34.565663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:35.565767      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:36.565865      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:37.566609      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:38.566837      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:39.566952      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:40.567231      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:41.568293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:42.568598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:43.568690      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:44.568801      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:45.569312      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:46.569412      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:47.570384      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:48.570533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:49.570627      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:50.570733      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:51.571217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:52.572296      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:53.573243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:54.573347      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:55.573479      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:56.573579      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:57.574320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:58.574431      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:23:59.574491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:00.574707      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:01.574805      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:02.574913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:03.575278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:04.576298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:05.576395      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:06.576517      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:07.576619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:08.576732      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:09.576848      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:10.576950      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:11.577058      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:12.577175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:13.577286      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:14.578082      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:15.578182      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:16.578285      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:17.578395      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:18.578512      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:19.578876      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:20.579054      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:21.579522      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:22.579842      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:23.580111      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:24.580931      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:25.581028      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:26.581137      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:27.581517      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:28.581627      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:29.582444      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:30.582571      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:31.583322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:32.583414      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:33.584084      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:34.584187      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:35.584249      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:36.584339      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:37.585019      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:38.585224      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:39.585331      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:40.585451      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:41.586280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:42.586638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:43.587316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:44.598914      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:45.598912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:46.599194      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:47.599996      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:48.600144      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:49.601039      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:50.602048      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:51.602276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:52.602670      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:53.602938      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:54.603886      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:55.604815      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:56.604931      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:57.605743      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:58.605885      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:24:59.606070      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:00.606331      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:01.606967      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:02.607211      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:03.607400      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:04.607493      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:05.607845      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:06.608094      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:07.609035      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:08.610047      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:09.610194      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:10.610265      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:11.611336      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:12.611745      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:13.612102      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:14.612412      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:15.613297      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:16.613554      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:17.614372      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:18.614677      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:19.614842      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:20.615220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:21.615955      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:22.616928      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:23.617512      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:24.617612      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:25.618396      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:26.618501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:27.618844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:28.619600      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:29.619719      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:30.620470      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:31.621182      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:32.621275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:33.621372      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:34.621519      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:35.621607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:36.621723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:37.622493      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:38.622577      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:39.623705      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:40.624285      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:41.624485      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:42.624606      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:43.624781      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:44.624889      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:45.625002      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:46.625101      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:47.625954      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:48.626062      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:49.627143      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:50.627217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:51.627337      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:52.627692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:53.628203      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:54.628358      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:55.629302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:56.629414      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:57.630252      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:58.630359      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:25:59.631206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:00.632282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:01.632917      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:02.633530      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:03.634131      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:04.634434      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:05.635020      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:06.635176      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:07.635653      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:08.635758      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:09.636289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:10.636386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:11.637451      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:12.638429      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:13.638522      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:14.638633      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:15.639548      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:16.639641      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:17.640151      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:18.640381      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:19.641292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:20.641393      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:21.641969      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:22.642071      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:23.642435      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:24.642562      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:25.643219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:26.644283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:27.644385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:28.644510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:29.645554      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:30.645663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:31.646736      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:32.646849      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:26:33.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/27/23 13:26:33.421
  STEP: Destroying namespace "container-probe-130" for this suite. @ 05/27/23 13:26:33.44
• [242.741 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 05/27/23 13:26:33.454
  May 27 13:26:33.454: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename configmap @ 05/27/23 13:26:33.455
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:26:33.478
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:26:33.484
  STEP: Creating configMap with name cm-test-opt-del-13263e0d-8e1a-4a1b-a469-40fa3b47823c @ 05/27/23 13:26:33.493
  STEP: Creating configMap with name cm-test-opt-upd-93e8d5cc-b3ac-4b27-8d39-a7e84f3f9188 @ 05/27/23 13:26:33.5
  STEP: Creating the pod @ 05/27/23 13:26:33.505
  E0527 13:26:33.647735      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:34.647884      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-13263e0d-8e1a-4a1b-a469-40fa3b47823c @ 05/27/23 13:26:35.578
  STEP: Updating configmap cm-test-opt-upd-93e8d5cc-b3ac-4b27-8d39-a7e84f3f9188 @ 05/27/23 13:26:35.587
  STEP: Creating configMap with name cm-test-opt-create-c93b7bd4-a7f7-4062-95a7-5d0cf06f359e @ 05/27/23 13:26:35.594
  STEP: waiting to observe update in volume @ 05/27/23 13:26:35.602
  E0527 13:26:35.648327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:36.648426      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:37.648895      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:38.649020      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:39.649484      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:40.649787      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:41.649991      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:42.650127      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:43.651189      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:44.651836      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:45.652334      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:46.652643      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:47.652929      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:48.653287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:49.653416      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:50.653553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:51.653682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:52.654427      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:53.654556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:54.654645      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:55.654759      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:56.655255      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:57.655917      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:58.656022      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:26:59.656130      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:00.656243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:01.656407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:02.656376      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:03.656491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:04.656814      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:05.657138      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:06.657221      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:07.657395      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:08.658467      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:09.658601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:10.658627      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:11.658728      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:12.659660      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:13.659808      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:14.660279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:15.660411      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:16.661381      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:17.661494      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:18.662584      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:19.662746      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:20.663362      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:21.664304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:22.665013      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:23.665212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:24.665405      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:25.665517      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:26.666213      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:27.666857      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:28.667890      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:29.668738      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:30.669636      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:31.669751      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:32.670616      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:33.670714      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:34.671234      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:35.671466      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:36.671579      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:37.672086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:38.672181      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:39.672279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:27:39.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3285" for this suite. @ 05/27/23 13:27:39.981
• [66.534 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 05/27/23 13:27:39.992
  May 27 13:27:39.992: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 13:27:39.993
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:27:40.016
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:27:40.021
  STEP: Creating projection with secret that has name projected-secret-test-map-8f2f193d-4d7a-4300-ac55-d587d0ae1f62 @ 05/27/23 13:27:40.026
  STEP: Creating a pod to test consume secrets @ 05/27/23 13:27:40.032
  E0527 13:27:40.673245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:41.673348      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:42.674004      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:43.674231      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:27:44.07
  May 27 13:27:44.074: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-projected-secrets-bdfa9033-9603-46ca-beb1-3c5fd8eff8cb container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/27/23 13:27:44.084
  May 27 13:27:44.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5225" for this suite. @ 05/27/23 13:27:44.111
• [4.127 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 05/27/23 13:27:44.121
  May 27 13:27:44.121: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 13:27:44.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:27:44.143
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:27:44.149
  STEP: Setting up server cert @ 05/27/23 13:27:44.181
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 13:27:44.562
  STEP: Deploying the webhook pod @ 05/27/23 13:27:44.572
  STEP: Wait for the deployment to be ready @ 05/27/23 13:27:44.589
  May 27 13:27:44.598: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0527 13:27:44.674857      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:45.675045      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/27/23 13:27:46.61
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 13:27:46.622
  E0527 13:27:46.675486      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:27:47.623: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 27 13:27:47.629: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  E0527 13:27:47.675821      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9533-crds.webhook.example.com via the AdmissionRegistration API @ 05/27/23 13:27:48.15
  STEP: Creating a custom resource while v1 is storage version @ 05/27/23 13:27:48.217
  E0527 13:27:48.676517      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:49.676945      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 05/27/23 13:27:50.335
  STEP: Patching the custom resource while v2 is storage version @ 05/27/23 13:27:50.355
  May 27 13:27:50.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0527 13:27:50.677719      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-6620" for this suite. @ 05/27/23 13:27:51.048
  STEP: Destroying namespace "webhook-markers-3995" for this suite. @ 05/27/23 13:27:51.056
• [6.942 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 05/27/23 13:27:51.067
  May 27 13:27:51.067: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename var-expansion @ 05/27/23 13:27:51.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:27:51.086
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:27:51.092
  STEP: Creating a pod to test substitution in container's command @ 05/27/23 13:27:51.097
  E0527 13:27:51.677872      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:52.677948      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:53.678092      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:54.678423      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:27:55.123
  May 27 13:27:55.129: INFO: Trying to get logs from node ip-172-31-68-172 pod var-expansion-baa0824d-b40d-44a3-93ee-1d15ddd8e147 container dapi-container: <nil>
  STEP: delete the pod @ 05/27/23 13:27:55.136
  May 27 13:27:55.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-310" for this suite. @ 05/27/23 13:27:55.162
• [4.105 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 05/27/23 13:27:55.172
  May 27 13:27:55.172: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir @ 05/27/23 13:27:55.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:27:55.19
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:27:55.197
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/27/23 13:27:55.201
  E0527 13:27:55.679122      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:56.679241      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:57.679538      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:27:58.680568      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:27:59.235
  May 27 13:27:59.240: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-4c11286d-572a-45c9-8e73-d1f2b54a8c46 container test-container: <nil>
  STEP: delete the pod @ 05/27/23 13:27:59.248
  May 27 13:27:59.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1513" for this suite. @ 05/27/23 13:27:59.273
• [4.111 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 05/27/23 13:27:59.285
  May 27 13:27:59.285: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-runtime @ 05/27/23 13:27:59.286
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:27:59.305
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:27:59.311
  STEP: create the container @ 05/27/23 13:27:59.316
  W0527 13:27:59.326970      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/27/23 13:27:59.327
  E0527 13:27:59.680697      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:00.681625      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:01.682499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/27/23 13:28:02.351
  STEP: the container should be terminated @ 05/27/23 13:28:02.356
  STEP: the termination message should be set @ 05/27/23 13:28:02.356
  May 27 13:28:02.356: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 05/27/23 13:28:02.356
  May 27 13:28:02.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3289" for this suite. @ 05/27/23 13:28:02.382
• [3.108 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 05/27/23 13:28:02.393
  May 27 13:28:02.393: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename security-context-test @ 05/27/23 13:28:02.395
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:28:02.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:28:02.421
  E0527 13:28:02.683453      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:03.683528      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:04.683650      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:05.683827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:28:06.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-2814" for this suite. @ 05/27/23 13:28:06.457
• [4.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 05/27/23 13:28:06.469
  May 27 13:28:06.469: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename services @ 05/27/23 13:28:06.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:28:06.488
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:28:06.496
  STEP: creating service in namespace services-9912 @ 05/27/23 13:28:06.5
  STEP: creating service affinity-nodeport-transition in namespace services-9912 @ 05/27/23 13:28:06.5
  STEP: creating replication controller affinity-nodeport-transition in namespace services-9912 @ 05/27/23 13:28:06.531
  I0527 13:28:06.542221      18 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-9912, replica count: 3
  E0527 13:28:06.684146      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:07.684638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:08.685267      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0527 13:28:09.593540      18 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 27 13:28:09.607: INFO: Creating new exec pod
  E0527 13:28:09.685873      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:10.686036      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:11.686418      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:28:12.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-9912 exec execpod-affinitym6zs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  E0527 13:28:12.689633      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:28:12.794: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  May 27 13:28:12.794: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 13:28:12.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-9912 exec execpod-affinitym6zs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.147 80'
  May 27 13:28:12.957: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.147 80\nConnection to 10.152.183.147 80 port [tcp/http] succeeded!\n"
  May 27 13:28:12.957: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 13:28:12.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-9912 exec execpod-affinitym6zs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.22.3 32336'
  May 27 13:28:13.117: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.22.3 32336\nConnection to 172.31.22.3 32336 port [tcp/*] succeeded!\n"
  May 27 13:28:13.117: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 13:28:13.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-9912 exec execpod-affinitym6zs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.10.136 32336'
  May 27 13:28:13.271: INFO: stderr: "+ nc -v -t -w 2 172.31.10.136 32336\n+ echo hostName\nConnection to 172.31.10.136 32336 port [tcp/*] succeeded!\n"
  May 27 13:28:13.271: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 13:28:13.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-9912 exec execpod-affinitym6zs5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.10.136:32336/ ; done'
  May 27 13:28:13.554: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n"
  May 27 13:28:13.554: INFO: stdout: "\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-k6dh8\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-k6dh8\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-k6dh8\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-k6dh8\naffinity-nodeport-transition-fx92g\naffinity-nodeport-transition-fx92g\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-fx92g\naffinity-nodeport-transition-k6dh8\naffinity-nodeport-transition-fx92g\naffinity-nodeport-transition-fx92g\naffinity-nodeport-transition-k6dh8"
  May 27 13:28:13.554: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.554: INFO: Received response from host: affinity-nodeport-transition-k6dh8
  May 27 13:28:13.554: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.554: INFO: Received response from host: affinity-nodeport-transition-k6dh8
  May 27 13:28:13.554: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.554: INFO: Received response from host: affinity-nodeport-transition-k6dh8
  May 27 13:28:13.554: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.554: INFO: Received response from host: affinity-nodeport-transition-k6dh8
  May 27 13:28:13.554: INFO: Received response from host: affinity-nodeport-transition-fx92g
  May 27 13:28:13.554: INFO: Received response from host: affinity-nodeport-transition-fx92g
  May 27 13:28:13.554: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.554: INFO: Received response from host: affinity-nodeport-transition-fx92g
  May 27 13:28:13.554: INFO: Received response from host: affinity-nodeport-transition-k6dh8
  May 27 13:28:13.554: INFO: Received response from host: affinity-nodeport-transition-fx92g
  May 27 13:28:13.554: INFO: Received response from host: affinity-nodeport-transition-fx92g
  May 27 13:28:13.554: INFO: Received response from host: affinity-nodeport-transition-k6dh8
  May 27 13:28:13.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-9912 exec execpod-affinitym6zs5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.10.136:32336/ ; done'
  E0527 13:28:13.690222      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:28:13.837: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.10.136:32336/\n"
  May 27 13:28:13.837: INFO: stdout: "\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-d6qdb\naffinity-nodeport-transition-d6qdb"
  May 27 13:28:13.837: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.837: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.837: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.837: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.837: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.837: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.837: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.837: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.837: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.837: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.837: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.837: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.837: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.837: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.837: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.837: INFO: Received response from host: affinity-nodeport-transition-d6qdb
  May 27 13:28:13.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 27 13:28:13.842: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9912, will wait for the garbage collector to delete the pods @ 05/27/23 13:28:13.855
  May 27 13:28:13.920: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.096638ms
  May 27 13:28:14.020: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.617581ms
  E0527 13:28:14.690479      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:15.691522      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-9912" for this suite. @ 05/27/23 13:28:16.459
• [9.999 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 05/27/23 13:28:16.471
  May 27 13:28:16.471: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename events @ 05/27/23 13:28:16.472
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:28:16.493
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:28:16.497
  STEP: creating a test event @ 05/27/23 13:28:16.502
  STEP: listing events in all namespaces @ 05/27/23 13:28:16.511
  STEP: listing events in test namespace @ 05/27/23 13:28:16.518
  STEP: listing events with field selection filtering on source @ 05/27/23 13:28:16.522
  STEP: listing events with field selection filtering on reportingController @ 05/27/23 13:28:16.526
  STEP: getting the test event @ 05/27/23 13:28:16.53
  STEP: patching the test event @ 05/27/23 13:28:16.535
  STEP: getting the test event @ 05/27/23 13:28:16.546
  STEP: updating the test event @ 05/27/23 13:28:16.551
  STEP: getting the test event @ 05/27/23 13:28:16.559
  STEP: deleting the test event @ 05/27/23 13:28:16.564
  STEP: listing events in all namespaces @ 05/27/23 13:28:16.576
  STEP: listing events in test namespace @ 05/27/23 13:28:16.58
  May 27 13:28:16.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-3576" for this suite. @ 05/27/23 13:28:16.592
• [0.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 05/27/23 13:28:16.602
  May 27 13:28:16.602: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename pods @ 05/27/23 13:28:16.603
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:28:16.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:28:16.638
  STEP: creating the pod @ 05/27/23 13:28:16.644
  STEP: submitting the pod to kubernetes @ 05/27/23 13:28:16.644
  STEP: verifying QOS class is set on the pod @ 05/27/23 13:28:16.657
  May 27 13:28:16.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7702" for this suite. @ 05/27/23 13:28:16.67
• [0.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 05/27/23 13:28:16.68
  May 27 13:28:16.680: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename field-validation @ 05/27/23 13:28:16.681
  E0527 13:28:16.691833      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:28:16.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:28:16.717
  STEP: apply creating a deployment @ 05/27/23 13:28:16.722
  May 27 13:28:16.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-600" for this suite. @ 05/27/23 13:28:16.748
• [0.076 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 05/27/23 13:28:16.756
  May 27 13:28:16.756: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename services @ 05/27/23 13:28:16.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:28:16.775
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:28:16.779
  STEP: creating service endpoint-test2 in namespace services-7501 @ 05/27/23 13:28:16.784
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7501 to expose endpoints map[] @ 05/27/23 13:28:16.794
  May 27 13:28:16.800: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E0527 13:28:17.692549      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:28:17.811: INFO: successfully validated that service endpoint-test2 in namespace services-7501 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-7501 @ 05/27/23 13:28:17.812
  E0527 13:28:18.693294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:19.693454      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7501 to expose endpoints map[pod1:[80]] @ 05/27/23 13:28:19.843
  May 27 13:28:19.858: INFO: successfully validated that service endpoint-test2 in namespace services-7501 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 05/27/23 13:28:19.858
  May 27 13:28:19.858: INFO: Creating new exec pod
  E0527 13:28:20.693526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:21.693642      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:22.693844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:28:22.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-7501 exec execpodntklk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 27 13:28:23.034: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 27 13:28:23.034: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 13:28:23.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-7501 exec execpodntklk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.223 80'
  May 27 13:28:23.204: INFO: stderr: "+ nc -v -t -w 2 10.152.183.223 80\nConnection to 10.152.183.223 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  May 27 13:28:23.204: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-7501 @ 05/27/23 13:28:23.205
  E0527 13:28:23.694247      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:24.694343      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7501 to expose endpoints map[pod1:[80] pod2:[80]] @ 05/27/23 13:28:25.229
  May 27 13:28:25.247: INFO: successfully validated that service endpoint-test2 in namespace services-7501 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 05/27/23 13:28:25.247
  E0527 13:28:25.695498      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:28:26.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-7501 exec execpodntklk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 27 13:28:26.406: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 27 13:28:26.406: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 13:28:26.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-7501 exec execpodntklk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.223 80'
  May 27 13:28:26.577: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.223 80\nConnection to 10.152.183.223 80 port [tcp/http] succeeded!\n"
  May 27 13:28:26.577: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-7501 @ 05/27/23 13:28:26.577
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7501 to expose endpoints map[pod2:[80]] @ 05/27/23 13:28:26.6
  E0527 13:28:26.696538      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:28:27.626: INFO: successfully validated that service endpoint-test2 in namespace services-7501 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 05/27/23 13:28:27.626
  E0527 13:28:27.697175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:28:28.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-7501 exec execpodntklk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0527 13:28:28.697515      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:28:28.796: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 27 13:28:28.796: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 13:28:28.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-7501 exec execpodntklk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.223 80'
  May 27 13:28:28.970: INFO: stderr: "+ nc -v -t -w 2 10.152.183.223 80\n+ echo hostName\nConnection to 10.152.183.223 80 port [tcp/http] succeeded!\n"
  May 27 13:28:28.970: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-7501 @ 05/27/23 13:28:28.97
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7501 to expose endpoints map[] @ 05/27/23 13:28:28.987
  E0527 13:28:29.698433      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:28:30.012: INFO: successfully validated that service endpoint-test2 in namespace services-7501 exposes endpoints map[]
  May 27 13:28:30.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7501" for this suite. @ 05/27/23 13:28:30.044
• [13.299 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 05/27/23 13:28:30.06
  May 27 13:28:30.060: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename field-validation @ 05/27/23 13:28:30.062
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:28:30.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:28:30.085
  STEP: apply creating a deployment @ 05/27/23 13:28:30.091
  May 27 13:28:30.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8640" for this suite. @ 05/27/23 13:28:30.117
• [0.067 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 05/27/23 13:28:30.128
  May 27 13:28:30.128: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename resourcequota @ 05/27/23 13:28:30.129
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:28:30.151
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:28:30.156
  STEP: Creating a ResourceQuota with terminating scope @ 05/27/23 13:28:30.161
  STEP: Ensuring ResourceQuota status is calculated @ 05/27/23 13:28:30.169
  E0527 13:28:30.698536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:31.698620      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 05/27/23 13:28:32.174
  STEP: Ensuring ResourceQuota status is calculated @ 05/27/23 13:28:32.181
  E0527 13:28:32.699685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:33.700437      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 05/27/23 13:28:34.186
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 05/27/23 13:28:34.204
  E0527 13:28:34.701056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:35.701513      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 05/27/23 13:28:36.209
  E0527 13:28:36.702274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:37.702360      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/27/23 13:28:38.216
  STEP: Ensuring resource quota status released the pod usage @ 05/27/23 13:28:38.228
  E0527 13:28:38.702481      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:39.702614      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 05/27/23 13:28:40.232
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 05/27/23 13:28:40.248
  E0527 13:28:40.703116      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:41.703340      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 05/27/23 13:28:42.254
  E0527 13:28:42.703445      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:43.703599      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/27/23 13:28:44.259
  STEP: Ensuring resource quota status released the pod usage @ 05/27/23 13:28:44.274
  E0527 13:28:44.703660      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:45.703793      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:28:46.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7387" for this suite. @ 05/27/23 13:28:46.286
• [16.169 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 05/27/23 13:28:46.302
  May 27 13:28:46.302: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-probe @ 05/27/23 13:28:46.303
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:28:46.324
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:28:46.33
  E0527 13:28:46.704402      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:47.705189      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:48.706022      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:49.706101      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:50.706202      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:51.706350      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:52.707203      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:53.707336      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:54.707785      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:55.708473      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:56.708910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:57.709606      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:58.710532      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:28:59.710658      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:00.711033      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:01.711351      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:02.711501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:03.711616      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:04.711969      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:05.712055      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:06.712212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:07.712311      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:29:08.421: INFO: Container started at 2023-05-27 13:28:47 +0000 UTC, pod became ready at 2023-05-27 13:29:06 +0000 UTC
  May 27 13:29:08.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-4" for this suite. @ 05/27/23 13:29:08.427
• [22.134 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 05/27/23 13:29:08.437
  May 27 13:29:08.437: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename downward-api @ 05/27/23 13:29:08.438
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:29:08.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:29:08.465
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 13:29:08.47
  E0527 13:29:08.713258      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:09.713388      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:10.713793      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:11.713904      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:29:12.5
  May 27 13:29:12.505: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-a6043f79-0de3-4f0a-a0ef-d624f895934d container client-container: <nil>
  STEP: delete the pod @ 05/27/23 13:29:12.513
  May 27 13:29:12.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6708" for this suite. @ 05/27/23 13:29:12.536
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 05/27/23 13:29:12.549
  May 27 13:29:12.549: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/27/23 13:29:12.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:29:12.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:29:12.573
  May 27 13:29:12.579: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  E0527 13:29:12.714205      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:13.714669      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/27/23 13:29:14.196
  May 27 13:29:14.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-6050 --namespace=crd-publish-openapi-6050 create -f -'
  E0527 13:29:14.719371      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:29:14.925: INFO: stderr: ""
  May 27 13:29:14.925: INFO: stdout: "e2e-test-crd-publish-openapi-6784-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May 27 13:29:14.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-6050 --namespace=crd-publish-openapi-6050 delete e2e-test-crd-publish-openapi-6784-crds test-cr'
  May 27 13:29:15.012: INFO: stderr: ""
  May 27 13:29:15.012: INFO: stdout: "e2e-test-crd-publish-openapi-6784-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  May 27 13:29:15.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-6050 --namespace=crd-publish-openapi-6050 apply -f -'
  E0527 13:29:15.720208      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:29:15.821: INFO: stderr: ""
  May 27 13:29:15.821: INFO: stdout: "e2e-test-crd-publish-openapi-6784-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May 27 13:29:15.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-6050 --namespace=crd-publish-openapi-6050 delete e2e-test-crd-publish-openapi-6784-crds test-cr'
  May 27 13:29:15.920: INFO: stderr: ""
  May 27 13:29:15.920: INFO: stdout: "e2e-test-crd-publish-openapi-6784-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/27/23 13:29:15.92
  May 27 13:29:15.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-6050 explain e2e-test-crd-publish-openapi-6784-crds'
  May 27 13:29:16.166: INFO: stderr: ""
  May 27 13:29:16.166: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-6784-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0527 13:29:16.721221      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:29:17.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6050" for this suite. @ 05/27/23 13:29:17.62
• [5.082 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 05/27/23 13:29:17.631
  May 27 13:29:17.631: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir @ 05/27/23 13:29:17.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:29:17.656
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:29:17.661
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/27/23 13:29:17.667
  E0527 13:29:17.722282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:18.723193      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:19.723502      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:20.723649      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:29:21.699
  May 27 13:29:21.703: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-beaf735d-d933-4c3b-8d74-87acf944709a container test-container: <nil>
  STEP: delete the pod @ 05/27/23 13:29:21.71
  E0527 13:29:21.724056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:29:21.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5654" for this suite. @ 05/27/23 13:29:21.736
• [4.113 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 05/27/23 13:29:21.747
  May 27 13:29:21.748: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename statefulset @ 05/27/23 13:29:21.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:29:21.768
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:29:21.773
  STEP: Creating service test in namespace statefulset-4275 @ 05/27/23 13:29:21.777
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 05/27/23 13:29:21.789
  STEP: Creating stateful set ss in namespace statefulset-4275 @ 05/27/23 13:29:21.796
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4275 @ 05/27/23 13:29:21.805
  May 27 13:29:21.809: INFO: Found 0 stateful pods, waiting for 1
  E0527 13:29:22.724368      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:23.724345      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:24.724509      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:25.724574      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:26.724727      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:27.725141      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:28.725248      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:29.725458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:30.725528      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:31.725662      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:29:31.816: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 05/27/23 13:29:31.817
  May 27 13:29:31.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-4275 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 27 13:29:31.983: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 27 13:29:31.983: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 27 13:29:31.983: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 27 13:29:31.988: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0527 13:29:32.726694      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:33.726819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:34.726910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:35.727036      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:36.727328      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:37.727477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:38.727566      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:39.727638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:40.727770      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:41.727932      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:29:41.994: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 27 13:29:41.994: INFO: Waiting for statefulset status.replicas updated to 0
  May 27 13:29:42.018: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999978s
  E0527 13:29:42.728026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:29:43.023: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993302029s
  E0527 13:29:43.728112      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:29:44.029: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988454726s
  E0527 13:29:44.728228      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:29:45.034: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.982242115s
  E0527 13:29:45.728951      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:29:46.038: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.977534926s
  E0527 13:29:46.729539      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:29:47.043: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.972727604s
  E0527 13:29:47.730585      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:29:48.048: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.96780022s
  E0527 13:29:48.731547      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:29:49.053: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.962740023s
  E0527 13:29:49.732448      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:29:50.059: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.957901236s
  E0527 13:29:50.732494      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:29:51.064: INFO: Verifying statefulset ss doesn't scale past 1 for another 952.050232ms
  E0527 13:29:51.732847      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4275 @ 05/27/23 13:29:52.065
  May 27 13:29:52.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-4275 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 27 13:29:52.272: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 27 13:29:52.272: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 27 13:29:52.272: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 27 13:29:52.279: INFO: Found 1 stateful pods, waiting for 3
  E0527 13:29:52.733603      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:53.734135      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:54.734745      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:55.734982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:56.735388      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:57.735473      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:58.735603      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:29:59.735692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:00.735817      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:01.735929      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:30:02.284: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 27 13:30:02.284: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May 27 13:30:02.284: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 05/27/23 13:30:02.284
  STEP: Scale down will halt with unhealthy stateful pod @ 05/27/23 13:30:02.284
  May 27 13:30:02.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-4275 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 27 13:30:02.469: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 27 13:30:02.469: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 27 13:30:02.469: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 27 13:30:02.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-4275 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 27 13:30:02.638: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 27 13:30:02.638: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 27 13:30:02.638: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 27 13:30:02.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-4275 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0527 13:30:02.736599      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:30:02.820: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 27 13:30:02.820: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 27 13:30:02.820: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 27 13:30:02.820: INFO: Waiting for statefulset status.replicas updated to 0
  May 27 13:30:02.825: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0527 13:30:03.737473      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:04.737590      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:05.737717      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:06.737810      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:07.737897      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:08.737988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:09.738111      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:10.738237      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:11.738334      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:12.738643      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:30:12.844: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 27 13:30:12.844: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May 27 13:30:12.844: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May 27 13:30:12.870: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999962s
  E0527 13:30:13.739362      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:30:13.876: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98999032s
  E0527 13:30:14.740360      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:30:14.881: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984055974s
  E0527 13:30:15.740687      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:30:15.889: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977480466s
  E0527 13:30:16.740886      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:30:16.898: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.97008744s
  E0527 13:30:17.741783      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:30:17.904: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.962232245s
  E0527 13:30:18.742140      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:30:18.909: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.956207899s
  E0527 13:30:19.742206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:30:19.915: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.950131621s
  E0527 13:30:20.742834      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:30:20.922: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.944314359s
  E0527 13:30:21.742882      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:30:21.927: INFO: Verifying statefulset ss doesn't scale past 3 for another 938.217729ms
  E0527 13:30:22.743344      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4275 @ 05/27/23 13:30:22.927
  May 27 13:30:22.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-4275 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 27 13:30:23.100: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 27 13:30:23.100: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 27 13:30:23.100: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 27 13:30:23.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-4275 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 27 13:30:23.283: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 27 13:30:23.283: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 27 13:30:23.283: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 27 13:30:23.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-4275 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 27 13:30:23.449: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 27 13:30:23.449: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 27 13:30:23.449: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 27 13:30:23.449: INFO: Scaling statefulset ss to 0
  E0527 13:30:23.743420      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:24.744107      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:25.745103      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:26.745516      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:27.746320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:28.746413      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:29.746500      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:30.746682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:31.747056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:32.747301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 05/27/23 13:30:33.472
  May 27 13:30:33.472: INFO: Deleting all statefulset in ns statefulset-4275
  May 27 13:30:33.476: INFO: Scaling statefulset ss to 0
  May 27 13:30:33.490: INFO: Waiting for statefulset status.replicas updated to 0
  May 27 13:30:33.494: INFO: Deleting statefulset ss
  May 27 13:30:33.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4275" for this suite. @ 05/27/23 13:30:33.518
• [71.780 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 05/27/23 13:30:33.529
  May 27 13:30:33.529: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl @ 05/27/23 13:30:33.53
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:30:33.549
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:30:33.554
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/27/23 13:30:33.562
  May 27 13:30:33.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6481 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May 27 13:30:33.654: INFO: stderr: ""
  May 27 13:30:33.654: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 05/27/23 13:30:33.654
  E0527 13:30:33.747743      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:34.747814      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:35.748549      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:36.749595      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:37.750080      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/27/23 13:30:38.704
  May 27 13:30:38.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6481 get pod e2e-test-httpd-pod -o json'
  E0527 13:30:38.750399      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:30:38.792: INFO: stderr: ""
  May 27 13:30:38.792: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-05-27T13:30:33Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6481\",\n        \"resourceVersion\": \"32387\",\n        \"uid\": \"91e9119b-942d-4274-9b85-7682f8d01585\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-fv246\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-68-172\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-fv246\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-27T13:30:33Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-27T13:30:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-27T13:30:35Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-27T13:30:33Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://6550257f998ad8e44e303460f7a7180a1b996e925260fe7c4096fe705b4fe478\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-27T13:30:34Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.68.172\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.19.104\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.19.104\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-27T13:30:33Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 05/27/23 13:30:38.792
  May 27 13:30:38.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6481 replace -f -'
  May 27 13:30:39.145: INFO: stderr: ""
  May 27 13:30:39.145: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 05/27/23 13:30:39.145
  May 27 13:30:39.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6481 delete pods e2e-test-httpd-pod'
  E0527 13:30:39.751411      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:40.751461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:30:41.239: INFO: stderr: ""
  May 27 13:30:41.239: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 27 13:30:41.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6481" for this suite. @ 05/27/23 13:30:41.246
• [7.727 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 05/27/23 13:30:41.257
  May 27 13:30:41.257: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/27/23 13:30:41.258
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:30:41.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:30:41.288
  STEP: set up a multi version CRD @ 05/27/23 13:30:41.295
  May 27 13:30:41.296: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  E0527 13:30:41.752302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:42.753304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:43.754029      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:44.754077      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 05/27/23 13:30:45.13
  STEP: check the unserved version gets removed @ 05/27/23 13:30:45.156
  E0527 13:30:45.754558      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 05/27/23 13:30:46.629
  E0527 13:30:46.754722      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:47.755261      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:48.755769      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:49.755957      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:30:50.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-640" for this suite. @ 05/27/23 13:30:50.114
• [8.864 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 05/27/23 13:30:50.121
  May 27 13:30:50.121: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename resourcequota @ 05/27/23 13:30:50.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:30:50.144
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:30:50.147
  STEP: Creating a ResourceQuota with best effort scope @ 05/27/23 13:30:50.151
  STEP: Ensuring ResourceQuota status is calculated @ 05/27/23 13:30:50.158
  E0527 13:30:50.756005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:51.756403      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 05/27/23 13:30:52.162
  STEP: Ensuring ResourceQuota status is calculated @ 05/27/23 13:30:52.168
  E0527 13:30:52.757315      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:53.757395      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 05/27/23 13:30:54.174
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 05/27/23 13:30:54.188
  E0527 13:30:54.758432      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:55.758781      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 05/27/23 13:30:56.194
  E0527 13:30:56.759275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:57.760260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/27/23 13:30:58.199
  STEP: Ensuring resource quota status released the pod usage @ 05/27/23 13:30:58.215
  E0527 13:30:58.761302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:30:59.761429      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 05/27/23 13:31:00.219
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 05/27/23 13:31:00.234
  E0527 13:31:00.761454      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:01.761728      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 05/27/23 13:31:02.239
  E0527 13:31:02.762702      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:03.763233      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/27/23 13:31:04.246
  STEP: Ensuring resource quota status released the pod usage @ 05/27/23 13:31:04.264
  E0527 13:31:04.763260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:05.763369      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:31:06.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5170" for this suite. @ 05/27/23 13:31:06.275
• [16.163 seconds]
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 05/27/23 13:31:06.284
  May 27 13:31:06.284: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename deployment @ 05/27/23 13:31:06.285
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:31:06.305
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:31:06.308
  May 27 13:31:06.322: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0527 13:31:06.763479      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:07.764168      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:08.764247      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:09.764342      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:10.764694      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:31:11.326: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/27/23 13:31:11.327
  May 27 13:31:11.327: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 05/27/23 13:31:11.339
  May 27 13:31:11.355: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2535  d6a405d9-aafc-44ad-af97-369936442b5a 32586 1 2023-05-27 13:31:11 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-05-27 13:31:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a09418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  May 27 13:31:11.365: INFO: New ReplicaSet "test-cleanup-deployment-68b75d69f8" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-68b75d69f8  deployment-2535  b050e7af-f00f-4b84-ae70-ea5da94db9ec 32588 1 2023-05-27 13:31:11 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment d6a405d9-aafc-44ad-af97-369936442b5a 0xc004ce9477 0xc004ce9478}] [] [{kube-controller-manager Update apps/v1 2023-05-27 13:31:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6a405d9-aafc-44ad-af97-369936442b5a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 68b75d69f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ce9518 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 27 13:31:11.365: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  May 27 13:31:11.365: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2535  b83a95f5-d2be-4f9b-b9dc-ad27f2ce75be 32587 1 2023-05-27 13:31:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment d6a405d9-aafc-44ad-af97-369936442b5a 0xc004ce9347 0xc004ce9348}] [] [{e2e.test Update apps/v1 2023-05-27 13:31:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 13:31:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-27 13:31:11 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"d6a405d9-aafc-44ad-af97-369936442b5a\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004ce9408 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 27 13:31:11.369: INFO: Pod "test-cleanup-controller-2dcfx" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-2dcfx test-cleanup-controller- deployment-2535  0d77238a-314c-4fc0-8568-79c2bb33643c 32573 0 2023-05-27 13:31:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller b83a95f5-d2be-4f9b-b9dc-ad27f2ce75be 0xc004ce9977 0xc004ce9978}] [] [{kube-controller-manager Update v1 2023-05-27 13:31:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b83a95f5-d2be-4f9b-b9dc-ad27f2ce75be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 13:31:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.19.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-clc9k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-clc9k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:31:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:31:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:31:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:31:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.172,PodIP:192.168.19.106,StartTime:2023-05-27 13:31:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-27 13:31:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://70537cc276df0fa7d2b96c9fccdb8895515456fa8efa127b23683325ac7b6cb9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.19.106,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 13:31:11.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2535" for this suite. @ 05/27/23 13:31:11.379
• [5.109 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 05/27/23 13:31:11.395
  May 27 13:31:11.395: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename podtemplate @ 05/27/23 13:31:11.396
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:31:11.423
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:31:11.427
  STEP: Create a pod template @ 05/27/23 13:31:11.431
  STEP: Replace a pod template @ 05/27/23 13:31:11.438
  May 27 13:31:11.448: INFO: Found updated podtemplate annotation: "true"

  May 27 13:31:11.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1545" for this suite. @ 05/27/23 13:31:11.452
• [0.067 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 05/27/23 13:31:11.462
  May 27 13:31:11.463: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename statefulset @ 05/27/23 13:31:11.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:31:11.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:31:11.491
  STEP: Creating service test in namespace statefulset-2495 @ 05/27/23 13:31:11.495
  STEP: Creating stateful set ss in namespace statefulset-2495 @ 05/27/23 13:31:11.506
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2495 @ 05/27/23 13:31:11.515
  May 27 13:31:11.520: INFO: Found 0 stateful pods, waiting for 1
  E0527 13:31:11.764785      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:12.765048      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:13.765183      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:14.765251      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:15.765792      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:16.765910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:17.766742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:18.767229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:19.767283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:20.767409      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:31:21.526: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 05/27/23 13:31:21.526
  May 27 13:31:21.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-2495 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 27 13:31:21.695: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 27 13:31:21.695: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 27 13:31:21.695: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 27 13:31:21.699: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0527 13:31:21.767557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:22.767671      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:23.767790      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:24.767901      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:25.768015      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:26.768114      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:27.768245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:28.768341      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:29.768458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:30.768562      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:31:31.704: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 27 13:31:31.704: INFO: Waiting for statefulset status.replicas updated to 0
  May 27 13:31:31.724: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  May 27 13:31:31.724: INFO: ss-0  ip-172-31-68-172  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:11 +0000 UTC  }]
  May 27 13:31:31.724: INFO: 
  May 27 13:31:31.724: INFO: StatefulSet ss has not reached scale 3, at 1
  E0527 13:31:31.769027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:31:32.731: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994536149s
  E0527 13:31:32.769526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:31:33.735: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988020526s
  E0527 13:31:33.769632      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:31:34.740: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98277549s
  E0527 13:31:34.769873      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:31:35.747: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978074485s
  E0527 13:31:35.770449      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:31:36.753: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971262155s
  E0527 13:31:36.771479      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:31:37.759: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965561739s
  E0527 13:31:37.771997      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:31:38.764: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959487915s
  E0527 13:31:38.772758      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:31:39.768: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.954601356s
  E0527 13:31:39.773161      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:40.773536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:31:40.775: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.448333ms
  E0527 13:31:41.773632      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2495 @ 05/27/23 13:31:41.775
  May 27 13:31:41.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-2495 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 27 13:31:41.932: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 27 13:31:41.932: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 27 13:31:41.932: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 27 13:31:41.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-2495 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 27 13:31:42.092: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May 27 13:31:42.092: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 27 13:31:42.092: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 27 13:31:42.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-2495 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 27 13:31:42.258: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May 27 13:31:42.258: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 27 13:31:42.258: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 27 13:31:42.264: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  E0527 13:31:42.774338      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:43.774445      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:44.774550      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:45.774658      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:46.774768      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:47.775564      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:48.775678      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:49.775793      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:50.775898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:51.776023      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:31:52.271: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 27 13:31:52.271: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May 27 13:31:52.271: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 05/27/23 13:31:52.271
  May 27 13:31:52.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-2495 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 27 13:31:52.432: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 27 13:31:52.432: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 27 13:31:52.432: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 27 13:31:52.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-2495 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 27 13:31:52.593: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 27 13:31:52.593: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 27 13:31:52.593: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 27 13:31:52.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-2495 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 27 13:31:52.751: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 27 13:31:52.751: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 27 13:31:52.751: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 27 13:31:52.751: INFO: Waiting for statefulset status.replicas updated to 0
  May 27 13:31:52.755: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0527 13:31:52.777053      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:53.777337      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:54.777491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:55.777684      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:56.777903      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:57.778084      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:58.778193      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:31:59.778311      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:00.778422      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:01.778531      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:02.765: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 27 13:32:02.765: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May 27 13:32:02.765: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  E0527 13:32:02.778953      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:02.783: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  May 27 13:32:02.783: INFO: ss-0  ip-172-31-68-172  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:11 +0000 UTC  }]
  May 27 13:32:02.783: INFO: ss-1  ip-172-31-10-136  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:31 +0000 UTC  }]
  May 27 13:32:02.783: INFO: ss-2  ip-172-31-22-3    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:31 +0000 UTC  }]
  May 27 13:32:02.783: INFO: 
  May 27 13:32:02.783: INFO: StatefulSet ss has not reached scale 0, at 3
  E0527 13:32:03.779261      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:03.788: INFO: POD   NODE              PHASE      GRACE  CONDITIONS
  May 27 13:32:03.788: INFO: ss-1  ip-172-31-10-136  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:31 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:53 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:53 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-27 13:31:31 +0000 UTC  }]
  May 27 13:32:03.788: INFO: 
  May 27 13:32:03.788: INFO: StatefulSet ss has not reached scale 0, at 1
  E0527 13:32:04.780301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:04.794: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990584303s
  E0527 13:32:05.780441      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:05.799: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.98467553s
  E0527 13:32:06.781382      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:06.803: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.979590883s
  E0527 13:32:07.782315      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:07.808: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.974817973s
  E0527 13:32:08.782478      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:08.812: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.970630356s
  E0527 13:32:09.782556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:09.816: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.966381387s
  E0527 13:32:10.783272      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:10.822: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.961844032s
  E0527 13:32:11.784340      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:11.827: INFO: Verifying statefulset ss doesn't scale past 0 for another 955.788956ms
  E0527 13:32:12.785271      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2495 @ 05/27/23 13:32:12.828
  May 27 13:32:12.832: INFO: Scaling statefulset ss to 0
  May 27 13:32:12.845: INFO: Waiting for statefulset status.replicas updated to 0
  May 27 13:32:12.849: INFO: Deleting all statefulset in ns statefulset-2495
  May 27 13:32:12.852: INFO: Scaling statefulset ss to 0
  May 27 13:32:12.866: INFO: Waiting for statefulset status.replicas updated to 0
  May 27 13:32:12.869: INFO: Deleting statefulset ss
  May 27 13:32:12.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2495" for this suite. @ 05/27/23 13:32:12.891
• [61.437 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 05/27/23 13:32:12.901
  May 27 13:32:12.901: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename svcaccounts @ 05/27/23 13:32:12.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:32:12.923
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:32:12.927
  May 27 13:32:12.955: INFO: created pod pod-service-account-defaultsa
  May 27 13:32:12.955: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  May 27 13:32:12.964: INFO: created pod pod-service-account-mountsa
  May 27 13:32:12.965: INFO: pod pod-service-account-mountsa service account token volume mount: true
  May 27 13:32:12.975: INFO: created pod pod-service-account-nomountsa
  May 27 13:32:12.975: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  May 27 13:32:12.984: INFO: created pod pod-service-account-defaultsa-mountspec
  May 27 13:32:12.984: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  May 27 13:32:12.994: INFO: created pod pod-service-account-mountsa-mountspec
  May 27 13:32:12.995: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  May 27 13:32:13.001: INFO: created pod pod-service-account-nomountsa-mountspec
  May 27 13:32:13.001: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  May 27 13:32:13.012: INFO: created pod pod-service-account-defaultsa-nomountspec
  May 27 13:32:13.012: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  May 27 13:32:13.022: INFO: created pod pod-service-account-mountsa-nomountspec
  May 27 13:32:13.022: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  May 27 13:32:13.033: INFO: created pod pod-service-account-nomountsa-nomountspec
  May 27 13:32:13.033: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  May 27 13:32:13.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1834" for this suite. @ 05/27/23 13:32:13.05
• [0.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 05/27/23 13:32:13.088
  May 27 13:32:13.088: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename replicaset @ 05/27/23 13:32:13.091
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:32:13.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:32:13.123
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 05/27/23 13:32:13.127
  May 27 13:32:13.140: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0527 13:32:13.786060      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:14.786423      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:15.787151      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:16.787217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:17.787320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:18.147: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/27/23 13:32:18.148
  STEP: getting scale subresource @ 05/27/23 13:32:18.148
  STEP: updating a scale subresource @ 05/27/23 13:32:18.154
  STEP: verifying the replicaset Spec.Replicas was modified @ 05/27/23 13:32:18.16
  STEP: Patch a scale subresource @ 05/27/23 13:32:18.166
  May 27 13:32:18.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5461" for this suite. @ 05/27/23 13:32:18.188
• [5.112 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 05/27/23 13:32:18.2
  May 27 13:32:18.200: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/27/23 13:32:18.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:32:18.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:32:18.231
  May 27 13:32:18.238: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  E0527 13:32:18.788034      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:19.788682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/27/23 13:32:20.291
  May 27 13:32:20.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8119 --namespace=crd-publish-openapi-8119 create -f -'
  E0527 13:32:20.789060      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:21.140: INFO: stderr: ""
  May 27 13:32:21.140: INFO: stdout: "e2e-test-crd-publish-openapi-4797-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May 27 13:32:21.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8119 --namespace=crd-publish-openapi-8119 delete e2e-test-crd-publish-openapi-4797-crds test-cr'
  May 27 13:32:21.255: INFO: stderr: ""
  May 27 13:32:21.255: INFO: stdout: "e2e-test-crd-publish-openapi-4797-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  May 27 13:32:21.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8119 --namespace=crd-publish-openapi-8119 apply -f -'
  May 27 13:32:21.502: INFO: stderr: ""
  May 27 13:32:21.502: INFO: stdout: "e2e-test-crd-publish-openapi-4797-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May 27 13:32:21.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8119 --namespace=crd-publish-openapi-8119 delete e2e-test-crd-publish-openapi-4797-crds test-cr'
  May 27 13:32:21.597: INFO: stderr: ""
  May 27 13:32:21.597: INFO: stdout: "e2e-test-crd-publish-openapi-4797-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/27/23 13:32:21.597
  May 27 13:32:21.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8119 explain e2e-test-crd-publish-openapi-4797-crds'
  E0527 13:32:21.789318      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:22.260: INFO: stderr: ""
  May 27 13:32:22.260: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-4797-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0527 13:32:22.790204      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:23.790854      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:24.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8119" for this suite. @ 05/27/23 13:32:24.166
• [5.974 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 05/27/23 13:32:24.176
  May 27 13:32:24.176: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename proxy @ 05/27/23 13:32:24.177
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:32:24.195
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:32:24.199
  STEP: starting an echo server on multiple ports @ 05/27/23 13:32:24.217
  STEP: creating replication controller proxy-service-7t2xx in namespace proxy-6491 @ 05/27/23 13:32:24.217
  I0527 13:32:24.226409      18 runners.go:194] Created replication controller with name: proxy-service-7t2xx, namespace: proxy-6491, replica count: 1
  E0527 13:32:24.791612      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0527 13:32:25.277710      18 runners.go:194] proxy-service-7t2xx Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0527 13:32:25.791860      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0527 13:32:26.278706      18 runners.go:194] proxy-service-7t2xx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0527 13:32:26.792828      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0527 13:32:27.279849      18 runners.go:194] proxy-service-7t2xx Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 27 13:32:27.284: INFO: setup took 3.081781531s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 05/27/23 13:32:27.284
  May 27 13:32:27.293: INFO: (0) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 8.55907ms)
  May 27 13:32:27.293: INFO: (0) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 8.472078ms)
  May 27 13:32:27.295: INFO: (0) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 11.009902ms)
  May 27 13:32:27.295: INFO: (0) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 10.621563ms)
  May 27 13:32:27.297: INFO: (0) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 12.366041ms)
  May 27 13:32:27.297: INFO: (0) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 12.462772ms)
  May 27 13:32:27.297: INFO: (0) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 13.212068ms)
  May 27 13:32:27.302: INFO: (0) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 17.375606ms)
  May 27 13:32:27.302: INFO: (0) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 17.781594ms)
  May 27 13:32:27.302: INFO: (0) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 17.829245ms)
  May 27 13:32:27.302: INFO: (0) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 18.259364ms)
  May 27 13:32:27.305: INFO: (0) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 20.420419ms)
  May 27 13:32:27.307: INFO: (0) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 22.160416ms)
  May 27 13:32:27.307: INFO: (0) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 22.566895ms)
  May 27 13:32:27.308: INFO: (0) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 23.128556ms)
  May 27 13:32:27.308: INFO: (0) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 23.244279ms)
  May 27 13:32:27.314: INFO: (1) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 5.997076ms)
  May 27 13:32:27.315: INFO: (1) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 6.576009ms)
  May 27 13:32:27.317: INFO: (1) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 8.611721ms)
  May 27 13:32:27.318: INFO: (1) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 9.219694ms)
  May 27 13:32:27.320: INFO: (1) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 11.476991ms)
  May 27 13:32:27.320: INFO: (1) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 10.975171ms)
  May 27 13:32:27.321: INFO: (1) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 12.212377ms)
  May 27 13:32:27.322: INFO: (1) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 13.046434ms)
  May 27 13:32:27.322: INFO: (1) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 13.692068ms)
  May 27 13:32:27.323: INFO: (1) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 14.199979ms)
  May 27 13:32:27.323: INFO: (1) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 14.413913ms)
  May 27 13:32:27.323: INFO: (1) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 14.370313ms)
  May 27 13:32:27.323: INFO: (1) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 14.471575ms)
  May 27 13:32:27.324: INFO: (1) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 15.18968ms)
  May 27 13:32:27.324: INFO: (1) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 15.499767ms)
  May 27 13:32:27.324: INFO: (1) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 15.058877ms)
  May 27 13:32:27.332: INFO: (2) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 8.081171ms)
  May 27 13:32:27.334: INFO: (2) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 9.02016ms)
  May 27 13:32:27.334: INFO: (2) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 9.48963ms)
  May 27 13:32:27.336: INFO: (2) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 11.923701ms)
  May 27 13:32:27.337: INFO: (2) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 12.32768ms)
  May 27 13:32:27.337: INFO: (2) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 12.406812ms)
  May 27 13:32:27.339: INFO: (2) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 14.527675ms)
  May 27 13:32:27.339: INFO: (2) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 13.928433ms)
  May 27 13:32:27.339: INFO: (2) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 14.381343ms)
  May 27 13:32:27.339: INFO: (2) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 14.75857ms)
  May 27 13:32:27.340: INFO: (2) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 15.872684ms)
  May 27 13:32:27.340: INFO: (2) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 15.337893ms)
  May 27 13:32:27.340: INFO: (2) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 15.69647ms)
  May 27 13:32:27.340: INFO: (2) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 15.643079ms)
  May 27 13:32:27.340: INFO: (2) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 15.614919ms)
  May 27 13:32:27.342: INFO: (2) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 16.852455ms)
  May 27 13:32:27.348: INFO: (3) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 6.132909ms)
  May 27 13:32:27.349: INFO: (3) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 7.543329ms)
  May 27 13:32:27.350: INFO: (3) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 7.789464ms)
  May 27 13:32:27.350: INFO: (3) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 7.994328ms)
  May 27 13:32:27.352: INFO: (3) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 9.610902ms)
  May 27 13:32:27.352: INFO: (3) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 9.997371ms)
  May 27 13:32:27.352: INFO: (3) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 10.242205ms)
  May 27 13:32:27.353: INFO: (3) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 10.555413ms)
  May 27 13:32:27.354: INFO: (3) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 11.498692ms)
  May 27 13:32:27.354: INFO: (3) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 11.547223ms)
  May 27 13:32:27.354: INFO: (3) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 12.257378ms)
  May 27 13:32:27.355: INFO: (3) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 12.8043ms)
  May 27 13:32:27.355: INFO: (3) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 12.956552ms)
  May 27 13:32:27.356: INFO: (3) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 13.997695ms)
  May 27 13:32:27.357: INFO: (3) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 14.147277ms)
  May 27 13:32:27.358: INFO: (3) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 15.900315ms)
  May 27 13:32:27.364: INFO: (4) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 6.010257ms)
  May 27 13:32:27.369: INFO: (4) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 10.737016ms)
  May 27 13:32:27.369: INFO: (4) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 10.501211ms)
  May 27 13:32:27.370: INFO: (4) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 11.039412ms)
  May 27 13:32:27.371: INFO: (4) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 12.454272ms)
  May 27 13:32:27.371: INFO: (4) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 12.492272ms)
  May 27 13:32:27.374: INFO: (4) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 15.099348ms)
  May 27 13:32:27.374: INFO: (4) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 15.765522ms)
  May 27 13:32:27.374: INFO: (4) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 15.566117ms)
  May 27 13:32:27.375: INFO: (4) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 16.15887ms)
  May 27 13:32:27.375: INFO: (4) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 16.14228ms)
  May 27 13:32:27.375: INFO: (4) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 16.785463ms)
  May 27 13:32:27.375: INFO: (4) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 17.060829ms)
  May 27 13:32:27.376: INFO: (4) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 17.362165ms)
  May 27 13:32:27.376: INFO: (4) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 17.814415ms)
  May 27 13:32:27.376: INFO: (4) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 18.073301ms)
  May 27 13:32:27.382: INFO: (5) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 5.987676ms)
  May 27 13:32:27.383: INFO: (5) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 7.011338ms)
  May 27 13:32:27.384: INFO: (5) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 7.519798ms)
  May 27 13:32:27.386: INFO: (5) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 9.606792ms)
  May 27 13:32:27.386: INFO: (5) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 9.810696ms)
  May 27 13:32:27.386: INFO: (5) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 9.714424ms)
  May 27 13:32:27.387: INFO: (5) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 11.029532ms)
  May 27 13:32:27.388: INFO: (5) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 11.364539ms)
  May 27 13:32:27.389: INFO: (5) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 12.259758ms)
  May 27 13:32:27.389: INFO: (5) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 12.701347ms)
  May 27 13:32:27.389: INFO: (5) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 12.650356ms)
  May 27 13:32:27.390: INFO: (5) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 13.449822ms)
  May 27 13:32:27.390: INFO: (5) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 12.898212ms)
  May 27 13:32:27.391: INFO: (5) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 13.811971ms)
  May 27 13:32:27.391: INFO: (5) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 14.206739ms)
  May 27 13:32:27.391: INFO: (5) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 14.526556ms)
  May 27 13:32:27.397: INFO: (6) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 5.756741ms)
  May 27 13:32:27.398: INFO: (6) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 6.778393ms)
  May 27 13:32:27.399: INFO: (6) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 7.434257ms)
  May 27 13:32:27.400: INFO: (6) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 8.202293ms)
  May 27 13:32:27.400: INFO: (6) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 8.600642ms)
  May 27 13:32:27.401: INFO: (6) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 8.745994ms)
  May 27 13:32:27.401: INFO: (6) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 9.819877ms)
  May 27 13:32:27.402: INFO: (6) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 9.99683ms)
  May 27 13:32:27.402: INFO: (6) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 10.607023ms)
  May 27 13:32:27.402: INFO: (6) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 10.712465ms)
  May 27 13:32:27.403: INFO: (6) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 11.371469ms)
  May 27 13:32:27.404: INFO: (6) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 12.038604ms)
  May 27 13:32:27.404: INFO: (6) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 11.830099ms)
  May 27 13:32:27.404: INFO: (6) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 12.36209ms)
  May 27 13:32:27.404: INFO: (6) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 12.684737ms)
  May 27 13:32:27.405: INFO: (6) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 13.347411ms)
  May 27 13:32:27.411: INFO: (7) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 5.417814ms)
  May 27 13:32:27.412: INFO: (7) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 6.984928ms)
  May 27 13:32:27.414: INFO: (7) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 8.501259ms)
  May 27 13:32:27.415: INFO: (7) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 8.931678ms)
  May 27 13:32:27.415: INFO: (7) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 9.280725ms)
  May 27 13:32:27.416: INFO: (7) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 10.615664ms)
  May 27 13:32:27.416: INFO: (7) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 10.531891ms)
  May 27 13:32:27.417: INFO: (7) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 10.994661ms)
  May 27 13:32:27.417: INFO: (7) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 11.209295ms)
  May 27 13:32:27.418: INFO: (7) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 12.33367ms)
  May 27 13:32:27.418: INFO: (7) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 11.937191ms)
  May 27 13:32:27.419: INFO: (7) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 13.445843ms)
  May 27 13:32:27.419: INFO: (7) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 13.492684ms)
  May 27 13:32:27.419: INFO: (7) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 13.515744ms)
  May 27 13:32:27.420: INFO: (7) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 13.546075ms)
  May 27 13:32:27.420: INFO: (7) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 13.964954ms)
  May 27 13:32:27.427: INFO: (8) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 6.912446ms)
  May 27 13:32:27.428: INFO: (8) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 8.398747ms)
  May 27 13:32:27.430: INFO: (8) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 9.820507ms)
  May 27 13:32:27.430: INFO: (8) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 9.94524ms)
  May 27 13:32:27.431: INFO: (8) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 10.176935ms)
  May 27 13:32:27.431: INFO: (8) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 10.985641ms)
  May 27 13:32:27.432: INFO: (8) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 11.519193ms)
  May 27 13:32:27.432: INFO: (8) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 11.929911ms)
  May 27 13:32:27.433: INFO: (8) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 12.394821ms)
  May 27 13:32:27.433: INFO: (8) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 12.649646ms)
  May 27 13:32:27.433: INFO: (8) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 13.075956ms)
  May 27 13:32:27.434: INFO: (8) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 13.252889ms)
  May 27 13:32:27.434: INFO: (8) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 13.740239ms)
  May 27 13:32:27.435: INFO: (8) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 14.409073ms)
  May 27 13:32:27.435: INFO: (8) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 14.678589ms)
  May 27 13:32:27.437: INFO: (8) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 16.390245ms)
  May 27 13:32:27.444: INFO: (9) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 6.812143ms)
  May 27 13:32:27.444: INFO: (9) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 7.174311ms)
  May 27 13:32:27.446: INFO: (9) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 8.323066ms)
  May 27 13:32:27.446: INFO: (9) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 8.5253ms)
  May 27 13:32:27.447: INFO: (9) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 9.96283ms)
  May 27 13:32:27.448: INFO: (9) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 10.735636ms)
  May 27 13:32:27.450: INFO: (9) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 12.074214ms)
  May 27 13:32:27.450: INFO: (9) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 12.651356ms)
  May 27 13:32:27.450: INFO: (9) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 12.760608ms)
  May 27 13:32:27.450: INFO: (9) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 12.82136ms)
  May 27 13:32:27.450: INFO: (9) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 12.86261ms)
  May 27 13:32:27.451: INFO: (9) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 13.705888ms)
  May 27 13:32:27.451: INFO: (9) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 14.097747ms)
  May 27 13:32:27.451: INFO: (9) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 13.671248ms)
  May 27 13:32:27.452: INFO: (9) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 14.69403ms)
  May 27 13:32:27.452: INFO: (9) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 14.807222ms)
  May 27 13:32:27.459: INFO: (10) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 6.716882ms)
  May 27 13:32:27.461: INFO: (10) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 7.57808ms)
  May 27 13:32:27.461: INFO: (10) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 8.643502ms)
  May 27 13:32:27.462: INFO: (10) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 9.113072ms)
  May 27 13:32:27.463: INFO: (10) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 10.025101ms)
  May 27 13:32:27.464: INFO: (10) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 10.861239ms)
  May 27 13:32:27.465: INFO: (10) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 12.120796ms)
  May 27 13:32:27.467: INFO: (10) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 13.659478ms)
  May 27 13:32:27.467: INFO: (10) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 14.652388ms)
  May 27 13:32:27.468: INFO: (10) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 14.400113ms)
  May 27 13:32:27.468: INFO: (10) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 14.68092ms)
  May 27 13:32:27.468: INFO: (10) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 14.827872ms)
  May 27 13:32:27.468: INFO: (10) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 14.760761ms)
  May 27 13:32:27.468: INFO: (10) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 15.460595ms)
  May 27 13:32:27.469: INFO: (10) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 15.586328ms)
  May 27 13:32:27.469: INFO: (10) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 15.846174ms)
  May 27 13:32:27.477: INFO: (11) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 7.342205ms)
  May 27 13:32:27.478: INFO: (11) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 8.767274ms)
  May 27 13:32:27.479: INFO: (11) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 9.50896ms)
  May 27 13:32:27.480: INFO: (11) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 9.861988ms)
  May 27 13:32:27.481: INFO: (11) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 11.335148ms)
  May 27 13:32:27.481: INFO: (11) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 11.421481ms)
  May 27 13:32:27.482: INFO: (11) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 11.8657ms)
  May 27 13:32:27.483: INFO: (11) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 13.341821ms)
  May 27 13:32:27.483: INFO: (11) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 13.970454ms)
  May 27 13:32:27.483: INFO: (11) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 13.945593ms)
  May 27 13:32:27.484: INFO: (11) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 13.809051ms)
  May 27 13:32:27.484: INFO: (11) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 14.564886ms)
  May 27 13:32:27.485: INFO: (11) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 14.960275ms)
  May 27 13:32:27.485: INFO: (11) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 15.324392ms)
  May 27 13:32:27.485: INFO: (11) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 15.452855ms)
  May 27 13:32:27.485: INFO: (11) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 15.455626ms)
  May 27 13:32:27.492: INFO: (12) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 6.855884ms)
  May 27 13:32:27.492: INFO: (12) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 6.949146ms)
  May 27 13:32:27.495: INFO: (12) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 9.154552ms)
  May 27 13:32:27.496: INFO: (12) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 10.841808ms)
  May 27 13:32:27.497: INFO: (12) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 10.45136ms)
  May 27 13:32:27.497: INFO: (12) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 10.322177ms)
  May 27 13:32:27.497: INFO: (12) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 11.289678ms)
  May 27 13:32:27.497: INFO: (12) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 10.4566ms)
  May 27 13:32:27.498: INFO: (12) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 11.720487ms)
  May 27 13:32:27.498: INFO: (12) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 12.371611ms)
  May 27 13:32:27.500: INFO: (12) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 13.711708ms)
  May 27 13:32:27.500: INFO: (12) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 14.286101ms)
  May 27 13:32:27.501: INFO: (12) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 13.825021ms)
  May 27 13:32:27.501: INFO: (12) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 15.20447ms)
  May 27 13:32:27.501: INFO: (12) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 15.473216ms)
  May 27 13:32:27.501: INFO: (12) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 14.545126ms)
  May 27 13:32:27.507: INFO: (13) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 5.887924ms)
  May 27 13:32:27.512: INFO: (13) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 9.849647ms)
  May 27 13:32:27.512: INFO: (13) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 10.107473ms)
  May 27 13:32:27.512: INFO: (13) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 10.401059ms)
  May 27 13:32:27.512: INFO: (13) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 9.906079ms)
  May 27 13:32:27.514: INFO: (13) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 12.571015ms)
  May 27 13:32:27.514: INFO: (13) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 12.670916ms)
  May 27 13:32:27.514: INFO: (13) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 12.414862ms)
  May 27 13:32:27.515: INFO: (13) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 13.237249ms)
  May 27 13:32:27.516: INFO: (13) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 14.198529ms)
  May 27 13:32:27.516: INFO: (13) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 14.078256ms)
  May 27 13:32:27.517: INFO: (13) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 14.523166ms)
  May 27 13:32:27.517: INFO: (13) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 15.024246ms)
  May 27 13:32:27.520: INFO: (13) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 17.57904ms)
  May 27 13:32:27.520: INFO: (13) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 17.833545ms)
  May 27 13:32:27.520: INFO: (13) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 18.426158ms)
  May 27 13:32:27.540: INFO: (14) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 19.758776ms)
  May 27 13:32:27.548: INFO: (14) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 26.972238ms)
  May 27 13:32:27.557: INFO: (14) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 35.774243ms)
  May 27 13:32:27.557: INFO: (14) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 35.921176ms)
  May 27 13:32:27.557: INFO: (14) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 35.738512ms)
  May 27 13:32:27.557: INFO: (14) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 36.756424ms)
  May 27 13:32:27.558: INFO: (14) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 36.60585ms)
  May 27 13:32:27.558: INFO: (14) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 36.532289ms)
  May 27 13:32:27.560: INFO: (14) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 38.984141ms)
  May 27 13:32:27.560: INFO: (14) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 39.299578ms)
  May 27 13:32:27.560: INFO: (14) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 39.642034ms)
  May 27 13:32:27.560: INFO: (14) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 39.622664ms)
  May 27 13:32:27.561: INFO: (14) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 39.966861ms)
  May 27 13:32:27.562: INFO: (14) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 41.690528ms)
  May 27 13:32:27.562: INFO: (14) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 41.290419ms)
  May 27 13:32:27.563: INFO: (14) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 41.621777ms)
  May 27 13:32:27.575: INFO: (15) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 11.856609ms)
  May 27 13:32:27.575: INFO: (15) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 11.86102ms)
  May 27 13:32:27.576: INFO: (15) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 13.535124ms)
  May 27 13:32:27.579: INFO: (15) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 16.233971ms)
  May 27 13:32:27.586: INFO: (15) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 22.617086ms)
  May 27 13:32:27.586: INFO: (15) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 23.046764ms)
  May 27 13:32:27.587: INFO: (15) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 23.930663ms)
  May 27 13:32:27.588: INFO: (15) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 24.22237ms)
  May 27 13:32:27.588: INFO: (15) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 24.858853ms)
  May 27 13:32:27.589: INFO: (15) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 25.218011ms)
  May 27 13:32:27.589: INFO: (15) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 26.259152ms)
  May 27 13:32:27.589: INFO: (15) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 26.102159ms)
  May 27 13:32:27.589: INFO: (15) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 25.935105ms)
  May 27 13:32:27.590: INFO: (15) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 26.659631ms)
  May 27 13:32:27.591: INFO: (15) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 27.261254ms)
  May 27 13:32:27.591: INFO: (15) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 28.119082ms)
  May 27 13:32:27.606: INFO: (16) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 14.478714ms)
  May 27 13:32:27.607: INFO: (16) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 15.332022ms)
  May 27 13:32:27.607: INFO: (16) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 15.859844ms)
  May 27 13:32:27.608: INFO: (16) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 16.882375ms)
  May 27 13:32:27.609: INFO: (16) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 17.731503ms)
  May 27 13:32:27.611: INFO: (16) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 19.47589ms)
  May 27 13:32:27.611: INFO: (16) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 19.211675ms)
  May 27 13:32:27.612: INFO: (16) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 20.103373ms)
  May 27 13:32:27.613: INFO: (16) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 20.762816ms)
  May 27 13:32:27.613: INFO: (16) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 21.055323ms)
  May 27 13:32:27.613: INFO: (16) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 21.792378ms)
  May 27 13:32:27.620: INFO: (16) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 28.636662ms)
  May 27 13:32:27.620: INFO: (16) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 28.396668ms)
  May 27 13:32:27.622: INFO: (16) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 29.765186ms)
  May 27 13:32:27.623: INFO: (16) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 31.241578ms)
  May 27 13:32:27.630: INFO: (16) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 38.050571ms)
  May 27 13:32:27.638: INFO: (17) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 7.986498ms)
  May 27 13:32:27.639: INFO: (17) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 8.671533ms)
  May 27 13:32:27.640: INFO: (17) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 9.942739ms)
  May 27 13:32:27.640: INFO: (17) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 9.865918ms)
  May 27 13:32:27.643: INFO: (17) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 12.861941ms)
  May 27 13:32:27.643: INFO: (17) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 13.084796ms)
  May 27 13:32:27.643: INFO: (17) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 12.964463ms)
  May 27 13:32:27.644: INFO: (17) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 13.560845ms)
  May 27 13:32:27.646: INFO: (17) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 15.801712ms)
  May 27 13:32:27.648: INFO: (17) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 17.910447ms)
  May 27 13:32:27.648: INFO: (17) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 17.821295ms)
  May 27 13:32:27.648: INFO: (17) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 17.503309ms)
  May 27 13:32:27.649: INFO: (17) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 18.431548ms)
  May 27 13:32:27.650: INFO: (17) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 19.612403ms)
  May 27 13:32:27.650: INFO: (17) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 19.714635ms)
  May 27 13:32:27.651: INFO: (17) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 20.481752ms)
  May 27 13:32:27.670: INFO: (18) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 19.083272ms)
  May 27 13:32:27.670: INFO: (18) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 18.904228ms)
  May 27 13:32:27.670: INFO: (18) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 19.087012ms)
  May 27 13:32:27.671: INFO: (18) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 19.814687ms)
  May 27 13:32:27.686: INFO: (18) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 34.852174ms)
  May 27 13:32:27.686: INFO: (18) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 34.951135ms)
  May 27 13:32:27.686: INFO: (18) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 35.004777ms)
  May 27 13:32:27.686: INFO: (18) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 34.70261ms)
  May 27 13:32:27.686: INFO: (18) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 34.806613ms)
  May 27 13:32:27.686: INFO: (18) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 34.862683ms)
  May 27 13:32:27.690: INFO: (18) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 38.350177ms)
  May 27 13:32:27.690: INFO: (18) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 38.409258ms)
  May 27 13:32:27.691: INFO: (18) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 39.275297ms)
  May 27 13:32:27.691: INFO: (18) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 39.843229ms)
  May 27 13:32:27.693: INFO: (18) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 41.137646ms)
  May 27 13:32:27.693: INFO: (18) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 41.81916ms)
  May 27 13:32:27.707: INFO: (19) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 14.24675ms)
  May 27 13:32:27.708: INFO: (19) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:460/proxy/: tls baz (200; 15.089917ms)
  May 27 13:32:27.709: INFO: (19) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:160/proxy/: foo (200; 16.261953ms)
  May 27 13:32:27.711: INFO: (19) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:462/proxy/: tls qux (200; 18.041889ms)
  May 27 13:32:27.712: INFO: (19) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname1/proxy/: foo (200; 18.194313ms)
  May 27 13:32:27.712: INFO: (19) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 18.355836ms)
  May 27 13:32:27.714: INFO: (19) /api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/https:proxy-service-7t2xx-94ffb:443/proxy/tlsrewritem... (200; 21.068274ms)
  May 27 13:32:27.714: INFO: (19) /api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/http:proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">... (200; 21.097433ms)
  May 27 13:32:27.715: INFO: (19) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:1080/proxy/rewriteme">test<... (200; 21.40063ms)
  May 27 13:32:27.715: INFO: (19) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb:162/proxy/: bar (200; 21.161775ms)
  May 27 13:32:27.715: INFO: (19) /api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/: <a href="/api/v1/namespaces/proxy-6491/pods/proxy-service-7t2xx-94ffb/proxy/rewriteme">test</a> (200; 21.434762ms)
  May 27 13:32:27.716: INFO: (19) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname2/proxy/: tls qux (200; 22.997604ms)
  May 27 13:32:27.717: INFO: (19) /api/v1/namespaces/proxy-6491/services/proxy-service-7t2xx:portname2/proxy/: bar (200; 23.510775ms)
  May 27 13:32:27.718: INFO: (19) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname1/proxy/: foo (200; 24.169559ms)
  May 27 13:32:27.718: INFO: (19) /api/v1/namespaces/proxy-6491/services/http:proxy-service-7t2xx:portname2/proxy/: bar (200; 24.418014ms)
  May 27 13:32:27.718: INFO: (19) /api/v1/namespaces/proxy-6491/services/https:proxy-service-7t2xx:tlsportname1/proxy/: tls baz (200; 25.123639ms)
  May 27 13:32:27.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-7t2xx in namespace proxy-6491, will wait for the garbage collector to delete the pods @ 05/27/23 13:32:27.724
  May 27 13:32:27.787: INFO: Deleting ReplicationController proxy-service-7t2xx took: 7.604581ms
  E0527 13:32:27.792999      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:27.888: INFO: Terminating ReplicationController proxy-service-7t2xx pods took: 100.828681ms
  E0527 13:32:28.793169      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-6491" for this suite. @ 05/27/23 13:32:29.49
• [5.322 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 05/27/23 13:32:29.499
  May 27 13:32:29.499: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename subpath @ 05/27/23 13:32:29.5
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:32:29.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:32:29.537
  STEP: Setting up data @ 05/27/23 13:32:29.541
  STEP: Creating pod pod-subpath-test-projected-zws9 @ 05/27/23 13:32:29.555
  STEP: Creating a pod to test atomic-volume-subpath @ 05/27/23 13:32:29.555
  E0527 13:32:29.793278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:30.793549      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:31.794553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:32.794974      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:33.795423      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:34.795591      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:35.796165      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:36.796287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:37.796988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:38.797105      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:39.797795      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:40.798260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:41.799307      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:42.799609      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:43.799658      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:44.799744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:45.800589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:46.800673      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:47.801582      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:48.801956      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:49.801968      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:50.802179      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:51.802890      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:52.803139      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:32:53.635
  May 27 13:32:53.640: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-subpath-test-projected-zws9 container test-container-subpath-projected-zws9: <nil>
  STEP: delete the pod @ 05/27/23 13:32:53.668
  STEP: Deleting pod pod-subpath-test-projected-zws9 @ 05/27/23 13:32:53.688
  May 27 13:32:53.688: INFO: Deleting pod "pod-subpath-test-projected-zws9" in namespace "subpath-7071"
  May 27 13:32:53.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7071" for this suite. @ 05/27/23 13:32:53.697
• [24.204 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 05/27/23 13:32:53.709
  May 27 13:32:53.709: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename pods @ 05/27/23 13:32:53.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:32:53.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:32:53.739
  STEP: creating the pod @ 05/27/23 13:32:53.742
  STEP: setting up watch @ 05/27/23 13:32:53.742
  E0527 13:32:53.803971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: submitting the pod to kubernetes @ 05/27/23 13:32:53.848
  STEP: verifying the pod is in kubernetes @ 05/27/23 13:32:53.857
  STEP: verifying pod creation was observed @ 05/27/23 13:32:53.865
  E0527 13:32:54.804076      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:55.804323      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 05/27/23 13:32:55.881
  STEP: verifying pod deletion was observed @ 05/27/23 13:32:55.889
  E0527 13:32:56.804464      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:57.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5307" for this suite. @ 05/27/23 13:32:57.575
• [3.877 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 05/27/23 13:32:57.59
  May 27 13:32:57.590: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename pods @ 05/27/23 13:32:57.591
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:32:57.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:32:57.619
  May 27 13:32:57.623: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: creating the pod @ 05/27/23 13:32:57.623
  STEP: submitting the pod to kubernetes @ 05/27/23 13:32:57.624
  E0527 13:32:57.804801      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:32:58.804947      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:32:59.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-569" for this suite. @ 05/27/23 13:32:59.69
• [2.110 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 05/27/23 13:32:59.701
  May 27 13:32:59.701: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 13:32:59.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:32:59.724
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:32:59.729
  STEP: Setting up server cert @ 05/27/23 13:32:59.763
  E0527 13:32:59.805897      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 13:33:00.641
  STEP: Deploying the webhook pod @ 05/27/23 13:33:00.651
  STEP: Wait for the deployment to be ready @ 05/27/23 13:33:00.666
  May 27 13:33:00.677: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0527 13:33:00.806507      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:01.806668      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/27/23 13:33:02.691
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 13:33:02.703
  E0527 13:33:02.807421      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:33:03.704: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 05/27/23 13:33:03.708
  STEP: create a namespace for the webhook @ 05/27/23 13:33:03.731
  STEP: create a configmap should be unconditionally rejected by the webhook @ 05/27/23 13:33:03.751
  E0527 13:33:03.808326      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:33:03.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5263" for this suite. @ 05/27/23 13:33:03.928
  STEP: Destroying namespace "webhook-markers-2174" for this suite. @ 05/27/23 13:33:03.94
  STEP: Destroying namespace "fail-closed-namespace-4717" for this suite. @ 05/27/23 13:33:03.954
• [4.264 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 05/27/23 13:33:03.968
  May 27 13:33:03.968: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-runtime @ 05/27/23 13:33:03.969
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:33:03.99
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:33:03.993
  STEP: create the container @ 05/27/23 13:33:03.997
  W0527 13:33:04.011688      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 05/27/23 13:33:04.012
  E0527 13:33:04.808638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:05.808744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:06.808849      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/27/23 13:33:07.032
  STEP: the container should be terminated @ 05/27/23 13:33:07.035
  STEP: the termination message should be set @ 05/27/23 13:33:07.035
  May 27 13:33:07.036: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/27/23 13:33:07.036
  May 27 13:33:07.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9738" for this suite. @ 05/27/23 13:33:07.057
• [3.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 05/27/23 13:33:07.071
  May 27 13:33:07.071: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 13:33:07.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:33:07.092
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:33:07.096
  STEP: Creating configMap with name projected-configmap-test-volume-eb1c0152-9a87-40fa-a73a-b1555a300fc7 @ 05/27/23 13:33:07.104
  STEP: Creating a pod to test consume configMaps @ 05/27/23 13:33:07.11
  E0527 13:33:07.808997      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:08.809232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:09.809894      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:10.810212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:33:11.134
  May 27 13:33:11.139: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-projected-configmaps-7f046d9a-6675-4412-85e2-a5cc80198e2e container agnhost-container: <nil>
  STEP: delete the pod @ 05/27/23 13:33:11.147
  May 27 13:33:11.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6934" for this suite. @ 05/27/23 13:33:11.169
• [4.106 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 05/27/23 13:33:11.178
  May 27 13:33:11.178: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename events @ 05/27/23 13:33:11.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:33:11.199
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:33:11.202
  STEP: creating a test event @ 05/27/23 13:33:11.206
  STEP: listing all events in all namespaces @ 05/27/23 13:33:11.212
  STEP: patching the test event @ 05/27/23 13:33:11.216
  STEP: fetching the test event @ 05/27/23 13:33:11.225
  STEP: updating the test event @ 05/27/23 13:33:11.228
  STEP: getting the test event @ 05/27/23 13:33:11.239
  STEP: deleting the test event @ 05/27/23 13:33:11.243
  STEP: listing all events in all namespaces @ 05/27/23 13:33:11.252
  May 27 13:33:11.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-2827" for this suite. @ 05/27/23 13:33:11.26
• [0.091 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 05/27/23 13:33:11.27
  May 27 13:33:11.270: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename deployment @ 05/27/23 13:33:11.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:33:11.335
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:33:11.338
  May 27 13:33:11.343: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  May 27 13:33:11.353: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0527 13:33:11.810731      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:12.810866      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:13.811246      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:14.811378      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:15.811488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:33:16.358: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/27/23 13:33:16.358
  May 27 13:33:16.358: INFO: Creating deployment "test-rolling-update-deployment"
  May 27 13:33:16.365: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  May 27 13:33:16.378: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0527 13:33:16.811667      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:17.811794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:33:18.386: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  May 27 13:33:18.391: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  May 27 13:33:18.402: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-79  dac0796f-61e2-4af0-ae24-99ab978ba51f 33691 1 2023-05-27 13:33:16 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-27 13:33:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 13:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fc22d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-27 13:33:16 +0000 UTC,LastTransitionTime:2023-05-27 13:33:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-05-27 13:33:18 +0000 UTC,LastTransitionTime:2023-05-27 13:33:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 27 13:33:18.407: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-79  936f0150-ef8b-4926-b936-1ba1f6580f6e 33681 1 2023-05-27 13:33:16 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment dac0796f-61e2-4af0-ae24-99ab978ba51f 0xc003fc27d7 0xc003fc27d8}] [] [{kube-controller-manager Update apps/v1 2023-05-27 13:33:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dac0796f-61e2-4af0-ae24-99ab978ba51f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 13:33:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fc2898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 27 13:33:18.407: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  May 27 13:33:18.407: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-79  a77678cd-a20a-4202-96cc-bb588d0a8dcc 33690 2 2023-05-27 13:33:11 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment dac0796f-61e2-4af0-ae24-99ab978ba51f 0xc003fc26a7 0xc003fc26a8}] [] [{e2e.test Update apps/v1 2023-05-27 13:33:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-27 13:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dac0796f-61e2-4af0-ae24-99ab978ba51f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-27 13:33:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003fc2768 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 27 13:33:18.411: INFO: Pod "test-rolling-update-deployment-656d657cd8-28qzj" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-28qzj test-rolling-update-deployment-656d657cd8- deployment-79  d380e8d1-0238-49fe-82da-5979ddb48bea 33680 0 2023-05-27 13:33:16 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 936f0150-ef8b-4926-b936-1ba1f6580f6e 0xc003fc2d17 0xc003fc2d18}] [] [{kube-controller-manager Update v1 2023-05-27 13:33:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"936f0150-ef8b-4926-b936-1ba1f6580f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-27 13:33:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttsmc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttsmc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:33:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:33:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:33:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-27 13:33:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.136,PodIP:192.168.0.50,StartTime:2023-05-27 13:33:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-27 13:33:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://e4dc88b51d69b501efc0dc3c1111142575002733e70e46cc6148ea9c42cde990,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.0.50,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 27 13:33:18.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-79" for this suite. @ 05/27/23 13:33:18.417
• [7.155 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 05/27/23 13:33:18.427
  May 27 13:33:18.427: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename resourcequota @ 05/27/23 13:33:18.428
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:33:18.449
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:33:18.452
  STEP: Discovering how many secrets are in namespace by default @ 05/27/23 13:33:18.456
  E0527 13:33:18.812313      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:19.812968      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:20.813201      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:21.814109      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:22.815113      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 05/27/23 13:33:23.46
  E0527 13:33:23.815316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:24.815374      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:25.815497      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:26.816188      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:27.816883      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/27/23 13:33:28.465
  STEP: Ensuring resource quota status is calculated @ 05/27/23 13:33:28.473
  E0527 13:33:28.816947      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:29.817056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 05/27/23 13:33:30.478
  STEP: Ensuring resource quota status captures secret creation @ 05/27/23 13:33:30.491
  E0527 13:33:30.818068      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:31.818603      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 05/27/23 13:33:32.497
  STEP: Ensuring resource quota status released usage @ 05/27/23 13:33:32.504
  E0527 13:33:32.818794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:33.819229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:33:34.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1954" for this suite. @ 05/27/23 13:33:34.515
• [16.096 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 05/27/23 13:33:34.524
  May 27 13:33:34.524: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubelet-test @ 05/27/23 13:33:34.525
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:33:34.548
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:33:34.552
  E0527 13:33:34.820139      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:35.820229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:36.820346      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:37.821024      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:33:38.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4460" for this suite. @ 05/27/23 13:33:38.583
• [4.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 05/27/23 13:33:38.594
  May 27 13:33:38.594: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename replicaset @ 05/27/23 13:33:38.595
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:33:38.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:33:38.618
  May 27 13:33:38.638: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0527 13:33:38.821521      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:39.821937      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:40.822052      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:41.822156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:42.822259      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:33:43.642: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/27/23 13:33:43.643
  STEP: Scaling up "test-rs" replicaset  @ 05/27/23 13:33:43.643
  May 27 13:33:43.653: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 05/27/23 13:33:43.653
  W0527 13:33:43.662802      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 27 13:33:43.664: INFO: observed ReplicaSet test-rs in namespace replicaset-4600 with ReadyReplicas 1, AvailableReplicas 1
  May 27 13:33:43.692: INFO: observed ReplicaSet test-rs in namespace replicaset-4600 with ReadyReplicas 1, AvailableReplicas 1
  May 27 13:33:43.716: INFO: observed ReplicaSet test-rs in namespace replicaset-4600 with ReadyReplicas 1, AvailableReplicas 1
  May 27 13:33:43.725: INFO: observed ReplicaSet test-rs in namespace replicaset-4600 with ReadyReplicas 1, AvailableReplicas 1
  E0527 13:33:43.823270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:33:44.639: INFO: observed ReplicaSet test-rs in namespace replicaset-4600 with ReadyReplicas 2, AvailableReplicas 2
  E0527 13:33:44.824280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:33:45.085: INFO: observed Replicaset test-rs in namespace replicaset-4600 with ReadyReplicas 3 found true
  May 27 13:33:45.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4600" for this suite. @ 05/27/23 13:33:45.09
• [6.505 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 05/27/23 13:33:45.1
  May 27 13:33:45.100: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename resourcequota @ 05/27/23 13:33:45.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:33:45.123
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:33:45.127
  STEP: Counting existing ResourceQuota @ 05/27/23 13:33:45.13
  E0527 13:33:45.824415      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:46.824805      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:47.825326      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:48.825453      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:49.825483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/27/23 13:33:50.135
  STEP: Ensuring resource quota status is calculated @ 05/27/23 13:33:50.14
  E0527 13:33:50.825805      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:51.825732      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 05/27/23 13:33:52.146
  STEP: Ensuring ResourceQuota status captures the pod usage @ 05/27/23 13:33:52.165
  E0527 13:33:52.826342      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:53.826446      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 05/27/23 13:33:54.17
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 05/27/23 13:33:54.173
  STEP: Ensuring a pod cannot update its resource requirements @ 05/27/23 13:33:54.175
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 05/27/23 13:33:54.179
  E0527 13:33:54.827491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:55.827574      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/27/23 13:33:56.185
  STEP: Ensuring resource quota status released the pod usage @ 05/27/23 13:33:56.202
  E0527 13:33:56.827689      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:57.827812      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:33:58.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2119" for this suite. @ 05/27/23 13:33:58.213
• [13.121 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 05/27/23 13:33:58.222
  May 27 13:33:58.222: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename gc @ 05/27/23 13:33:58.223
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:33:58.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:33:58.248
  STEP: create the rc @ 05/27/23 13:33:58.258
  W0527 13:33:58.263480      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0527 13:33:58.828402      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:33:59.828639      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:00.828747      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:01.828859      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:02.829638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:03.829938      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/27/23 13:34:04.269
  STEP: wait for the rc to be deleted @ 05/27/23 13:34:04.283
  E0527 13:34:04.830999      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:34:05.310: INFO: 80 pods remaining
  May 27 13:34:05.310: INFO: 80 pods has nil DeletionTimestamp
  May 27 13:34:05.311: INFO: 
  E0527 13:34:05.831814      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:34:06.308: INFO: 71 pods remaining
  May 27 13:34:06.309: INFO: 71 pods has nil DeletionTimestamp
  May 27 13:34:06.309: INFO: 
  E0527 13:34:06.837249      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:34:07.315: INFO: 60 pods remaining
  May 27 13:34:07.315: INFO: 60 pods has nil DeletionTimestamp
  May 27 13:34:07.315: INFO: 
  E0527 13:34:07.837454      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:34:08.302: INFO: 40 pods remaining
  May 27 13:34:08.303: INFO: 40 pods has nil DeletionTimestamp
  May 27 13:34:08.303: INFO: 
  E0527 13:34:08.838460      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:34:09.307: INFO: 31 pods remaining
  May 27 13:34:09.307: INFO: 31 pods has nil DeletionTimestamp
  May 27 13:34:09.307: INFO: 
  E0527 13:34:09.838430      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:34:10.314: INFO: 20 pods remaining
  May 27 13:34:10.315: INFO: 20 pods has nil DeletionTimestamp
  May 27 13:34:10.315: INFO: 
  E0527 13:34:10.839422      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/27/23 13:34:11.292
  W0527 13:34:11.296695      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May 27 13:34:11.296: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 27 13:34:11.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3390" for this suite. @ 05/27/23 13:34:11.308
• [13.097 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 05/27/23 13:34:11.322
  May 27 13:34:11.322: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename watch @ 05/27/23 13:34:11.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:34:11.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:34:11.362
  STEP: getting a starting resourceVersion @ 05/27/23 13:34:11.367
  STEP: starting a background goroutine to produce watch events @ 05/27/23 13:34:11.372
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 05/27/23 13:34:11.372
  E0527 13:34:11.839831      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:12.840817      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:13.841502      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:34:14.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4861" for this suite. @ 05/27/23 13:34:14.183
• [2.915 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 05/27/23 13:34:14.238
  May 27 13:34:14.238: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename dns @ 05/27/23 13:34:14.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:34:14.258
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:34:14.262
  STEP: Creating a test headless service @ 05/27/23 13:34:14.266
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7215 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7215;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7215 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7215;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7215.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7215.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7215.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7215.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7215.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7215.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7215.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7215.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7215.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7215.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7215.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7215.svc;check="$$(dig +notcp +noall +answer +search 222.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.222_udp@PTR;check="$$(dig +tcp +noall +answer +search 222.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.222_tcp@PTR;sleep 1; done
   @ 05/27/23 13:34:14.288
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7215 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7215;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7215 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7215;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7215.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7215.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7215.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7215.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7215.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7215.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7215.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7215.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7215.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7215.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7215.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7215.svc;check="$$(dig +notcp +noall +answer +search 222.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.222_udp@PTR;check="$$(dig +tcp +noall +answer +search 222.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.222_tcp@PTR;sleep 1; done
   @ 05/27/23 13:34:14.288
  STEP: creating a pod to probe DNS @ 05/27/23 13:34:14.288
  STEP: submitting the pod to kubernetes @ 05/27/23 13:34:14.288
  E0527 13:34:14.842578      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:15.843014      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:16.843396      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:17.844070      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:18.844718      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:19.845156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:20.845360      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:21.845726      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:22.846139      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:23.847104      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:24.847613      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:25.848296      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/27/23 13:34:26.346
  STEP: looking for the results for each expected name from probers @ 05/27/23 13:34:26.363
  May 27 13:34:26.373: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3: the server could not find the requested resource (get pods dns-test-62fef82b-a355-43b6-9f15-182ac18732e3)
  May 27 13:34:26.390: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3: the server could not find the requested resource (get pods dns-test-62fef82b-a355-43b6-9f15-182ac18732e3)
  May 27 13:34:26.407: INFO: Unable to read wheezy_udp@dns-test-service.dns-7215 from pod dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3: the server could not find the requested resource (get pods dns-test-62fef82b-a355-43b6-9f15-182ac18732e3)
  May 27 13:34:26.418: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7215 from pod dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3: the server could not find the requested resource (get pods dns-test-62fef82b-a355-43b6-9f15-182ac18732e3)
  May 27 13:34:26.424: INFO: Unable to read wheezy_udp@dns-test-service.dns-7215.svc from pod dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3: the server could not find the requested resource (get pods dns-test-62fef82b-a355-43b6-9f15-182ac18732e3)
  May 27 13:34:26.444: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7215.svc from pod dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3: the server could not find the requested resource (get pods dns-test-62fef82b-a355-43b6-9f15-182ac18732e3)
  May 27 13:34:26.451: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7215.svc from pod dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3: the server could not find the requested resource (get pods dns-test-62fef82b-a355-43b6-9f15-182ac18732e3)
  May 27 13:34:26.457: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7215.svc from pod dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3: the server could not find the requested resource (get pods dns-test-62fef82b-a355-43b6-9f15-182ac18732e3)
  May 27 13:34:26.492: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3: the server could not find the requested resource (get pods dns-test-62fef82b-a355-43b6-9f15-182ac18732e3)
  May 27 13:34:26.502: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3: the server could not find the requested resource (get pods dns-test-62fef82b-a355-43b6-9f15-182ac18732e3)
  May 27 13:34:26.507: INFO: Unable to read jessie_udp@dns-test-service.dns-7215 from pod dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3: the server could not find the requested resource (get pods dns-test-62fef82b-a355-43b6-9f15-182ac18732e3)
  May 27 13:34:26.512: INFO: Unable to read jessie_tcp@dns-test-service.dns-7215 from pod dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3: the server could not find the requested resource (get pods dns-test-62fef82b-a355-43b6-9f15-182ac18732e3)
  May 27 13:34:26.517: INFO: Unable to read jessie_udp@dns-test-service.dns-7215.svc from pod dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3: the server could not find the requested resource (get pods dns-test-62fef82b-a355-43b6-9f15-182ac18732e3)
  May 27 13:34:26.522: INFO: Unable to read jessie_tcp@dns-test-service.dns-7215.svc from pod dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3: the server could not find the requested resource (get pods dns-test-62fef82b-a355-43b6-9f15-182ac18732e3)
  May 27 13:34:26.528: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7215.svc from pod dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3: the server could not find the requested resource (get pods dns-test-62fef82b-a355-43b6-9f15-182ac18732e3)
  May 27 13:34:26.534: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7215.svc from pod dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3: the server could not find the requested resource (get pods dns-test-62fef82b-a355-43b6-9f15-182ac18732e3)
  May 27 13:34:26.554: INFO: Lookups using dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7215 wheezy_tcp@dns-test-service.dns-7215 wheezy_udp@dns-test-service.dns-7215.svc wheezy_tcp@dns-test-service.dns-7215.svc wheezy_udp@_http._tcp.dns-test-service.dns-7215.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7215.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7215 jessie_tcp@dns-test-service.dns-7215 jessie_udp@dns-test-service.dns-7215.svc jessie_tcp@dns-test-service.dns-7215.svc jessie_udp@_http._tcp.dns-test-service.dns-7215.svc jessie_tcp@_http._tcp.dns-test-service.dns-7215.svc]

  E0527 13:34:26.848626      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:27.850215      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:28.850567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:29.851369      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:30.851558      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:34:31.676: INFO: DNS probes using dns-7215/dns-test-62fef82b-a355-43b6-9f15-182ac18732e3 succeeded

  May 27 13:34:31.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/27/23 13:34:31.681
  STEP: deleting the test service @ 05/27/23 13:34:31.704
  STEP: deleting the test headless service @ 05/27/23 13:34:31.733
  STEP: Destroying namespace "dns-7215" for this suite. @ 05/27/23 13:34:31.75
• [17.523 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 05/27/23 13:34:31.766
  May 27 13:34:31.766: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename pods @ 05/27/23 13:34:31.767
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:34:31.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:34:31.796
  May 27 13:34:31.801: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: creating the pod @ 05/27/23 13:34:31.803
  STEP: submitting the pod to kubernetes @ 05/27/23 13:34:31.803
  E0527 13:34:31.852305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:32.852497      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:33.853353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:34:33.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3862" for this suite. @ 05/27/23 13:34:33.916
• [2.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 05/27/23 13:34:33.926
  May 27 13:34:33.926: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/27/23 13:34:33.927
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:34:33.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:34:33.949
  STEP: Creating 50 configmaps @ 05/27/23 13:34:33.953
  STEP: Creating RC which spawns configmap-volume pods @ 05/27/23 13:34:34.263
  May 27 13:34:34.283: INFO: Pod name wrapped-volume-race-f52b1f50-4897-47c8-845a-b98a875617e3: Found 0 pods out of 5
  E0527 13:34:34.854091      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:35.854476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:36.854613      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:37.857065      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:38.857466      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:34:39.291: INFO: Pod name wrapped-volume-race-f52b1f50-4897-47c8-845a-b98a875617e3: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/27/23 13:34:39.292
  STEP: Creating RC which spawns configmap-volume pods @ 05/27/23 13:34:39.315
  May 27 13:34:39.337: INFO: Pod name wrapped-volume-race-45db960d-629c-4fff-bb6b-aa853af60f21: Found 0 pods out of 5
  E0527 13:34:39.857939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:40.858415      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:41.859068      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:42.859289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:43.859384      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:34:44.348: INFO: Pod name wrapped-volume-race-45db960d-629c-4fff-bb6b-aa853af60f21: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/27/23 13:34:44.348
  STEP: Creating RC which spawns configmap-volume pods @ 05/27/23 13:34:44.377
  May 27 13:34:44.397: INFO: Pod name wrapped-volume-race-5884649c-af97-4c7e-9a64-5ca2b99bcacd: Found 0 pods out of 5
  E0527 13:34:44.860026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:45.860132      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:46.860728      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:47.860992      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:48.861422      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:34:49.406: INFO: Pod name wrapped-volume-race-5884649c-af97-4c7e-9a64-5ca2b99bcacd: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/27/23 13:34:49.406
  May 27 13:34:49.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-5884649c-af97-4c7e-9a64-5ca2b99bcacd in namespace emptydir-wrapper-5818, will wait for the garbage collector to delete the pods @ 05/27/23 13:34:49.437
  May 27 13:34:49.501: INFO: Deleting ReplicationController wrapped-volume-race-5884649c-af97-4c7e-9a64-5ca2b99bcacd took: 10.033112ms
  May 27 13:34:49.602: INFO: Terminating ReplicationController wrapped-volume-race-5884649c-af97-4c7e-9a64-5ca2b99bcacd pods took: 100.627351ms
  E0527 13:34:49.861781      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:50.862019      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:51.862827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-45db960d-629c-4fff-bb6b-aa853af60f21 in namespace emptydir-wrapper-5818, will wait for the garbage collector to delete the pods @ 05/27/23 13:34:52.303
  May 27 13:34:52.367: INFO: Deleting ReplicationController wrapped-volume-race-45db960d-629c-4fff-bb6b-aa853af60f21 took: 7.957828ms
  May 27 13:34:52.468: INFO: Terminating ReplicationController wrapped-volume-race-45db960d-629c-4fff-bb6b-aa853af60f21 pods took: 101.10004ms
  E0527 13:34:52.863975      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:53.863939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-f52b1f50-4897-47c8-845a-b98a875617e3 in namespace emptydir-wrapper-5818, will wait for the garbage collector to delete the pods @ 05/27/23 13:34:54.469
  May 27 13:34:54.534: INFO: Deleting ReplicationController wrapped-volume-race-f52b1f50-4897-47c8-845a-b98a875617e3 took: 8.438378ms
  May 27 13:34:54.635: INFO: Terminating ReplicationController wrapped-volume-race-f52b1f50-4897-47c8-845a-b98a875617e3 pods took: 100.61475ms
  E0527 13:34:54.864477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:55.864898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:56.865532      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 05/27/23 13:34:57.036
  STEP: Destroying namespace "emptydir-wrapper-5818" for this suite. @ 05/27/23 13:34:57.429
• [23.512 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 05/27/23 13:34:57.441
  May 27 13:34:57.441: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename statefulset @ 05/27/23 13:34:57.441
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:34:57.463
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:34:57.467
  STEP: Creating service test in namespace statefulset-9099 @ 05/27/23 13:34:57.471
  May 27 13:34:57.493: INFO: Found 0 stateful pods, waiting for 1
  E0527 13:34:57.865656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:58.865921      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:34:59.866033      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:00.866144      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:01.866450      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:02.866564      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:03.867012      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:04.867215      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:05.867345      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:06.868325      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:35:07.500: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 05/27/23 13:35:07.507
  W0527 13:35:07.520300      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 27 13:35:07.527: INFO: Found 1 stateful pods, waiting for 2
  E0527 13:35:07.869071      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:08.869149      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:09.869735      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:10.869997      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:11.870355      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:12.870941      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:13.871230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:14.871339      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:15.871395      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:16.871560      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:35:17.533: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 27 13:35:17.533: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 05/27/23 13:35:17.542
  STEP: Delete all of the StatefulSets @ 05/27/23 13:35:17.547
  STEP: Verify that StatefulSets have been deleted @ 05/27/23 13:35:17.558
  May 27 13:35:17.564: INFO: Deleting all statefulset in ns statefulset-9099
  May 27 13:35:17.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9099" for this suite. @ 05/27/23 13:35:17.604
• [20.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 05/27/23 13:35:17.618
  May 27 13:35:17.618: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename services @ 05/27/23 13:35:17.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:35:17.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:35:17.649
  May 27 13:35:17.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7184" for this suite. @ 05/27/23 13:35:17.668
• [0.060 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 05/27/23 13:35:17.679
  May 27 13:35:17.679: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-probe @ 05/27/23 13:35:17.68
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:35:17.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:35:17.712
  STEP: Creating pod liveness-5a9dd8c8-e80b-4e88-a1e2-57b0891694fc in namespace container-probe-585 @ 05/27/23 13:35:17.717
  E0527 13:35:17.872434      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:18.872633      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:35:19.749: INFO: Started pod liveness-5a9dd8c8-e80b-4e88-a1e2-57b0891694fc in namespace container-probe-585
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/27/23 13:35:19.75
  May 27 13:35:19.753: INFO: Initial restart count of pod liveness-5a9dd8c8-e80b-4e88-a1e2-57b0891694fc is 0
  E0527 13:35:19.873061      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:20.873177      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:21.873671      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:22.873781      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:23.874777      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:24.874881      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:25.875216      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:26.875343      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:27.875874      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:28.875970      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:29.876893      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:30.877001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:31.877619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:32.878188      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:33.878533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:34.878680      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:35.878912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:36.879294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:37.879384      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:38.879523      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:39.879650      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:40.879758      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:41.879975      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:42.880381      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:43.881269      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:44.882250      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:45.883139      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:46.883326      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:47.883377      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:48.883483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:49.884385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:50.884491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:51.885237      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:52.885578      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:53.886043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:54.886117      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:55.886321      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:56.886882      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:57.887971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:58.888060      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:35:59.889100      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:00.889216      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:01.890219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:02.890484      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:03.891280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:04.891405      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:05.891696      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:06.891770      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:07.892633      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:08.892719      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:09.893334      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:10.893433      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:11.894400      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:12.894555      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:13.894747      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:14.895217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:15.895341      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:16.895461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:17.895551      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:18.896278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:19.896383      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:20.897283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:21.897349      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:22.897451      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:23.897543      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:24.897603      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:25.897878      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:26.897984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:27.898836      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:28.899222      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:29.900328      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:30.900559      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:31.900776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:32.900869      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:33.900998      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:34.901467      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:35.901573      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:36.901667      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:37.901763      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:38.901899      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:39.901998      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:40.902109      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:41.902217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:42.902308      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:43.902677      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:44.903261      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:45.903353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:46.903495      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:47.903623      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:48.903693      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:49.903784      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:50.903910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:51.904027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:52.904418      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:53.904241      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:54.904342      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:55.904529      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:56.904711      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:57.904752      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:58.904864      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:36:59.904983      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:00.905117      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:01.905421      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:02.905540      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:03.905643      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:04.905788      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:05.905908      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:06.906019      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:07.906139      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:08.906232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:09.906627      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:10.906826      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:11.906988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:12.907876      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:13.907987      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:14.908794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:15.908986      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:16.909093      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:17.909206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:18.909369      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:19.909503      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:20.909612      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:21.909910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:22.910780      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:23.911391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:24.911511      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:25.911617      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:26.911738      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:27.911860      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:28.912021      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:29.912312      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:30.912422      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:31.912678      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:32.912746      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:33.912860      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:34.913908      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:35.914026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:36.914093      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:37.915107      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:38.915289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:39.916304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:40.917234      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:41.917721      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:42.917817      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:43.917930      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:44.918032      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:45.918141      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:46.919235      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:47.919332      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:48.919447      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:49.919819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:50.919948      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:51.919966      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:52.920169      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:53.920399      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:54.921290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:55.921396      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:56.922457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:57.923312      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:58.923464      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:37:59.923533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:00.924357      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:01.924433      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:02.925207      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:03.925329      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:04.926227      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:05.926773      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:06.927269      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:07.927338      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:08.927456      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:09.928299      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:10.928407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:11.928541      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:12.929405      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:13.929503      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:14.929604      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:15.929702      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:16.929811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:17.929922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:18.930221      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:19.930362      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:20.930642      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:21.930736      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:22.931299      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:23.932303      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:24.933275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:25.934050      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:26.934159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:27.934265      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:28.934382      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:29.934786      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:30.935753      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:31.935861      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:32.935990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:33.936055      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:34.937090      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:35.937173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:36.937248      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:37.937530      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:38.937954      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:39.938053      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:40.938174      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:41.938282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:42.938387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:43.938606      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:44.938743      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:45.939005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:46.939189      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:47.940043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:48.940999      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:49.941504      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:50.941605      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:51.941704      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:52.941827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:53.941974      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:54.942025      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:55.942135      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:56.942348      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:57.943027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:58.943872      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:38:59.943979      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:00.945011      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:01.945126      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:02.945939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:03.946638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:04.947656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:05.948280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:06.949364      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:07.949481      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:08.950223      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:09.950324      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:10.951353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:11.951482      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:12.952293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:13.952376      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:14.953264      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:15.953382      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:16.953480      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:17.953582      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:18.953868      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:19.954027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:39:20.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/27/23 13:39:20.38
  STEP: Destroying namespace "container-probe-585" for this suite. @ 05/27/23 13:39:20.393
• [242.725 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 05/27/23 13:39:20.404
  May 27 13:39:20.405: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename namespaces @ 05/27/23 13:39:20.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:39:20.448
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:39:20.452
  STEP: Creating a test namespace @ 05/27/23 13:39:20.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:39:20.479
  STEP: Creating a service in the namespace @ 05/27/23 13:39:20.484
  STEP: Deleting the namespace @ 05/27/23 13:39:20.496
  STEP: Waiting for the namespace to be removed. @ 05/27/23 13:39:20.508
  E0527 13:39:20.954477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:21.954646      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:22.954978      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:23.955163      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:24.955296      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:25.955367      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 05/27/23 13:39:26.514
  STEP: Verifying there is no service in the namespace @ 05/27/23 13:39:26.536
  May 27 13:39:26.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8783" for this suite. @ 05/27/23 13:39:26.551
  STEP: Destroying namespace "nsdeletetest-9477" for this suite. @ 05/27/23 13:39:26.56
  May 27 13:39:26.565: INFO: Namespace nsdeletetest-9477 was already deleted
  STEP: Destroying namespace "nsdeletetest-8341" for this suite. @ 05/27/23 13:39:26.565
• [6.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 05/27/23 13:39:26.576
  May 27 13:39:26.576: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 13:39:26.577
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:39:26.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:39:26.603
  STEP: Setting up server cert @ 05/27/23 13:39:26.635
  E0527 13:39:26.956402      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 13:39:27.064
  STEP: Deploying the webhook pod @ 05/27/23 13:39:27.075
  STEP: Wait for the deployment to be ready @ 05/27/23 13:39:27.091
  May 27 13:39:27.099: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0527 13:39:27.956501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:28.956673      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/27/23 13:39:29.114
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 13:39:29.124
  E0527 13:39:29.956768      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:39:30.125: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/27/23 13:39:30.13
  STEP: create a pod that should be denied by the webhook @ 05/27/23 13:39:30.152
  STEP: create a pod that causes the webhook to hang @ 05/27/23 13:39:30.176
  E0527 13:39:30.956846      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:31.957179      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:32.957298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:33.957405      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:34.957514      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:35.957636      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:36.957749      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:37.957856      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:38.957965      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:39.958040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 05/27/23 13:39:40.188
  STEP: create a configmap that should be admitted by the webhook @ 05/27/23 13:39:40.202
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/27/23 13:39:40.212
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/27/23 13:39:40.226
  STEP: create a namespace that bypass the webhook @ 05/27/23 13:39:40.231
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 05/27/23 13:39:40.25
  May 27 13:39:40.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2259" for this suite. @ 05/27/23 13:39:40.328
  STEP: Destroying namespace "webhook-markers-1855" for this suite. @ 05/27/23 13:39:40.337
  STEP: Destroying namespace "exempted-namespace-9051" for this suite. @ 05/27/23 13:39:40.347
• [13.781 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 05/27/23 13:39:40.361
  May 27 13:39:40.361: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename secrets @ 05/27/23 13:39:40.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:39:40.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:39:40.382
  STEP: Creating secret with name secret-test-map-2162b28b-67f8-412e-9667-067808a31d6c @ 05/27/23 13:39:40.394
  STEP: Creating a pod to test consume secrets @ 05/27/23 13:39:40.401
  E0527 13:39:40.958847      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:41.958967      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:42.959874      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:43.959942      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:39:44.427
  May 27 13:39:44.430: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-secrets-f5ca3fe5-23ab-4cac-9614-770004d2af89 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/27/23 13:39:44.454
  May 27 13:39:44.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4515" for this suite. @ 05/27/23 13:39:44.478
• [4.125 seconds]
------------------------------
S
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 05/27/23 13:39:44.486
  May 27 13:39:44.486: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename runtimeclass @ 05/27/23 13:39:44.487
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:39:44.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:39:44.512
  May 27 13:39:44.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2041" for this suite. @ 05/27/23 13:39:44.53
• [0.052 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 05/27/23 13:39:44.539
  May 27 13:39:44.539: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename prestop @ 05/27/23 13:39:44.54
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:39:44.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:39:44.563
  STEP: Creating server pod server in namespace prestop-4978 @ 05/27/23 13:39:44.567
  STEP: Waiting for pods to come up. @ 05/27/23 13:39:44.582
  E0527 13:39:44.960152      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:45.960277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-4978 @ 05/27/23 13:39:46.598
  E0527 13:39:46.960764      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:47.961429      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 05/27/23 13:39:48.615
  E0527 13:39:48.961638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:49.961642      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:50.961736      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:51.961858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:52.962676      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:39:53.628: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  May 27 13:39:53.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 05/27/23 13:39:53.634
  STEP: Destroying namespace "prestop-4978" for this suite. @ 05/27/23 13:39:53.651
• [9.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 05/27/23 13:39:53.662
  May 27 13:39:53.662: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename configmap @ 05/27/23 13:39:53.663
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:39:53.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:39:53.688
  STEP: Creating configMap configmap-570/configmap-test-b86e79c4-72ec-43f5-bd2a-98fafc18a169 @ 05/27/23 13:39:53.693
  STEP: Creating a pod to test consume configMaps @ 05/27/23 13:39:53.699
  E0527 13:39:53.962794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:54.963211      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:55.963691      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:56.963850      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:39:57.725
  May 27 13:39:57.731: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-configmaps-40c4ee9d-6cf8-428c-a663-4cc578405992 container env-test: <nil>
  STEP: delete the pod @ 05/27/23 13:39:57.741
  May 27 13:39:57.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-570" for this suite. @ 05/27/23 13:39:57.766
• [4.113 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 05/27/23 13:39:57.777
  May 27 13:39:57.777: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename security-context-test @ 05/27/23 13:39:57.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:39:57.799
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:39:57.803
  E0527 13:39:57.964432      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:58.964585      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:39:59.965058      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:00.965120      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:40:01.842: INFO: Got logs for pod "busybox-privileged-false-fafc49d9-5647-4ddd-8102-30cf860626c0": "ip: RTNETLINK answers: Operation not permitted\n"
  May 27 13:40:01.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5278" for this suite. @ 05/27/23 13:40:01.849
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 05/27/23 13:40:01.862
  May 27 13:40:01.862: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/27/23 13:40:01.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:40:01.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:40:01.895
  May 27 13:40:01.902: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  E0527 13:40:01.965984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:02.966048      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 05/27/23 13:40:03.599
  May 27 13:40:03.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8186 --namespace=crd-publish-openapi-8186 create -f -'
  E0527 13:40:03.966863      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:40:04.860: INFO: stderr: ""
  May 27 13:40:04.860: INFO: stdout: "e2e-test-crd-publish-openapi-9983-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May 27 13:40:04.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8186 --namespace=crd-publish-openapi-8186 delete e2e-test-crd-publish-openapi-9983-crds test-foo'
  May 27 13:40:04.958: INFO: stderr: ""
  May 27 13:40:04.958: INFO: stdout: "e2e-test-crd-publish-openapi-9983-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  May 27 13:40:04.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8186 --namespace=crd-publish-openapi-8186 apply -f -'
  E0527 13:40:04.967903      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:40:05.631: INFO: stderr: ""
  May 27 13:40:05.631: INFO: stdout: "e2e-test-crd-publish-openapi-9983-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May 27 13:40:05.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8186 --namespace=crd-publish-openapi-8186 delete e2e-test-crd-publish-openapi-9983-crds test-foo'
  May 27 13:40:05.733: INFO: stderr: ""
  May 27 13:40:05.733: INFO: stdout: "e2e-test-crd-publish-openapi-9983-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 05/27/23 13:40:05.733
  May 27 13:40:05.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8186 --namespace=crd-publish-openapi-8186 create -f -'
  E0527 13:40:05.968792      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:40:05.982: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 05/27/23 13:40:05.982
  May 27 13:40:05.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8186 --namespace=crd-publish-openapi-8186 create -f -'
  May 27 13:40:06.238: INFO: rc: 1
  May 27 13:40:06.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8186 --namespace=crd-publish-openapi-8186 apply -f -'
  May 27 13:40:06.484: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 05/27/23 13:40:06.484
  May 27 13:40:06.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8186 --namespace=crd-publish-openapi-8186 create -f -'
  May 27 13:40:06.744: INFO: rc: 1
  May 27 13:40:06.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8186 --namespace=crd-publish-openapi-8186 apply -f -'
  E0527 13:40:06.969022      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:40:06.993: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 05/27/23 13:40:06.993
  May 27 13:40:06.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8186 explain e2e-test-crd-publish-openapi-9983-crds'
  May 27 13:40:07.243: INFO: stderr: ""
  May 27 13:40:07.243: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9983-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 05/27/23 13:40:07.243
  May 27 13:40:07.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8186 explain e2e-test-crd-publish-openapi-9983-crds.metadata'
  May 27 13:40:07.472: INFO: stderr: ""
  May 27 13:40:07.472: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9983-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  May 27 13:40:07.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8186 explain e2e-test-crd-publish-openapi-9983-crds.spec'
  May 27 13:40:07.728: INFO: stderr: ""
  May 27 13:40:07.728: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9983-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  May 27 13:40:07.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8186 explain e2e-test-crd-publish-openapi-9983-crds.spec.bars'
  E0527 13:40:07.969520      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:40:07.984: INFO: stderr: ""
  May 27 13:40:07.984: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9983-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 05/27/23 13:40:07.984
  May 27 13:40:07.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=crd-publish-openapi-8186 explain e2e-test-crd-publish-openapi-9983-crds.spec.bars2'
  May 27 13:40:08.247: INFO: rc: 1
  E0527 13:40:08.970474      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:40:09.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8186" for this suite. @ 05/27/23 13:40:09.906
• [8.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 05/27/23 13:40:09.918
  May 27 13:40:09.918: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename subpath @ 05/27/23 13:40:09.919
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:40:09.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:40:09.945
  STEP: Setting up data @ 05/27/23 13:40:09.95
  STEP: Creating pod pod-subpath-test-downwardapi-mjkh @ 05/27/23 13:40:09.963
  STEP: Creating a pod to test atomic-volume-subpath @ 05/27/23 13:40:09.963
  E0527 13:40:09.970637      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:10.971610      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:11.971695      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:12.972171      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:13.972385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:14.972476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:15.972591      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:16.972699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:17.973282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:18.973405      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:19.973688      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:20.973848      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:21.974269      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:22.974592      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:23.974672      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:24.974742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:25.974948      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:26.975256      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:27.975381      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:28.976294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:29.977289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:30.977833      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:31.977926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:32.978052      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:33.978216      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:40:34.055
  May 27 13:40:34.059: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-subpath-test-downwardapi-mjkh container test-container-subpath-downwardapi-mjkh: <nil>
  STEP: delete the pod @ 05/27/23 13:40:34.08
  STEP: Deleting pod pod-subpath-test-downwardapi-mjkh @ 05/27/23 13:40:34.101
  May 27 13:40:34.101: INFO: Deleting pod "pod-subpath-test-downwardapi-mjkh" in namespace "subpath-9716"
  May 27 13:40:34.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9716" for this suite. @ 05/27/23 13:40:34.115
• [24.210 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 05/27/23 13:40:34.136
  May 27 13:40:34.136: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename ingress @ 05/27/23 13:40:34.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:40:34.164
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:40:34.168
  STEP: getting /apis @ 05/27/23 13:40:34.173
  STEP: getting /apis/networking.k8s.io @ 05/27/23 13:40:34.179
  STEP: getting /apis/networking.k8s.iov1 @ 05/27/23 13:40:34.18
  STEP: creating @ 05/27/23 13:40:34.182
  STEP: getting @ 05/27/23 13:40:34.209
  STEP: listing @ 05/27/23 13:40:34.215
  STEP: watching @ 05/27/23 13:40:34.221
  May 27 13:40:34.222: INFO: starting watch
  STEP: cluster-wide listing @ 05/27/23 13:40:34.226
  STEP: cluster-wide watching @ 05/27/23 13:40:34.231
  May 27 13:40:34.231: INFO: starting watch
  STEP: patching @ 05/27/23 13:40:34.234
  STEP: updating @ 05/27/23 13:40:34.245
  May 27 13:40:34.258: INFO: waiting for watch events with expected annotations
  May 27 13:40:34.259: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/27/23 13:40:34.26
  STEP: updating /status @ 05/27/23 13:40:34.272
  STEP: get /status @ 05/27/23 13:40:34.294
  STEP: deleting @ 05/27/23 13:40:34.31
  STEP: deleting a collection @ 05/27/23 13:40:34.335
  May 27 13:40:34.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-9087" for this suite. @ 05/27/23 13:40:34.368
• [0.242 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 05/27/23 13:40:34.378
  May 27 13:40:34.378: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename namespaces @ 05/27/23 13:40:34.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:40:34.405
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:40:34.411
  STEP: Creating namespace "e2e-ns-8lrqf" @ 05/27/23 13:40:34.417
  May 27 13:40:34.438: INFO: Namespace "e2e-ns-8lrqf-8335" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-8lrqf-8335" @ 05/27/23 13:40:34.438
  May 27 13:40:34.452: INFO: Namespace "e2e-ns-8lrqf-8335" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-8lrqf-8335" @ 05/27/23 13:40:34.452
  May 27 13:40:34.465: INFO: Namespace "e2e-ns-8lrqf-8335" has []v1.FinalizerName{"kubernetes"}
  May 27 13:40:34.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9094" for this suite. @ 05/27/23 13:40:34.471
  STEP: Destroying namespace "e2e-ns-8lrqf-8335" for this suite. @ 05/27/23 13:40:34.479
• [0.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 05/27/23 13:40:34.494
  May 27 13:40:34.494: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename security-context @ 05/27/23 13:40:34.495
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:40:34.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:40:34.524
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/27/23 13:40:34.531
  E0527 13:40:34.978307      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:35.978474      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:36.978502      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:37.978607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:40:38.56
  May 27 13:40:38.565: INFO: Trying to get logs from node ip-172-31-68-172 pod security-context-cc5b13a7-194b-49ba-8e7e-d5960dc6be2a container test-container: <nil>
  STEP: delete the pod @ 05/27/23 13:40:38.574
  May 27 13:40:38.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-7627" for this suite. @ 05/27/23 13:40:38.599
• [4.114 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 05/27/23 13:40:38.609
  May 27 13:40:38.609: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename statefulset @ 05/27/23 13:40:38.61
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:40:38.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:40:38.635
  STEP: Creating service test in namespace statefulset-5734 @ 05/27/23 13:40:38.64
  STEP: Looking for a node to schedule stateful set and pod @ 05/27/23 13:40:38.648
  STEP: Creating pod with conflicting port in namespace statefulset-5734 @ 05/27/23 13:40:38.66
  STEP: Waiting until pod test-pod will start running in namespace statefulset-5734 @ 05/27/23 13:40:38.675
  E0527 13:40:38.978795      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:39.978883      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-5734 @ 05/27/23 13:40:40.685
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5734 @ 05/27/23 13:40:40.692
  May 27 13:40:40.708: INFO: Observed stateful pod in namespace: statefulset-5734, name: ss-0, uid: db3501d6-85d4-4ed6-bd5e-bb29a1f259b5, status phase: Pending. Waiting for statefulset controller to delete.
  May 27 13:40:40.731: INFO: Observed stateful pod in namespace: statefulset-5734, name: ss-0, uid: db3501d6-85d4-4ed6-bd5e-bb29a1f259b5, status phase: Failed. Waiting for statefulset controller to delete.
  May 27 13:40:40.755: INFO: Observed stateful pod in namespace: statefulset-5734, name: ss-0, uid: db3501d6-85d4-4ed6-bd5e-bb29a1f259b5, status phase: Failed. Waiting for statefulset controller to delete.
  May 27 13:40:40.759: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5734
  STEP: Removing pod with conflicting port in namespace statefulset-5734 @ 05/27/23 13:40:40.759
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5734 and will be in running state @ 05/27/23 13:40:40.789
  E0527 13:40:40.979211      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:41.979237      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:40:42.803: INFO: Deleting all statefulset in ns statefulset-5734
  May 27 13:40:42.809: INFO: Scaling statefulset ss to 0
  E0527 13:40:42.980122      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:43.980222      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:44.980399      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:45.981391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:46.981532      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:47.981599      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:48.981682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:49.982193      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:50.982688      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:51.982990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:40:52.833: INFO: Waiting for statefulset status.replicas updated to 0
  May 27 13:40:52.837: INFO: Deleting statefulset ss
  May 27 13:40:52.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5734" for this suite. @ 05/27/23 13:40:52.861
• [14.260 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 05/27/23 13:40:52.871
  May 27 13:40:52.871: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename svcaccounts @ 05/27/23 13:40:52.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:40:52.899
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:40:52.904
  STEP: creating a ServiceAccount @ 05/27/23 13:40:52.908
  STEP: watching for the ServiceAccount to be added @ 05/27/23 13:40:52.92
  STEP: patching the ServiceAccount @ 05/27/23 13:40:52.926
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 05/27/23 13:40:52.934
  STEP: deleting the ServiceAccount @ 05/27/23 13:40:52.939
  May 27 13:40:52.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6218" for this suite. @ 05/27/23 13:40:52.967
• [0.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 05/27/23 13:40:52.98
  May 27 13:40:52.980: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename daemonsets @ 05/27/23 13:40:52.981
  E0527 13:40:52.983541      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:40:53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:40:53.006
  STEP: Creating simple DaemonSet "daemon-set" @ 05/27/23 13:40:53.037
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/27/23 13:40:53.043
  May 27 13:40:53.049: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:40:53.049: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:40:53.054: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 13:40:53.054: INFO: Node ip-172-31-10-136 is running 0 daemon pod, expected 1
  E0527 13:40:53.984294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:40:54.061: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:40:54.061: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:40:54.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 13:40:54.066: INFO: Node ip-172-31-10-136 is running 0 daemon pod, expected 1
  E0527 13:40:54.984451      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:40:55.060: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:40:55.060: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:40:55.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 27 13:40:55.065: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 05/27/23 13:40:55.07
  STEP: DeleteCollection of the DaemonSets @ 05/27/23 13:40:55.075
  STEP: Verify that ReplicaSets have been deleted @ 05/27/23 13:40:55.085
  May 27 13:40:55.112: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38244"},"items":null}

  May 27 13:40:55.117: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38244"},"items":[{"metadata":{"name":"daemon-set-6hbr9","generateName":"daemon-set-","namespace":"daemonsets-8793","uid":"3656e093-3617-490b-bccf-9b94b8e7a422","resourceVersion":"38241","creationTimestamp":"2023-05-27T13:40:53Z","deletionTimestamp":"2023-05-27T13:41:25Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"41ceb38a-9c2f-408e-bea0-6589171207de","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-27T13:40:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41ceb38a-9c2f-408e-bea0-6589171207de\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-27T13:40:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.0.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-lxn4b","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-lxn4b","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-10-136","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-10-136"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-27T13:40:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-27T13:40:54Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-27T13:40:54Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-27T13:40:53Z"}],"hostIP":"172.31.10.136","podIP":"192.168.0.33","podIPs":[{"ip":"192.168.0.33"}],"startTime":"2023-05-27T13:40:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-27T13:40:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://937d770bae33dd608d73da9c0d8bf73f5d35b6e2a18486e044417c2eb08250e4","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-dfrdx","generateName":"daemon-set-","namespace":"daemonsets-8793","uid":"7f5df417-1a34-4369-91fe-36646fc2cf3d","resourceVersion":"38242","creationTimestamp":"2023-05-27T13:40:53Z","deletionTimestamp":"2023-05-27T13:41:25Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"41ceb38a-9c2f-408e-bea0-6589171207de","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-27T13:40:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41ceb38a-9c2f-408e-bea0-6589171207de\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-27T13:40:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qkkb7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qkkb7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-22-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-22-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-27T13:40:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-27T13:40:54Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-27T13:40:54Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-27T13:40:53Z"}],"hostIP":"172.31.22.3","podIP":"192.168.7.109","podIPs":[{"ip":"192.168.7.109"}],"startTime":"2023-05-27T13:40:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-27T13:40:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://158108bf5b2f8e48039a46fecd557b619bc28bbcefca3dfb38173e1b8ee579b7","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-trx45","generateName":"daemon-set-","namespace":"daemonsets-8793","uid":"fb8086fd-ceed-4881-987f-11c69228218f","resourceVersion":"38243","creationTimestamp":"2023-05-27T13:40:53Z","deletionTimestamp":"2023-05-27T13:41:25Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"41ceb38a-9c2f-408e-bea0-6589171207de","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-27T13:40:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41ceb38a-9c2f-408e-bea0-6589171207de\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-27T13:40:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.19.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-r72hl","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-r72hl","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-68-172","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-68-172"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-27T13:40:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-27T13:40:54Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-27T13:40:54Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-27T13:40:53Z"}],"hostIP":"172.31.68.172","podIP":"192.168.19.105","podIPs":[{"ip":"192.168.19.105"}],"startTime":"2023-05-27T13:40:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-27T13:40:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://aeda4b5141f2bf5d77e59aa6fef46423a2b4b6c795a8f7def4f1cf043fabd326","started":true}],"qosClass":"BestEffort"}}]}

  May 27 13:40:55.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8793" for this suite. @ 05/27/23 13:40:55.142
• [2.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 05/27/23 13:40:55.153
  May 27 13:40:55.153: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename watch @ 05/27/23 13:40:55.154
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:40:55.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:40:55.18
  STEP: creating a watch on configmaps with label A @ 05/27/23 13:40:55.185
  STEP: creating a watch on configmaps with label B @ 05/27/23 13:40:55.187
  STEP: creating a watch on configmaps with label A or B @ 05/27/23 13:40:55.189
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 05/27/23 13:40:55.191
  May 27 13:40:55.197: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1103  2eb3005c-9288-4014-a9fd-be265181d1d1 38251 0 2023-05-27 13:40:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-27 13:40:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 27 13:40:55.197: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1103  2eb3005c-9288-4014-a9fd-be265181d1d1 38251 0 2023-05-27 13:40:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-27 13:40:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 05/27/23 13:40:55.197
  May 27 13:40:55.210: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1103  2eb3005c-9288-4014-a9fd-be265181d1d1 38252 0 2023-05-27 13:40:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-27 13:40:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 27 13:40:55.210: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1103  2eb3005c-9288-4014-a9fd-be265181d1d1 38252 0 2023-05-27 13:40:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-27 13:40:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 05/27/23 13:40:55.21
  May 27 13:40:55.223: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1103  2eb3005c-9288-4014-a9fd-be265181d1d1 38253 0 2023-05-27 13:40:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-27 13:40:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 27 13:40:55.223: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1103  2eb3005c-9288-4014-a9fd-be265181d1d1 38253 0 2023-05-27 13:40:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-27 13:40:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 05/27/23 13:40:55.223
  May 27 13:40:55.231: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1103  2eb3005c-9288-4014-a9fd-be265181d1d1 38254 0 2023-05-27 13:40:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-27 13:40:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 27 13:40:55.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1103  2eb3005c-9288-4014-a9fd-be265181d1d1 38254 0 2023-05-27 13:40:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-27 13:40:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 05/27/23 13:40:55.232
  May 27 13:40:55.238: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1103  7e60bb05-66cb-44ad-81d5-30ef68981a35 38255 0 2023-05-27 13:40:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-27 13:40:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 27 13:40:55.238: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1103  7e60bb05-66cb-44ad-81d5-30ef68981a35 38255 0 2023-05-27 13:40:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-27 13:40:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0527 13:40:55.984593      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:56.984693      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:57.984836      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:58.984939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:40:59.985090      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:00.985168      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:01.985283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:02.985356      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:03.985458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:04.985570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 05/27/23 13:41:05.24
  May 27 13:41:05.248: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1103  7e60bb05-66cb-44ad-81d5-30ef68981a35 38345 0 2023-05-27 13:40:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-27 13:40:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 27 13:41:05.248: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1103  7e60bb05-66cb-44ad-81d5-30ef68981a35 38345 0 2023-05-27 13:40:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-27 13:40:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0527 13:41:05.985753      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:06.985908      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:07.985976      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:08.986078      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:09.986236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:10.986834      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:11.987187      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:12.987559      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:13.987725      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:14.987864      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:41:15.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1103" for this suite. @ 05/27/23 13:41:15.256
• [20.114 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 05/27/23 13:41:15.267
  May 27 13:41:15.267: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl @ 05/27/23 13:41:15.268
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:41:15.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:41:15.293
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/27/23 13:41:15.297
  May 27 13:41:15.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-7989 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  May 27 13:41:15.405: INFO: stderr: ""
  May 27 13:41:15.405: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/27/23 13:41:15.405
  May 27 13:41:15.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-7989 delete pods e2e-test-httpd-pod'
  E0527 13:41:15.987930      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:16.988129      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:41:17.405: INFO: stderr: ""
  May 27 13:41:17.405: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 27 13:41:17.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7989" for this suite. @ 05/27/23 13:41:17.412
• [2.152 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 05/27/23 13:41:17.421
  May 27 13:41:17.421: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename downward-api @ 05/27/23 13:41:17.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:41:17.447
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:41:17.453
  STEP: Creating a pod to test downward api env vars @ 05/27/23 13:41:17.457
  E0527 13:41:17.989038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:18.989142      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:19.989258      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:20.989445      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:41:21.488
  May 27 13:41:21.496: INFO: Trying to get logs from node ip-172-31-68-172 pod downward-api-804881e0-38f9-47b4-9e90-23c9fc4612b9 container dapi-container: <nil>
  STEP: delete the pod @ 05/27/23 13:41:21.506
  May 27 13:41:21.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3981" for this suite. @ 05/27/23 13:41:21.531
• [4.121 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 05/27/23 13:41:21.544
  May 27 13:41:21.544: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 13:41:21.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:41:21.573
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:41:21.578
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 13:41:21.583
  E0527 13:41:21.989627      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:22.989730      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:23.990000      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:24.990100      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:41:25.616
  May 27 13:41:25.623: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-19aa5d63-1c0e-4eda-b996-56551363bcc7 container client-container: <nil>
  STEP: delete the pod @ 05/27/23 13:41:25.63
  May 27 13:41:25.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4745" for this suite. @ 05/27/23 13:41:25.664
• [4.132 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 05/27/23 13:41:25.677
  May 27 13:41:25.677: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename watch @ 05/27/23 13:41:25.678
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:41:25.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:41:25.731
  STEP: creating a new configmap @ 05/27/23 13:41:25.736
  STEP: modifying the configmap once @ 05/27/23 13:41:25.742
  STEP: modifying the configmap a second time @ 05/27/23 13:41:25.753
  STEP: deleting the configmap @ 05/27/23 13:41:25.766
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 05/27/23 13:41:25.776
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 05/27/23 13:41:25.778
  May 27 13:41:25.778: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9191  2a1590e1-6d51-4abe-9870-2b61c97fd1d9 38483 0 2023-05-27 13:41:25 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-27 13:41:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 27 13:41:25.779: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9191  2a1590e1-6d51-4abe-9870-2b61c97fd1d9 38484 0 2023-05-27 13:41:25 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-27 13:41:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 27 13:41:25.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9191" for this suite. @ 05/27/23 13:41:25.785
• [0.118 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 05/27/23 13:41:25.796
  May 27 13:41:25.796: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename svcaccounts @ 05/27/23 13:41:25.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:41:25.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:41:25.819
  E0527 13:41:25.990328      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:26.990374      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 05/27/23 13:41:27.857
  May 27 13:41:27.857: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-282 pod-service-account-c5ae47b9-ea5a-4ab3-ab28-c795e16d2a65 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  E0527 13:41:27.990483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 05/27/23 13:41:28.012
  May 27 13:41:28.012: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-282 pod-service-account-c5ae47b9-ea5a-4ab3-ab28-c795e16d2a65 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 05/27/23 13:41:28.172
  May 27 13:41:28.172: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-282 pod-service-account-c5ae47b9-ea5a-4ab3-ab28-c795e16d2a65 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  May 27 13:41:28.347: INFO: Got root ca configmap in namespace "svcaccounts-282"
  May 27 13:41:28.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-282" for this suite. @ 05/27/23 13:41:28.356
• [2.570 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 05/27/23 13:41:28.366
  May 27 13:41:28.366: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir @ 05/27/23 13:41:28.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:41:28.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:41:28.392
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/27/23 13:41:28.397
  E0527 13:41:28.991000      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:29.991229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:30.991418      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:31.991640      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:41:32.428
  May 27 13:41:32.433: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-999c9965-734a-4a9e-9521-8f388ba9d3a8 container test-container: <nil>
  STEP: delete the pod @ 05/27/23 13:41:32.441
  May 27 13:41:32.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3059" for this suite. @ 05/27/23 13:41:32.464
• [4.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 05/27/23 13:41:32.476
  May 27 13:41:32.476: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename secrets @ 05/27/23 13:41:32.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:41:32.5
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:41:32.505
  STEP: creating secret secrets-9129/secret-test-db76103e-2d77-4d9b-a0f1-c480bcfbb92a @ 05/27/23 13:41:32.51
  STEP: Creating a pod to test consume secrets @ 05/27/23 13:41:32.515
  E0527 13:41:32.992109      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:33.992275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:34.992381      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:35.992661      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:41:36.548
  May 27 13:41:36.553: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-configmaps-329c5021-963f-4534-adb2-08a0aecd4a7c container env-test: <nil>
  STEP: delete the pod @ 05/27/23 13:41:36.563
  May 27 13:41:36.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9129" for this suite. @ 05/27/23 13:41:36.586
• [4.118 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 05/27/23 13:41:36.595
  May 27 13:41:36.595: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir @ 05/27/23 13:41:36.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:41:36.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:41:36.622
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 05/27/23 13:41:36.631
  E0527 13:41:36.993642      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:37.994699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:38.994854      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:39.995021      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:41:40.664
  May 27 13:41:40.668: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-0ed19409-c590-4471-9f41-c2abe36705cb container test-container: <nil>
  STEP: delete the pod @ 05/27/23 13:41:40.677
  May 27 13:41:40.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5620" for this suite. @ 05/27/23 13:41:40.713
• [4.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 05/27/23 13:41:40.728
  May 27 13:41:40.728: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename secrets @ 05/27/23 13:41:40.729
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:41:40.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:41:40.771
  STEP: Creating secret with name secret-test-map-d8fd0788-c722-4a1f-83ba-73927c2ef82c @ 05/27/23 13:41:40.777
  STEP: Creating a pod to test consume secrets @ 05/27/23 13:41:40.785
  E0527 13:41:40.995549      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:41.996329      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:42.997324      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:43.997443      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:41:44.819
  May 27 13:41:44.823: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-secrets-d379a5e2-c6bf-4d3f-99e1-86c0047bb1bc container secret-volume-test: <nil>
  STEP: delete the pod @ 05/27/23 13:41:44.833
  May 27 13:41:44.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3373" for this suite. @ 05/27/23 13:41:44.857
• [4.138 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 05/27/23 13:41:44.868
  May 27 13:41:44.868: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename runtimeclass @ 05/27/23 13:41:44.869
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:41:44.892
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:41:44.896
  May 27 13:41:44.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6697" for this suite. @ 05/27/23 13:41:44.944
• [0.086 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 05/27/23 13:41:44.955
  May 27 13:41:44.956: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 13:41:44.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:41:44.972
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:41:44.979
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 13:41:44.984
  E0527 13:41:44.997496      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:45.997681      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:46.997928      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:47.998146      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:48.998473      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:41:49.014
  May 27 13:41:49.019: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-6b45e354-3768-4ce1-9374-27bc85a0117f container client-container: <nil>
  STEP: delete the pod @ 05/27/23 13:41:49.028
  May 27 13:41:49.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8168" for this suite. @ 05/27/23 13:41:49.054
• [4.112 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 05/27/23 13:41:49.068
  May 27 13:41:49.068: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir @ 05/27/23 13:41:49.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:41:49.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:41:49.096
  STEP: Creating Pod @ 05/27/23 13:41:49.101
  E0527 13:41:49.998658      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:50.998734      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 05/27/23 13:41:51.127
  May 27 13:41:51.128: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2763 PodName:pod-sharedvolume-a0145486-0a1c-4c90-a3b0-4c8c3986f4c8 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 13:41:51.128: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:41:51.128: INFO: ExecWithOptions: Clientset creation
  May 27 13:41:51.128: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-2763/pods/pod-sharedvolume-a0145486-0a1c-4c90-a3b0-4c8c3986f4c8/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  May 27 13:41:51.220: INFO: Exec stderr: ""
  May 27 13:41:51.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2763" for this suite. @ 05/27/23 13:41:51.228
• [2.169 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 05/27/23 13:41:51.237
  May 27 13:41:51.237: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename discovery @ 05/27/23 13:41:51.238
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:41:51.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:41:51.264
  STEP: Setting up server cert @ 05/27/23 13:41:51.271
  May 27 13:41:51.918: INFO: Checking APIGroup: apiregistration.k8s.io
  May 27 13:41:51.920: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  May 27 13:41:51.920: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  May 27 13:41:51.920: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  May 27 13:41:51.920: INFO: Checking APIGroup: apps
  May 27 13:41:51.922: INFO: PreferredVersion.GroupVersion: apps/v1
  May 27 13:41:51.922: INFO: Versions found [{apps/v1 v1}]
  May 27 13:41:51.922: INFO: apps/v1 matches apps/v1
  May 27 13:41:51.922: INFO: Checking APIGroup: events.k8s.io
  May 27 13:41:51.923: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  May 27 13:41:51.924: INFO: Versions found [{events.k8s.io/v1 v1}]
  May 27 13:41:51.924: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  May 27 13:41:51.924: INFO: Checking APIGroup: authentication.k8s.io
  May 27 13:41:51.926: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  May 27 13:41:51.926: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  May 27 13:41:51.926: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  May 27 13:41:51.926: INFO: Checking APIGroup: authorization.k8s.io
  May 27 13:41:51.927: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  May 27 13:41:51.927: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  May 27 13:41:51.927: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  May 27 13:41:51.927: INFO: Checking APIGroup: autoscaling
  May 27 13:41:51.929: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  May 27 13:41:51.929: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  May 27 13:41:51.929: INFO: autoscaling/v2 matches autoscaling/v2
  May 27 13:41:51.929: INFO: Checking APIGroup: batch
  May 27 13:41:51.931: INFO: PreferredVersion.GroupVersion: batch/v1
  May 27 13:41:51.931: INFO: Versions found [{batch/v1 v1}]
  May 27 13:41:51.931: INFO: batch/v1 matches batch/v1
  May 27 13:41:51.931: INFO: Checking APIGroup: certificates.k8s.io
  May 27 13:41:51.932: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  May 27 13:41:51.933: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  May 27 13:41:51.933: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  May 27 13:41:51.933: INFO: Checking APIGroup: networking.k8s.io
  May 27 13:41:51.934: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  May 27 13:41:51.935: INFO: Versions found [{networking.k8s.io/v1 v1}]
  May 27 13:41:51.935: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  May 27 13:41:51.935: INFO: Checking APIGroup: policy
  May 27 13:41:51.936: INFO: PreferredVersion.GroupVersion: policy/v1
  May 27 13:41:51.936: INFO: Versions found [{policy/v1 v1}]
  May 27 13:41:51.936: INFO: policy/v1 matches policy/v1
  May 27 13:41:51.936: INFO: Checking APIGroup: rbac.authorization.k8s.io
  May 27 13:41:51.938: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  May 27 13:41:51.938: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  May 27 13:41:51.938: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  May 27 13:41:51.938: INFO: Checking APIGroup: storage.k8s.io
  May 27 13:41:51.939: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  May 27 13:41:51.939: INFO: Versions found [{storage.k8s.io/v1 v1}]
  May 27 13:41:51.939: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  May 27 13:41:51.939: INFO: Checking APIGroup: admissionregistration.k8s.io
  May 27 13:41:51.941: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  May 27 13:41:51.941: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  May 27 13:41:51.941: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  May 27 13:41:51.941: INFO: Checking APIGroup: apiextensions.k8s.io
  May 27 13:41:51.943: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  May 27 13:41:51.943: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  May 27 13:41:51.943: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  May 27 13:41:51.943: INFO: Checking APIGroup: scheduling.k8s.io
  May 27 13:41:51.945: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  May 27 13:41:51.945: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  May 27 13:41:51.945: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  May 27 13:41:51.945: INFO: Checking APIGroup: coordination.k8s.io
  May 27 13:41:51.946: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  May 27 13:41:51.946: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  May 27 13:41:51.946: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  May 27 13:41:51.946: INFO: Checking APIGroup: node.k8s.io
  May 27 13:41:51.948: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  May 27 13:41:51.948: INFO: Versions found [{node.k8s.io/v1 v1}]
  May 27 13:41:51.948: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  May 27 13:41:51.948: INFO: Checking APIGroup: discovery.k8s.io
  May 27 13:41:51.949: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  May 27 13:41:51.949: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  May 27 13:41:51.949: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  May 27 13:41:51.949: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  May 27 13:41:51.951: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  May 27 13:41:51.951: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  May 27 13:41:51.951: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  May 27 13:41:51.951: INFO: Checking APIGroup: metrics.k8s.io
  May 27 13:41:51.953: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  May 27 13:41:51.953: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  May 27 13:41:51.953: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  May 27 13:41:51.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-1966" for this suite. @ 05/27/23 13:41:51.958
• [0.731 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 05/27/23 13:41:51.974
  May 27 13:41:51.974: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-runtime @ 05/27/23 13:41:51.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:41:51.994
  E0527 13:41:51.998773      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:41:52.001
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 05/27/23 13:41:52.018
  E0527 13:41:52.999226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:53.999364      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:54.999515      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:55.999603      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:57.000248      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:58.000353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:41:59.001439      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:00.001529      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:01.001564      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:02.002454      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:03.003568      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:04.003662      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:05.004506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:06.004665      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:07.004756      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:08.004874      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:09.005022      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:10.005556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:11.006570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 05/27/23 13:42:11.132
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 05/27/23 13:42:11.136
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 05/27/23 13:42:11.146
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 05/27/23 13:42:11.146
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 05/27/23 13:42:11.175
  E0527 13:42:12.006680      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:13.006822      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:14.006969      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 05/27/23 13:42:14.197
  E0527 13:42:15.007263      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 05/27/23 13:42:15.208
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 05/27/23 13:42:15.215
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 05/27/23 13:42:15.216
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 05/27/23 13:42:15.248
  E0527 13:42:16.008305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 05/27/23 13:42:16.262
  E0527 13:42:17.008918      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:18.009814      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 05/27/23 13:42:18.277
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 05/27/23 13:42:18.288
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 05/27/23 13:42:18.288
  May 27 13:42:18.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1123" for this suite. @ 05/27/23 13:42:18.325
• [26.364 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 05/27/23 13:42:18.339
  May 27 13:42:18.339: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename field-validation @ 05/27/23 13:42:18.34
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:42:18.361
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:42:18.368
  May 27 13:42:18.373: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  E0527 13:42:19.010909      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:20.011225      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0527 13:42:20.945623      18 warnings.go:70] unknown field "alpha"
  W0527 13:42:20.945805      18 warnings.go:70] unknown field "beta"
  W0527 13:42:20.945899      18 warnings.go:70] unknown field "delta"
  W0527 13:42:20.946024      18 warnings.go:70] unknown field "epsilon"
  W0527 13:42:20.946140      18 warnings.go:70] unknown field "gamma"
  May 27 13:42:20.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3636" for this suite. @ 05/27/23 13:42:20.99
• [2.659 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 05/27/23 13:42:21.001
  May 27 13:42:21.001: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename ingressclass @ 05/27/23 13:42:21.002
  E0527 13:42:21.011961      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:42:21.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:42:21.025
  STEP: getting /apis @ 05/27/23 13:42:21.029
  STEP: getting /apis/networking.k8s.io @ 05/27/23 13:42:21.035
  STEP: getting /apis/networking.k8s.iov1 @ 05/27/23 13:42:21.037
  STEP: creating @ 05/27/23 13:42:21.039
  STEP: getting @ 05/27/23 13:42:21.058
  STEP: listing @ 05/27/23 13:42:21.062
  STEP: watching @ 05/27/23 13:42:21.067
  May 27 13:42:21.067: INFO: starting watch
  STEP: patching @ 05/27/23 13:42:21.069
  STEP: updating @ 05/27/23 13:42:21.076
  May 27 13:42:21.082: INFO: waiting for watch events with expected annotations
  May 27 13:42:21.082: INFO: saw patched and updated annotations
  STEP: deleting @ 05/27/23 13:42:21.082
  STEP: deleting a collection @ 05/27/23 13:42:21.099
  May 27 13:42:21.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-7781" for this suite. @ 05/27/23 13:42:21.123
• [0.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 05/27/23 13:42:21.131
  May 27 13:42:21.131: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename hostport @ 05/27/23 13:42:21.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:42:21.151
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:42:21.157
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 05/27/23 13:42:21.172
  E0527 13:42:22.012308      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:23.012798      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.22.3 on the node which pod1 resides and expect scheduled @ 05/27/23 13:42:23.192
  E0527 13:42:24.012937      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:25.013045      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.22.3 but use UDP protocol on the node which pod2 resides @ 05/27/23 13:42:25.22
  E0527 13:42:26.013073      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:27.013148      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:28.013282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:29.013395      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 05/27/23 13:42:29.263
  May 27 13:42:29.263: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.22.3 http://127.0.0.1:54323/hostname] Namespace:hostport-3440 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 13:42:29.263: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:42:29.263: INFO: ExecWithOptions: Clientset creation
  May 27 13:42:29.264: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-3440/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.22.3+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.22.3, port: 54323 @ 05/27/23 13:42:29.347
  May 27 13:42:29.347: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.22.3:54323/hostname] Namespace:hostport-3440 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 13:42:29.347: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:42:29.348: INFO: ExecWithOptions: Clientset creation
  May 27 13:42:29.348: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-3440/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.22.3%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.22.3, port: 54323 UDP @ 05/27/23 13:42:29.431
  May 27 13:42:29.431: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.22.3 54323] Namespace:hostport-3440 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 13:42:29.431: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:42:29.432: INFO: ExecWithOptions: Clientset creation
  May 27 13:42:29.432: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-3440/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.22.3+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0527 13:42:30.013488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:31.013616      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:32.013692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:33.014535      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:34.014636      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:42:34.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-3440" for this suite. @ 05/27/23 13:42:34.525
• [13.402 seconds]
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 05/27/23 13:42:34.534
  May 27 13:42:34.534: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename field-validation @ 05/27/23 13:42:34.535
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:42:34.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:42:34.563
  May 27 13:42:34.569: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  E0527 13:42:35.014962      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:36.016000      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:37.016542      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0527 13:42:37.155020      18 warnings.go:70] unknown field "alpha"
  W0527 13:42:37.155250      18 warnings.go:70] unknown field "beta"
  W0527 13:42:37.155352      18 warnings.go:70] unknown field "delta"
  W0527 13:42:37.155462      18 warnings.go:70] unknown field "epsilon"
  W0527 13:42:37.155569      18 warnings.go:70] unknown field "gamma"
  May 27 13:42:37.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5690" for this suite. @ 05/27/23 13:42:37.207
• [2.681 seconds]
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 05/27/23 13:42:37.215
  May 27 13:42:37.215: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename cronjob @ 05/27/23 13:42:37.216
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:42:37.238
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:42:37.244
  STEP: Creating a cronjob @ 05/27/23 13:42:37.249
  STEP: creating @ 05/27/23 13:42:37.249
  STEP: getting @ 05/27/23 13:42:37.258
  STEP: listing @ 05/27/23 13:42:37.262
  STEP: watching @ 05/27/23 13:42:37.266
  May 27 13:42:37.266: INFO: starting watch
  STEP: cluster-wide listing @ 05/27/23 13:42:37.269
  STEP: cluster-wide watching @ 05/27/23 13:42:37.275
  May 27 13:42:37.275: INFO: starting watch
  STEP: patching @ 05/27/23 13:42:37.277
  STEP: updating @ 05/27/23 13:42:37.287
  May 27 13:42:37.300: INFO: waiting for watch events with expected annotations
  May 27 13:42:37.300: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/27/23 13:42:37.3
  STEP: updating /status @ 05/27/23 13:42:37.309
  STEP: get /status @ 05/27/23 13:42:37.32
  STEP: deleting @ 05/27/23 13:42:37.325
  STEP: deleting a collection @ 05/27/23 13:42:37.346
  May 27 13:42:37.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6594" for this suite. @ 05/27/23 13:42:37.368
• [0.161 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 05/27/23 13:42:37.379
  May 27 13:42:37.379: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename containers @ 05/27/23 13:42:37.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:42:37.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:42:37.405
  STEP: Creating a pod to test override command @ 05/27/23 13:42:37.409
  E0527 13:42:38.017572      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:39.018376      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:40.018535      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:41.018598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:42:41.439
  May 27 13:42:41.443: INFO: Trying to get logs from node ip-172-31-68-172 pod client-containers-a777b68a-451c-4dfb-9214-b168c2ea4de1 container agnhost-container: <nil>
  STEP: delete the pod @ 05/27/23 13:42:41.453
  May 27 13:42:41.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2308" for this suite. @ 05/27/23 13:42:41.481
• [4.112 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 05/27/23 13:42:41.491
  May 27 13:42:41.491: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename secrets @ 05/27/23 13:42:41.492
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:42:41.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:42:41.517
  STEP: Creating secret with name secret-test-16f00d0a-9652-4c0e-b5df-c1d45e0c3fc6 @ 05/27/23 13:42:41.521
  STEP: Creating a pod to test consume secrets @ 05/27/23 13:42:41.529
  E0527 13:42:42.019652      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:43.019674      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:44.019803      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:45.019921      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:42:45.56
  May 27 13:42:45.563: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-secrets-fde85c87-1bef-474d-8b96-aac39eab4e30 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/27/23 13:42:45.572
  May 27 13:42:45.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9413" for this suite. @ 05/27/23 13:42:45.597
• [4.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 05/27/23 13:42:45.611
  May 27 13:42:45.611: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl @ 05/27/23 13:42:45.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:42:45.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:42:45.64
  STEP: creating a replication controller @ 05/27/23 13:42:45.645
  May 27 13:42:45.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6013 create -f -'
  E0527 13:42:46.020250      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:42:46.623: INFO: stderr: ""
  May 27 13:42:46.623: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/27/23 13:42:46.623
  May 27 13:42:46.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6013 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 27 13:42:46.719: INFO: stderr: ""
  May 27 13:42:46.719: INFO: stdout: "update-demo-nautilus-p92r9 update-demo-nautilus-q7tx8 "
  May 27 13:42:46.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6013 get pods update-demo-nautilus-p92r9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 27 13:42:46.807: INFO: stderr: ""
  May 27 13:42:46.807: INFO: stdout: ""
  May 27 13:42:46.807: INFO: update-demo-nautilus-p92r9 is created but not running
  E0527 13:42:47.020920      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:48.021018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:49.021226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:50.021448      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:51.021929      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:42:51.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6013 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 27 13:42:51.891: INFO: stderr: ""
  May 27 13:42:51.891: INFO: stdout: "update-demo-nautilus-p92r9 update-demo-nautilus-q7tx8 "
  May 27 13:42:51.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6013 get pods update-demo-nautilus-p92r9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 27 13:42:51.974: INFO: stderr: ""
  May 27 13:42:51.974: INFO: stdout: "true"
  May 27 13:42:51.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6013 get pods update-demo-nautilus-p92r9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0527 13:42:52.022364      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:42:52.058: INFO: stderr: ""
  May 27 13:42:52.058: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 27 13:42:52.058: INFO: validating pod update-demo-nautilus-p92r9
  May 27 13:42:52.066: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 27 13:42:52.066: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 27 13:42:52.066: INFO: update-demo-nautilus-p92r9 is verified up and running
  May 27 13:42:52.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6013 get pods update-demo-nautilus-q7tx8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 27 13:42:52.147: INFO: stderr: ""
  May 27 13:42:52.147: INFO: stdout: "true"
  May 27 13:42:52.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6013 get pods update-demo-nautilus-q7tx8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 27 13:42:52.233: INFO: stderr: ""
  May 27 13:42:52.233: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 27 13:42:52.233: INFO: validating pod update-demo-nautilus-q7tx8
  May 27 13:42:52.240: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 27 13:42:52.240: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 27 13:42:52.240: INFO: update-demo-nautilus-q7tx8 is verified up and running
  STEP: using delete to clean up resources @ 05/27/23 13:42:52.24
  May 27 13:42:52.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6013 delete --grace-period=0 --force -f -'
  May 27 13:42:52.324: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 27 13:42:52.324: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May 27 13:42:52.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6013 get rc,svc -l name=update-demo --no-headers'
  May 27 13:42:52.469: INFO: stderr: "No resources found in kubectl-6013 namespace.\n"
  May 27 13:42:52.469: INFO: stdout: ""
  May 27 13:42:52.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-6013 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 27 13:42:52.589: INFO: stderr: ""
  May 27 13:42:52.589: INFO: stdout: ""
  May 27 13:42:52.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6013" for this suite. @ 05/27/23 13:42:52.595
• [6.993 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 05/27/23 13:42:52.604
  May 27 13:42:52.604: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename daemonsets @ 05/27/23 13:42:52.605
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:42:52.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:42:52.629
  STEP: Creating a simple DaemonSet "daemon-set" @ 05/27/23 13:42:52.662
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/27/23 13:42:52.672
  May 27 13:42:52.679: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:42:52.679: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:42:52.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 13:42:52.684: INFO: Node ip-172-31-10-136 is running 0 daemon pod, expected 1
  E0527 13:42:53.022581      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:42:53.691: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:42:53.691: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:42:53.697: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 27 13:42:53.697: INFO: Node ip-172-31-10-136 is running 0 daemon pod, expected 1
  E0527 13:42:54.022757      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:42:54.691: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:42:54.691: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:42:54.695: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 27 13:42:54.695: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 05/27/23 13:42:54.7
  May 27 13:42:54.724: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:42:54.724: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:42:54.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 27 13:42:54.730: INFO: Node ip-172-31-22-3 is running 0 daemon pod, expected 1
  E0527 13:42:55.022911      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:42:55.737: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:42:55.737: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:42:55.742: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 27 13:42:55.742: INFO: Node ip-172-31-22-3 is running 0 daemon pod, expected 1
  E0527 13:42:56.023831      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:42:56.742: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:42:56.742: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:42:56.746: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 27 13:42:56.746: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 05/27/23 13:42:56.746
  STEP: Deleting DaemonSet "daemon-set" @ 05/27/23 13:42:56.755
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9907, will wait for the garbage collector to delete the pods @ 05/27/23 13:42:56.755
  May 27 13:42:56.822: INFO: Deleting DaemonSet.extensions daemon-set took: 10.532614ms
  May 27 13:42:56.922: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.385432ms
  E0527 13:42:57.024337      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:42:58.024776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:42:58.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 13:42:58.127: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 27 13:42:58.132: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"39469"},"items":null}

  May 27 13:42:58.138: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"39469"},"items":null}

  May 27 13:42:58.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9907" for this suite. @ 05/27/23 13:42:58.164
• [5.571 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 05/27/23 13:42:58.18
  May 27 13:42:58.180: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename init-container @ 05/27/23 13:42:58.18
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:42:58.199
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:42:58.206
  STEP: creating the pod @ 05/27/23 13:42:58.212
  May 27 13:42:58.212: INFO: PodSpec: initContainers in spec.initContainers
  E0527 13:42:59.024912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:00.025854      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:01.025914      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:02.026056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:03.026356      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:04.026525      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:05.026983      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:06.027243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:07.027331      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:08.027417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:09.027515      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:10.027659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:11.028316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:12.028407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:13.028501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:14.028917      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:15.028965      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:16.029065      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:17.029412      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:18.029511      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:19.029593      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:20.029692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:21.030443      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:22.030493      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:23.030992      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:24.031235      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:25.031308      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:26.031414      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:27.031775      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:28.032298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:29.032563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:30.032630      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:31.032819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:32.033074      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:33.033155      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:34.033448      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:35.033827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:36.033945      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:37.034176      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:38.034413      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:43:38.769: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c522d20a-78c4-43e4-abc0-13e6124a03e4", GenerateName:"", Namespace:"init-container-699", SelfLink:"", UID:"c0c6e37a-8fb2-40f0-ab13-028c50218b7d", ResourceVersion:"39615", Generation:0, CreationTimestamp:time.Date(2023, time.May, 27, 13, 42, 58, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"212774417"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 27, 13, 42, 58, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005b557d0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 27, 13, 43, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005b55800), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-hpmbv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000254de0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hpmbv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hpmbv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hpmbv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00350db68), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-68-172", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc005dc3570), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00350dbf0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00350dc10)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00350dc18), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00350dc1c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005a14040), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 27, 13, 42, 58, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 27, 13, 42, 58, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 27, 13, 42, 58, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 27, 13, 42, 58, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.68.172", PodIP:"192.168.19.78", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.19.78"}}, StartTime:time.Date(2023, time.May, 27, 13, 42, 58, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc005dc36c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc005dc3730)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://1ce937d2e10d318a6893ea4620302cca8dfbdb71d9905b7826660e6d532b636b", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000254e80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000254e60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00350dc94), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  May 27 13:43:38.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-699" for this suite. @ 05/27/23 13:43:38.775
• [40.603 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 05/27/23 13:43:38.783
  May 27 13:43:38.783: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl @ 05/27/23 13:43:38.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:43:38.808
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:43:38.812
  STEP: create deployment with httpd image @ 05/27/23 13:43:38.817
  May 27 13:43:38.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-391 create -f -'
  E0527 13:43:39.034589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:43:39.884: INFO: stderr: ""
  May 27 13:43:39.884: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 05/27/23 13:43:39.884
  May 27 13:43:39.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-391 diff -f -'
  E0527 13:43:40.035304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:43:40.155: INFO: rc: 1
  May 27 13:43:40.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-391 delete -f -'
  May 27 13:43:40.243: INFO: stderr: ""
  May 27 13:43:40.243: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  May 27 13:43:40.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-391" for this suite. @ 05/27/23 13:43:40.247
• [1.472 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 05/27/23 13:43:40.255
  May 27 13:43:40.255: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename downward-api @ 05/27/23 13:43:40.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:43:40.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:43:40.281
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 13:43:40.285
  E0527 13:43:41.036390      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:42.036763      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:43.037165      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:44.037401      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:43:44.315
  May 27 13:43:44.320: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-34bc0406-10a4-421f-b7d0-c7a055997361 container client-container: <nil>
  STEP: delete the pod @ 05/27/23 13:43:44.33
  May 27 13:43:44.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2108" for this suite. @ 05/27/23 13:43:44.352
• [4.105 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 05/27/23 13:43:44.361
  May 27 13:43:44.361: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename replicaset @ 05/27/23 13:43:44.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:43:44.381
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:43:44.387
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 05/27/23 13:43:44.392
  E0527 13:43:45.037475      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:46.037572      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 05/27/23 13:43:46.421
  STEP: Then the orphan pod is adopted @ 05/27/23 13:43:46.428
  E0527 13:43:47.038267      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 05/27/23 13:43:47.44
  May 27 13:43:47.445: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/27/23 13:43:47.46
  E0527 13:43:48.038464      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:43:48.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8260" for this suite. @ 05/27/23 13:43:48.478
• [4.127 seconds]
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 05/27/23 13:43:48.489
  May 27 13:43:48.489: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename pod-network-test @ 05/27/23 13:43:48.49
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:43:48.509
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:43:48.513
  STEP: Performing setup for networking test in namespace pod-network-test-1821 @ 05/27/23 13:43:48.518
  STEP: creating a selector @ 05/27/23 13:43:48.519
  STEP: Creating the service pods in kubernetes @ 05/27/23 13:43:48.519
  May 27 13:43:48.519: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0527 13:43:49.038856      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:50.038923      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:51.039242      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:52.039362      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:53.040038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:54.040155      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:55.040668      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:56.040747      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:57.041609      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:58.041744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:43:59.041744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:00.041866      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:01.042043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:02.042360      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:03.042456      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:04.042721      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:05.042917      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:06.043270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:07.044323      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:08.044684      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:09.044844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:10.045327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/27/23 13:44:10.655
  E0527 13:44:11.046021      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:12.045995      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:44:12.680: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May 27 13:44:12.680: INFO: Breadth first check of 192.168.0.23 on host 172.31.10.136...
  May 27 13:44:12.684: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.19.123:9080/dial?request=hostname&protocol=udp&host=192.168.0.23&port=8081&tries=1'] Namespace:pod-network-test-1821 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 13:44:12.684: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:44:12.685: INFO: ExecWithOptions: Clientset creation
  May 27 13:44:12.685: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1821/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.19.123%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.0.23%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 27 13:44:12.758: INFO: Waiting for responses: map[]
  May 27 13:44:12.758: INFO: reached 192.168.0.23 after 0/1 tries
  May 27 13:44:12.758: INFO: Breadth first check of 192.168.7.90 on host 172.31.22.3...
  May 27 13:44:12.764: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.19.123:9080/dial?request=hostname&protocol=udp&host=192.168.7.90&port=8081&tries=1'] Namespace:pod-network-test-1821 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 13:44:12.764: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:44:12.765: INFO: ExecWithOptions: Clientset creation
  May 27 13:44:12.765: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1821/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.19.123%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.7.90%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 27 13:44:12.850: INFO: Waiting for responses: map[]
  May 27 13:44:12.850: INFO: reached 192.168.7.90 after 0/1 tries
  May 27 13:44:12.850: INFO: Breadth first check of 192.168.19.122 on host 172.31.68.172...
  May 27 13:44:12.855: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.19.123:9080/dial?request=hostname&protocol=udp&host=192.168.19.122&port=8081&tries=1'] Namespace:pod-network-test-1821 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 13:44:12.855: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:44:12.856: INFO: ExecWithOptions: Clientset creation
  May 27 13:44:12.856: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1821/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.19.123%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.19.122%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 27 13:44:12.941: INFO: Waiting for responses: map[]
  May 27 13:44:12.941: INFO: reached 192.168.19.122 after 0/1 tries
  May 27 13:44:12.941: INFO: Going to retry 0 out of 3 pods....
  May 27 13:44:12.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1821" for this suite. @ 05/27/23 13:44:12.946
• [24.467 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 05/27/23 13:44:12.966
  May 27 13:44:12.966: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 05/27/23 13:44:12.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:44:12.996
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:44:13
  STEP: creating a target pod @ 05/27/23 13:44:13.004
  E0527 13:44:13.046466      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:14.046567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 05/27/23 13:44:15.033
  E0527 13:44:15.046743      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:16.047234      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:17.047353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 05/27/23 13:44:17.061
  May 27 13:44:17.061: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6186 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 13:44:17.061: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:44:17.061: INFO: ExecWithOptions: Clientset creation
  May 27 13:44:17.062: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-6186/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  May 27 13:44:17.138: INFO: Exec stderr: ""
  May 27 13:44:17.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-6186" for this suite. @ 05/27/23 13:44:17.155
• [4.197 seconds]
------------------------------
S
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 05/27/23 13:44:17.164
  May 27 13:44:17.164: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename endpointslice @ 05/27/23 13:44:17.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:44:17.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:44:17.188
  May 27 13:44:17.206: INFO: Endpoints addresses: [172.31.13.250 172.31.77.211] , ports: [6443]
  May 27 13:44:17.206: INFO: EndpointSlices addresses: [172.31.13.250 172.31.77.211] , ports: [6443]
  May 27 13:44:17.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7140" for this suite. @ 05/27/23 13:44:17.212
• [0.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 05/27/23 13:44:17.226
  May 27 13:44:17.226: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename svcaccounts @ 05/27/23 13:44:17.227
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:44:17.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:44:17.25
  May 27 13:44:17.274: INFO: created pod
  E0527 13:44:18.047519      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:19.047727      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:20.047769      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:21.047995      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:44:21.294
  E0527 13:44:22.048169      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:23.048358      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:24.048823      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:25.048937      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:26.049039      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:27.049143      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:28.049287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:29.049583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:30.050001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:31.050121      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:32.050266      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:33.050993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:34.051210      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:35.053421      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:36.053513      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:37.053800      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:38.054247      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:39.054369      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:40.054471      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:41.054966      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:42.055142      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:43.056032      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:44.056134      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:45.056578      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:46.056682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:47.056786      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:48.057882      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:49.058526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:50.058854      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:51.058903      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:44:51.295: INFO: polling logs
  May 27 13:44:51.308: INFO: Pod logs: 
  I0527 13:44:18.100592       1 log.go:198] OK: Got token
  I0527 13:44:18.100725       1 log.go:198] validating with in-cluster discovery
  I0527 13:44:18.101108       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0527 13:44:18.101199       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1980:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1685195657, NotBefore:1685195057, IssuedAt:1685195057, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1980", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"f3a8721b-3b95-4670-baf5-148d9eb51841"}}}
  I0527 13:44:18.113675       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0527 13:44:18.122033       1 log.go:198] OK: Validated signature on JWT
  I0527 13:44:18.122173       1 log.go:198] OK: Got valid claims from token!
  I0527 13:44:18.122212       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1980:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1685195657, NotBefore:1685195057, IssuedAt:1685195057, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1980", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"f3a8721b-3b95-4670-baf5-148d9eb51841"}}}

  May 27 13:44:51.308: INFO: completed pod
  May 27 13:44:51.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1980" for this suite. @ 05/27/23 13:44:51.322
• [34.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 05/27/23 13:44:51.332
  May 27 13:44:51.332: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename downward-api @ 05/27/23 13:44:51.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:44:51.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:44:51.362
  STEP: Creating the pod @ 05/27/23 13:44:51.366
  E0527 13:44:52.059232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:53.060329      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:44:53.915: INFO: Successfully updated pod "annotationupdate37f4089d-23b5-4e92-83c1-d4285c05c6f8"
  E0527 13:44:54.060633      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:55.061490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:44:55.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9873" for this suite. @ 05/27/23 13:44:55.941
• [4.618 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 05/27/23 13:44:55.951
  May 27 13:44:55.951: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename replication-controller @ 05/27/23 13:44:55.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:44:55.974
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:44:55.981
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 05/27/23 13:44:55.985
  E0527 13:44:56.062409      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:44:57.062462      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replication controller with a matching selector is created @ 05/27/23 13:44:58.018
  STEP: Then the orphan pod is adopted @ 05/27/23 13:44:58.026
  E0527 13:44:58.063312      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:44:59.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2587" for this suite. @ 05/27/23 13:44:59.045
• [3.101 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 05/27/23 13:44:59.053
  May 27 13:44:59.053: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename services @ 05/27/23 13:44:59.054
  E0527 13:44:59.063329      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:44:59.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:44:59.08
  STEP: creating service in namespace services-1080 @ 05/27/23 13:44:59.086
  STEP: creating service affinity-clusterip-transition in namespace services-1080 @ 05/27/23 13:44:59.086
  STEP: creating replication controller affinity-clusterip-transition in namespace services-1080 @ 05/27/23 13:44:59.106
  I0527 13:44:59.125156      18 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-1080, replica count: 3
  E0527 13:45:00.064473      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:01.065402      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:02.066591      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0527 13:45:02.177284      18 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 27 13:45:02.187: INFO: Creating new exec pod
  E0527 13:45:03.066653      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:04.066716      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:05.067294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:45:05.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-1080 exec execpod-affinityc7266 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  May 27 13:45:05.371: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  May 27 13:45:05.371: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 13:45:05.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-1080 exec execpod-affinityc7266 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.248 80'
  May 27 13:45:05.573: INFO: stderr: "+ nc -v -t -w 2 10.152.183.248 80\n+ echo hostName\nConnection to 10.152.183.248 80 port [tcp/http] succeeded!\n"
  May 27 13:45:05.573: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 27 13:45:05.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-1080 exec execpod-affinityc7266 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.248:80/ ; done'
  May 27 13:45:05.847: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n"
  May 27 13:45:05.847: INFO: stdout: "\naffinity-clusterip-transition-r4bfg\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-bdwdd\naffinity-clusterip-transition-bdwdd\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-bdwdd\naffinity-clusterip-transition-r4bfg\naffinity-clusterip-transition-r4bfg\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-r4bfg\naffinity-clusterip-transition-bdwdd\naffinity-clusterip-transition-r4bfg\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-r4bfg"
  May 27 13:45:05.847: INFO: Received response from host: affinity-clusterip-transition-r4bfg
  May 27 13:45:05.847: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:05.847: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:05.847: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:05.847: INFO: Received response from host: affinity-clusterip-transition-bdwdd
  May 27 13:45:05.847: INFO: Received response from host: affinity-clusterip-transition-bdwdd
  May 27 13:45:05.847: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:05.847: INFO: Received response from host: affinity-clusterip-transition-bdwdd
  May 27 13:45:05.847: INFO: Received response from host: affinity-clusterip-transition-r4bfg
  May 27 13:45:05.847: INFO: Received response from host: affinity-clusterip-transition-r4bfg
  May 27 13:45:05.847: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:05.847: INFO: Received response from host: affinity-clusterip-transition-r4bfg
  May 27 13:45:05.847: INFO: Received response from host: affinity-clusterip-transition-bdwdd
  May 27 13:45:05.847: INFO: Received response from host: affinity-clusterip-transition-r4bfg
  May 27 13:45:05.847: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:05.847: INFO: Received response from host: affinity-clusterip-transition-r4bfg
  May 27 13:45:05.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-1080 exec execpod-affinityc7266 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.248:80/ ; done'
  E0527 13:45:06.067734      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:45:06.124: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.248:80/\n"
  May 27 13:45:06.124: INFO: stdout: "\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9\naffinity-clusterip-transition-f8fq9"
  May 27 13:45:06.124: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:06.124: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:06.124: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:06.124: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:06.124: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:06.124: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:06.124: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:06.124: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:06.124: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:06.124: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:06.124: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:06.124: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:06.124: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:06.124: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:06.124: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:06.124: INFO: Received response from host: affinity-clusterip-transition-f8fq9
  May 27 13:45:06.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 27 13:45:06.130: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1080, will wait for the garbage collector to delete the pods @ 05/27/23 13:45:06.145
  May 27 13:45:06.218: INFO: Deleting ReplicationController affinity-clusterip-transition took: 9.691746ms
  May 27 13:45:06.319: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.923654ms
  E0527 13:45:07.068181      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:08.069204      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-1080" for this suite. @ 05/27/23 13:45:08.447
• [9.416 seconds]
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 05/27/23 13:45:08.469
  May 27 13:45:08.469: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename var-expansion @ 05/27/23 13:45:08.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:45:08.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:45:08.51
  STEP: creating the pod @ 05/27/23 13:45:08.522
  STEP: waiting for pod running @ 05/27/23 13:45:08.545
  E0527 13:45:09.069315      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:10.070228      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 05/27/23 13:45:10.556
  May 27 13:45:10.562: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3236 PodName:var-expansion-e56fc66c-f2f5-4aa8-b122-9e86cdd117df ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 13:45:10.562: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:45:10.562: INFO: ExecWithOptions: Clientset creation
  May 27 13:45:10.563: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-3236/pods/var-expansion-e56fc66c-f2f5-4aa8-b122-9e86cdd117df/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 05/27/23 13:45:10.642
  May 27 13:45:10.646: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3236 PodName:var-expansion-e56fc66c-f2f5-4aa8-b122-9e86cdd117df ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 13:45:10.646: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:45:10.647: INFO: ExecWithOptions: Clientset creation
  May 27 13:45:10.647: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-3236/pods/var-expansion-e56fc66c-f2f5-4aa8-b122-9e86cdd117df/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 05/27/23 13:45:10.725
  E0527 13:45:11.070819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:45:11.240: INFO: Successfully updated pod "var-expansion-e56fc66c-f2f5-4aa8-b122-9e86cdd117df"
  STEP: waiting for annotated pod running @ 05/27/23 13:45:11.24
  STEP: deleting the pod gracefully @ 05/27/23 13:45:11.246
  May 27 13:45:11.246: INFO: Deleting pod "var-expansion-e56fc66c-f2f5-4aa8-b122-9e86cdd117df" in namespace "var-expansion-3236"
  May 27 13:45:11.256: INFO: Wait up to 5m0s for pod "var-expansion-e56fc66c-f2f5-4aa8-b122-9e86cdd117df" to be fully deleted
  E0527 13:45:12.071294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:13.071390      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:14.072304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:15.072914      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:16.073038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:17.073126      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:18.074131      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:19.074676      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:20.074765      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:21.074868      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:22.075213      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:23.075303      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:24.075454      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:25.075568      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:26.076295      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:27.076366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:28.076515      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:29.076589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:30.077490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:31.077920      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:32.078217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:33.078317      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:34.078430      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:35.078852      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:36.078932      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:37.079264      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:38.079318      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:39.080322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:40.081030      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:41.081824      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:42.081926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:43.082156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:45:43.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3236" for this suite. @ 05/27/23 13:45:43.348
• [34.889 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 05/27/23 13:45:43.359
  May 27 13:45:43.359: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename dns @ 05/27/23 13:45:43.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:45:43.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:45:43.389
  STEP: Creating a test headless service @ 05/27/23 13:45:43.393
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9299.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9299.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9299.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9299.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9299.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9299.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9299.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9299.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9299.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9299.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9299.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9299.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 249.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.249_udp@PTR;check="$$(dig +tcp +noall +answer +search 249.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.249_tcp@PTR;sleep 1; done
   @ 05/27/23 13:45:43.416
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9299.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9299.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9299.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9299.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9299.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9299.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9299.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9299.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9299.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9299.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9299.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9299.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 249.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.249_udp@PTR;check="$$(dig +tcp +noall +answer +search 249.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.249_tcp@PTR;sleep 1; done
   @ 05/27/23 13:45:43.416
  STEP: creating a pod to probe DNS @ 05/27/23 13:45:43.416
  STEP: submitting the pod to kubernetes @ 05/27/23 13:45:43.416
  E0527 13:45:44.082867      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:45.083237      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/27/23 13:45:45.448
  STEP: looking for the results for each expected name from probers @ 05/27/23 13:45:45.453
  May 27 13:45:45.458: INFO: Unable to read wheezy_udp@dns-test-service.dns-9299.svc.cluster.local from pod dns-9299/dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d: the server could not find the requested resource (get pods dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d)
  May 27 13:45:45.463: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9299.svc.cluster.local from pod dns-9299/dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d: the server could not find the requested resource (get pods dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d)
  May 27 13:45:45.468: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9299.svc.cluster.local from pod dns-9299/dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d: the server could not find the requested resource (get pods dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d)
  May 27 13:45:45.472: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9299.svc.cluster.local from pod dns-9299/dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d: the server could not find the requested resource (get pods dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d)
  May 27 13:45:45.496: INFO: Unable to read jessie_udp@dns-test-service.dns-9299.svc.cluster.local from pod dns-9299/dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d: the server could not find the requested resource (get pods dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d)
  May 27 13:45:45.500: INFO: Unable to read jessie_tcp@dns-test-service.dns-9299.svc.cluster.local from pod dns-9299/dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d: the server could not find the requested resource (get pods dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d)
  May 27 13:45:45.506: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9299.svc.cluster.local from pod dns-9299/dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d: the server could not find the requested resource (get pods dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d)
  May 27 13:45:45.511: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9299.svc.cluster.local from pod dns-9299/dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d: the server could not find the requested resource (get pods dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d)
  May 27 13:45:45.529: INFO: Lookups using dns-9299/dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d failed for: [wheezy_udp@dns-test-service.dns-9299.svc.cluster.local wheezy_tcp@dns-test-service.dns-9299.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9299.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9299.svc.cluster.local jessie_udp@dns-test-service.dns-9299.svc.cluster.local jessie_tcp@dns-test-service.dns-9299.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9299.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9299.svc.cluster.local]

  E0527 13:45:46.083337      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:47.083448      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:48.083548      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:49.084527      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:50.084657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:45:50.613: INFO: DNS probes using dns-9299/dns-test-89814a5e-12e3-41f7-adbc-667a93ea809d succeeded

  May 27 13:45:50.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/27/23 13:45:50.618
  STEP: deleting the test service @ 05/27/23 13:45:50.639
  STEP: deleting the test headless service @ 05/27/23 13:45:50.683
  STEP: Destroying namespace "dns-9299" for this suite. @ 05/27/23 13:45:50.704
• [7.358 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 05/27/23 13:45:50.718
  May 27 13:45:50.718: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/27/23 13:45:50.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:45:50.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:45:50.744
  STEP: create the container to handle the HTTPGet hook request. @ 05/27/23 13:45:50.755
  E0527 13:45:51.084935      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:52.085025      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/27/23 13:45:52.783
  E0527 13:45:53.085739      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:54.085863      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 05/27/23 13:45:54.805
  STEP: delete the pod with lifecycle hook @ 05/27/23 13:45:54.815
  E0527 13:45:55.086646      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:56.086832      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:45:56.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-7749" for this suite. @ 05/27/23 13:45:56.841
• [6.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 05/27/23 13:45:56.854
  May 27 13:45:56.854: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 13:45:56.855
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:45:56.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:45:56.884
  STEP: Setting up server cert @ 05/27/23 13:45:56.923
  E0527 13:45:57.087855      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 13:45:57.279
  STEP: Deploying the webhook pod @ 05/27/23 13:45:57.288
  STEP: Wait for the deployment to be ready @ 05/27/23 13:45:57.304
  May 27 13:45:57.314: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0527 13:45:58.088138      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:45:59.088271      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/27/23 13:45:59.331
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 13:45:59.347
  E0527 13:46:00.089250      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:46:00.348: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 05/27/23 13:46:00.354
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 05/27/23 13:46:00.375
  STEP: Creating a configMap that should not be mutated @ 05/27/23 13:46:00.385
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 05/27/23 13:46:00.399
  STEP: Creating a configMap that should be mutated @ 05/27/23 13:46:00.408
  May 27 13:46:00.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6086" for this suite. @ 05/27/23 13:46:00.508
  STEP: Destroying namespace "webhook-markers-7840" for this suite. @ 05/27/23 13:46:00.521
• [3.682 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 05/27/23 13:46:00.537
  May 27 13:46:00.537: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename namespaces @ 05/27/23 13:46:00.538
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:46:00.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:46:00.562
  STEP: Updating Namespace "namespaces-8831" @ 05/27/23 13:46:00.566
  May 27 13:46:00.577: INFO: Namespace "namespaces-8831" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"7bfe1a40-97ab-42a2-b1e3-e2a98f36b04a", "kubernetes.io/metadata.name":"namespaces-8831", "namespaces-8831":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  May 27 13:46:00.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8831" for this suite. @ 05/27/23 13:46:00.583
• [0.058 seconds]
------------------------------
S
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 05/27/23 13:46:00.596
  May 27 13:46:00.596: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename secrets @ 05/27/23 13:46:00.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:46:00.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:46:00.62
  STEP: Creating secret with name secret-test-967b0d0d-e44b-44e1-ac56-15cc56f1c7e4 @ 05/27/23 13:46:00.625
  STEP: Creating a pod to test consume secrets @ 05/27/23 13:46:00.633
  E0527 13:46:01.089446      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:02.089529      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:03.089643      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:04.090194      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:46:04.66
  May 27 13:46:04.664: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-secrets-c5d1ad31-b8fd-4849-9b03-484e3e519a7a container secret-volume-test: <nil>
  STEP: delete the pod @ 05/27/23 13:46:04.673
  May 27 13:46:04.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1961" for this suite. @ 05/27/23 13:46:04.699
• [4.114 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 05/27/23 13:46:04.715
  May 27 13:46:04.715: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 13:46:04.717
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:46:04.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:46:04.742
  STEP: Setting up server cert @ 05/27/23 13:46:04.774
  E0527 13:46:05.090818      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 13:46:05.109
  STEP: Deploying the webhook pod @ 05/27/23 13:46:05.116
  STEP: Wait for the deployment to be ready @ 05/27/23 13:46:05.131
  May 27 13:46:05.143: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0527 13:46:06.091754      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:07.091884      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/27/23 13:46:07.157
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 13:46:07.17
  E0527 13:46:08.092498      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:46:08.171: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 05/27/23 13:46:08.176
  STEP: create a pod that should be updated by the webhook @ 05/27/23 13:46:08.195
  May 27 13:46:08.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8946" for this suite. @ 05/27/23 13:46:08.296
  STEP: Destroying namespace "webhook-markers-7032" for this suite. @ 05/27/23 13:46:08.311
• [3.605 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 05/27/23 13:46:08.322
  May 27 13:46:08.322: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename job @ 05/27/23 13:46:08.323
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:46:08.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:46:08.349
  STEP: Creating a job @ 05/27/23 13:46:08.354
  STEP: Ensuring job reaches completions @ 05/27/23 13:46:08.364
  E0527 13:46:09.092623      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:10.092754      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:11.092912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:12.093001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:13.094186      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:14.094343      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:15.095274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:16.095444      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:17.095562      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:18.096337      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:46:18.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2271" for this suite. @ 05/27/23 13:46:18.375
• [10.065 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 05/27/23 13:46:18.388
  May 27 13:46:18.388: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename sched-pred @ 05/27/23 13:46:18.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:46:18.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:46:18.413
  May 27 13:46:18.418: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 27 13:46:18.428: INFO: Waiting for terminating namespaces to be deleted...
  May 27 13:46:18.432: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-10-136 before test
  May 27 13:46:18.441: INFO: default-http-backend-kubernetes-worker-65fc475d49-8hlsq from ingress-nginx-kubernetes-worker started at 2023-05-27 12:22:35 +0000 UTC (1 container statuses recorded)
  May 27 13:46:18.441: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  May 27 13:46:18.441: INFO: nginx-ingress-controller-kubernetes-worker-jkk42 from ingress-nginx-kubernetes-worker started at 2023-05-27 12:11:29 +0000 UTC (1 container statuses recorded)
  May 27 13:46:18.442: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  May 27 13:46:18.442: INFO: calico-kube-controllers-79678b7759-2xtvf from kube-system started at 2023-05-27 12:22:35 +0000 UTC (1 container statuses recorded)
  May 27 13:46:18.442: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 27 13:46:18.442: INFO: sonobuoy from sonobuoy started at 2023-05-27 12:15:40 +0000 UTC (1 container statuses recorded)
  May 27 13:46:18.442: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 27 13:46:18.442: INFO: sonobuoy-e2e-job-0cf94b30a28f4573 from sonobuoy started at 2023-05-27 12:15:43 +0000 UTC (2 container statuses recorded)
  May 27 13:46:18.442: INFO: 	Container e2e ready: true, restart count 0
  May 27 13:46:18.442: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 27 13:46:18.442: INFO: sonobuoy-systemd-logs-daemon-set-f9890aca51654f74-zr2qt from sonobuoy started at 2023-05-27 12:15:43 +0000 UTC (2 container statuses recorded)
  May 27 13:46:18.442: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 27 13:46:18.442: INFO: 	Container systemd-logs ready: true, restart count 0
  May 27 13:46:18.442: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-22-3 before test
  May 27 13:46:18.449: INFO: nginx-ingress-controller-kubernetes-worker-2hlwz from ingress-nginx-kubernetes-worker started at 2023-05-27 11:59:07 +0000 UTC (1 container statuses recorded)
  May 27 13:46:18.449: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  May 27 13:46:18.449: INFO: coredns-5c7f76ccb8-b4zh4 from kube-system started at 2023-05-27 11:59:00 +0000 UTC (1 container statuses recorded)
  May 27 13:46:18.449: INFO: 	Container coredns ready: true, restart count 0
  May 27 13:46:18.449: INFO: kube-state-metrics-5b95b4459c-8rvst from kube-system started at 2023-05-27 11:59:00 +0000 UTC (1 container statuses recorded)
  May 27 13:46:18.449: INFO: 	Container kube-state-metrics ready: true, restart count 0
  May 27 13:46:18.449: INFO: metrics-server-v0.5.2-6cf8c8b69c-t499d from kube-system started at 2023-05-27 11:59:00 +0000 UTC (2 container statuses recorded)
  May 27 13:46:18.449: INFO: 	Container metrics-server ready: true, restart count 0
  May 27 13:46:18.449: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  May 27 13:46:18.449: INFO: dashboard-metrics-scraper-6b8586b5c9-7kfbs from kubernetes-dashboard started at 2023-05-27 11:59:00 +0000 UTC (1 container statuses recorded)
  May 27 13:46:18.449: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  May 27 13:46:18.449: INFO: kubernetes-dashboard-6869f4cd5f-js8mr from kubernetes-dashboard started at 2023-05-27 11:59:00 +0000 UTC (1 container statuses recorded)
  May 27 13:46:18.449: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  May 27 13:46:18.449: INFO: sonobuoy-systemd-logs-daemon-set-f9890aca51654f74-c4nxx from sonobuoy started at 2023-05-27 12:15:43 +0000 UTC (2 container statuses recorded)
  May 27 13:46:18.449: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 27 13:46:18.449: INFO: 	Container systemd-logs ready: true, restart count 0
  May 27 13:46:18.449: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-68-172 before test
  May 27 13:46:18.455: INFO: nginx-ingress-controller-kubernetes-worker-8g7rf from ingress-nginx-kubernetes-worker started at 2023-05-27 13:07:09 +0000 UTC (1 container statuses recorded)
  May 27 13:46:18.455: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  May 27 13:46:18.455: INFO: fail-once-local-7jzwf from job-2271 started at 2023-05-27 13:46:13 +0000 UTC (1 container statuses recorded)
  May 27 13:46:18.455: INFO: 	Container c ready: false, restart count 1
  May 27 13:46:18.455: INFO: fail-once-local-mgpsf from job-2271 started at 2023-05-27 13:46:08 +0000 UTC (1 container statuses recorded)
  May 27 13:46:18.455: INFO: 	Container c ready: false, restart count 1
  May 27 13:46:18.455: INFO: fail-once-local-mrl7k from job-2271 started at 2023-05-27 13:46:08 +0000 UTC (1 container statuses recorded)
  May 27 13:46:18.455: INFO: 	Container c ready: false, restart count 1
  May 27 13:46:18.455: INFO: fail-once-local-wnzhh from job-2271 started at 2023-05-27 13:46:13 +0000 UTC (1 container statuses recorded)
  May 27 13:46:18.455: INFO: 	Container c ready: false, restart count 1
  May 27 13:46:18.455: INFO: sonobuoy-systemd-logs-daemon-set-f9890aca51654f74-mmhtp from sonobuoy started at 2023-05-27 12:15:43 +0000 UTC (2 container statuses recorded)
  May 27 13:46:18.455: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 27 13:46:18.455: INFO: 	Container systemd-logs ready: true, restart count 0
  May 27 13:46:18.455: INFO: webhook-to-be-mutated from webhook-8946 started at 2023-05-27 13:46:08 +0000 UTC (1 container statuses recorded)
  May 27 13:46:18.455: INFO: 	Container example ready: false, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 05/27/23 13:46:18.456
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.1763041616b5a077], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] @ 05/27/23 13:46:18.493
  E0527 13:46:19.097230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:46:19.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-4021" for this suite. @ 05/27/23 13:46:19.496
• [1.117 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 05/27/23 13:46:19.507
  May 27 13:46:19.507: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename cronjob @ 05/27/23 13:46:19.508
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:46:19.524
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:46:19.53
  STEP: Creating a ReplaceConcurrent cronjob @ 05/27/23 13:46:19.534
  STEP: Ensuring a job is scheduled @ 05/27/23 13:46:19.541
  E0527 13:46:20.097410      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:21.097554      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:22.098434      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:23.098534      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:24.098684      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:25.098794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:26.098906      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:27.099255      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:28.100301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:29.100484      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:30.101119      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:31.101231      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:32.101336      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:33.101469      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:34.101626      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:35.101994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:36.102664      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:37.102725      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:38.103407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:39.104287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:40.104737      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:41.104792      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:42.105344      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:43.105598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:44.106083      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:45.106643      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:46.106988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:47.107221      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:48.107355      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:49.107457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:50.107984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:51.108302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:52.108334      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:53.108423      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:54.108819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:55.109026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:56.109115      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:57.109338      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:58.110108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:46:59.110225      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:00.110335      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:01.110870      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 05/27/23 13:47:01.548
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/27/23 13:47:01.552
  STEP: Ensuring the job is replaced with a new one @ 05/27/23 13:47:01.555
  E0527 13:47:02.111389      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:03.111632      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:04.112310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:05.112407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:06.113415      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:07.113523      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:08.114426      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:09.114525      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:10.114670      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:11.114883      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:12.115202      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:13.116284      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:14.117134      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:15.117244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:16.117970      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:17.118396      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:18.118509      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:19.118968      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:20.120040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:21.121072      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:22.121181      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:23.121287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:24.121388      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:25.121505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:26.122387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:27.123413      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:28.123505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:29.123628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:30.124290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:31.124605      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:32.124656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:33.124726      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:34.124835      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:35.124941      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:36.125027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:37.125667      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:38.125789      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:39.125901      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:40.126350      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:41.126782      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:42.126819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:43.126920      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:44.126989      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:45.127204      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:46.127327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:47.127429      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:48.127741      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:49.128292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:50.128383      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:51.128782      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:52.129292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:53.129707      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:54.130352      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:55.130607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:56.131388      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:57.132458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:58.132494      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:47:59.133421      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:00.134169      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:01.134291      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 05/27/23 13:48:01.561
  May 27 13:48:01.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8009" for this suite. @ 05/27/23 13:48:01.575
• [102.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 05/27/23 13:48:01.588
  May 27 13:48:01.588: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename dns @ 05/27/23 13:48:01.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:48:01.618
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:48:01.624
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/27/23 13:48:01.628
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/27/23 13:48:01.629
  STEP: creating a pod to probe DNS @ 05/27/23 13:48:01.629
  STEP: submitting the pod to kubernetes @ 05/27/23 13:48:01.629
  E0527 13:48:02.135268      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:03.135392      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/27/23 13:48:03.655
  STEP: looking for the results for each expected name from probers @ 05/27/23 13:48:03.66
  May 27 13:48:03.678: INFO: DNS probes using dns-4806/dns-test-ae84abf0-1460-476c-940c-d95bc05bcd99 succeeded

  May 27 13:48:03.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/27/23 13:48:03.683
  STEP: Destroying namespace "dns-4806" for this suite. @ 05/27/23 13:48:03.697
• [2.117 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 05/27/23 13:48:03.705
  May 27 13:48:03.705: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename services @ 05/27/23 13:48:03.706
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:48:03.725
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:48:03.73
  STEP: fetching services @ 05/27/23 13:48:03.736
  May 27 13:48:03.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4210" for this suite. @ 05/27/23 13:48:03.745
• [0.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 05/27/23 13:48:03.754
  May 27 13:48:03.754: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename var-expansion @ 05/27/23 13:48:03.755
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:48:03.773
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:48:03.784
  STEP: Creating a pod to test substitution in volume subpath @ 05/27/23 13:48:03.788
  E0527 13:48:04.135475      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:05.135955      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:06.136107      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:07.136679      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:48:07.819
  May 27 13:48:07.824: INFO: Trying to get logs from node ip-172-31-68-172 pod var-expansion-ca996a06-883d-4dc1-8866-9d2851847832 container dapi-container: <nil>
  STEP: delete the pod @ 05/27/23 13:48:07.845
  May 27 13:48:07.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5139" for this suite. @ 05/27/23 13:48:07.866
• [4.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 05/27/23 13:48:07.878
  May 27 13:48:07.878: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename statefulset @ 05/27/23 13:48:07.879
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:48:07.899
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:48:07.903
  STEP: Creating service test in namespace statefulset-8259 @ 05/27/23 13:48:07.908
  STEP: Creating a new StatefulSet @ 05/27/23 13:48:07.915
  May 27 13:48:07.934: INFO: Found 0 stateful pods, waiting for 3
  E0527 13:48:08.137124      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:09.137220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:10.137735      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:11.137977      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:12.138497      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:13.138776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:14.139212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:15.139316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:16.139413      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:17.139719      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:48:17.942: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 27 13:48:17.942: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 27 13:48:17.942: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  May 27 13:48:17.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-8259 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 27 13:48:18.122: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 27 13:48:18.122: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 27 13:48:18.122: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0527 13:48:18.140413      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:19.141510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:20.141560      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:21.141686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:22.142174      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:23.142297      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:24.142412      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:25.142507      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:26.142614      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:27.143135      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:28.143173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/27/23 13:48:28.145
  May 27 13:48:28.171: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/27/23 13:48:28.171
  E0527 13:48:29.143226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:30.143338      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:31.143447      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:32.143838      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:33.143909      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:34.144295      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:35.144566      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:36.144732      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:37.145546      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:38.145686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 05/27/23 13:48:38.194
  May 27 13:48:38.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-8259 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 27 13:48:38.356: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 27 13:48:38.356: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 27 13:48:38.356: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0527 13:48:39.146647      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:40.146776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:41.147417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:42.147816      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:43.147909      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:44.148018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:45.148142      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:46.148441      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:47.148670      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:48.148744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 05/27/23 13:48:48.387
  May 27 13:48:48.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-8259 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 27 13:48:48.547: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 27 13:48:48.547: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 27 13:48:48.547: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0527 13:48:49.149557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:50.149683      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:51.149999      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:52.150160      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:53.150258      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:54.150381      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:55.150490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:56.150591      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:57.151559      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:48:58.151679      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:48:58.590: INFO: Updating stateful set ss2
  E0527 13:48:59.151795      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:00.151886      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:01.151992      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:02.152164      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:03.152283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:04.152394      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:05.153268      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:06.153358      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:07.153550      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:08.153666      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 05/27/23 13:49:08.61
  May 27 13:49:08.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=statefulset-8259 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 27 13:49:08.772: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 27 13:49:08.772: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 27 13:49:08.772: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0527 13:49:09.153954      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:10.154049      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:11.155162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:12.155509      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:13.155572      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:14.155806      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:15.156127      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:16.156205      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:17.156633      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:18.156724      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:49:18.802: INFO: Deleting all statefulset in ns statefulset-8259
  May 27 13:49:18.805: INFO: Scaling statefulset ss2 to 0
  E0527 13:49:19.156836      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:20.157272      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:21.157876      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:22.158202      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:23.158307      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:24.158431      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:25.158541      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:26.158658      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:27.159636      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:28.160280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:49:28.831: INFO: Waiting for statefulset status.replicas updated to 0
  May 27 13:49:28.835: INFO: Deleting statefulset ss2
  May 27 13:49:28.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8259" for this suite. @ 05/27/23 13:49:28.873
• [81.006 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 05/27/23 13:49:28.884
  May 27 13:49:28.884: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename services @ 05/27/23 13:49:28.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:49:28.912
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:49:28.925
  STEP: creating a collection of services @ 05/27/23 13:49:28.931
  May 27 13:49:28.931: INFO: Creating e2e-svc-a-6b7rm
  May 27 13:49:28.954: INFO: Creating e2e-svc-b-gc426
  May 27 13:49:28.970: INFO: Creating e2e-svc-c-m7kfm
  STEP: deleting service collection @ 05/27/23 13:49:28.995
  May 27 13:49:29.054: INFO: Collection of services has been deleted
  May 27 13:49:29.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8651" for this suite. @ 05/27/23 13:49:29.06
• [0.184 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 05/27/23 13:49:29.072
  May 27 13:49:29.072: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename webhook @ 05/27/23 13:49:29.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:49:29.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:49:29.103
  STEP: Setting up server cert @ 05/27/23 13:49:29.156
  E0527 13:49:29.161236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/27/23 13:49:29.722
  STEP: Deploying the webhook pod @ 05/27/23 13:49:29.736
  STEP: Wait for the deployment to be ready @ 05/27/23 13:49:29.751
  May 27 13:49:29.763: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0527 13:49:30.162273      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:31.162421      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/27/23 13:49:31.777
  STEP: Verifying the service has paired with the endpoint @ 05/27/23 13:49:31.79
  E0527 13:49:32.163132      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:49:32.791: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 05/27/23 13:49:32.798
  STEP: Creating a custom resource definition that should be denied by the webhook @ 05/27/23 13:49:32.818
  May 27 13:49:32.818: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:49:32.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6494" for this suite. @ 05/27/23 13:49:32.934
  STEP: Destroying namespace "webhook-markers-6424" for this suite. @ 05/27/23 13:49:32.95
• [3.901 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 05/27/23 13:49:32.973
  May 27 13:49:32.973: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir @ 05/27/23 13:49:32.974
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:49:33.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:49:33.018
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/27/23 13:49:33.024
  E0527 13:49:33.163510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:34.163881      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:35.164724      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:36.164936      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:49:37.059
  May 27 13:49:37.064: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-34e62f60-daa2-464d-b7a6-eb3e7c7a005c container test-container: <nil>
  STEP: delete the pod @ 05/27/23 13:49:37.073
  May 27 13:49:37.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3693" for this suite. @ 05/27/23 13:49:37.102
• [4.136 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 05/27/23 13:49:37.11
  May 27 13:49:37.110: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename replicaset @ 05/27/23 13:49:37.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:49:37.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:49:37.141
  STEP: Create a Replicaset @ 05/27/23 13:49:37.151
  STEP: Verify that the required pods have come up. @ 05/27/23 13:49:37.159
  E0527 13:49:37.165557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:49:37.166: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0527 13:49:38.165701      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:39.166053      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:40.166112      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:41.166525      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:42.166667      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:49:42.170: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/27/23 13:49:42.17
  STEP: Getting /status @ 05/27/23 13:49:42.17
  May 27 13:49:42.176: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 05/27/23 13:49:42.176
  May 27 13:49:42.189: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 05/27/23 13:49:42.189
  May 27 13:49:42.192: INFO: Observed &ReplicaSet event: ADDED
  May 27 13:49:42.192: INFO: Observed &ReplicaSet event: MODIFIED
  May 27 13:49:42.193: INFO: Observed &ReplicaSet event: MODIFIED
  May 27 13:49:42.193: INFO: Observed &ReplicaSet event: MODIFIED
  May 27 13:49:42.193: INFO: Found replicaset test-rs in namespace replicaset-2522 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 27 13:49:42.193: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 05/27/23 13:49:42.193
  May 27 13:49:42.193: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 27 13:49:42.202: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 05/27/23 13:49:42.202
  May 27 13:49:42.205: INFO: Observed &ReplicaSet event: ADDED
  May 27 13:49:42.206: INFO: Observed &ReplicaSet event: MODIFIED
  May 27 13:49:42.206: INFO: Observed &ReplicaSet event: MODIFIED
  May 27 13:49:42.206: INFO: Observed &ReplicaSet event: MODIFIED
  May 27 13:49:42.206: INFO: Observed replicaset test-rs in namespace replicaset-2522 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 27 13:49:42.206: INFO: Observed &ReplicaSet event: MODIFIED
  May 27 13:49:42.207: INFO: Found replicaset test-rs in namespace replicaset-2522 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  May 27 13:49:42.207: INFO: Replicaset test-rs has a patched status
  May 27 13:49:42.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2522" for this suite. @ 05/27/23 13:49:42.212
• [5.113 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 05/27/23 13:49:42.228
  May 27 13:49:42.228: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename downward-api @ 05/27/23 13:49:42.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:49:42.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:49:42.257
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 13:49:42.261
  E0527 13:49:43.167240      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:44.167403      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:45.167897      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:46.168303      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:49:46.293
  May 27 13:49:46.297: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-3ca2e815-2b2f-4a17-8a0b-4d51a196a139 container client-container: <nil>
  STEP: delete the pod @ 05/27/23 13:49:46.307
  May 27 13:49:46.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7732" for this suite. @ 05/27/23 13:49:46.332
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 05/27/23 13:49:46.35
  May 27 13:49:46.350: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename svcaccounts @ 05/27/23 13:49:46.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:49:46.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:49:46.374
  STEP: Creating ServiceAccount "e2e-sa-fwcgj"  @ 05/27/23 13:49:46.379
  May 27 13:49:46.386: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-fwcgj"  @ 05/27/23 13:49:46.386
  May 27 13:49:46.396: INFO: AutomountServiceAccountToken: true
  May 27 13:49:46.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6658" for this suite. @ 05/27/23 13:49:46.401
• [0.059 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 05/27/23 13:49:46.41
  May 27 13:49:46.410: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 13:49:46.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:49:46.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:49:46.439
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 13:49:46.443
  E0527 13:49:47.169309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:48.169408      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:49.169570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:50.170003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:49:50.476
  May 27 13:49:50.480: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-33688521-c70e-4fc9-b4aa-65765d105d7c container client-container: <nil>
  STEP: delete the pod @ 05/27/23 13:49:50.491
  May 27 13:49:50.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3770" for this suite. @ 05/27/23 13:49:50.514
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 05/27/23 13:49:50.526
  May 27 13:49:50.526: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubelet-test @ 05/27/23 13:49:50.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:49:50.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:49:50.554
  E0527 13:49:51.170646      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:52.171285      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:49:52.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1141" for this suite. @ 05/27/23 13:49:52.608
• [2.091 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 05/27/23 13:49:52.617
  May 27 13:49:52.617: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename var-expansion @ 05/27/23 13:49:52.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:49:52.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:49:52.643
  E0527 13:49:53.172066      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:54.172314      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:49:54.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 27 13:49:54.675: INFO: Deleting pod "var-expansion-6cf7999a-4bc1-469e-ae05-0a03a70c9178" in namespace "var-expansion-4785"
  May 27 13:49:54.686: INFO: Wait up to 5m0s for pod "var-expansion-6cf7999a-4bc1-469e-ae05-0a03a70c9178" to be fully deleted
  E0527 13:49:55.172501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:56.173097      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-4785" for this suite. @ 05/27/23 13:49:56.696
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 05/27/23 13:49:56.708
  May 27 13:49:56.708: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename limitrange @ 05/27/23 13:49:56.709
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:49:56.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:49:56.735
  STEP: Creating LimitRange "e2e-limitrange-vdmcs" in namespace "limitrange-6222" @ 05/27/23 13:49:56.745
  STEP: Creating another limitRange in another namespace @ 05/27/23 13:49:56.752
  May 27 13:49:56.773: INFO: Namespace "e2e-limitrange-vdmcs-4569" created
  May 27 13:49:56.773: INFO: Creating LimitRange "e2e-limitrange-vdmcs" in namespace "e2e-limitrange-vdmcs-4569"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-vdmcs" @ 05/27/23 13:49:56.781
  May 27 13:49:56.787: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-vdmcs" in "limitrange-6222" namespace @ 05/27/23 13:49:56.787
  May 27 13:49:56.797: INFO: LimitRange "e2e-limitrange-vdmcs" has been patched
  STEP: Delete LimitRange "e2e-limitrange-vdmcs" by Collection with labelSelector: "e2e-limitrange-vdmcs=patched" @ 05/27/23 13:49:56.797
  STEP: Confirm that the limitRange "e2e-limitrange-vdmcs" has been deleted @ 05/27/23 13:49:56.808
  May 27 13:49:56.808: INFO: Requesting list of LimitRange to confirm quantity
  May 27 13:49:56.813: INFO: Found 0 LimitRange with label "e2e-limitrange-vdmcs=patched"
  May 27 13:49:56.813: INFO: LimitRange "e2e-limitrange-vdmcs" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-vdmcs" @ 05/27/23 13:49:56.813
  May 27 13:49:56.817: INFO: Found 1 limitRange
  May 27 13:49:56.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-6222" for this suite. @ 05/27/23 13:49:56.823
  STEP: Destroying namespace "e2e-limitrange-vdmcs-4569" for this suite. @ 05/27/23 13:49:56.833
• [0.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 05/27/23 13:49:56.844
  May 27 13:49:56.844: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 13:49:56.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:49:56.864
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:49:56.87
  STEP: Creating configMap with name configmap-projected-all-test-volume-7ced011f-5ea9-48a8-96bd-150ba877d373 @ 05/27/23 13:49:56.875
  STEP: Creating secret with name secret-projected-all-test-volume-70014454-a5b2-41dd-a747-53bf6e9669f9 @ 05/27/23 13:49:56.883
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 05/27/23 13:49:56.89
  E0527 13:49:57.173501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:58.173660      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:49:59.174024      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:00.174151      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:50:00.923
  May 27 13:50:00.927: INFO: Trying to get logs from node ip-172-31-68-172 pod projected-volume-c80af7ba-64a7-4464-94e5-e35254d499f0 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 05/27/23 13:50:00.935
  May 27 13:50:00.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5715" for this suite. @ 05/27/23 13:50:00.96
• [4.125 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 05/27/23 13:50:00.971
  May 27 13:50:00.971: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename services @ 05/27/23 13:50:00.972
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:50:00.992
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:50:00.997
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-9004 @ 05/27/23 13:50:01.002
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/27/23 13:50:01.023
  STEP: creating service externalsvc in namespace services-9004 @ 05/27/23 13:50:01.023
  STEP: creating replication controller externalsvc in namespace services-9004 @ 05/27/23 13:50:01.049
  I0527 13:50:01.059691      18 runners.go:194] Created replication controller with name: externalsvc, namespace: services-9004, replica count: 2
  E0527 13:50:01.174898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:02.175218      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:03.175255      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0527 13:50:04.111487      18 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 05/27/23 13:50:04.116
  May 27 13:50:04.138: INFO: Creating new exec pod
  E0527 13:50:04.175879      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:05.176331      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:50:06.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=services-9004 exec execpodcdb4b -- /bin/sh -x -c nslookup nodeport-service.services-9004.svc.cluster.local'
  E0527 13:50:06.177241      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:50:06.364: INFO: stderr: "+ nslookup nodeport-service.services-9004.svc.cluster.local\n"
  May 27 13:50:06.364: INFO: stdout: "Server:\t\t10.152.183.112\nAddress:\t10.152.183.112#53\n\nnodeport-service.services-9004.svc.cluster.local\tcanonical name = externalsvc.services-9004.svc.cluster.local.\nName:\texternalsvc.services-9004.svc.cluster.local\nAddress: 10.152.183.187\n\n"
  May 27 13:50:06.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-9004, will wait for the garbage collector to delete the pods @ 05/27/23 13:50:06.371
  May 27 13:50:06.435: INFO: Deleting ReplicationController externalsvc took: 9.173295ms
  May 27 13:50:06.535: INFO: Terminating ReplicationController externalsvc pods took: 100.63167ms
  E0527 13:50:07.178131      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:08.179040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:50:09.059: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-9004" for this suite. @ 05/27/23 13:50:09.073
• [8.113 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 05/27/23 13:50:09.085
  May 27 13:50:09.085: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename downward-api @ 05/27/23 13:50:09.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:50:09.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:50:09.122
  STEP: Creating the pod @ 05/27/23 13:50:09.127
  E0527 13:50:09.179295      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:10.179374      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:11.179777      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:50:11.681: INFO: Successfully updated pod "labelsupdate1b52c878-e6b6-48b6-b102-4ea0be32665b"
  E0527 13:50:12.180495      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:13.181532      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:14.181936      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:15.181923      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:50:15.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6610" for this suite. @ 05/27/23 13:50:15.724
• [6.650 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 05/27/23 13:50:15.737
  May 27 13:50:15.737: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename daemonsets @ 05/27/23 13:50:15.738
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:50:15.756
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:50:15.765
  May 27 13:50:15.798: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/27/23 13:50:15.805
  May 27 13:50:15.812: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:15.812: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:15.817: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 13:50:15.818: INFO: Node ip-172-31-10-136 is running 0 daemon pod, expected 1
  E0527 13:50:16.182544      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:50:16.823: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:16.823: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:16.829: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 27 13:50:16.829: INFO: Node ip-172-31-10-136 is running 0 daemon pod, expected 1
  E0527 13:50:17.183626      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:50:17.823: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:17.823: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:17.828: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 27 13:50:17.828: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 05/27/23 13:50:17.845
  STEP: Check that daemon pods images are updated. @ 05/27/23 13:50:17.863
  May 27 13:50:17.873: INFO: Wrong image for pod: daemon-set-6bw5l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 27 13:50:17.873: INFO: Wrong image for pod: daemon-set-dh49r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 27 13:50:17.873: INFO: Wrong image for pod: daemon-set-krhwh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 27 13:50:17.879: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:17.879: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0527 13:50:18.184330      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:50:18.885: INFO: Wrong image for pod: daemon-set-6bw5l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 27 13:50:18.885: INFO: Wrong image for pod: daemon-set-krhwh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 27 13:50:18.885: INFO: Pod daemon-set-twwhl is not available
  May 27 13:50:18.891: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:18.891: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0527 13:50:19.185240      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:50:19.884: INFO: Wrong image for pod: daemon-set-6bw5l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 27 13:50:19.890: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:19.890: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0527 13:50:20.185643      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:50:20.886: INFO: Wrong image for pod: daemon-set-6bw5l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 27 13:50:20.891: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:20.891: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0527 13:50:21.185861      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:50:21.884: INFO: Wrong image for pod: daemon-set-6bw5l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 27 13:50:21.885: INFO: Pod daemon-set-p5txs is not available
  May 27 13:50:21.891: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:21.891: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0527 13:50:22.185931      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:50:22.890: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:22.891: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0527 13:50:23.186020      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:50:23.884: INFO: Pod daemon-set-24x8m is not available
  May 27 13:50:23.889: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:23.889: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 05/27/23 13:50:23.889
  May 27 13:50:23.894: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:23.894: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:23.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 27 13:50:23.901: INFO: Node ip-172-31-68-172 is running 0 daemon pod, expected 1
  E0527 13:50:24.186167      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:50:24.909: INFO: DaemonSet pods can't tolerate node ip-172-31-13-250 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:24.909: INFO: DaemonSet pods can't tolerate node ip-172-31-77-211 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 27 13:50:24.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 27 13:50:24.913: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/27/23 13:50:24.938
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5535, will wait for the garbage collector to delete the pods @ 05/27/23 13:50:24.938
  May 27 13:50:25.003: INFO: Deleting DaemonSet.extensions daemon-set took: 10.390873ms
  May 27 13:50:25.103: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.481456ms
  E0527 13:50:25.186880      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:50:26.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 27 13:50:26.110: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 27 13:50:26.114: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42873"},"items":null}

  May 27 13:50:26.119: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42873"},"items":null}

  May 27 13:50:26.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5535" for this suite. @ 05/27/23 13:50:26.147
• [10.423 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 05/27/23 13:50:26.163
  May 27 13:50:26.163: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename sched-preemption @ 05/27/23 13:50:26.164
  E0527 13:50:26.186875      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:50:26.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:50:26.192
  May 27 13:50:26.219: INFO: Waiting up to 1m0s for all nodes to be ready
  E0527 13:50:27.187582      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:28.187727      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:29.188028      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:30.188052      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:31.188174      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:32.189183      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:33.189257      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:34.189333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:35.189751      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:36.189589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:37.190460      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:38.190573      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:39.191236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:40.191325      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:41.191605      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:42.192647      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:43.192944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:44.193054      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:45.193305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:46.193497      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:47.193831      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:48.193906      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:49.194012      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:50.194108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:51.194232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:52.195271      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:53.196146      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:54.196538      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:55.196600      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:56.196720      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:57.197002      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:58.197141      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:50:59.197275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:00.197351      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:01.197466      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:02.198476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:03.199320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:04.200309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:05.200412      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:06.200772      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:07.201545      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:08.201753      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:09.201937      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:10.202002      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:11.202668      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:12.203275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:13.203914      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:14.204074      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:15.204181      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:16.204300      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:17.204688      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:18.204811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:19.204947      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:20.204958      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:21.205077      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:22.205173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:23.205308      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:24.205650      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:25.205899      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:26.205988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:51:26.247: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/27/23 13:51:26.253
  May 27 13:51:26.280: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May 27 13:51:26.291: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May 27 13:51:26.318: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May 27 13:51:26.328: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  May 27 13:51:26.360: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  May 27 13:51:26.372: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/27/23 13:51:26.372
  E0527 13:51:27.207026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:28.207416      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 05/27/23 13:51:28.412
  E0527 13:51:29.208611      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:30.208615      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:31.209770      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:32.210559      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:51:32.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-3662" for this suite. @ 05/27/23 13:51:32.52
• [66.369 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 05/27/23 13:51:32.533
  May 27 13:51:32.533: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubelet-test @ 05/27/23 13:51:32.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:51:32.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:51:32.558
  May 27 13:51:32.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9193" for this suite. @ 05/27/23 13:51:32.599
• [0.075 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 05/27/23 13:51:32.609
  May 27 13:51:32.609: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename security-context-test @ 05/27/23 13:51:32.61
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:51:32.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:51:32.636
  E0527 13:51:33.211197      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:34.211247      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:35.211365      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:36.212309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:51:36.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-3713" for this suite. @ 05/27/23 13:51:36.684
• [4.085 seconds]
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 05/27/23 13:51:36.694
  May 27 13:51:36.694: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename pod-network-test @ 05/27/23 13:51:36.695
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:51:36.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:51:36.721
  STEP: Performing setup for networking test in namespace pod-network-test-9493 @ 05/27/23 13:51:36.727
  STEP: creating a selector @ 05/27/23 13:51:36.727
  STEP: Creating the service pods in kubernetes @ 05/27/23 13:51:36.727
  May 27 13:51:36.727: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0527 13:51:37.213052      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:38.214217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:39.214958      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:40.215173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:41.215948      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:42.216304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:43.216470      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:44.216511      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:45.217111      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:46.217218      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:47.218285      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:48.218420      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/27/23 13:51:48.836
  E0527 13:51:49.219391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:50.219489      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:51:50.882: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May 27 13:51:50.883: INFO: Going to poll 192.168.0.31 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  May 27 13:51:50.887: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.0.31:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9493 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 13:51:50.887: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:51:50.888: INFO: ExecWithOptions: Clientset creation
  May 27 13:51:50.888: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9493/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.0.31%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 27 13:51:50.971: INFO: Found all 1 expected endpoints: [netserver-0]
  May 27 13:51:50.971: INFO: Going to poll 192.168.7.99 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  May 27 13:51:50.977: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.7.99:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9493 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 13:51:50.977: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:51:50.978: INFO: ExecWithOptions: Clientset creation
  May 27 13:51:50.978: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9493/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.7.99%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 27 13:51:51.067: INFO: Found all 1 expected endpoints: [netserver-1]
  May 27 13:51:51.068: INFO: Going to poll 192.168.19.104 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  May 27 13:51:51.072: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.19.104:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9493 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 27 13:51:51.072: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  May 27 13:51:51.073: INFO: ExecWithOptions: Clientset creation
  May 27 13:51:51.073: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9493/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.19.104%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 27 13:51:51.150: INFO: Found all 1 expected endpoints: [netserver-2]
  May 27 13:51:51.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-9493" for this suite. @ 05/27/23 13:51:51.156
• [14.472 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 05/27/23 13:51:51.167
  May 27 13:51:51.167: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename subpath @ 05/27/23 13:51:51.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:51:51.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:51:51.192
  STEP: Setting up data @ 05/27/23 13:51:51.201
  STEP: Creating pod pod-subpath-test-configmap-wc82 @ 05/27/23 13:51:51.215
  STEP: Creating a pod to test atomic-volume-subpath @ 05/27/23 13:51:51.216
  E0527 13:51:51.220162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:52.221174      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:53.221271      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:54.221378      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:55.222136      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:56.222258      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:57.222870      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:58.223357      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:51:59.224329      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:00.224938      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:01.225263      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:02.226246      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:03.226848      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:04.226959      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:05.227236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:06.227367      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:07.227390      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:08.227504      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:09.227614      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:10.227723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:11.228302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:12.228409      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:13.228523      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:14.228633      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:15.228741      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:52:15.3
  May 27 13:52:15.304: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-subpath-test-configmap-wc82 container test-container-subpath-configmap-wc82: <nil>
  STEP: delete the pod @ 05/27/23 13:52:15.332
  STEP: Deleting pod pod-subpath-test-configmap-wc82 @ 05/27/23 13:52:15.354
  May 27 13:52:15.355: INFO: Deleting pod "pod-subpath-test-configmap-wc82" in namespace "subpath-5998"
  May 27 13:52:15.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5998" for this suite. @ 05/27/23 13:52:15.366
• [24.210 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 05/27/23 13:52:15.382
  May 27 13:52:15.382: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 13:52:15.383
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:52:15.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:52:15.419
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 13:52:15.424
  E0527 13:52:16.228818      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:17.229306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:18.229334      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:19.229461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:52:19.457
  May 27 13:52:19.462: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-97271c0e-ad86-4d90-bedf-181db0400094 container client-container: <nil>
  STEP: delete the pod @ 05/27/23 13:52:19.474
  May 27 13:52:19.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9167" for this suite. @ 05/27/23 13:52:19.503
• [4.129 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 05/27/23 13:52:19.512
  May 27 13:52:19.512: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename emptydir @ 05/27/23 13:52:19.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:52:19.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:52:19.538
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/27/23 13:52:19.542
  E0527 13:52:20.230436      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:21.230460      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:22.231213      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:23.232300      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:52:23.575
  May 27 13:52:23.581: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-5e0a6636-b1bb-401a-9291-ef93bd1c7c55 container test-container: <nil>
  STEP: delete the pod @ 05/27/23 13:52:23.59
  May 27 13:52:23.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3731" for this suite. @ 05/27/23 13:52:23.614
• [4.112 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 05/27/23 13:52:23.624
  May 27 13:52:23.624: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/27/23 13:52:23.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:52:23.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:52:23.65
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 05/27/23 13:52:23.656
  May 27 13:52:23.656: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  E0527 13:52:24.233170      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:25.233443      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:26.233756      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:27.233799      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:28.234252      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:29.234752      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:30.235010      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 05/27/23 13:52:30.375
  May 27 13:52:30.376: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  E0527 13:52:31.235937      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:52:31.863: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  E0527 13:52:32.236320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:33.237387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:34.238309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:35.238764      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:36.239852      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:37.240427      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:38.240721      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:52:38.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6273" for this suite. @ 05/27/23 13:52:38.271
• [14.655 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 05/27/23 13:52:38.281
  May 27 13:52:38.281: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename sched-preemption @ 05/27/23 13:52:38.282
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:52:38.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:52:38.306
  May 27 13:52:38.325: INFO: Waiting up to 1m0s for all nodes to be ready
  E0527 13:52:39.241032      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:40.241095      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:41.241222      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:42.242246      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:43.242344      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:44.242723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:45.242843      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:46.242969      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:47.243489      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:48.244309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:49.244748      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:50.244948      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:51.244999      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:52.245265      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:53.245442      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:54.245825      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:55.246024      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:56.246096      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:57.246175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:58.246260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:52:59.246420      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:00.246489      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:01.247186      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:02.247404      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:03.248013      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:04.248172      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:05.248285      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:06.248385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:07.248491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:08.248718      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:09.248886      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:10.248942      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:11.249029      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:12.249433      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:13.249632      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:14.249768      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:15.250477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:16.250714      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:17.251664      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:18.252313      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:19.253090      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:20.254047      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:21.254870      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:22.255386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:23.256441      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:24.256550      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:25.256652      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:26.256773      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:27.257652      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:28.257917      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:29.258318      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:30.258420      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:31.258736      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:32.259438      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:33.260293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:34.260944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:35.261692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:36.261962      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:37.262778      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:38.262887      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:53:38.345: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/27/23 13:53:38.349
  May 27 13:53:38.371: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May 27 13:53:38.380: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May 27 13:53:38.396: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May 27 13:53:38.407: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  May 27 13:53:38.428: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  May 27 13:53:38.437: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/27/23 13:53:38.437
  E0527 13:53:39.265161      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:40.265440      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 05/27/23 13:53:40.473
  E0527 13:53:41.265722      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:42.266563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:43.266768      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:44.266811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:45.267585      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:46.267641      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:53:46.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-7718" for this suite. @ 05/27/23 13:53:46.627
• [68.357 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 05/27/23 13:53:46.639
  May 27 13:53:46.639: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename var-expansion @ 05/27/23 13:53:46.64
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:53:46.662
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:53:46.667
  E0527 13:53:47.268212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:48.268293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:53:48.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 27 13:53:48.709: INFO: Deleting pod "var-expansion-27b7486c-e965-403a-aa24-8ef0217a5f45" in namespace "var-expansion-9732"
  May 27 13:53:48.718: INFO: Wait up to 5m0s for pod "var-expansion-27b7486c-e965-403a-aa24-8ef0217a5f45" to be fully deleted
  E0527 13:53:49.269176      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:50.269563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-9732" for this suite. @ 05/27/23 13:53:50.729
• [4.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 05/27/23 13:53:50.739
  May 27 13:53:50.739: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename downward-api @ 05/27/23 13:53:50.74
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:53:50.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:53:50.765
  STEP: Creating a pod to test downward API volume plugin @ 05/27/23 13:53:50.769
  E0527 13:53:51.270158      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:52.270927      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:53.270852      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:54.271294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:53:54.795
  May 27 13:53:54.798: INFO: Trying to get logs from node ip-172-31-68-172 pod downwardapi-volume-571be8fa-afe3-41cd-99a5-140d4a9a770a container client-container: <nil>
  STEP: delete the pod @ 05/27/23 13:53:54.818
  May 27 13:53:54.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5366" for this suite. @ 05/27/23 13:53:54.843
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 05/27/23 13:53:54.854
  May 27 13:53:54.854: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename container-probe @ 05/27/23 13:53:54.855
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:53:54.874
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:53:54.878
  STEP: Creating pod test-grpc-73aaa7c5-49a0-4b4f-a6d6-ced854123ded in namespace container-probe-5858 @ 05/27/23 13:53:54.883
  E0527 13:53:55.272245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:56.272735      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:53:56.904: INFO: Started pod test-grpc-73aaa7c5-49a0-4b4f-a6d6-ced854123ded in namespace container-probe-5858
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/27/23 13:53:56.905
  May 27 13:53:56.909: INFO: Initial restart count of pod test-grpc-73aaa7c5-49a0-4b4f-a6d6-ced854123ded is 0
  E0527 13:53:57.273725      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:58.274022      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:53:59.274864      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:00.275248      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:01.275349      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:02.276335      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:03.276664      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:04.276755      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:05.276827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:06.277160      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:07.277843      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:08.278255      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:09.278392      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:10.278614      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:11.279559      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:12.280295      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:13.281099      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:14.281225      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:15.281685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:16.281810      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:17.282789      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:18.282898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:19.283717      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:20.284256      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:21.285339      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:22.285918      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:23.286367      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:24.286749      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:25.287630      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:26.288305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:27.288574      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:28.288674      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:29.288779      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:30.288875      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:31.289645      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:32.290723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:33.291185      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:34.291312      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:35.292127      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:36.292220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:37.292318      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:38.293241      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:39.294287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:40.294386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:41.295456      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:42.296437      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:43.296465      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:44.296902      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:45.296982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:46.297117      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:47.297217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:48.297336      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:49.298095      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:50.298183      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:51.299524      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:52.300361      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:53.300862      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:54.301324      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:55.302183      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:56.303144      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:57.303741      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:58.304327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:54:59.304370      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:00.304437      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:55:01.078: INFO: Restart count of pod container-probe-5858/test-grpc-73aaa7c5-49a0-4b4f-a6d6-ced854123ded is now 1 (1m4.169254943s elapsed)
  May 27 13:55:01.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/27/23 13:55:01.083
  STEP: Destroying namespace "container-probe-5858" for this suite. @ 05/27/23 13:55:01.1
• [66.255 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 05/27/23 13:55:01.11
  May 27 13:55:01.110: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename kubectl @ 05/27/23 13:55:01.111
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:55:01.131
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:55:01.137
  STEP: creating Agnhost RC @ 05/27/23 13:55:01.162
  May 27 13:55:01.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-8507 create -f -'
  E0527 13:55:01.305115      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:55:01.478: INFO: stderr: ""
  May 27 13:55:01.478: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/27/23 13:55:01.478
  E0527 13:55:02.305124      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:55:02.483: INFO: Selector matched 1 pods for map[app:agnhost]
  May 27 13:55:02.483: INFO: Found 0 / 1
  E0527 13:55:03.306147      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:55:03.483: INFO: Selector matched 1 pods for map[app:agnhost]
  May 27 13:55:03.483: INFO: Found 1 / 1
  May 27 13:55:03.483: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 05/27/23 13:55:03.483
  May 27 13:55:03.488: INFO: Selector matched 1 pods for map[app:agnhost]
  May 27 13:55:03.488: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 27 13:55:03.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-434279256 --namespace=kubectl-8507 patch pod agnhost-primary-hfjdc -p {"metadata":{"annotations":{"x":"y"}}}'
  May 27 13:55:03.584: INFO: stderr: ""
  May 27 13:55:03.584: INFO: stdout: "pod/agnhost-primary-hfjdc patched\n"
  STEP: checking annotations @ 05/27/23 13:55:03.584
  May 27 13:55:03.589: INFO: Selector matched 1 pods for map[app:agnhost]
  May 27 13:55:03.589: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 27 13:55:03.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8507" for this suite. @ 05/27/23 13:55:03.598
• [2.496 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 05/27/23 13:55:03.606
  May 27 13:55:03.606: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename statefulset @ 05/27/23 13:55:03.607
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:55:03.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:55:03.631
  STEP: Creating service test in namespace statefulset-5222 @ 05/27/23 13:55:03.637
  STEP: Creating statefulset ss in namespace statefulset-5222 @ 05/27/23 13:55:03.645
  May 27 13:55:03.654: INFO: Found 0 stateful pods, waiting for 1
  E0527 13:55:04.307258      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:05.307415      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:06.308443      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:07.308769      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:08.308868      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:09.309856      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:10.309990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:11.310195      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:12.311278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:13.312297      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:55:13.661: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 05/27/23 13:55:13.67
  STEP: updating a scale subresource @ 05/27/23 13:55:13.674
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/27/23 13:55:13.679
  STEP: Patch a scale subresource @ 05/27/23 13:55:13.684
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/27/23 13:55:13.692
  May 27 13:55:13.698: INFO: Deleting all statefulset in ns statefulset-5222
  May 27 13:55:13.704: INFO: Scaling statefulset ss to 0
  E0527 13:55:14.312873      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:15.313137      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:16.313647      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:17.314577      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:18.314964      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:19.315397      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:20.316097      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:21.316533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:22.317505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:23.317879      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:55:23.742: INFO: Waiting for statefulset status.replicas updated to 0
  May 27 13:55:23.746: INFO: Deleting statefulset ss
  May 27 13:55:23.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5222" for this suite. @ 05/27/23 13:55:23.768
• [20.169 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 05/27/23 13:55:23.778
  May 27 13:55:23.778: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename projected @ 05/27/23 13:55:23.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:55:23.802
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:55:23.805
  STEP: Creating projection with secret that has name projected-secret-test-ecbfe828-a972-484b-b0c6-8cbec24b3706 @ 05/27/23 13:55:23.808
  STEP: Creating a pod to test consume secrets @ 05/27/23 13:55:23.814
  E0527 13:55:24.318047      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:25.318391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:26.319436      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:27.320204      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/27/23 13:55:27.84
  May 27 13:55:27.843: INFO: Trying to get logs from node ip-172-31-68-172 pod pod-projected-secrets-b1522ac1-512c-4f1e-ab4a-494124bfd57b container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/27/23 13:55:27.869
  May 27 13:55:27.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-672" for this suite. @ 05/27/23 13:55:27.893
• [4.122 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 05/27/23 13:55:27.902
  May 27 13:55:27.902: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename sched-preemption @ 05/27/23 13:55:27.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:55:27.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:55:27.925
  May 27 13:55:27.944: INFO: Waiting up to 1m0s for all nodes to be ready
  E0527 13:55:28.320305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:29.321272      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:30.322185      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:31.322482      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:32.322921      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:33.323229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:34.324242      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:35.324414      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:36.324523      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:37.324629      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:38.325583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:39.325842      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:40.326309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:41.326880      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:42.327856      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:43.328054      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:44.328289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:45.328543      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:46.329274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:47.329606      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:48.330377      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:49.331119      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:50.331246      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:51.331422      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:52.332302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:53.332462      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:54.332556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:55.332666      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:56.332792      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:57.333576      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:58.334210      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:55:59.334346      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:00.334933      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:01.335222      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:02.335607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:03.335654      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:04.336244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:05.336628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:06.337581      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:07.337913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:08.338731      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:09.338863      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:10.339346      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:11.339475      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:12.339542      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:13.339690      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:14.339707      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:15.340839      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:16.340984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:17.341536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:18.342235      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:19.342656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:20.343669      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:21.343861      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:22.344626      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:23.344782      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:24.344821      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:25.345019      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:26.345584      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:27.346565      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:56:27.966: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/27/23 13:56:27.97
  May 27 13:56:27.970: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/27/23 13:56:27.971
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:56:27.996
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:56:28
  STEP: Finding an available node @ 05/27/23 13:56:28.003
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/27/23 13:56:28.003
  E0527 13:56:28.346614      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:29.347219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/27/23 13:56:30.027
  May 27 13:56:30.044: INFO: found a healthy node: ip-172-31-68-172
  E0527 13:56:30.347826      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:31.347952      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:32.348051      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:33.348150      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:34.348783      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:35.348910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:56:36.133: INFO: pods created so far: [1 1 1]
  May 27 13:56:36.133: INFO: length of pods created so far: 3
  E0527 13:56:36.349797      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:37.350531      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:56:38.150: INFO: pods created so far: [2 2 1]
  E0527 13:56:38.351108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:39.351220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:40.351588      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:41.351678      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:42.352292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:43.352411      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0527 13:56:44.352516      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 27 13:56:45.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 27 13:56:45.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-2407" for this suite. @ 05/27/23 13:56:45.24
  STEP: Destroying namespace "sched-preemption-1952" for this suite. @ 05/27/23 13:56:45.25
• [77.356 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 05/27/23 13:56:45.259
  May 27 13:56:45.259: INFO: >>> kubeConfig: /tmp/kubeconfig-434279256
  STEP: Building a namespace api object, basename namespaces @ 05/27/23 13:56:45.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/27/23 13:56:45.285
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/27/23 13:56:45.289
  STEP: creating a Namespace @ 05/27/23 13:56:45.293
  STEP: patching the Namespace @ 05/27/23 13:56:45.313
  STEP: get the Namespace and ensuring it has the label @ 05/27/23 13:56:45.323
  May 27 13:56:45.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8914" for this suite. @ 05/27/23 13:56:45.331
  STEP: Destroying namespace "nspatchtest-4f19b9aa-0533-470c-bc2d-125204016227-1589" for this suite. @ 05/27/23 13:56:45.338
• [0.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  May 27 13:56:45.350: INFO: Running AfterSuite actions on node 1
  May 27 13:56:45.350: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
  E0527 13:56:45.356341      18 retrywatcher.go:130] "Watch failed" err="context canceled"
[ReportAfterSuite] PASSED [0.127 seconds]
------------------------------

Ran 378 of 7207 Specs in 6047.932 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h40m48.491948077s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

