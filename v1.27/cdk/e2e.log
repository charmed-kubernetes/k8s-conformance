  I0708 12:01:31.175007      20 e2e.go:117] Starting e2e run "65e215ab-230f-404c-8004-75735dff06e4" on Ginkgo node 1
  Jul  8 12:01:31.203: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1688817691 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Jul  8 12:01:31.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 12:01:31.341: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Jul  8 12:01:31.370: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Jul  8 12:01:31.374: INFO: e2e test version: v1.27.3
  Jul  8 12:01:31.375: INFO: kube-apiserver version: v1.27.3
  Jul  8 12:01:31.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 12:01:31.380: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.040 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 07/08/23 12:01:31.763
  Jul  8 12:01:31.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/08/23 12:01:31.764
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:01:31.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:01:31.786
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 07/08/23 12:01:31.79
  Jul  8 12:01:31.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 07/08/23 12:01:36.923
  Jul  8 12:01:36.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 12:01:38.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 12:01:43.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-964" for this suite. @ 07/08/23 12:01:43.793
• [12.037 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 07/08/23 12:01:43.8
  Jul  8 12:01:43.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename var-expansion @ 07/08/23 12:01:43.801
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:01:43.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:01:43.824
  STEP: Creating a pod to test substitution in container's args @ 07/08/23 12:01:43.827
  STEP: Saw pod success @ 07/08/23 12:01:49.856
  Jul  8 12:01:49.859: INFO: Trying to get logs from node ip-172-31-93-234 pod var-expansion-430d2498-4c02-4509-8d9e-f24807b0a819 container dapi-container: <nil>
  STEP: delete the pod @ 07/08/23 12:01:49.877
  Jul  8 12:01:49.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4680" for this suite. @ 07/08/23 12:01:49.898
• [6.105 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 07/08/23 12:01:49.905
  Jul  8 12:01:49.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename configmap @ 07/08/23 12:01:49.906
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:01:49.925
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:01:49.929
  STEP: Creating configMap with name configmap-test-volume-a530b188-d62a-4847-9999-728a1ddb89a6 @ 07/08/23 12:01:49.932
  STEP: Creating a pod to test consume configMaps @ 07/08/23 12:01:49.937
  STEP: Saw pod success @ 07/08/23 12:01:55.965
  Jul  8 12:01:55.968: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-configmaps-4f451123-d537-416b-822b-70c944e6f7af container configmap-volume-test: <nil>
  STEP: delete the pod @ 07/08/23 12:01:55.976
  Jul  8 12:01:55.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2846" for this suite. @ 07/08/23 12:01:55.998
• [6.099 seconds]
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 07/08/23 12:01:56.005
  Jul  8 12:01:56.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename podtemplate @ 07/08/23 12:01:56.006
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:01:56.02
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:01:56.024
  STEP: Create set of pod templates @ 07/08/23 12:01:56.027
  Jul  8 12:01:56.033: INFO: created test-podtemplate-1
  Jul  8 12:01:56.039: INFO: created test-podtemplate-2
  Jul  8 12:01:56.045: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 07/08/23 12:01:56.045
  STEP: delete collection of pod templates @ 07/08/23 12:01:56.049
  Jul  8 12:01:56.049: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 07/08/23 12:01:56.067
  Jul  8 12:01:56.067: INFO: requesting list of pod templates to confirm quantity
  Jul  8 12:01:56.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4795" for this suite. @ 07/08/23 12:01:56.075
• [0.077 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 07/08/23 12:01:56.082
  Jul  8 12:01:56.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir @ 07/08/23 12:01:56.083
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:01:56.101
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:01:56.105
  STEP: Creating a pod to test emptydir volume type on node default medium @ 07/08/23 12:01:56.108
  STEP: Saw pod success @ 07/08/23 12:02:00.132
  Jul  8 12:02:00.136: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-d6029ebc-70a5-4f8c-9a82-2c19f75655e3 container test-container: <nil>
  STEP: delete the pod @ 07/08/23 12:02:00.143
  Jul  8 12:02:00.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9632" for this suite. @ 07/08/23 12:02:00.166
• [4.092 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 07/08/23 12:02:00.174
  Jul  8 12:02:00.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename endpointslice @ 07/08/23 12:02:00.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:02:00.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:02:00.194
  Jul  8 12:02:00.208: INFO: Endpoints addresses: [172.31.42.234 172.31.91.17] , ports: [6443]
  Jul  8 12:02:00.208: INFO: EndpointSlices addresses: [172.31.42.234 172.31.91.17] , ports: [6443]
  Jul  8 12:02:00.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6854" for this suite. @ 07/08/23 12:02:00.211
• [0.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 07/08/23 12:02:00.219
  Jul  8 12:02:00.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename endpointslicemirroring @ 07/08/23 12:02:00.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:02:00.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:02:00.239
  STEP: mirroring a new custom Endpoint @ 07/08/23 12:02:00.261
  Jul  8 12:02:00.272: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 07/08/23 12:02:02.278
  Jul  8 12:02:02.303: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  STEP: mirroring deletion of a custom Endpoint @ 07/08/23 12:02:04.308
  Jul  8 12:02:04.320: INFO: Waiting for 0 EndpointSlices to exist, got 1
  Jul  8 12:02:06.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-4151" for this suite. @ 07/08/23 12:02:06.329
• [6.117 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 07/08/23 12:02:06.336
  Jul  8 12:02:06.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename csistoragecapacity @ 07/08/23 12:02:06.337
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:02:06.354
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:02:06.357
  STEP: getting /apis @ 07/08/23 12:02:06.363
  STEP: getting /apis/storage.k8s.io @ 07/08/23 12:02:06.367
  STEP: getting /apis/storage.k8s.io/v1 @ 07/08/23 12:02:06.368
  STEP: creating @ 07/08/23 12:02:06.37
  STEP: watching @ 07/08/23 12:02:06.387
  Jul  8 12:02:06.387: INFO: starting watch
  STEP: getting @ 07/08/23 12:02:06.395
  STEP: listing in namespace @ 07/08/23 12:02:06.398
  STEP: listing across namespaces @ 07/08/23 12:02:06.401
  STEP: patching @ 07/08/23 12:02:06.405
  STEP: updating @ 07/08/23 12:02:06.411
  Jul  8 12:02:06.416: INFO: waiting for watch events with expected annotations in namespace
  Jul  8 12:02:06.416: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 07/08/23 12:02:06.416
  STEP: deleting a collection @ 07/08/23 12:02:06.43
  Jul  8 12:02:06.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-9497" for this suite. @ 07/08/23 12:02:06.45
• [0.124 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 07/08/23 12:02:06.46
  Jul  8 12:02:06.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:02:06.461
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:02:06.482
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:02:06.485
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 12:02:06.489
  STEP: Saw pod success @ 07/08/23 12:02:10.512
  Jul  8 12:02:10.517: INFO: Trying to get logs from node ip-172-31-93-234 pod downwardapi-volume-a9cd05e4-791d-48a5-a514-63cdb8231bf1 container client-container: <nil>
  STEP: delete the pod @ 07/08/23 12:02:10.524
  Jul  8 12:02:10.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1770" for this suite. @ 07/08/23 12:02:10.545
• [4.091 seconds]
------------------------------
SS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 07/08/23 12:02:10.552
  Jul  8 12:02:10.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename downward-api @ 07/08/23 12:02:10.553
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:02:10.571
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:02:10.574
  STEP: Creating a pod to test downward api env vars @ 07/08/23 12:02:10.577
  STEP: Saw pod success @ 07/08/23 12:02:14.601
  Jul  8 12:02:14.605: INFO: Trying to get logs from node ip-172-31-93-234 pod downward-api-72192f5e-30a4-436f-9cac-85307bac043e container dapi-container: <nil>
  STEP: delete the pod @ 07/08/23 12:02:14.613
  Jul  8 12:02:14.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2628" for this suite. @ 07/08/23 12:02:14.632
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 07/08/23 12:02:14.641
  Jul  8 12:02:14.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename replicaset @ 07/08/23 12:02:14.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:02:14.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:02:14.661
  STEP: Create a Replicaset @ 07/08/23 12:02:14.669
  STEP: Verify that the required pods have come up. @ 07/08/23 12:02:14.676
  Jul  8 12:02:14.680: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul  8 12:02:19.685: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/08/23 12:02:19.685
  STEP: Getting /status @ 07/08/23 12:02:19.685
  Jul  8 12:02:19.690: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 07/08/23 12:02:19.69
  Jul  8 12:02:19.700: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 07/08/23 12:02:19.7
  Jul  8 12:02:19.702: INFO: Observed &ReplicaSet event: ADDED
  Jul  8 12:02:19.702: INFO: Observed &ReplicaSet event: MODIFIED
  Jul  8 12:02:19.703: INFO: Observed &ReplicaSet event: MODIFIED
  Jul  8 12:02:19.703: INFO: Observed &ReplicaSet event: MODIFIED
  Jul  8 12:02:19.703: INFO: Found replicaset test-rs in namespace replicaset-2235 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul  8 12:02:19.703: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 07/08/23 12:02:19.703
  Jul  8 12:02:19.703: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul  8 12:02:19.710: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 07/08/23 12:02:19.71
  Jul  8 12:02:19.712: INFO: Observed &ReplicaSet event: ADDED
  Jul  8 12:02:19.712: INFO: Observed &ReplicaSet event: MODIFIED
  Jul  8 12:02:19.712: INFO: Observed &ReplicaSet event: MODIFIED
  Jul  8 12:02:19.712: INFO: Observed &ReplicaSet event: MODIFIED
  Jul  8 12:02:19.712: INFO: Observed replicaset test-rs in namespace replicaset-2235 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul  8 12:02:19.713: INFO: Observed &ReplicaSet event: MODIFIED
  Jul  8 12:02:19.713: INFO: Found replicaset test-rs in namespace replicaset-2235 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Jul  8 12:02:19.713: INFO: Replicaset test-rs has a patched status
  Jul  8 12:02:19.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2235" for this suite. @ 07/08/23 12:02:19.718
• [5.084 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 07/08/23 12:02:19.724
  Jul  8 12:02:19.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename configmap @ 07/08/23 12:02:19.725
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:02:19.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:02:19.746
  STEP: Creating configMap with name configmap-test-upd-ec5fbb81-93bd-4966-8d54-4b57d9f3a941 @ 07/08/23 12:02:19.753
  STEP: Creating the pod @ 07/08/23 12:02:19.759
  STEP: Waiting for pod with text data @ 07/08/23 12:02:21.779
  STEP: Waiting for pod with binary data @ 07/08/23 12:02:21.792
  Jul  8 12:02:21.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-800" for this suite. @ 07/08/23 12:02:21.804
• [2.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 07/08/23 12:02:21.812
  Jul  8 12:02:21.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename watch @ 07/08/23 12:02:21.812
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:02:21.83
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:02:21.833
  STEP: creating a watch on configmaps with label A @ 07/08/23 12:02:21.837
  STEP: creating a watch on configmaps with label B @ 07/08/23 12:02:21.838
  STEP: creating a watch on configmaps with label A or B @ 07/08/23 12:02:21.84
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 07/08/23 12:02:21.841
  Jul  8 12:02:21.848: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2332  f0556147-b8c1-424f-92fa-cb80ef6573a6 2983 0 2023-07-08 12:02:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-08 12:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 12:02:21.848: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2332  f0556147-b8c1-424f-92fa-cb80ef6573a6 2983 0 2023-07-08 12:02:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-08 12:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 07/08/23 12:02:21.848
  Jul  8 12:02:21.856: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2332  f0556147-b8c1-424f-92fa-cb80ef6573a6 2984 0 2023-07-08 12:02:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-08 12:02:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 12:02:21.856: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2332  f0556147-b8c1-424f-92fa-cb80ef6573a6 2984 0 2023-07-08 12:02:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-08 12:02:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 07/08/23 12:02:21.856
  Jul  8 12:02:21.866: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2332  f0556147-b8c1-424f-92fa-cb80ef6573a6 2986 0 2023-07-08 12:02:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-08 12:02:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 12:02:21.866: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2332  f0556147-b8c1-424f-92fa-cb80ef6573a6 2986 0 2023-07-08 12:02:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-08 12:02:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 07/08/23 12:02:21.866
  Jul  8 12:02:21.872: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2332  f0556147-b8c1-424f-92fa-cb80ef6573a6 2987 0 2023-07-08 12:02:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-08 12:02:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 12:02:21.872: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2332  f0556147-b8c1-424f-92fa-cb80ef6573a6 2987 0 2023-07-08 12:02:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-08 12:02:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 07/08/23 12:02:21.872
  Jul  8 12:02:21.877: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2332  70806179-93da-4d96-99e0-bed26c73b2e1 2988 0 2023-07-08 12:02:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-08 12:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 12:02:21.877: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2332  70806179-93da-4d96-99e0-bed26c73b2e1 2988 0 2023-07-08 12:02:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-08 12:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 07/08/23 12:02:31.877
  Jul  8 12:02:31.885: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2332  70806179-93da-4d96-99e0-bed26c73b2e1 3045 0 2023-07-08 12:02:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-08 12:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 12:02:31.885: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2332  70806179-93da-4d96-99e0-bed26c73b2e1 3045 0 2023-07-08 12:02:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-08 12:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 12:02:41.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2332" for this suite. @ 07/08/23 12:02:41.891
• [20.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 07/08/23 12:02:41.899
  Jul  8 12:02:41.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:02:41.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:02:41.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:02:41.919
  STEP: Creating configMap with name projected-configmap-test-volume-map-01a0f669-da20-495b-8cb9-bde15efcefdb @ 07/08/23 12:02:41.922
  STEP: Creating a pod to test consume configMaps @ 07/08/23 12:02:41.927
  STEP: Saw pod success @ 07/08/23 12:02:45.95
  Jul  8 12:02:45.954: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-projected-configmaps-d93885be-d1e2-4601-8a18-9b4ea24d9937 container agnhost-container: <nil>
  STEP: delete the pod @ 07/08/23 12:02:45.962
  Jul  8 12:02:45.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9814" for this suite. @ 07/08/23 12:02:45.985
• [4.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 07/08/23 12:02:45.997
  Jul  8 12:02:45.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir @ 07/08/23 12:02:45.998
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:02:46.016
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:02:46.019
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 07/08/23 12:02:46.023
  STEP: Saw pod success @ 07/08/23 12:02:50.046
  Jul  8 12:02:50.049: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-2ef920d5-21a1-4139-b328-0f733433369f container test-container: <nil>
  STEP: delete the pod @ 07/08/23 12:02:50.058
  Jul  8 12:02:50.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7335" for this suite. @ 07/08/23 12:02:50.081
• [4.092 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 07/08/23 12:02:50.089
  Jul  8 12:02:50.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename var-expansion @ 07/08/23 12:02:50.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:02:50.108
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:02:50.111
  STEP: creating the pod with failed condition @ 07/08/23 12:02:50.115
  STEP: updating the pod @ 07/08/23 12:04:50.124
  Jul  8 12:04:50.637: INFO: Successfully updated pod "var-expansion-a057e0c4-42ea-471b-b83e-c323117646ac"
  STEP: waiting for pod running @ 07/08/23 12:04:50.637
  STEP: deleting the pod gracefully @ 07/08/23 12:04:52.647
  Jul  8 12:04:52.647: INFO: Deleting pod "var-expansion-a057e0c4-42ea-471b-b83e-c323117646ac" in namespace "var-expansion-4789"
  Jul  8 12:04:52.655: INFO: Wait up to 5m0s for pod "var-expansion-a057e0c4-42ea-471b-b83e-c323117646ac" to be fully deleted
  Jul  8 12:05:24.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4789" for this suite. @ 07/08/23 12:05:24.757
• [154.675 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 07/08/23 12:05:24.765
  Jul  8 12:05:24.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename deployment @ 07/08/23 12:05:24.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:05:24.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:05:24.787
  STEP: creating a Deployment @ 07/08/23 12:05:24.795
  Jul  8 12:05:24.795: INFO: Creating simple deployment test-deployment-jlcbd
  Jul  8 12:05:24.807: INFO: deployment "test-deployment-jlcbd" doesn't have the required revision set
  STEP: Getting /status @ 07/08/23 12:05:26.823
  Jul  8 12:05:26.827: INFO: Deployment test-deployment-jlcbd has Conditions: [{Available True 2023-07-08 12:05:25 +0000 UTC 2023-07-08 12:05:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-07-08 12:05:25 +0000 UTC 2023-07-08 12:05:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jlcbd-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 07/08/23 12:05:26.827
  Jul  8 12:05:26.843: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 12, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 5, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 12, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 5, 24, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-jlcbd-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 07/08/23 12:05:26.844
  Jul  8 12:05:26.845: INFO: Observed &Deployment event: ADDED
  Jul  8 12:05:26.845: INFO: Observed Deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-08 12:05:24 +0000 UTC 2023-07-08 12:05:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jlcbd-5994cf9475"}
  Jul  8 12:05:26.846: INFO: Observed &Deployment event: MODIFIED
  Jul  8 12:05:26.846: INFO: Observed Deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-08 12:05:24 +0000 UTC 2023-07-08 12:05:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jlcbd-5994cf9475"}
  Jul  8 12:05:26.846: INFO: Observed Deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-08 12:05:24 +0000 UTC 2023-07-08 12:05:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul  8 12:05:26.846: INFO: Observed &Deployment event: MODIFIED
  Jul  8 12:05:26.846: INFO: Observed Deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-08 12:05:24 +0000 UTC 2023-07-08 12:05:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul  8 12:05:26.846: INFO: Observed Deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-08 12:05:24 +0000 UTC 2023-07-08 12:05:24 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jlcbd-5994cf9475" is progressing.}
  Jul  8 12:05:26.846: INFO: Observed &Deployment event: MODIFIED
  Jul  8 12:05:26.846: INFO: Observed Deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-08 12:05:25 +0000 UTC 2023-07-08 12:05:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul  8 12:05:26.846: INFO: Observed Deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-08 12:05:25 +0000 UTC 2023-07-08 12:05:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jlcbd-5994cf9475" has successfully progressed.}
  Jul  8 12:05:26.846: INFO: Observed &Deployment event: MODIFIED
  Jul  8 12:05:26.846: INFO: Observed Deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-08 12:05:25 +0000 UTC 2023-07-08 12:05:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul  8 12:05:26.846: INFO: Observed Deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-08 12:05:25 +0000 UTC 2023-07-08 12:05:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jlcbd-5994cf9475" has successfully progressed.}
  Jul  8 12:05:26.846: INFO: Found Deployment test-deployment-jlcbd in namespace deployment-2067 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul  8 12:05:26.846: INFO: Deployment test-deployment-jlcbd has an updated status
  STEP: patching the Statefulset Status @ 07/08/23 12:05:26.846
  Jul  8 12:05:26.846: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul  8 12:05:26.853: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 07/08/23 12:05:26.853
  Jul  8 12:05:26.855: INFO: Observed &Deployment event: ADDED
  Jul  8 12:05:26.855: INFO: Observed deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-08 12:05:24 +0000 UTC 2023-07-08 12:05:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jlcbd-5994cf9475"}
  Jul  8 12:05:26.855: INFO: Observed &Deployment event: MODIFIED
  Jul  8 12:05:26.855: INFO: Observed deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-08 12:05:24 +0000 UTC 2023-07-08 12:05:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jlcbd-5994cf9475"}
  Jul  8 12:05:26.855: INFO: Observed deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-08 12:05:24 +0000 UTC 2023-07-08 12:05:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul  8 12:05:26.855: INFO: Observed &Deployment event: MODIFIED
  Jul  8 12:05:26.855: INFO: Observed deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-08 12:05:24 +0000 UTC 2023-07-08 12:05:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul  8 12:05:26.855: INFO: Observed deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-08 12:05:24 +0000 UTC 2023-07-08 12:05:24 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jlcbd-5994cf9475" is progressing.}
  Jul  8 12:05:26.856: INFO: Observed &Deployment event: MODIFIED
  Jul  8 12:05:26.856: INFO: Observed deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-08 12:05:25 +0000 UTC 2023-07-08 12:05:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul  8 12:05:26.856: INFO: Observed deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-08 12:05:25 +0000 UTC 2023-07-08 12:05:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jlcbd-5994cf9475" has successfully progressed.}
  Jul  8 12:05:26.856: INFO: Observed &Deployment event: MODIFIED
  Jul  8 12:05:26.856: INFO: Observed deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-08 12:05:25 +0000 UTC 2023-07-08 12:05:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul  8 12:05:26.856: INFO: Observed deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-08 12:05:25 +0000 UTC 2023-07-08 12:05:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jlcbd-5994cf9475" has successfully progressed.}
  Jul  8 12:05:26.856: INFO: Observed deployment test-deployment-jlcbd in namespace deployment-2067 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul  8 12:05:26.856: INFO: Observed &Deployment event: MODIFIED
  Jul  8 12:05:26.856: INFO: Found deployment test-deployment-jlcbd in namespace deployment-2067 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Jul  8 12:05:26.856: INFO: Deployment test-deployment-jlcbd has a patched status
  Jul  8 12:05:26.860: INFO: Deployment "test-deployment-jlcbd":
  &Deployment{ObjectMeta:{test-deployment-jlcbd  deployment-2067  200b17e0-667d-433c-9f0b-b016b0f8a4a9 3531 1 2023-07-08 12:05:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-07-08 12:05:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-07-08 12:05:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-07-08 12:05:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ca2638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-jlcbd-5994cf9475",LastUpdateTime:2023-07-08 12:05:26 +0000 UTC,LastTransitionTime:2023-07-08 12:05:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul  8 12:05:26.863: INFO: New ReplicaSet "test-deployment-jlcbd-5994cf9475" of Deployment "test-deployment-jlcbd":
  &ReplicaSet{ObjectMeta:{test-deployment-jlcbd-5994cf9475  deployment-2067  046ef376-38ce-4566-8c33-9ab515bad997 3526 1 2023-07-08 12:05:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-jlcbd 200b17e0-667d-433c-9f0b-b016b0f8a4a9 0xc002784627 0xc002784628}] [] [{kube-controller-manager Update apps/v1 2023-07-08 12:05:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"200b17e0-667d-433c-9f0b-b016b0f8a4a9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 12:05:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027846d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul  8 12:05:26.867: INFO: Pod "test-deployment-jlcbd-5994cf9475-9m9km" is available:
  &Pod{ObjectMeta:{test-deployment-jlcbd-5994cf9475-9m9km test-deployment-jlcbd-5994cf9475- deployment-2067  87e2905c-7c57-4d77-a2e1-76063a9c7aff 3525 0 2023-07-08 12:05:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-jlcbd-5994cf9475 046ef376-38ce-4566-8c33-9ab515bad997 0xc002784a67 0xc002784a68}] [] [{kube-controller-manager Update v1 2023-07-08 12:05:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"046ef376-38ce-4566-8c33-9ab515bad997\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 12:05:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.205\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d82z2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d82z2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-93-234,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 12:05:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 12:05:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 12:05:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 12:05:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.93.234,PodIP:192.168.7.205,StartTime:2023-07-08 12:05:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-08 12:05:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9cd2935f3b321202e218bca24bbc8a40562539404507161e8d4082b9a702d54a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.205,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 12:05:26.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2067" for this suite. @ 07/08/23 12:05:26.872
• [2.114 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 07/08/23 12:05:26.879
  Jul  8 12:05:26.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/08/23 12:05:26.88
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:05:26.894
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:05:26.897
  STEP: create the container to handle the HTTPGet hook request. @ 07/08/23 12:05:26.904
  STEP: create the pod with lifecycle hook @ 07/08/23 12:05:32.936
  STEP: check poststart hook @ 07/08/23 12:05:34.96
  STEP: delete the pod with lifecycle hook @ 07/08/23 12:05:34.976
  Jul  8 12:05:36.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1312" for this suite. @ 07/08/23 12:05:36.997
• [10.125 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 07/08/23 12:05:37.004
  Jul  8 12:05:37.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename subpath @ 07/08/23 12:05:37.005
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:05:37.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:05:37.025
  STEP: Setting up data @ 07/08/23 12:05:37.029
  STEP: Creating pod pod-subpath-test-configmap-52kp @ 07/08/23 12:05:37.038
  STEP: Creating a pod to test atomic-volume-subpath @ 07/08/23 12:05:37.038
  STEP: Saw pod success @ 07/08/23 12:05:59.107
  Jul  8 12:05:59.110: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-subpath-test-configmap-52kp container test-container-subpath-configmap-52kp: <nil>
  STEP: delete the pod @ 07/08/23 12:05:59.125
  STEP: Deleting pod pod-subpath-test-configmap-52kp @ 07/08/23 12:05:59.142
  Jul  8 12:05:59.143: INFO: Deleting pod "pod-subpath-test-configmap-52kp" in namespace "subpath-256"
  Jul  8 12:05:59.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-256" for this suite. @ 07/08/23 12:05:59.153
• [22.156 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 07/08/23 12:05:59.16
  Jul  8 12:05:59.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:05:59.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:05:59.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:05:59.179
  STEP: Creating projection with secret that has name projected-secret-test-map-d525131c-b269-4a7d-9694-c230ba85dcd7 @ 07/08/23 12:05:59.182
  STEP: Creating a pod to test consume secrets @ 07/08/23 12:05:59.187
  STEP: Saw pod success @ 07/08/23 12:06:03.212
  Jul  8 12:06:03.216: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-projected-secrets-03ba5393-8801-4421-a851-f2546a734b14 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/08/23 12:06:03.224
  Jul  8 12:06:03.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5332" for this suite. @ 07/08/23 12:06:03.247
• [4.094 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 07/08/23 12:06:03.254
  Jul  8 12:06:03.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/08/23 12:06:03.255
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:06:03.272
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:06:03.275
  STEP: create the container to handle the HTTPGet hook request. @ 07/08/23 12:06:03.283
  STEP: create the pod with lifecycle hook @ 07/08/23 12:06:07.309
  STEP: check poststart hook @ 07/08/23 12:06:09.327
  STEP: delete the pod with lifecycle hook @ 07/08/23 12:06:09.345
  Jul  8 12:06:13.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8576" for this suite. @ 07/08/23 12:06:13.37
• [10.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 07/08/23 12:06:13.378
  Jul  8 12:06:13.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename containers @ 07/08/23 12:06:13.379
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:06:13.397
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:06:13.4
  STEP: Creating a pod to test override command @ 07/08/23 12:06:13.404
  STEP: Saw pod success @ 07/08/23 12:06:17.427
  Jul  8 12:06:17.431: INFO: Trying to get logs from node ip-172-31-93-234 pod client-containers-5c822f66-1f06-4269-a0d5-f7d37713c160 container agnhost-container: <nil>
  STEP: delete the pod @ 07/08/23 12:06:17.438
  Jul  8 12:06:17.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7705" for this suite. @ 07/08/23 12:06:17.459
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 07/08/23 12:06:17.468
  Jul  8 12:06:17.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename replication-controller @ 07/08/23 12:06:17.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:06:17.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:06:17.49
  STEP: creating a ReplicationController @ 07/08/23 12:06:17.497
  STEP: waiting for RC to be added @ 07/08/23 12:06:17.504
  STEP: waiting for available Replicas @ 07/08/23 12:06:17.504
  STEP: patching ReplicationController @ 07/08/23 12:06:18.892
  STEP: waiting for RC to be modified @ 07/08/23 12:06:18.9
  STEP: patching ReplicationController status @ 07/08/23 12:06:18.9
  STEP: waiting for RC to be modified @ 07/08/23 12:06:18.907
  STEP: waiting for available Replicas @ 07/08/23 12:06:18.907
  STEP: fetching ReplicationController status @ 07/08/23 12:06:18.913
  STEP: patching ReplicationController scale @ 07/08/23 12:06:18.918
  STEP: waiting for RC to be modified @ 07/08/23 12:06:18.924
  STEP: waiting for ReplicationController's scale to be the max amount @ 07/08/23 12:06:18.924
  STEP: fetching ReplicationController; ensuring that it's patched @ 07/08/23 12:06:21.336
  STEP: updating ReplicationController status @ 07/08/23 12:06:21.34
  STEP: waiting for RC to be modified @ 07/08/23 12:06:21.346
  STEP: listing all ReplicationControllers @ 07/08/23 12:06:21.346
  STEP: checking that ReplicationController has expected values @ 07/08/23 12:06:21.351
  STEP: deleting ReplicationControllers by collection @ 07/08/23 12:06:21.351
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 07/08/23 12:06:21.361
  Jul  8 12:06:21.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0708 12:06:21.408621      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-4389" for this suite. @ 07/08/23 12:06:21.412
• [3.953 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 07/08/23 12:06:21.421
  Jul  8 12:06:21.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename events @ 07/08/23 12:06:21.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:06:21.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:06:21.441
  STEP: creating a test event @ 07/08/23 12:06:21.447
  STEP: listing events in all namespaces @ 07/08/23 12:06:21.455
  STEP: listing events in test namespace @ 07/08/23 12:06:21.462
  STEP: listing events with field selection filtering on source @ 07/08/23 12:06:21.466
  STEP: listing events with field selection filtering on reportingController @ 07/08/23 12:06:21.47
  STEP: getting the test event @ 07/08/23 12:06:21.473
  STEP: patching the test event @ 07/08/23 12:06:21.477
  STEP: getting the test event @ 07/08/23 12:06:21.485
  STEP: updating the test event @ 07/08/23 12:06:21.488
  STEP: getting the test event @ 07/08/23 12:06:21.498
  STEP: deleting the test event @ 07/08/23 12:06:21.502
  STEP: listing events in all namespaces @ 07/08/23 12:06:21.51
  STEP: listing events in test namespace @ 07/08/23 12:06:21.518
  Jul  8 12:06:21.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8696" for this suite. @ 07/08/23 12:06:21.525
• [0.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 07/08/23 12:06:21.534
  Jul  8 12:06:21.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename namespaces @ 07/08/23 12:06:21.535
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:06:21.551
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:06:21.555
  STEP: Creating a test namespace @ 07/08/23 12:06:21.558
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:06:21.573
  STEP: Creating a service in the namespace @ 07/08/23 12:06:21.576
  STEP: Deleting the namespace @ 07/08/23 12:06:21.587
  STEP: Waiting for the namespace to be removed. @ 07/08/23 12:06:21.595
  E0708 12:06:22.408938      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:23.409013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:24.409106      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:25.409275      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:26.409948      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:27.411021      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 07/08/23 12:06:27.6
  STEP: Verifying there is no service in the namespace @ 07/08/23 12:06:27.617
  Jul  8 12:06:27.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-876" for this suite. @ 07/08/23 12:06:27.626
  STEP: Destroying namespace "nsdeletetest-3900" for this suite. @ 07/08/23 12:06:27.633
  Jul  8 12:06:27.637: INFO: Namespace nsdeletetest-3900 was already deleted
  STEP: Destroying namespace "nsdeletetest-5578" for this suite. @ 07/08/23 12:06:27.637
• [6.110 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 07/08/23 12:06:27.644
  Jul  8 12:06:27.644: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 12:06:27.644
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:06:27.662
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:06:27.665
  STEP: Setting up server cert @ 07/08/23 12:06:27.699
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 12:06:27.858
  STEP: Deploying the webhook pod @ 07/08/23 12:06:27.867
  STEP: Wait for the deployment to be ready @ 07/08/23 12:06:27.879
  Jul  8 12:06:27.889: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0708 12:06:28.411072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:29.411227      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 12:06:29.901
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 12:06:29.916
  E0708 12:06:30.412083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:06:30.916: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 07/08/23 12:06:30.92
  STEP: create a pod @ 07/08/23 12:06:30.938
  E0708 12:06:31.413127      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:32.413275      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 07/08/23 12:06:32.956
  Jul  8 12:06:32.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=webhook-3907 attach --namespace=webhook-3907 to-be-attached-pod -i -c=container1'
  Jul  8 12:06:33.031: INFO: rc: 1
  Jul  8 12:06:33.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3907" for this suite. @ 07/08/23 12:06:33.086
  STEP: Destroying namespace "webhook-markers-5864" for this suite. @ 07/08/23 12:06:33.094
• [5.458 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 07/08/23 12:06:33.102
  Jul  8 12:06:33.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename configmap @ 07/08/23 12:06:33.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:06:33.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:06:33.125
  STEP: Creating configMap with name cm-test-opt-del-b3660d62-cac6-474b-a58d-c7ad81c79b3e @ 07/08/23 12:06:33.133
  STEP: Creating configMap with name cm-test-opt-upd-392d8655-f0ec-47c6-b649-d0ae261ea9dd @ 07/08/23 12:06:33.138
  STEP: Creating the pod @ 07/08/23 12:06:33.143
  E0708 12:06:33.413750      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:34.413820      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-b3660d62-cac6-474b-a58d-c7ad81c79b3e @ 07/08/23 12:06:35.19
  STEP: Updating configmap cm-test-opt-upd-392d8655-f0ec-47c6-b649-d0ae261ea9dd @ 07/08/23 12:06:35.197
  STEP: Creating configMap with name cm-test-opt-create-5d0fd933-42f3-4ecd-a6a8-7df8b904519f @ 07/08/23 12:06:35.202
  STEP: waiting to observe update in volume @ 07/08/23 12:06:35.207
  E0708 12:06:35.414425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:36.414622      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:37.414883      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:38.415027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:39.415836      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:40.416048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:41.416270      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:42.417112      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:43.417187      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:44.418030      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:45.418072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:46.418787      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:47.418929      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:48.419040      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:49.419583      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:50.419833      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:51.420269      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:52.420346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:53.420826      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:54.420911      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:55.421745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:56.422707      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:57.423005      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:58.423188      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:06:59.424090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:00.425118      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:01.425180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:02.425278      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:03.425648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:04.425840      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:05.426672      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:06.426868      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:07.427444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:08.427527      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:09.428197      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:10.429112      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:11.429391      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:12.429599      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:13.430390      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:14.430522      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:15.430689      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:16.431483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:17.431495      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:18.431610      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:19.431733      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:20.431813      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:21.431807      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:22.431978      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:23.432165      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:24.432272      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:25.433121      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:26.433966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:27.434080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:28.434151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:29.434347      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:30.434421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:31.435474      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:32.436050      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:33.436166      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:34.436239      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:35.437107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:36.437524      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:37.437619      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:38.438095      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:39.438187      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:40.438253      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:41.438347      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:42.438523      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:43.438681      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:44.438954      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:45.439173      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:46.440105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:47.440180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:48.440249      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:49.441080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:50.441517      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:51.441604      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:52.441745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:53.441995      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:54.442040      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:55.442204      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:56.442287      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:57.442428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:58.443154      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:07:59.443299      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:00.443384      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:01.443433      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:08:01.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-93" for this suite. @ 07/08/23 12:08:01.63
• [88.534 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 07/08/23 12:08:01.639
  Jul  8 12:08:01.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 12:08:01.639
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:08:01.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:08:01.662
  STEP: Setting up server cert @ 07/08/23 12:08:01.687
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 12:08:01.923
  STEP: Deploying the webhook pod @ 07/08/23 12:08:01.933
  STEP: Wait for the deployment to be ready @ 07/08/23 12:08:01.944
  Jul  8 12:08:01.951: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0708 12:08:02.444442      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:03.444533      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 12:08:03.964
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 12:08:03.975
  E0708 12:08:04.444776      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:08:04.975: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul  8 12:08:04.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 12:08:05.445547      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6794-crds.webhook.example.com via the AdmissionRegistration API @ 07/08/23 12:08:05.492
  STEP: Creating a custom resource that should be mutated by the webhook @ 07/08/23 12:08:05.508
  E0708 12:08:06.446214      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:07.446312      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:08:07.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2081" for this suite. @ 07/08/23 12:08:08.12
  STEP: Destroying namespace "webhook-markers-271" for this suite. @ 07/08/23 12:08:08.128
• [6.496 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 07/08/23 12:08:08.135
  Jul  8 12:08:08.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename secrets @ 07/08/23 12:08:08.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:08:08.149
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:08:08.152
  STEP: Creating secret with name secret-test-558ec0d7-d29d-4a80-ae7c-0c37cb8f799c @ 07/08/23 12:08:08.155
  STEP: Creating a pod to test consume secrets @ 07/08/23 12:08:08.16
  E0708 12:08:08.446997      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:09.447237      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:10.447976      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:11.448044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:08:12.182
  Jul  8 12:08:12.186: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-secrets-77789305-7afe-4af4-889f-ae29d4739c77 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/08/23 12:08:12.194
  Jul  8 12:08:12.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4213" for this suite. @ 07/08/23 12:08:12.216
• [4.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 07/08/23 12:08:12.224
  Jul  8 12:08:12.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:08:12.224
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:08:12.238
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:08:12.242
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 12:08:12.245
  E0708 12:08:12.449021      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:13.449100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:08:14.326
  Jul  8 12:08:14.330: INFO: Trying to get logs from node ip-172-31-93-234 pod downwardapi-volume-3970b4fb-13dc-43d6-8047-273fc9f516e5 container client-container: <nil>
  STEP: delete the pod @ 07/08/23 12:08:14.337
  Jul  8 12:08:14.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8933" for this suite. @ 07/08/23 12:08:14.358
• [2.141 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 07/08/23 12:08:14.365
  Jul  8 12:08:14.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir @ 07/08/23 12:08:14.366
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:08:14.382
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:08:14.385
  STEP: Creating Pod @ 07/08/23 12:08:14.388
  E0708 12:08:14.450080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:15.450235      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 07/08/23 12:08:16.408
  Jul  8 12:08:16.408: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1047 PodName:pod-sharedvolume-eb9dacea-363f-4596-a90d-e6295dfa9ba9 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 12:08:16.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 12:08:16.409: INFO: ExecWithOptions: Clientset creation
  Jul  8 12:08:16.409: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-1047/pods/pod-sharedvolume-eb9dacea-363f-4596-a90d-e6295dfa9ba9/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  E0708 12:08:16.450277      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:08:16.468: INFO: Exec stderr: ""
  Jul  8 12:08:16.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1047" for this suite. @ 07/08/23 12:08:16.473
• [2.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 07/08/23 12:08:16.481
  Jul  8 12:08:16.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename svcaccounts @ 07/08/23 12:08:16.482
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:08:16.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:08:16.499
  Jul  8 12:08:16.507: INFO: Got root ca configmap in namespace "svcaccounts-2306"
  Jul  8 12:08:16.513: INFO: Deleted root ca configmap in namespace "svcaccounts-2306"
  STEP: waiting for a new root ca configmap created @ 07/08/23 12:08:17.014
  Jul  8 12:08:17.018: INFO: Recreated root ca configmap in namespace "svcaccounts-2306"
  Jul  8 12:08:17.024: INFO: Updated root ca configmap in namespace "svcaccounts-2306"
  E0708 12:08:17.450753      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for the root ca configmap reconciled @ 07/08/23 12:08:17.524
  Jul  8 12:08:17.529: INFO: Reconciled root ca configmap in namespace "svcaccounts-2306"
  Jul  8 12:08:17.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2306" for this suite. @ 07/08/23 12:08:17.533
• [1.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 07/08/23 12:08:17.54
  Jul  8 12:08:17.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename svcaccounts @ 07/08/23 12:08:17.541
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:08:17.565
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:08:17.569
  STEP: Creating a pod to test service account token:  @ 07/08/23 12:08:17.572
  E0708 12:08:18.450863      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:19.451047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:20.451080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:21.451276      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:08:21.594
  Jul  8 12:08:21.598: INFO: Trying to get logs from node ip-172-31-29-188 pod test-pod-63493050-de42-4237-8f26-c7bfbbfde8ae container agnhost-container: <nil>
  STEP: delete the pod @ 07/08/23 12:08:21.613
  Jul  8 12:08:21.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9975" for this suite. @ 07/08/23 12:08:21.636
• [4.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 07/08/23 12:08:21.645
  Jul  8 12:08:21.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename secrets @ 07/08/23 12:08:21.645
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:08:21.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:08:21.667
  STEP: Creating secret with name secret-test-4965e891-9b49-4904-9196-07837a1c5573 @ 07/08/23 12:08:21.67
  STEP: Creating a pod to test consume secrets @ 07/08/23 12:08:21.676
  E0708 12:08:22.451366      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:23.451553      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:24.451671      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:25.451745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:08:25.701
  Jul  8 12:08:25.704: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-secrets-efc947a5-0313-4eca-8129-3afa1692728b container secret-volume-test: <nil>
  STEP: delete the pod @ 07/08/23 12:08:25.712
  Jul  8 12:08:25.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5547" for this suite. @ 07/08/23 12:08:25.734
• [4.096 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 07/08/23 12:08:25.742
  Jul  8 12:08:25.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename replicaset @ 07/08/23 12:08:25.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:08:25.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:08:25.763
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 07/08/23 12:08:25.766
  Jul  8 12:08:25.776: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0708 12:08:26.452679      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:27.452782      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:28.452871      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:29.452976      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:30.453040      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:08:30.782: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/08/23 12:08:30.782
  STEP: getting scale subresource @ 07/08/23 12:08:30.782
  STEP: updating a scale subresource @ 07/08/23 12:08:30.786
  STEP: verifying the replicaset Spec.Replicas was modified @ 07/08/23 12:08:30.793
  STEP: Patch a scale subresource @ 07/08/23 12:08:30.797
  Jul  8 12:08:30.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-536" for this suite. @ 07/08/23 12:08:30.812
• [5.081 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 07/08/23 12:08:30.822
  Jul  8 12:08:30.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename downward-api @ 07/08/23 12:08:30.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:08:30.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:08:30.848
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 12:08:30.851
  E0708 12:08:31.453206      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:32.453293      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:33.453399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:34.453470      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:08:34.877
  Jul  8 12:08:34.881: INFO: Trying to get logs from node ip-172-31-93-234 pod downwardapi-volume-821ac13f-8698-4279-b41f-8f570636df3c container client-container: <nil>
  STEP: delete the pod @ 07/08/23 12:08:34.889
  Jul  8 12:08:34.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2532" for this suite. @ 07/08/23 12:08:34.91
• [4.095 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 07/08/23 12:08:34.917
  Jul  8 12:08:34.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename sysctl @ 07/08/23 12:08:34.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:08:34.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:08:34.938
  STEP: Creating a pod with one valid and two invalid sysctls @ 07/08/23 12:08:34.942
  Jul  8 12:08:34.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-8633" for this suite. @ 07/08/23 12:08:34.95
• [0.041 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 07/08/23 12:08:34.96
  Jul  8 12:08:34.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename discovery @ 07/08/23 12:08:34.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:08:34.976
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:08:34.979
  STEP: Setting up server cert @ 07/08/23 12:08:34.984
  Jul  8 12:08:35.297: INFO: Checking APIGroup: apiregistration.k8s.io
  Jul  8 12:08:35.298: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Jul  8 12:08:35.298: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Jul  8 12:08:35.298: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Jul  8 12:08:35.298: INFO: Checking APIGroup: apps
  Jul  8 12:08:35.300: INFO: PreferredVersion.GroupVersion: apps/v1
  Jul  8 12:08:35.300: INFO: Versions found [{apps/v1 v1}]
  Jul  8 12:08:35.300: INFO: apps/v1 matches apps/v1
  Jul  8 12:08:35.300: INFO: Checking APIGroup: events.k8s.io
  Jul  8 12:08:35.301: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Jul  8 12:08:35.301: INFO: Versions found [{events.k8s.io/v1 v1}]
  Jul  8 12:08:35.301: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Jul  8 12:08:35.301: INFO: Checking APIGroup: authentication.k8s.io
  Jul  8 12:08:35.303: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Jul  8 12:08:35.303: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Jul  8 12:08:35.303: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Jul  8 12:08:35.303: INFO: Checking APIGroup: authorization.k8s.io
  Jul  8 12:08:35.304: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Jul  8 12:08:35.304: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Jul  8 12:08:35.304: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Jul  8 12:08:35.304: INFO: Checking APIGroup: autoscaling
  Jul  8 12:08:35.306: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Jul  8 12:08:35.306: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Jul  8 12:08:35.306: INFO: autoscaling/v2 matches autoscaling/v2
  Jul  8 12:08:35.306: INFO: Checking APIGroup: batch
  Jul  8 12:08:35.307: INFO: PreferredVersion.GroupVersion: batch/v1
  Jul  8 12:08:35.307: INFO: Versions found [{batch/v1 v1}]
  Jul  8 12:08:35.307: INFO: batch/v1 matches batch/v1
  Jul  8 12:08:35.307: INFO: Checking APIGroup: certificates.k8s.io
  Jul  8 12:08:35.309: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Jul  8 12:08:35.309: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Jul  8 12:08:35.309: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Jul  8 12:08:35.309: INFO: Checking APIGroup: networking.k8s.io
  Jul  8 12:08:35.310: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Jul  8 12:08:35.310: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Jul  8 12:08:35.310: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Jul  8 12:08:35.310: INFO: Checking APIGroup: policy
  Jul  8 12:08:35.312: INFO: PreferredVersion.GroupVersion: policy/v1
  Jul  8 12:08:35.312: INFO: Versions found [{policy/v1 v1}]
  Jul  8 12:08:35.312: INFO: policy/v1 matches policy/v1
  Jul  8 12:08:35.312: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Jul  8 12:08:35.313: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Jul  8 12:08:35.313: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Jul  8 12:08:35.313: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Jul  8 12:08:35.313: INFO: Checking APIGroup: storage.k8s.io
  Jul  8 12:08:35.314: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Jul  8 12:08:35.314: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Jul  8 12:08:35.314: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Jul  8 12:08:35.314: INFO: Checking APIGroup: admissionregistration.k8s.io
  Jul  8 12:08:35.316: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Jul  8 12:08:35.316: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Jul  8 12:08:35.316: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Jul  8 12:08:35.316: INFO: Checking APIGroup: apiextensions.k8s.io
  Jul  8 12:08:35.317: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Jul  8 12:08:35.317: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Jul  8 12:08:35.317: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Jul  8 12:08:35.317: INFO: Checking APIGroup: scheduling.k8s.io
  Jul  8 12:08:35.319: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Jul  8 12:08:35.319: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Jul  8 12:08:35.319: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Jul  8 12:08:35.319: INFO: Checking APIGroup: coordination.k8s.io
  Jul  8 12:08:35.320: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Jul  8 12:08:35.320: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Jul  8 12:08:35.320: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Jul  8 12:08:35.320: INFO: Checking APIGroup: node.k8s.io
  Jul  8 12:08:35.322: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Jul  8 12:08:35.322: INFO: Versions found [{node.k8s.io/v1 v1}]
  Jul  8 12:08:35.322: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Jul  8 12:08:35.322: INFO: Checking APIGroup: discovery.k8s.io
  Jul  8 12:08:35.323: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Jul  8 12:08:35.323: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Jul  8 12:08:35.323: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Jul  8 12:08:35.323: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Jul  8 12:08:35.325: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Jul  8 12:08:35.325: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Jul  8 12:08:35.325: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Jul  8 12:08:35.325: INFO: Checking APIGroup: metrics.k8s.io
  Jul  8 12:08:35.326: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Jul  8 12:08:35.326: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Jul  8 12:08:35.326: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Jul  8 12:08:35.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-4464" for this suite. @ 07/08/23 12:08:35.33
• [0.378 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 07/08/23 12:08:35.338
  Jul  8 12:08:35.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename field-validation @ 07/08/23 12:08:35.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:08:35.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:08:35.358
  Jul  8 12:08:35.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 12:08:35.454343      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:36.455129      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:37.455285      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0708 12:08:37.911886      20 warnings.go:70] unknown field "alpha"
  W0708 12:08:37.911902      20 warnings.go:70] unknown field "beta"
  W0708 12:08:37.911905      20 warnings.go:70] unknown field "delta"
  W0708 12:08:37.911925      20 warnings.go:70] unknown field "epsilon"
  W0708 12:08:37.911928      20 warnings.go:70] unknown field "gamma"
  Jul  8 12:08:37.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8271" for this suite. @ 07/08/23 12:08:37.949
• [2.618 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 07/08/23 12:08:37.957
  Jul  8 12:08:37.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename statefulset @ 07/08/23 12:08:37.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:08:37.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:08:37.978
  STEP: Creating service test in namespace statefulset-3687 @ 07/08/23 12:08:37.981
  Jul  8 12:08:37.999: INFO: Found 0 stateful pods, waiting for 1
  E0708 12:08:38.455637      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:39.455733      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:40.455892      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:41.456038      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:42.457116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:43.457901      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:44.458101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:45.458165      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:46.458673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:47.458790      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:08:48.004: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 07/08/23 12:08:48.012
  W0708 12:08:48.021548      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul  8 12:08:48.029: INFO: Found 1 stateful pods, waiting for 2
  E0708 12:08:48.458941      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:49.459136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:50.459216      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:51.459438      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:52.459530      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:53.459595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:54.459693      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:55.459854      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:56.460043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:57.461120      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:08:58.034: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul  8 12:08:58.034: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 07/08/23 12:08:58.042
  STEP: Delete all of the StatefulSets @ 07/08/23 12:08:58.046
  STEP: Verify that StatefulSets have been deleted @ 07/08/23 12:08:58.055
  Jul  8 12:08:58.059: INFO: Deleting all statefulset in ns statefulset-3687
  Jul  8 12:08:58.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3687" for this suite. @ 07/08/23 12:08:58.081
• [20.135 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 07/08/23 12:08:58.092
  Jul  8 12:08:58.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename field-validation @ 07/08/23 12:08:58.093
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:08:58.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:08:58.114
  Jul  8 12:08:58.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 12:08:58.461838      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:08:59.462644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:00.462766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0708 12:09:00.664150      20 warnings.go:70] unknown field "alpha"
  W0708 12:09:00.664254      20 warnings.go:70] unknown field "beta"
  W0708 12:09:00.664285      20 warnings.go:70] unknown field "delta"
  W0708 12:09:00.664314      20 warnings.go:70] unknown field "epsilon"
  W0708 12:09:00.664343      20 warnings.go:70] unknown field "gamma"
  Jul  8 12:09:00.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6413" for this suite. @ 07/08/23 12:09:00.705
• [2.621 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 07/08/23 12:09:00.714
  Jul  8 12:09:00.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl @ 07/08/23 12:09:00.715
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:09:00.729
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:09:00.733
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/08/23 12:09:00.737
  Jul  8 12:09:00.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6457 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Jul  8 12:09:00.878: INFO: stderr: ""
  Jul  8 12:09:00.878: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 07/08/23 12:09:00.878
  Jul  8 12:09:00.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6457 delete pods e2e-test-httpd-pod'
  E0708 12:09:01.463708      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:02.463800      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:09:03.223: INFO: stderr: ""
  Jul  8 12:09:03.223: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul  8 12:09:03.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6457" for this suite. @ 07/08/23 12:09:03.228
• [2.521 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 07/08/23 12:09:03.235
  Jul  8 12:09:03.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename resourcequota @ 07/08/23 12:09:03.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:09:03.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:09:03.26
  STEP: Creating a ResourceQuota @ 07/08/23 12:09:03.266
  STEP: Getting a ResourceQuota @ 07/08/23 12:09:03.273
  STEP: Listing all ResourceQuotas with LabelSelector @ 07/08/23 12:09:03.277
  STEP: Patching the ResourceQuota @ 07/08/23 12:09:03.281
  STEP: Deleting a Collection of ResourceQuotas @ 07/08/23 12:09:03.289
  STEP: Verifying the deleted ResourceQuota @ 07/08/23 12:09:03.299
  Jul  8 12:09:03.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5189" for this suite. @ 07/08/23 12:09:03.308
• [0.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 07/08/23 12:09:03.317
  Jul  8 12:09:03.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir @ 07/08/23 12:09:03.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:09:03.342
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:09:03.346
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 07/08/23 12:09:03.35
  E0708 12:09:03.464551      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:04.465108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:05.465140      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:06.465226      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:09:07.376
  Jul  8 12:09:07.380: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-5921d6fc-5ea8-4eea-9313-9f007fa2a53d container test-container: <nil>
  STEP: delete the pod @ 07/08/23 12:09:07.393
  Jul  8 12:09:07.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1333" for this suite. @ 07/08/23 12:09:07.417
• [4.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 07/08/23 12:09:07.427
  Jul  8 12:09:07.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename downward-api @ 07/08/23 12:09:07.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:09:07.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:09:07.449
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 12:09:07.454
  E0708 12:09:07.465366      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:08.466254      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:09.467304      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:09:09.475
  Jul  8 12:09:09.480: INFO: Trying to get logs from node ip-172-31-93-234 pod downwardapi-volume-68c222e7-9738-4575-a7c8-2a2a7a44468e container client-container: <nil>
  STEP: delete the pod @ 07/08/23 12:09:09.487
  Jul  8 12:09:09.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7433" for this suite. @ 07/08/23 12:09:09.514
• [2.094 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 07/08/23 12:09:09.521
  Jul  8 12:09:09.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename cronjob @ 07/08/23 12:09:09.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:09:09.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:09:09.541
  STEP: Creating a ReplaceConcurrent cronjob @ 07/08/23 12:09:09.544
  STEP: Ensuring a job is scheduled @ 07/08/23 12:09:09.551
  E0708 12:09:10.467632      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:11.467824      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:12.468049      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:13.468150      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:14.469055      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:15.469235      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:16.469472      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:17.469598      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:18.469669      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:19.469832      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:20.469942      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:21.470256      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:22.471253      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:23.471420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:24.471514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:25.471688      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:26.472579      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:27.472675      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:28.472751      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:29.473099      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:30.473370      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:31.473482      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:32.473558      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:33.473703      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:34.473772      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:35.473956      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:36.474425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:37.474494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:38.474603      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:39.474686      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:40.474757      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:41.474872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:42.474958      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:43.475050      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:44.475527      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:45.475733      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:46.476254      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:47.476330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:48.477048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:49.477199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:50.477595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:51.477872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:52.478042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:53.479023      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:54.479180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:55.479327      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:56.480272      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:57.481091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:58.481327      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:09:59.481711      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:00.482033      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:01.482136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 07/08/23 12:10:01.556
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 07/08/23 12:10:01.56
  STEP: Ensuring the job is replaced with a new one @ 07/08/23 12:10:01.564
  E0708 12:10:02.482239      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:03.482419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:04.482502      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:05.482684      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:06.483582      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:07.483759      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:08.484205      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:09.485101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:10.485192      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:11.485474      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:12.485564      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:13.485718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:14.485811      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:15.485889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:16.486434      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:17.486594      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:18.486673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:19.487115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:20.487904      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:21.488038      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:22.488119      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:23.488223      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:24.489097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:25.489184      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:26.489351      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:27.489625      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:28.490177      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:29.490345      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:30.490432      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:31.491205      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:32.491292      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:33.492193      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:34.492280      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:35.492452      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:36.493108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:37.493129      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:38.493207      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:39.493360      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:40.493458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:41.493481      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:42.493560      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:43.493733      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:44.493836      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:45.494010      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:46.494094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:47.494255      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:48.494403      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:49.494494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:50.494592      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:51.494995      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:52.496043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:53.496107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:54.496212      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:55.496301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:56.496425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:57.497432      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:58.497669      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:10:59.498641      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:00.498864      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:01.499862      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 07/08/23 12:11:01.569
  Jul  8 12:11:01.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-534" for this suite. @ 07/08/23 12:11:01.58
• [112.069 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 07/08/23 12:11:01.591
  Jul  8 12:11:01.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename replication-controller @ 07/08/23 12:11:01.591
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:11:01.609
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:11:01.613
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 07/08/23 12:11:01.616
  E0708 12:11:02.500037      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:03.500654      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replication controller with a matching selector is created @ 07/08/23 12:11:03.637
  STEP: Then the orphan pod is adopted @ 07/08/23 12:11:03.643
  E0708 12:11:04.500742      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:11:04.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4118" for this suite. @ 07/08/23 12:11:04.657
• [3.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 07/08/23 12:11:04.665
  Jul  8 12:11:04.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:11:04.666
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:11:04.682
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:11:04.685
  STEP: Creating configMap with name projected-configmap-test-volume-7077b6cc-7842-4d19-857a-3ebed686abab @ 07/08/23 12:11:04.688
  STEP: Creating a pod to test consume configMaps @ 07/08/23 12:11:04.693
  E0708 12:11:05.500990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:06.501283      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:07.501368      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:08.501549      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:11:08.716
  Jul  8 12:11:08.719: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-projected-configmaps-19c629c4-a6b5-4586-8207-bcaaad1e21e3 container agnhost-container: <nil>
  STEP: delete the pod @ 07/08/23 12:11:08.739
  Jul  8 12:11:08.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9397" for this suite. @ 07/08/23 12:11:08.76
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 07/08/23 12:11:08.767
  Jul  8 12:11:08.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl @ 07/08/23 12:11:08.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:11:08.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:11:08.787
  STEP: creating a replication controller @ 07/08/23 12:11:08.79
  Jul  8 12:11:08.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7673 create -f -'
  Jul  8 12:11:08.991: INFO: stderr: ""
  Jul  8 12:11:08.991: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/08/23 12:11:08.991
  Jul  8 12:11:08.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7673 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul  8 12:11:09.043: INFO: stderr: ""
  Jul  8 12:11:09.043: INFO: stdout: "update-demo-nautilus-r4nhw update-demo-nautilus-zf4cm "
  Jul  8 12:11:09.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7673 get pods update-demo-nautilus-r4nhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  8 12:11:09.090: INFO: stderr: ""
  Jul  8 12:11:09.090: INFO: stdout: ""
  Jul  8 12:11:09.090: INFO: update-demo-nautilus-r4nhw is created but not running
  E0708 12:11:09.502338      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:10.502417      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:11.502500      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:12.502542      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:13.502738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:11:14.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7673 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul  8 12:11:14.138: INFO: stderr: ""
  Jul  8 12:11:14.138: INFO: stdout: "update-demo-nautilus-r4nhw update-demo-nautilus-zf4cm "
  Jul  8 12:11:14.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7673 get pods update-demo-nautilus-r4nhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  8 12:11:14.183: INFO: stderr: ""
  Jul  8 12:11:14.183: INFO: stdout: "true"
  Jul  8 12:11:14.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7673 get pods update-demo-nautilus-r4nhw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul  8 12:11:14.230: INFO: stderr: ""
  Jul  8 12:11:14.230: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul  8 12:11:14.230: INFO: validating pod update-demo-nautilus-r4nhw
  Jul  8 12:11:14.237: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul  8 12:11:14.237: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul  8 12:11:14.237: INFO: update-demo-nautilus-r4nhw is verified up and running
  Jul  8 12:11:14.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7673 get pods update-demo-nautilus-zf4cm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  8 12:11:14.284: INFO: stderr: ""
  Jul  8 12:11:14.284: INFO: stdout: "true"
  Jul  8 12:11:14.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7673 get pods update-demo-nautilus-zf4cm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul  8 12:11:14.331: INFO: stderr: ""
  Jul  8 12:11:14.331: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul  8 12:11:14.331: INFO: validating pod update-demo-nautilus-zf4cm
  Jul  8 12:11:14.337: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul  8 12:11:14.337: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul  8 12:11:14.337: INFO: update-demo-nautilus-zf4cm is verified up and running
  STEP: using delete to clean up resources @ 07/08/23 12:11:14.337
  Jul  8 12:11:14.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7673 delete --grace-period=0 --force -f -'
  Jul  8 12:11:14.387: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  8 12:11:14.387: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jul  8 12:11:14.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7673 get rc,svc -l name=update-demo --no-headers'
  Jul  8 12:11:14.438: INFO: stderr: "No resources found in kubectl-7673 namespace.\n"
  Jul  8 12:11:14.439: INFO: stdout: ""
  Jul  8 12:11:14.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7673 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul  8 12:11:14.488: INFO: stderr: ""
  Jul  8 12:11:14.488: INFO: stdout: ""
  Jul  8 12:11:14.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7673" for this suite. @ 07/08/23 12:11:14.492
• [5.732 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 07/08/23 12:11:14.499
  Jul  8 12:11:14.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir @ 07/08/23 12:11:14.5
  E0708 12:11:14.503371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:11:14.515
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:11:14.518
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 07/08/23 12:11:14.524
  E0708 12:11:15.503515      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:16.504490      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:17.504663      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:18.505097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:11:18.546
  Jul  8 12:11:18.550: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-3008e78a-bf0d-48e5-a411-c20473890f16 container test-container: <nil>
  STEP: delete the pod @ 07/08/23 12:11:18.558
  Jul  8 12:11:18.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1225" for this suite. @ 07/08/23 12:11:18.58
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 07/08/23 12:11:18.589
  Jul  8 12:11:18.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename security-context-test @ 07/08/23 12:11:18.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:11:18.607
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:11:18.61
  E0708 12:11:19.505600      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:20.505782      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:21.506286      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:22.506395      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:11:22.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-194" for this suite. @ 07/08/23 12:11:22.641
• [4.061 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 07/08/23 12:11:22.65
  Jul  8 12:11:22.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename downward-api @ 07/08/23 12:11:22.651
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:11:22.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:11:22.67
  STEP: Creating the pod @ 07/08/23 12:11:22.674
  E0708 12:11:23.506482      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:24.506672      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:11:25.216: INFO: Successfully updated pod "annotationupdate7bc2019c-9a23-454d-89e6-589478f465ee"
  E0708 12:11:25.507658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:26.508018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:27.508046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:28.508138      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:11:29.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2089" for this suite. @ 07/08/23 12:11:29.246
• [6.602 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 07/08/23 12:11:29.253
  Jul  8 12:11:29.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:11:29.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:11:29.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:11:29.276
  STEP: Creating secret with name s-test-opt-del-d377a2f1-e9d0-4183-bfd9-e10a1e7c8655 @ 07/08/23 12:11:29.284
  STEP: Creating secret with name s-test-opt-upd-88a10c32-27d2-40c7-bbcb-4c2485fa81c4 @ 07/08/23 12:11:29.288
  STEP: Creating the pod @ 07/08/23 12:11:29.294
  E0708 12:11:29.508691      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:30.508805      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-d377a2f1-e9d0-4183-bfd9-e10a1e7c8655 @ 07/08/23 12:11:31.348
  STEP: Updating secret s-test-opt-upd-88a10c32-27d2-40c7-bbcb-4c2485fa81c4 @ 07/08/23 12:11:31.355
  STEP: Creating secret with name s-test-opt-create-80639aa6-384e-408c-b282-ec38a82c352d @ 07/08/23 12:11:31.361
  STEP: waiting to observe update in volume @ 07/08/23 12:11:31.368
  E0708 12:11:31.509423      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:32.509563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:33.509934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:34.510097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:35.510906      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:36.511036      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:37.511902      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:38.512048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:39.512089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:40.513104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:41.513759      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:42.513903      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:43.514113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:44.514291      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:45.514604      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:46.515363      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:47.515668      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:48.516533      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:49.517445      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:50.517553      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:51.518289      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:52.518746      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:53.518920      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:54.519225      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:55.519864      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:56.520587      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:57.521396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:58.521498      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:11:59.521828      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:00.522581      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:01.523200      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:02.523566      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:03.524471      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:04.524584      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:05.524846      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:06.525827      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:07.526313      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:08.526391      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:09.526891      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:10.526976      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:11.527311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:12.528360      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:13.529205      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:14.529444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:15.529634      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:16.530339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:17.530519      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:18.530611      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:19.530778      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:20.530844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:21.531301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:22.532189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:23.532261      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:24.532360      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:25.533105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:26.533246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:27.533348      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:28.533412      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:29.534126      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:30.534784      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:31.535176      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:32.536211      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:33.536314      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:34.537089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:35.537182      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:36.537303      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:37.537377      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:38.538017      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:39.538861      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:40.539456      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:41.540449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:42.540552      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:43.541556      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:44.541638      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:45.542333      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:46.543292      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:47.543383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:48.543754      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:49.543845      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:50.544024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:51.545091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:52.545177      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:53.545269      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:54.545300      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:55.545393      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:56.545579      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:57.545668      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:58.546647      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:12:59.546976      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:12:59.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-604" for this suite. @ 07/08/23 12:12:59.811
• [90.564 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 07/08/23 12:12:59.818
  Jul  8 12:12:59.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename configmap @ 07/08/23 12:12:59.818
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:12:59.839
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:12:59.843
  STEP: Creating configMap with name configmap-test-upd-ca6927a3-b313-440f-b52d-47dc18c96d3d @ 07/08/23 12:12:59.85
  STEP: Creating the pod @ 07/08/23 12:12:59.856
  E0708 12:13:00.547311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:01.547543      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap configmap-test-upd-ca6927a3-b313-440f-b52d-47dc18c96d3d @ 07/08/23 12:13:01.895
  STEP: waiting to observe update in volume @ 07/08/23 12:13:01.901
  E0708 12:13:02.547655      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:03.547948      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:04.548035      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:05.549088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:13:05.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1654" for this suite. @ 07/08/23 12:13:05.93
• [6.119 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 07/08/23 12:13:05.937
  Jul  8 12:13:05.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename cronjob @ 07/08/23 12:13:05.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:13:05.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:13:05.962
  STEP: Creating a suspended cronjob @ 07/08/23 12:13:05.965
  STEP: Ensuring no jobs are scheduled @ 07/08/23 12:13:05.973
  E0708 12:13:06.549955      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:07.550151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:08.550784      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:09.551280      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:10.551394      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:11.551557      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:12.551938      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:13.552038      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:14.552289      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:15.552400      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:16.553101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:17.553191      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:18.553659      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:19.553847      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:20.553940      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:21.554278      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:22.554366      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:23.555138      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:24.555221      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:25.555316      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:26.556236      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:27.556334      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:28.556426      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:29.557102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:30.557187      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:31.557730      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:32.557877      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:33.557951      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:34.558686      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:35.558776      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:36.559006      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:37.559658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:38.560043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:39.561093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:40.561206      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:41.561275      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:42.561375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:43.561455      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:44.561532      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:45.561740      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:46.561807      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:47.562064      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:48.562285      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:49.562362      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:50.563389      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:51.563521      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:52.564335      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:53.565084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:54.565173      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:55.565343      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:56.566004      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:57.566086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:58.566904      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:13:59.566990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:00.568014      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:01.569090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:02.570119      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:03.570883      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:04.570988      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:05.571155      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:06.571506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:07.571753      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:08.572018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:09.573087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:10.573175      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:11.573500      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:12.573845      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:13.573945      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:14.574125      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:15.574291      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:16.574549      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:17.574773      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:18.575377      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:19.575534      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:20.575660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:21.576032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:22.577085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:23.578044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:24.578965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:25.579865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:26.580148      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:27.581087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:28.581343      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:29.581432      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:30.581799      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:31.581887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:32.581966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:33.582117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:34.582329      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:35.582420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:36.583380      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:37.583552      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:38.583639      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:39.583807      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:40.584022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:41.585083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:42.585403      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:43.585692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:44.585775      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:45.586598      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:46.587594      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:47.587673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:48.588021      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:49.588140      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:50.588207      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:51.589220      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:52.589306      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:53.589471      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:54.590330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:55.590424      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:56.590941      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:57.591032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:58.591647      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:14:59.591738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:00.592514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:01.592561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:02.592815      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:03.593087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:04.593181      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:05.593973      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:06.594476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:07.594672      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:08.594852      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:09.594936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:10.595239      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:11.595548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:12.596481      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:13.597087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:14.597173      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:15.597831      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:16.598200      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:17.598974      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:18.599278      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:19.600290      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:20.600373      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:21.600561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:22.600959      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:23.601125      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:24.602045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:25.602111      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:26.602468      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:27.603360      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:28.603406      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:29.603564      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:30.604035      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:31.604737      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:32.605769      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:33.606610      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:34.606694      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:35.606866      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:36.607478      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:37.608017      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:38.608937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:39.609328      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:40.609632      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:41.609718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:42.609915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:43.610074      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:44.610309      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:45.610541      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:46.610634      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:47.610794      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:48.611817      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:49.611904      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:50.612024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:51.612114      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:52.612674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:53.613245      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:54.613375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:55.613532      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:56.614335      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:57.614505      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:58.614589      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:15:59.615213      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:00.615296      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:01.615578      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:02.615616      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:03.615767      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:04.616024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:05.616106      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:06.617088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:07.617251      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:08.617345      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:09.617527      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:10.618063      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:11.618405      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:12.618548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:13.618635      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:14.618844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:15.619017      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:16.619045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:17.619194      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:18.619433      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:19.619697      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:20.620024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:21.621086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:22.621619      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:23.621872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:24.622069      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:25.622165      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:26.622570      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:27.622661      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:28.623204      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:29.623276      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:30.623455      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:31.623605      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:32.624370      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:33.625083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:34.625177      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:35.625345      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:36.625427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:37.625698      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:38.626058      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:39.626585      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:40.626673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:41.626854      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:42.626984      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:43.627078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:44.627167      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:45.627312      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:46.627966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:47.628033      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:48.628331      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:49.629088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:50.629182      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:51.629491      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:52.630527      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:53.630616      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:54.630713      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:55.630869      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:56.631938      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:57.632026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:58.633090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:16:59.633257      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:00.633518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:01.633917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:02.634430      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:03.634515      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:04.634730      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:05.635683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:06.636022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:07.637091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:08.637179      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:09.638028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:10.638118      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:11.638169      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:12.639150      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:13.639241      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:14.639881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:15.640034      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:16.641088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:17.641566      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:18.641635      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:19.641875      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:20.641966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:21.642052      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:22.642271      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:23.642443      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:24.642838      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:25.643001      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:26.643081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:27.643639      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:28.643720      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:29.643813      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:30.644024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:31.645080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:32.645621      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:33.645789      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:34.645997      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:35.646154      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:36.646245      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:37.646407      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:38.646893      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:39.647048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:40.647134      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:41.647231      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:42.648013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:43.648092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:44.648179      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:45.648384      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:46.649191      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:47.649229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:48.649592      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:49.649762      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:50.650324      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:51.650936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:52.651995      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:53.652032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:54.653087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:55.653238      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:56.653936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:57.654107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:58.654761      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:17:59.654855      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:00.655130      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:01.655445      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:02.656375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:03.657088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:04.657340      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:05.657429      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 07/08/23 12:18:05.982
  STEP: Removing cronjob @ 07/08/23 12:18:05.985
  Jul  8 12:18:05.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-968" for this suite. @ 07/08/23 12:18:05.996
• [300.067 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 07/08/23 12:18:06.004
  Jul  8 12:18:06.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename configmap @ 07/08/23 12:18:06.005
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:18:06.02
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:18:06.024
  STEP: Creating configMap with name configmap-test-volume-map-672a32cb-afea-4e06-8479-c8e09348a415 @ 07/08/23 12:18:06.027
  STEP: Creating a pod to test consume configMaps @ 07/08/23 12:18:06.127
  E0708 12:18:06.657907      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:07.658091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:08.658166      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:09.658336      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:18:10.15
  Jul  8 12:18:10.154: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-configmaps-8d72ea6a-6db7-46cf-9887-b65d655124ac container agnhost-container: <nil>
  STEP: delete the pod @ 07/08/23 12:18:10.172
  Jul  8 12:18:10.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8600" for this suite. @ 07/08/23 12:18:10.193
• [4.196 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 07/08/23 12:18:10.202
  Jul  8 12:18:10.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename secrets @ 07/08/23 12:18:10.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:18:10.218
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:18:10.221
  STEP: Creating projection with secret that has name secret-emptykey-test-21a320cb-1c62-4069-ade7-f73a46678161 @ 07/08/23 12:18:10.225
  Jul  8 12:18:10.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7112" for this suite. @ 07/08/23 12:18:10.231
• [0.037 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 07/08/23 12:18:10.24
  Jul  8 12:18:10.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename namespaces @ 07/08/23 12:18:10.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:18:10.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:18:10.26
  STEP: Read namespace status @ 07/08/23 12:18:10.264
  Jul  8 12:18:10.271: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 07/08/23 12:18:10.271
  Jul  8 12:18:10.276: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 07/08/23 12:18:10.276
  Jul  8 12:18:10.286: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Jul  8 12:18:10.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-48" for this suite. @ 07/08/23 12:18:10.29
• [0.057 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 07/08/23 12:18:10.298
  Jul  8 12:18:10.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:18:10.299
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:18:10.321
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:18:10.324
  STEP: Creating projection with secret that has name projected-secret-test-map-4ff66f66-1cb8-43bd-b2a7-616dfdf07232 @ 07/08/23 12:18:10.328
  STEP: Creating a pod to test consume secrets @ 07/08/23 12:18:10.333
  E0708 12:18:10.658383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:11.658653      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:12.658738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:13.658832      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:18:14.355
  Jul  8 12:18:14.359: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-projected-secrets-3de0de77-09d8-48d9-b9db-699f96b2d73d container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/08/23 12:18:14.367
  Jul  8 12:18:14.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7080" for this suite. @ 07/08/23 12:18:14.386
• [4.095 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 07/08/23 12:18:14.393
  Jul  8 12:18:14.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 12:18:14.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:18:14.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:18:14.413
  STEP: Setting up server cert @ 07/08/23 12:18:14.437
  E0708 12:18:14.659408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 12:18:14.874
  STEP: Deploying the webhook pod @ 07/08/23 12:18:14.884
  STEP: Wait for the deployment to be ready @ 07/08/23 12:18:14.897
  Jul  8 12:18:14.904: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0708 12:18:15.659585      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:16.659712      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 12:18:16.917
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 12:18:16.927
  E0708 12:18:17.660500      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:18:17.927: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul  8 12:18:17.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4326-crds.webhook.example.com via the AdmissionRegistration API @ 07/08/23 12:18:18.443
  STEP: Creating a custom resource while v1 is storage version @ 07/08/23 12:18:18.458
  E0708 12:18:18.660490      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:19.661106      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 07/08/23 12:18:20.497
  STEP: Patching the custom resource while v2 is storage version @ 07/08/23 12:18:20.513
  Jul  8 12:18:20.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0708 12:18:20.661660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-2357" for this suite. @ 07/08/23 12:18:21.123
  STEP: Destroying namespace "webhook-markers-5563" for this suite. @ 07/08/23 12:18:21.13
• [6.744 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 07/08/23 12:18:21.138
  Jul  8 12:18:21.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename gc @ 07/08/23 12:18:21.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:18:21.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:18:21.159
  STEP: create the rc1 @ 07/08/23 12:18:21.167
  STEP: create the rc2 @ 07/08/23 12:18:21.173
  E0708 12:18:21.662082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:22.662228      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:23.662807      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:24.662895      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:25.662990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:26.663217      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 07/08/23 12:18:27.183
  STEP: delete the rc simpletest-rc-to-be-deleted @ 07/08/23 12:18:27.662
  E0708 12:18:27.668093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for the rc to be deleted @ 07/08/23 12:18:27.674
  E0708 12:18:28.668935      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:29.669068      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:30.669097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:31.669419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:32.669524      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:18:32.689: INFO: 73 pods remaining
  Jul  8 12:18:32.689: INFO: 73 pods has nil DeletionTimestamp
  Jul  8 12:18:32.689: INFO: 
  E0708 12:18:33.670446      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:34.670672      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:35.670791      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:36.671743      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:37.671804      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/08/23 12:18:37.687
  W0708 12:18:37.692648      20 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul  8 12:18:37.692: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul  8 12:18:37.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-25ll8" in namespace "gc-2531"
  Jul  8 12:18:37.710: INFO: Deleting pod "simpletest-rc-to-be-deleted-2j9m2" in namespace "gc-2531"
  Jul  8 12:18:37.724: INFO: Deleting pod "simpletest-rc-to-be-deleted-2pkfh" in namespace "gc-2531"
  Jul  8 12:18:37.739: INFO: Deleting pod "simpletest-rc-to-be-deleted-2w67r" in namespace "gc-2531"
  Jul  8 12:18:37.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-46cfn" in namespace "gc-2531"
  Jul  8 12:18:37.769: INFO: Deleting pod "simpletest-rc-to-be-deleted-4c6tq" in namespace "gc-2531"
  Jul  8 12:18:37.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mrqc" in namespace "gc-2531"
  Jul  8 12:18:37.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-4nlgw" in namespace "gc-2531"
  Jul  8 12:18:37.809: INFO: Deleting pod "simpletest-rc-to-be-deleted-4p4sd" in namespace "gc-2531"
  Jul  8 12:18:37.823: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vjk8" in namespace "gc-2531"
  Jul  8 12:18:37.837: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vxnj" in namespace "gc-2531"
  Jul  8 12:18:37.850: INFO: Deleting pod "simpletest-rc-to-be-deleted-4whbj" in namespace "gc-2531"
  Jul  8 12:18:37.865: INFO: Deleting pod "simpletest-rc-to-be-deleted-59jld" in namespace "gc-2531"
  Jul  8 12:18:37.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cqch" in namespace "gc-2531"
  Jul  8 12:18:37.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-5f5jg" in namespace "gc-2531"
  Jul  8 12:18:37.910: INFO: Deleting pod "simpletest-rc-to-be-deleted-5prdp" in namespace "gc-2531"
  Jul  8 12:18:37.925: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qx5j" in namespace "gc-2531"
  Jul  8 12:18:37.939: INFO: Deleting pod "simpletest-rc-to-be-deleted-62x66" in namespace "gc-2531"
  Jul  8 12:18:37.954: INFO: Deleting pod "simpletest-rc-to-be-deleted-6x7sj" in namespace "gc-2531"
  Jul  8 12:18:37.964: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zz28" in namespace "gc-2531"
  Jul  8 12:18:37.977: INFO: Deleting pod "simpletest-rc-to-be-deleted-7lft4" in namespace "gc-2531"
  Jul  8 12:18:37.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-844vr" in namespace "gc-2531"
  Jul  8 12:18:38.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-88qlm" in namespace "gc-2531"
  Jul  8 12:18:38.029: INFO: Deleting pod "simpletest-rc-to-be-deleted-8l4xr" in namespace "gc-2531"
  Jul  8 12:18:38.043: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xkjm" in namespace "gc-2531"
  Jul  8 12:18:38.057: INFO: Deleting pod "simpletest-rc-to-be-deleted-98n7c" in namespace "gc-2531"
  Jul  8 12:18:38.081: INFO: Deleting pod "simpletest-rc-to-be-deleted-9g9tm" in namespace "gc-2531"
  Jul  8 12:18:38.097: INFO: Deleting pod "simpletest-rc-to-be-deleted-9p2td" in namespace "gc-2531"
  Jul  8 12:18:38.112: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcqf7" in namespace "gc-2531"
  Jul  8 12:18:38.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjt7b" in namespace "gc-2531"
  Jul  8 12:18:38.144: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqnht" in namespace "gc-2531"
  Jul  8 12:18:38.214: INFO: Deleting pod "simpletest-rc-to-be-deleted-bwj9s" in namespace "gc-2531"
  Jul  8 12:18:38.238: INFO: Deleting pod "simpletest-rc-to-be-deleted-bzsmr" in namespace "gc-2531"
  Jul  8 12:18:38.251: INFO: Deleting pod "simpletest-rc-to-be-deleted-c94sm" in namespace "gc-2531"
  Jul  8 12:18:38.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9qzp" in namespace "gc-2531"
  Jul  8 12:18:38.279: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfzjv" in namespace "gc-2531"
  Jul  8 12:18:38.295: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctn5g" in namespace "gc-2531"
  Jul  8 12:18:38.310: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2vvs" in namespace "gc-2531"
  Jul  8 12:18:38.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2zff" in namespace "gc-2531"
  Jul  8 12:18:38.469: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4t6n" in namespace "gc-2531"
  Jul  8 12:18:38.483: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5wjs" in namespace "gc-2531"
  Jul  8 12:18:38.495: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgqgb" in namespace "gc-2531"
  Jul  8 12:18:38.509: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhs66" in namespace "gc-2531"
  Jul  8 12:18:38.531: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlzlj" in namespace "gc-2531"
  Jul  8 12:18:38.550: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgcnc" in namespace "gc-2531"
  Jul  8 12:18:38.567: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgdbg" in namespace "gc-2531"
  Jul  8 12:18:38.582: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjznk" in namespace "gc-2531"
  Jul  8 12:18:38.594: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkw4c" in namespace "gc-2531"
  Jul  8 12:18:38.609: INFO: Deleting pod "simpletest-rc-to-be-deleted-fpc8h" in namespace "gc-2531"
  Jul  8 12:18:38.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqzd5" in namespace "gc-2531"
  Jul  8 12:18:38.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2531" for this suite. @ 07/08/23 12:18:38.643
• [17.513 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 07/08/23 12:18:38.651
  Jul  8 12:18:38.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 12:18:38.652
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:18:38.668
  E0708 12:18:38.671800      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:18:38.671
  STEP: Setting up server cert @ 07/08/23 12:18:38.702
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 12:18:39.311
  STEP: Deploying the webhook pod @ 07/08/23 12:18:39.317
  STEP: Wait for the deployment to be ready @ 07/08/23 12:18:39.33
  Jul  8 12:18:39.365: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 12, 18, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 18, 39, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-7497495989\""}}, CollisionCount:(*int32)(nil)}
  E0708 12:18:39.672108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:40.672223      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 12:18:41.369
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 12:18:41.381
  E0708 12:18:41.673063      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:18:42.381: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 07/08/23 12:18:42.447
  STEP: Creating a configMap that should be mutated @ 07/08/23 12:18:42.46
  STEP: Deleting the collection of validation webhooks @ 07/08/23 12:18:42.488
  STEP: Creating a configMap that should not be mutated @ 07/08/23 12:18:42.54
  Jul  8 12:18:42.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1479" for this suite. @ 07/08/23 12:18:42.594
  STEP: Destroying namespace "webhook-markers-2725" for this suite. @ 07/08/23 12:18:42.603
• [3.959 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 07/08/23 12:18:42.61
  Jul  8 12:18:42.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename replicaset @ 07/08/23 12:18:42.61
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:18:42.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:18:42.629
  Jul  8 12:18:42.645: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0708 12:18:42.673907      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:43.674054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:44.674247      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:45.674298      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:46.674572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:18:47.652: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/08/23 12:18:47.652
  STEP: Scaling up "test-rs" replicaset  @ 07/08/23 12:18:47.652
  Jul  8 12:18:47.661: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 07/08/23 12:18:47.661
  W0708 12:18:47.669503      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul  8 12:18:47.671: INFO: observed ReplicaSet test-rs in namespace replicaset-8227 with ReadyReplicas 1, AvailableReplicas 1
  E0708 12:18:47.675313      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:18:47.688: INFO: observed ReplicaSet test-rs in namespace replicaset-8227 with ReadyReplicas 1, AvailableReplicas 1
  Jul  8 12:18:47.702: INFO: observed ReplicaSet test-rs in namespace replicaset-8227 with ReadyReplicas 1, AvailableReplicas 1
  Jul  8 12:18:47.709: INFO: observed ReplicaSet test-rs in namespace replicaset-8227 with ReadyReplicas 1, AvailableReplicas 1
  E0708 12:18:48.676343      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:18:48.746: INFO: observed ReplicaSet test-rs in namespace replicaset-8227 with ReadyReplicas 2, AvailableReplicas 2
  E0708 12:18:49.677102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:50.677195      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:51.677420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:18:52.413: INFO: observed Replicaset test-rs in namespace replicaset-8227 with ReadyReplicas 3 found true
  Jul  8 12:18:52.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8227" for this suite. @ 07/08/23 12:18:52.418
• [9.816 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 07/08/23 12:18:52.425
  Jul  8 12:18:52.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:18:52.426
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:18:52.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:18:52.446
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 12:18:52.449
  E0708 12:18:52.677786      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:53.677910      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:54.678711      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:55.678921      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:18:56.474
  Jul  8 12:18:56.478: INFO: Trying to get logs from node ip-172-31-93-234 pod downwardapi-volume-19c3efb6-3021-4244-8ffa-ee8bfc1f3f3c container client-container: <nil>
  STEP: delete the pod @ 07/08/23 12:18:56.486
  Jul  8 12:18:56.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4492" for this suite. @ 07/08/23 12:18:56.507
• [4.088 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 07/08/23 12:18:56.514
  Jul  8 12:18:56.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename configmap @ 07/08/23 12:18:56.515
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:18:56.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:18:56.533
  STEP: Creating configMap with name configmap-test-volume-map-4401d891-1a1c-42da-8d3e-8f01cc5eb8db @ 07/08/23 12:18:56.536
  STEP: Creating a pod to test consume configMaps @ 07/08/23 12:18:56.541
  E0708 12:18:56.679888      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:57.680120      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:58.680325      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:18:59.681105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:19:00.565
  Jul  8 12:19:00.569: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-configmaps-229d58d7-5da5-4e0e-99f0-2b70195ee238 container agnhost-container: <nil>
  STEP: delete the pod @ 07/08/23 12:19:00.576
  Jul  8 12:19:00.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8372" for this suite. @ 07/08/23 12:19:00.599
• [4.093 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 07/08/23 12:19:00.608
  Jul  8 12:19:00.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename job @ 07/08/23 12:19:00.608
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:19:00.623
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:19:00.626
  STEP: Creating Indexed job @ 07/08/23 12:19:00.632
  STEP: Ensuring job reaches completions @ 07/08/23 12:19:00.638
  E0708 12:19:00.681553      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:01.681665      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:02.682115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:03.682562      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:04.682993      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:05.683136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:06.684022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:07.685128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 07/08/23 12:19:08.643
  Jul  8 12:19:08.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7858" for this suite. @ 07/08/23 12:19:08.651
• [8.051 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 07/08/23 12:19:08.659
  Jul  8 12:19:08.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:19:08.66
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:19:08.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:19:08.679
  STEP: Creating configMap with name projected-configmap-test-volume-c0c523b6-cd8d-40b4-9aec-d722c5ccd078 @ 07/08/23 12:19:08.683
  E0708 12:19:08.685446      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a pod to test consume configMaps @ 07/08/23 12:19:08.687
  E0708 12:19:09.685593      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:10.685679      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:11.686323      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:12.686410      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:19:12.711
  Jul  8 12:19:12.715: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-projected-configmaps-8011caf3-3703-4fc2-884c-6a07de1a74a0 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 07/08/23 12:19:12.722
  Jul  8 12:19:12.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5817" for this suite. @ 07/08/23 12:19:12.867
• [4.218 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 07/08/23 12:19:12.881
  Jul  8 12:19:12.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename resourcequota @ 07/08/23 12:19:12.882
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:19:12.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:19:12.904
  STEP: Creating a ResourceQuota with terminating scope @ 07/08/23 12:19:12.908
  STEP: Ensuring ResourceQuota status is calculated @ 07/08/23 12:19:12.912
  E0708 12:19:13.686698      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:14.686901      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 07/08/23 12:19:14.917
  STEP: Ensuring ResourceQuota status is calculated @ 07/08/23 12:19:14.922
  E0708 12:19:15.686973      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:16.687210      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 07/08/23 12:19:16.927
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 07/08/23 12:19:16.943
  E0708 12:19:17.687687      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:18.687777      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 07/08/23 12:19:18.949
  E0708 12:19:19.688036      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:20.688123      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 07/08/23 12:19:20.954
  STEP: Ensuring resource quota status released the pod usage @ 07/08/23 12:19:20.967
  E0708 12:19:21.689115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:22.689314      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 07/08/23 12:19:22.972
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 07/08/23 12:19:22.984
  E0708 12:19:23.689410      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:24.689518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 07/08/23 12:19:24.99
  E0708 12:19:25.690200      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:26.690390      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 07/08/23 12:19:26.995
  STEP: Ensuring resource quota status released the pod usage @ 07/08/23 12:19:27.01
  E0708 12:19:27.690623      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:28.690667      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:19:29.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-64" for this suite. @ 07/08/23 12:19:29.021
• [16.149 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 07/08/23 12:19:29.03
  Jul  8 12:19:29.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl @ 07/08/23 12:19:29.03
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:19:29.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:19:29.053
  STEP: creating Agnhost RC @ 07/08/23 12:19:29.058
  Jul  8 12:19:29.058: INFO: namespace kubectl-5013
  Jul  8 12:19:29.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-5013 create -f -'
  E0708 12:19:29.694494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:19:29.745: INFO: stderr: ""
  Jul  8 12:19:29.745: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/08/23 12:19:29.745
  E0708 12:19:30.694587      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:19:30.750: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  8 12:19:30.750: INFO: Found 0 / 1
  E0708 12:19:31.695236      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:19:31.751: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  8 12:19:31.751: INFO: Found 1 / 1
  Jul  8 12:19:31.751: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jul  8 12:19:31.755: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  8 12:19:31.755: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul  8 12:19:31.756: INFO: wait on agnhost-primary startup in kubectl-5013 
  Jul  8 12:19:31.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-5013 logs agnhost-primary-vbtv2 agnhost-primary'
  Jul  8 12:19:31.846: INFO: stderr: ""
  Jul  8 12:19:31.846: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 07/08/23 12:19:31.846
  Jul  8 12:19:31.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-5013 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Jul  8 12:19:31.925: INFO: stderr: ""
  Jul  8 12:19:31.925: INFO: stdout: "service/rm2 exposed\n"
  Jul  8 12:19:31.929: INFO: Service rm2 in namespace kubectl-5013 found.
  E0708 12:19:32.695330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:33.695416      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 07/08/23 12:19:33.937
  Jul  8 12:19:33.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-5013 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Jul  8 12:19:33.999: INFO: stderr: ""
  Jul  8 12:19:33.999: INFO: stdout: "service/rm3 exposed\n"
  Jul  8 12:19:34.005: INFO: Service rm3 in namespace kubectl-5013 found.
  E0708 12:19:34.695505      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:35.695682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:19:36.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5013" for this suite. @ 07/08/23 12:19:36.018
• [6.995 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 07/08/23 12:19:36.025
  Jul  8 12:19:36.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename limitrange @ 07/08/23 12:19:36.026
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:19:36.041
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:19:36.044
  STEP: Creating a LimitRange @ 07/08/23 12:19:36.047
  STEP: Setting up watch @ 07/08/23 12:19:36.047
  STEP: Submitting a LimitRange @ 07/08/23 12:19:36.151
  STEP: Verifying LimitRange creation was observed @ 07/08/23 12:19:36.157
  STEP: Fetching the LimitRange to ensure it has proper values @ 07/08/23 12:19:36.157
  Jul  8 12:19:36.161: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jul  8 12:19:36.161: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 07/08/23 12:19:36.161
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 07/08/23 12:19:36.167
  Jul  8 12:19:36.171: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jul  8 12:19:36.171: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 07/08/23 12:19:36.171
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 07/08/23 12:19:36.179
  Jul  8 12:19:36.183: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Jul  8 12:19:36.183: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 07/08/23 12:19:36.183
  STEP: Failing to create a Pod with more than max resources @ 07/08/23 12:19:36.185
  STEP: Updating a LimitRange @ 07/08/23 12:19:36.187
  STEP: Verifying LimitRange updating is effective @ 07/08/23 12:19:36.191
  E0708 12:19:36.696024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:37.696134      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 07/08/23 12:19:38.196
  STEP: Failing to create a Pod with more than max resources @ 07/08/23 12:19:38.203
  STEP: Deleting a LimitRange @ 07/08/23 12:19:38.205
  STEP: Verifying the LimitRange was deleted @ 07/08/23 12:19:38.214
  E0708 12:19:38.696872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:39.697107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:40.697941      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:41.698284      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:42.698376      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:19:43.219: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 07/08/23 12:19:43.219
  Jul  8 12:19:43.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-5611" for this suite. @ 07/08/23 12:19:43.232
• [7.217 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 07/08/23 12:19:43.243
  Jul  8 12:19:43.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename controllerrevisions @ 07/08/23 12:19:43.244
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:19:43.265
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:19:43.268
  STEP: Creating DaemonSet "e2e-kf4ml-daemon-set" @ 07/08/23 12:19:43.291
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/08/23 12:19:43.3
  Jul  8 12:19:43.304: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:19:43.304: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:19:43.308: INFO: Number of nodes with available pods controlled by daemonset e2e-kf4ml-daemon-set: 0
  Jul  8 12:19:43.308: INFO: Node ip-172-31-12-67 is running 0 daemon pod, expected 1
  E0708 12:19:43.698477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:19:44.313: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:19:44.313: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:19:44.318: INFO: Number of nodes with available pods controlled by daemonset e2e-kf4ml-daemon-set: 1
  Jul  8 12:19:44.318: INFO: Node ip-172-31-12-67 is running 0 daemon pod, expected 1
  E0708 12:19:44.699334      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:19:45.313: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:19:45.313: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:19:45.317: INFO: Number of nodes with available pods controlled by daemonset e2e-kf4ml-daemon-set: 3
  Jul  8 12:19:45.317: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-kf4ml-daemon-set
  STEP: Confirm DaemonSet "e2e-kf4ml-daemon-set" successfully created with "daemonset-name=e2e-kf4ml-daemon-set" label @ 07/08/23 12:19:45.321
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-kf4ml-daemon-set" @ 07/08/23 12:19:45.328
  Jul  8 12:19:45.332: INFO: Located ControllerRevision: "e2e-kf4ml-daemon-set-d955996c9"
  STEP: Patching ControllerRevision "e2e-kf4ml-daemon-set-d955996c9" @ 07/08/23 12:19:45.336
  Jul  8 12:19:45.343: INFO: e2e-kf4ml-daemon-set-d955996c9 has been patched
  STEP: Create a new ControllerRevision @ 07/08/23 12:19:45.343
  Jul  8 12:19:45.349: INFO: Created ControllerRevision: e2e-kf4ml-daemon-set-6b64586b79
  STEP: Confirm that there are two ControllerRevisions @ 07/08/23 12:19:45.349
  Jul  8 12:19:45.349: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul  8 12:19:45.352: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-kf4ml-daemon-set-d955996c9" @ 07/08/23 12:19:45.352
  STEP: Confirm that there is only one ControllerRevision @ 07/08/23 12:19:45.359
  Jul  8 12:19:45.359: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul  8 12:19:45.362: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-kf4ml-daemon-set-6b64586b79" @ 07/08/23 12:19:45.365
  Jul  8 12:19:45.374: INFO: e2e-kf4ml-daemon-set-6b64586b79 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 07/08/23 12:19:45.374
  W0708 12:19:45.382579      20 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 07/08/23 12:19:45.382
  Jul  8 12:19:45.382: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0708 12:19:45.700151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:19:46.390: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul  8 12:19:46.394: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-kf4ml-daemon-set-6b64586b79=updated" @ 07/08/23 12:19:46.394
  STEP: Confirm that there is only one ControllerRevision @ 07/08/23 12:19:46.403
  Jul  8 12:19:46.403: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul  8 12:19:46.406: INFO: Found 1 ControllerRevisions
  Jul  8 12:19:46.410: INFO: ControllerRevision "e2e-kf4ml-daemon-set-67fdb7cf74" has revision 3
  STEP: Deleting DaemonSet "e2e-kf4ml-daemon-set" @ 07/08/23 12:19:46.414
  STEP: deleting DaemonSet.extensions e2e-kf4ml-daemon-set in namespace controllerrevisions-9909, will wait for the garbage collector to delete the pods @ 07/08/23 12:19:46.414
  Jul  8 12:19:46.474: INFO: Deleting DaemonSet.extensions e2e-kf4ml-daemon-set took: 6.2912ms
  Jul  8 12:19:46.575: INFO: Terminating DaemonSet.extensions e2e-kf4ml-daemon-set pods took: 100.941961ms
  E0708 12:19:46.700853      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:19:47.380: INFO: Number of nodes with available pods controlled by daemonset e2e-kf4ml-daemon-set: 0
  Jul  8 12:19:47.380: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-kf4ml-daemon-set
  Jul  8 12:19:47.384: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10270"},"items":null}

  Jul  8 12:19:47.388: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10270"},"items":null}

  Jul  8 12:19:47.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-9909" for this suite. @ 07/08/23 12:19:47.407
• [4.171 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 07/08/23 12:19:47.414
  Jul  8 12:19:47.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename dns @ 07/08/23 12:19:47.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:19:47.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:19:47.437
  STEP: Creating a test headless service @ 07/08/23 12:19:47.44
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1883.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1883.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1883.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1883.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1883.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1883.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1883.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1883.svc.cluster.local;sleep 1; done
   @ 07/08/23 12:19:47.445
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1883.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1883.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1883.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1883.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1883.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1883.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1883.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1883.svc.cluster.local;sleep 1; done
   @ 07/08/23 12:19:47.445
  STEP: creating a pod to probe DNS @ 07/08/23 12:19:47.445
  STEP: submitting the pod to kubernetes @ 07/08/23 12:19:47.445
  E0708 12:19:47.701305      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:48.701427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:49.702135      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:50.702316      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:51.702379      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:52.702476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/08/23 12:19:53.477
  STEP: looking for the results for each expected name from probers @ 07/08/23 12:19:53.481
  Jul  8 12:19:53.503: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1883.svc.cluster.local from pod dns-1883/dns-test-e9ae2585-b2f5-4806-bb95-98fafc29c937: the server could not find the requested resource (get pods dns-test-e9ae2585-b2f5-4806-bb95-98fafc29c937)
  Jul  8 12:19:53.508: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1883.svc.cluster.local from pod dns-1883/dns-test-e9ae2585-b2f5-4806-bb95-98fafc29c937: the server could not find the requested resource (get pods dns-test-e9ae2585-b2f5-4806-bb95-98fafc29c937)
  Jul  8 12:19:53.512: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1883.svc.cluster.local from pod dns-1883/dns-test-e9ae2585-b2f5-4806-bb95-98fafc29c937: the server could not find the requested resource (get pods dns-test-e9ae2585-b2f5-4806-bb95-98fafc29c937)
  Jul  8 12:19:53.516: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1883.svc.cluster.local from pod dns-1883/dns-test-e9ae2585-b2f5-4806-bb95-98fafc29c937: the server could not find the requested resource (get pods dns-test-e9ae2585-b2f5-4806-bb95-98fafc29c937)
  Jul  8 12:19:53.516: INFO: Lookups using dns-1883/dns-test-e9ae2585-b2f5-4806-bb95-98fafc29c937 failed for: [jessie_udp@dns-querier-2.dns-test-service-2.dns-1883.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1883.svc.cluster.local jessie_udp@dns-test-service-2.dns-1883.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1883.svc.cluster.local]

  E0708 12:19:53.703046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:54.703143      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:55.704110      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:56.704211      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:57.704299      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:19:58.553: INFO: DNS probes using dns-1883/dns-test-e9ae2585-b2f5-4806-bb95-98fafc29c937 succeeded

  Jul  8 12:19:58.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/08/23 12:19:58.557
  STEP: deleting the test headless service @ 07/08/23 12:19:58.574
  STEP: Destroying namespace "dns-1883" for this suite. @ 07/08/23 12:19:58.589
• [11.182 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 07/08/23 12:19:58.599
  Jul  8 12:19:58.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir @ 07/08/23 12:19:58.6
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:19:58.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:19:58.619
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 07/08/23 12:19:58.622
  E0708 12:19:58.705244      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:19:59.705394      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:00.705685      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:01.705970      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:20:02.644
  Jul  8 12:20:02.648: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-2cf32c02-cd1e-450b-9cf3-825627fdb5c8 container test-container: <nil>
  STEP: delete the pod @ 07/08/23 12:20:02.655
  Jul  8 12:20:02.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8059" for this suite. @ 07/08/23 12:20:02.678
• [4.085 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 07/08/23 12:20:02.685
  Jul  8 12:20:02.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/08/23 12:20:02.686
  E0708 12:20:02.706001      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:20:02.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:20:02.711
  Jul  8 12:20:02.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 12:20:03.706113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/08/23 12:20:03.988
  Jul  8 12:20:03.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-745 --namespace=crd-publish-openapi-745 create -f -'
  Jul  8 12:20:04.451: INFO: stderr: ""
  Jul  8 12:20:04.451: INFO: stdout: "e2e-test-crd-publish-openapi-3667-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jul  8 12:20:04.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-745 --namespace=crd-publish-openapi-745 delete e2e-test-crd-publish-openapi-3667-crds test-cr'
  Jul  8 12:20:04.504: INFO: stderr: ""
  Jul  8 12:20:04.504: INFO: stdout: "e2e-test-crd-publish-openapi-3667-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Jul  8 12:20:04.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-745 --namespace=crd-publish-openapi-745 apply -f -'
  E0708 12:20:04.706432      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:05.024: INFO: stderr: ""
  Jul  8 12:20:05.024: INFO: stdout: "e2e-test-crd-publish-openapi-3667-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jul  8 12:20:05.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-745 --namespace=crd-publish-openapi-745 delete e2e-test-crd-publish-openapi-3667-crds test-cr'
  Jul  8 12:20:05.077: INFO: stderr: ""
  Jul  8 12:20:05.077: INFO: stdout: "e2e-test-crd-publish-openapi-3667-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 07/08/23 12:20:05.077
  Jul  8 12:20:05.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-745 explain e2e-test-crd-publish-openapi-3667-crds'
  Jul  8 12:20:05.293: INFO: stderr: ""
  Jul  8 12:20:05.293: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-3667-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0708 12:20:05.707164      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:06.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-745" for this suite. @ 07/08/23 12:20:06.54
• [3.860 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 07/08/23 12:20:06.545
  Jul  8 12:20:06.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename svc-latency @ 07/08/23 12:20:06.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:20:06.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:20:06.568
  Jul  8 12:20:06.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-5876 @ 07/08/23 12:20:06.571
  I0708 12:20:06.577579      20 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5876, replica count: 1
  E0708 12:20:06.707705      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0708 12:20:07.629200      20 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0708 12:20:07.708344      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:07.739: INFO: Created: latency-svc-sz5qn
  Jul  8 12:20:07.747: INFO: Got endpoints: latency-svc-sz5qn [17.610683ms]
  Jul  8 12:20:07.762: INFO: Created: latency-svc-tvfgx
  Jul  8 12:20:07.768: INFO: Got endpoints: latency-svc-tvfgx [21.14548ms]
  Jul  8 12:20:07.773: INFO: Created: latency-svc-gwl6j
  Jul  8 12:20:07.778: INFO: Created: latency-svc-8sz5f
  Jul  8 12:20:07.781: INFO: Got endpoints: latency-svc-gwl6j [33.906673ms]
  Jul  8 12:20:07.785: INFO: Got endpoints: latency-svc-8sz5f [36.965881ms]
  Jul  8 12:20:07.798: INFO: Created: latency-svc-7p6dv
  Jul  8 12:20:07.806: INFO: Created: latency-svc-mrrrp
  Jul  8 12:20:07.809: INFO: Got endpoints: latency-svc-7p6dv [61.311663ms]
  Jul  8 12:20:07.827: INFO: Created: latency-svc-rrrwd
  Jul  8 12:20:07.832: INFO: Got endpoints: latency-svc-mrrrp [84.691271ms]
  Jul  8 12:20:07.839: INFO: Got endpoints: latency-svc-rrrwd [91.396094ms]
  Jul  8 12:20:07.856: INFO: Created: latency-svc-n464t
  Jul  8 12:20:07.856: INFO: Created: latency-svc-qwf9w
  Jul  8 12:20:07.858: INFO: Got endpoints: latency-svc-qwf9w [110.087957ms]
  Jul  8 12:20:07.865: INFO: Created: latency-svc-8dmlp
  Jul  8 12:20:07.871: INFO: Got endpoints: latency-svc-n464t [123.740131ms]
  Jul  8 12:20:07.873: INFO: Created: latency-svc-gppfm
  Jul  8 12:20:07.892: INFO: Created: latency-svc-6vvbr
  Jul  8 12:20:07.896: INFO: Got endpoints: latency-svc-gppfm [148.121026ms]
  Jul  8 12:20:07.896: INFO: Got endpoints: latency-svc-8dmlp [148.444384ms]
  Jul  8 12:20:07.900: INFO: Got endpoints: latency-svc-6vvbr [152.883719ms]
  Jul  8 12:20:07.902: INFO: Created: latency-svc-pbt5r
  Jul  8 12:20:07.914: INFO: Got endpoints: latency-svc-pbt5r [166.700747ms]
  Jul  8 12:20:07.916: INFO: Created: latency-svc-dx48b
  Jul  8 12:20:07.926: INFO: Created: latency-svc-xrpjn
  Jul  8 12:20:07.927: INFO: Got endpoints: latency-svc-dx48b [179.01248ms]
  Jul  8 12:20:07.935: INFO: Got endpoints: latency-svc-xrpjn [187.592981ms]
  Jul  8 12:20:08.002: INFO: Created: latency-svc-sz786
  Jul  8 12:20:08.002: INFO: Created: latency-svc-62275
  Jul  8 12:20:08.004: INFO: Created: latency-svc-fdhnl
  Jul  8 12:20:08.004: INFO: Created: latency-svc-lgp8c
  Jul  8 12:20:08.005: INFO: Created: latency-svc-j8hkm
  Jul  8 12:20:08.010: INFO: Created: latency-svc-dck7t
  Jul  8 12:20:08.010: INFO: Created: latency-svc-bhvqb
  Jul  8 12:20:08.011: INFO: Created: latency-svc-d4mpl
  Jul  8 12:20:08.011: INFO: Created: latency-svc-htwvm
  Jul  8 12:20:08.011: INFO: Created: latency-svc-rhq7h
  Jul  8 12:20:08.012: INFO: Created: latency-svc-g2gmv
  Jul  8 12:20:08.012: INFO: Created: latency-svc-b8r5t
  Jul  8 12:20:08.012: INFO: Created: latency-svc-lf4vk
  Jul  8 12:20:08.012: INFO: Created: latency-svc-69zf4
  Jul  8 12:20:08.013: INFO: Created: latency-svc-w7zrx
  Jul  8 12:20:08.025: INFO: Got endpoints: latency-svc-62275 [256.838024ms]
  Jul  8 12:20:08.025: INFO: Got endpoints: latency-svc-sz786 [89.734127ms]
  Jul  8 12:20:08.035: INFO: Got endpoints: latency-svc-lgp8c [177.145989ms]
  Jul  8 12:20:08.040: INFO: Got endpoints: latency-svc-fdhnl [144.039501ms]
  Jul  8 12:20:08.040: INFO: Got endpoints: latency-svc-j8hkm [201.236876ms]
  Jul  8 12:20:08.044: INFO: Got endpoints: latency-svc-b8r5t [259.628812ms]
  Jul  8 12:20:08.044: INFO: Got endpoints: latency-svc-lf4vk [212.596936ms]
  Jul  8 12:20:08.052: INFO: Got endpoints: latency-svc-w7zrx [124.584408ms]
  Jul  8 12:20:08.052: INFO: Got endpoints: latency-svc-dck7t [270.512027ms]
  Jul  8 12:20:08.052: INFO: Created: latency-svc-r2jst
  Jul  8 12:20:08.054: INFO: Got endpoints: latency-svc-htwvm [245.207444ms]
  Jul  8 12:20:08.055: INFO: Got endpoints: latency-svc-69zf4 [159.228951ms]
  Jul  8 12:20:08.063: INFO: Got endpoints: latency-svc-g2gmv [315.090406ms]
  Jul  8 12:20:08.063: INFO: Got endpoints: latency-svc-rhq7h [162.70472ms]
  Jul  8 12:20:08.070: INFO: Got endpoints: latency-svc-d4mpl [198.961821ms]
  Jul  8 12:20:08.071: INFO: Got endpoints: latency-svc-bhvqb [156.020329ms]
  Jul  8 12:20:08.072: INFO: Created: latency-svc-pchr9
  Jul  8 12:20:08.074: INFO: Got endpoints: latency-svc-r2jst [48.601752ms]
  Jul  8 12:20:08.077: INFO: Got endpoints: latency-svc-pchr9 [36.835761ms]
  Jul  8 12:20:08.078: INFO: Created: latency-svc-rjhcq
  Jul  8 12:20:08.084: INFO: Created: latency-svc-hmvmt
  Jul  8 12:20:08.085: INFO: Got endpoints: latency-svc-rjhcq [44.541922ms]
  Jul  8 12:20:08.097: INFO: Got endpoints: latency-svc-hmvmt [61.817306ms]
  Jul  8 12:20:08.097: INFO: Created: latency-svc-vct7q
  Jul  8 12:20:08.104: INFO: Got endpoints: latency-svc-vct7q [78.513117ms]
  Jul  8 12:20:08.108: INFO: Created: latency-svc-wvnrj
  Jul  8 12:20:08.113: INFO: Created: latency-svc-tp78n
  Jul  8 12:20:08.114: INFO: Got endpoints: latency-svc-wvnrj [69.661916ms]
  Jul  8 12:20:08.122: INFO: Got endpoints: latency-svc-tp78n [77.485332ms]
  Jul  8 12:20:08.123: INFO: Created: latency-svc-mk2pc
  Jul  8 12:20:08.131: INFO: Created: latency-svc-p4bnh
  Jul  8 12:20:08.137: INFO: Created: latency-svc-hl5cl
  Jul  8 12:20:08.145: INFO: Created: latency-svc-zg8n2
  Jul  8 12:20:08.145: INFO: Got endpoints: latency-svc-mk2pc [93.410963ms]
  Jul  8 12:20:08.152: INFO: Created: latency-svc-5gg75
  Jul  8 12:20:08.157: INFO: Created: latency-svc-9tp9c
  Jul  8 12:20:08.164: INFO: Created: latency-svc-l2vds
  Jul  8 12:20:08.171: INFO: Created: latency-svc-kjmlk
  Jul  8 12:20:08.174: INFO: Created: latency-svc-7lrfd
  Jul  8 12:20:08.181: INFO: Created: latency-svc-2w4tz
  Jul  8 12:20:08.188: INFO: Created: latency-svc-sdrzw
  Jul  8 12:20:08.194: INFO: Created: latency-svc-lwkwz
  Jul  8 12:20:08.196: INFO: Got endpoints: latency-svc-p4bnh [142.472866ms]
  Jul  8 12:20:08.201: INFO: Created: latency-svc-kvmhq
  Jul  8 12:20:08.208: INFO: Created: latency-svc-vpws9
  Jul  8 12:20:08.212: INFO: Created: latency-svc-4cvht
  Jul  8 12:20:08.219: INFO: Created: latency-svc-gkk6z
  Jul  8 12:20:08.225: INFO: Created: latency-svc-652lx
  Jul  8 12:20:08.248: INFO: Got endpoints: latency-svc-hl5cl [196.288509ms]
  Jul  8 12:20:08.256: INFO: Created: latency-svc-q5rt9
  Jul  8 12:20:08.297: INFO: Got endpoints: latency-svc-zg8n2 [241.771594ms]
  Jul  8 12:20:08.307: INFO: Created: latency-svc-qvmcq
  Jul  8 12:20:08.347: INFO: Got endpoints: latency-svc-5gg75 [283.444408ms]
  Jul  8 12:20:08.357: INFO: Created: latency-svc-5nfcv
  Jul  8 12:20:08.395: INFO: Got endpoints: latency-svc-9tp9c [332.293486ms]
  Jul  8 12:20:08.405: INFO: Created: latency-svc-7zmn4
  Jul  8 12:20:08.447: INFO: Got endpoints: latency-svc-l2vds [376.254409ms]
  Jul  8 12:20:08.456: INFO: Created: latency-svc-kchc7
  Jul  8 12:20:08.497: INFO: Got endpoints: latency-svc-kjmlk [426.579375ms]
  Jul  8 12:20:08.506: INFO: Created: latency-svc-9xb59
  Jul  8 12:20:08.551: INFO: Got endpoints: latency-svc-7lrfd [477.033026ms]
  Jul  8 12:20:08.559: INFO: Created: latency-svc-jkrtl
  Jul  8 12:20:08.596: INFO: Got endpoints: latency-svc-2w4tz [518.801694ms]
  Jul  8 12:20:08.606: INFO: Created: latency-svc-mnzkn
  Jul  8 12:20:08.646: INFO: Got endpoints: latency-svc-sdrzw [561.518939ms]
  Jul  8 12:20:08.656: INFO: Created: latency-svc-wqmd4
  Jul  8 12:20:08.697: INFO: Got endpoints: latency-svc-lwkwz [600.427821ms]
  Jul  8 12:20:08.706: INFO: Created: latency-svc-g2bgj
  E0708 12:20:08.709181      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:08.748: INFO: Got endpoints: latency-svc-kvmhq [644.52242ms]
  Jul  8 12:20:08.759: INFO: Created: latency-svc-hcxv6
  Jul  8 12:20:08.796: INFO: Got endpoints: latency-svc-vpws9 [682.39166ms]
  Jul  8 12:20:08.808: INFO: Created: latency-svc-bkpg8
  Jul  8 12:20:08.845: INFO: Got endpoints: latency-svc-4cvht [722.984214ms]
  Jul  8 12:20:08.855: INFO: Created: latency-svc-6vmlb
  Jul  8 12:20:08.896: INFO: Got endpoints: latency-svc-gkk6z [750.388429ms]
  Jul  8 12:20:08.906: INFO: Created: latency-svc-lqlwn
  Jul  8 12:20:08.947: INFO: Got endpoints: latency-svc-652lx [750.659758ms]
  Jul  8 12:20:08.957: INFO: Created: latency-svc-5j4xh
  Jul  8 12:20:08.998: INFO: Got endpoints: latency-svc-q5rt9 [749.15746ms]
  Jul  8 12:20:09.006: INFO: Created: latency-svc-rfs2z
  Jul  8 12:20:09.047: INFO: Got endpoints: latency-svc-qvmcq [749.910159ms]
  Jul  8 12:20:09.057: INFO: Created: latency-svc-5lmh7
  Jul  8 12:20:09.097: INFO: Got endpoints: latency-svc-5nfcv [749.723214ms]
  Jul  8 12:20:09.106: INFO: Created: latency-svc-tr77p
  Jul  8 12:20:09.147: INFO: Got endpoints: latency-svc-7zmn4 [751.62295ms]
  Jul  8 12:20:09.157: INFO: Created: latency-svc-rcxck
  Jul  8 12:20:09.195: INFO: Got endpoints: latency-svc-kchc7 [747.988305ms]
  Jul  8 12:20:09.206: INFO: Created: latency-svc-sp6m5
  Jul  8 12:20:09.245: INFO: Got endpoints: latency-svc-9xb59 [748.209412ms]
  Jul  8 12:20:09.255: INFO: Created: latency-svc-brj6z
  Jul  8 12:20:09.296: INFO: Got endpoints: latency-svc-jkrtl [745.437885ms]
  Jul  8 12:20:09.305: INFO: Created: latency-svc-kw7kf
  Jul  8 12:20:09.346: INFO: Got endpoints: latency-svc-mnzkn [750.281715ms]
  Jul  8 12:20:09.356: INFO: Created: latency-svc-2hrwj
  Jul  8 12:20:09.396: INFO: Got endpoints: latency-svc-wqmd4 [749.72488ms]
  Jul  8 12:20:09.407: INFO: Created: latency-svc-9rsg2
  Jul  8 12:20:09.447: INFO: Got endpoints: latency-svc-g2bgj [749.363926ms]
  Jul  8 12:20:09.458: INFO: Created: latency-svc-v7xtw
  Jul  8 12:20:09.497: INFO: Got endpoints: latency-svc-hcxv6 [748.752167ms]
  Jul  8 12:20:09.507: INFO: Created: latency-svc-xdz4c
  Jul  8 12:20:09.548: INFO: Got endpoints: latency-svc-bkpg8 [751.547566ms]
  Jul  8 12:20:09.558: INFO: Created: latency-svc-czf4t
  Jul  8 12:20:09.596: INFO: Got endpoints: latency-svc-6vmlb [751.10488ms]
  Jul  8 12:20:09.605: INFO: Created: latency-svc-prvkm
  Jul  8 12:20:09.647: INFO: Got endpoints: latency-svc-lqlwn [751.323676ms]
  Jul  8 12:20:09.657: INFO: Created: latency-svc-jxt2d
  Jul  8 12:20:09.695: INFO: Got endpoints: latency-svc-5j4xh [748.063887ms]
  Jul  8 12:20:09.706: INFO: Created: latency-svc-6c2kc
  E0708 12:20:09.709735      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:09.747: INFO: Got endpoints: latency-svc-rfs2z [749.28335ms]
  Jul  8 12:20:09.756: INFO: Created: latency-svc-cx6s8
  Jul  8 12:20:09.798: INFO: Got endpoints: latency-svc-5lmh7 [750.881195ms]
  Jul  8 12:20:09.807: INFO: Created: latency-svc-mrmv6
  Jul  8 12:20:09.846: INFO: Got endpoints: latency-svc-tr77p [749.571886ms]
  Jul  8 12:20:09.856: INFO: Created: latency-svc-lr52t
  Jul  8 12:20:09.897: INFO: Got endpoints: latency-svc-rcxck [750.279376ms]
  Jul  8 12:20:09.906: INFO: Created: latency-svc-v9ltq
  Jul  8 12:20:09.946: INFO: Got endpoints: latency-svc-sp6m5 [750.493722ms]
  Jul  8 12:20:09.955: INFO: Created: latency-svc-bp6ss
  Jul  8 12:20:09.997: INFO: Got endpoints: latency-svc-brj6z [751.144065ms]
  Jul  8 12:20:10.007: INFO: Created: latency-svc-8fdmb
  Jul  8 12:20:10.060: INFO: Got endpoints: latency-svc-kw7kf [763.282781ms]
  Jul  8 12:20:10.073: INFO: Created: latency-svc-zjtg5
  Jul  8 12:20:10.097: INFO: Got endpoints: latency-svc-2hrwj [750.124797ms]
  Jul  8 12:20:10.106: INFO: Created: latency-svc-nmdv8
  Jul  8 12:20:10.148: INFO: Got endpoints: latency-svc-9rsg2 [752.038106ms]
  Jul  8 12:20:10.158: INFO: Created: latency-svc-6gtqd
  Jul  8 12:20:10.197: INFO: Got endpoints: latency-svc-v7xtw [750.410872ms]
  Jul  8 12:20:10.206: INFO: Created: latency-svc-87k6d
  Jul  8 12:20:10.246: INFO: Got endpoints: latency-svc-xdz4c [748.371039ms]
  Jul  8 12:20:10.256: INFO: Created: latency-svc-8sh2b
  Jul  8 12:20:10.297: INFO: Got endpoints: latency-svc-czf4t [749.220048ms]
  Jul  8 12:20:10.307: INFO: Created: latency-svc-4lzqg
  Jul  8 12:20:10.347: INFO: Got endpoints: latency-svc-prvkm [750.521662ms]
  Jul  8 12:20:10.356: INFO: Created: latency-svc-tzg4j
  Jul  8 12:20:10.396: INFO: Got endpoints: latency-svc-jxt2d [748.33538ms]
  Jul  8 12:20:10.406: INFO: Created: latency-svc-sjcgd
  Jul  8 12:20:10.446: INFO: Got endpoints: latency-svc-6c2kc [750.565756ms]
  Jul  8 12:20:10.457: INFO: Created: latency-svc-9dvt2
  Jul  8 12:20:10.495: INFO: Got endpoints: latency-svc-cx6s8 [748.260576ms]
  Jul  8 12:20:10.504: INFO: Created: latency-svc-n68kj
  Jul  8 12:20:10.547: INFO: Got endpoints: latency-svc-mrmv6 [749.489718ms]
  Jul  8 12:20:10.557: INFO: Created: latency-svc-lcxrq
  Jul  8 12:20:10.596: INFO: Got endpoints: latency-svc-lr52t [749.796739ms]
  Jul  8 12:20:10.606: INFO: Created: latency-svc-gt6cj
  Jul  8 12:20:10.647: INFO: Got endpoints: latency-svc-v9ltq [749.020702ms]
  Jul  8 12:20:10.656: INFO: Created: latency-svc-5vl4n
  Jul  8 12:20:10.695: INFO: Got endpoints: latency-svc-bp6ss [749.205903ms]
  Jul  8 12:20:10.705: INFO: Created: latency-svc-lp57n
  E0708 12:20:10.710411      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:10.746: INFO: Got endpoints: latency-svc-8fdmb [749.935573ms]
  Jul  8 12:20:10.758: INFO: Created: latency-svc-28s78
  Jul  8 12:20:10.798: INFO: Got endpoints: latency-svc-zjtg5 [738.369281ms]
  Jul  8 12:20:10.806: INFO: Created: latency-svc-7nmrb
  Jul  8 12:20:10.847: INFO: Got endpoints: latency-svc-nmdv8 [750.736848ms]
  Jul  8 12:20:10.857: INFO: Created: latency-svc-zmz8t
  Jul  8 12:20:10.904: INFO: Got endpoints: latency-svc-6gtqd [755.179307ms]
  Jul  8 12:20:10.916: INFO: Created: latency-svc-25zqv
  Jul  8 12:20:10.945: INFO: Got endpoints: latency-svc-87k6d [747.921451ms]
  Jul  8 12:20:10.960: INFO: Created: latency-svc-8wdpv
  Jul  8 12:20:10.996: INFO: Got endpoints: latency-svc-8sh2b [750.248335ms]
  Jul  8 12:20:11.007: INFO: Created: latency-svc-tksbx
  Jul  8 12:20:11.047: INFO: Got endpoints: latency-svc-4lzqg [749.824887ms]
  Jul  8 12:20:11.056: INFO: Created: latency-svc-k6495
  Jul  8 12:20:11.098: INFO: Got endpoints: latency-svc-tzg4j [750.917863ms]
  Jul  8 12:20:11.106: INFO: Created: latency-svc-krl9r
  Jul  8 12:20:11.146: INFO: Got endpoints: latency-svc-sjcgd [750.614432ms]
  Jul  8 12:20:11.156: INFO: Created: latency-svc-kt6rt
  Jul  8 12:20:11.196: INFO: Got endpoints: latency-svc-9dvt2 [749.681993ms]
  Jul  8 12:20:11.205: INFO: Created: latency-svc-tt2m4
  Jul  8 12:20:11.247: INFO: Got endpoints: latency-svc-n68kj [750.955881ms]
  Jul  8 12:20:11.255: INFO: Created: latency-svc-7qjgn
  Jul  8 12:20:11.297: INFO: Got endpoints: latency-svc-lcxrq [749.922959ms]
  Jul  8 12:20:11.307: INFO: Created: latency-svc-2z2qd
  Jul  8 12:20:11.347: INFO: Got endpoints: latency-svc-gt6cj [751.128774ms]
  Jul  8 12:20:11.357: INFO: Created: latency-svc-xgwxt
  Jul  8 12:20:11.398: INFO: Got endpoints: latency-svc-5vl4n [751.138908ms]
  Jul  8 12:20:11.406: INFO: Created: latency-svc-q28l7
  Jul  8 12:20:11.445: INFO: Got endpoints: latency-svc-lp57n [750.15947ms]
  Jul  8 12:20:11.456: INFO: Created: latency-svc-p4bgt
  Jul  8 12:20:11.496: INFO: Got endpoints: latency-svc-28s78 [749.088496ms]
  Jul  8 12:20:11.506: INFO: Created: latency-svc-22jch
  Jul  8 12:20:11.547: INFO: Got endpoints: latency-svc-7nmrb [748.99024ms]
  Jul  8 12:20:11.556: INFO: Created: latency-svc-6jnlb
  Jul  8 12:20:11.597: INFO: Got endpoints: latency-svc-zmz8t [749.565927ms]
  Jul  8 12:20:11.604: INFO: Created: latency-svc-p6jv9
  Jul  8 12:20:11.646: INFO: Got endpoints: latency-svc-25zqv [742.562647ms]
  Jul  8 12:20:11.657: INFO: Created: latency-svc-rdxd9
  Jul  8 12:20:11.697: INFO: Got endpoints: latency-svc-8wdpv [751.449335ms]
  Jul  8 12:20:11.707: INFO: Created: latency-svc-6wqvv
  E0708 12:20:11.711165      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:11.746: INFO: Got endpoints: latency-svc-tksbx [749.420822ms]
  Jul  8 12:20:11.757: INFO: Created: latency-svc-w2lqh
  Jul  8 12:20:11.796: INFO: Got endpoints: latency-svc-k6495 [749.185506ms]
  Jul  8 12:20:11.808: INFO: Created: latency-svc-5f9kb
  Jul  8 12:20:11.847: INFO: Got endpoints: latency-svc-krl9r [749.117815ms]
  Jul  8 12:20:11.856: INFO: Created: latency-svc-s4994
  Jul  8 12:20:11.895: INFO: Got endpoints: latency-svc-kt6rt [748.547245ms]
  Jul  8 12:20:11.904: INFO: Created: latency-svc-8gd9s
  Jul  8 12:20:11.947: INFO: Got endpoints: latency-svc-tt2m4 [751.305497ms]
  Jul  8 12:20:11.958: INFO: Created: latency-svc-b2hrr
  Jul  8 12:20:11.997: INFO: Got endpoints: latency-svc-7qjgn [750.166392ms]
  Jul  8 12:20:12.006: INFO: Created: latency-svc-jrlv9
  Jul  8 12:20:12.047: INFO: Got endpoints: latency-svc-2z2qd [749.252226ms]
  Jul  8 12:20:12.054: INFO: Created: latency-svc-jr9hg
  Jul  8 12:20:12.097: INFO: Got endpoints: latency-svc-xgwxt [749.612184ms]
  Jul  8 12:20:12.108: INFO: Created: latency-svc-t68w4
  Jul  8 12:20:12.147: INFO: Got endpoints: latency-svc-q28l7 [748.925696ms]
  Jul  8 12:20:12.155: INFO: Created: latency-svc-jw4rs
  Jul  8 12:20:12.195: INFO: Got endpoints: latency-svc-p4bgt [750.196846ms]
  Jul  8 12:20:12.207: INFO: Created: latency-svc-mx6wv
  Jul  8 12:20:12.247: INFO: Got endpoints: latency-svc-22jch [750.658092ms]
  Jul  8 12:20:12.257: INFO: Created: latency-svc-b6w5d
  Jul  8 12:20:12.298: INFO: Got endpoints: latency-svc-6jnlb [750.874923ms]
  Jul  8 12:20:12.316: INFO: Created: latency-svc-kdgcc
  Jul  8 12:20:12.347: INFO: Got endpoints: latency-svc-p6jv9 [750.578365ms]
  Jul  8 12:20:12.354: INFO: Created: latency-svc-mxjcc
  Jul  8 12:20:12.397: INFO: Got endpoints: latency-svc-rdxd9 [750.55932ms]
  Jul  8 12:20:12.407: INFO: Created: latency-svc-56f62
  Jul  8 12:20:12.446: INFO: Got endpoints: latency-svc-6wqvv [749.591863ms]
  Jul  8 12:20:12.457: INFO: Created: latency-svc-c27c4
  Jul  8 12:20:12.496: INFO: Got endpoints: latency-svc-w2lqh [750.645205ms]
  Jul  8 12:20:12.504: INFO: Created: latency-svc-gjwv7
  Jul  8 12:20:12.545: INFO: Got endpoints: latency-svc-5f9kb [748.917963ms]
  Jul  8 12:20:12.555: INFO: Created: latency-svc-xjxbb
  Jul  8 12:20:12.671: INFO: Got endpoints: latency-svc-s4994 [823.972178ms]
  Jul  8 12:20:12.673: INFO: Got endpoints: latency-svc-8gd9s [777.411369ms]
  Jul  8 12:20:12.684: INFO: Created: latency-svc-sftd7
  Jul  8 12:20:12.686: INFO: Created: latency-svc-4prsw
  Jul  8 12:20:12.694: INFO: Got endpoints: latency-svc-b2hrr [747.24515ms]
  Jul  8 12:20:12.709: INFO: Created: latency-svc-gp4f2
  E0708 12:20:12.711677      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:12.748: INFO: Got endpoints: latency-svc-jrlv9 [750.933382ms]
  Jul  8 12:20:12.761: INFO: Created: latency-svc-8k6m8
  Jul  8 12:20:12.801: INFO: Got endpoints: latency-svc-jr9hg [754.224866ms]
  Jul  8 12:20:12.808: INFO: Created: latency-svc-gfv6j
  Jul  8 12:20:12.847: INFO: Got endpoints: latency-svc-t68w4 [749.508058ms]
  Jul  8 12:20:12.857: INFO: Created: latency-svc-4ghfr
  Jul  8 12:20:12.896: INFO: Got endpoints: latency-svc-jw4rs [749.068143ms]
  Jul  8 12:20:12.905: INFO: Created: latency-svc-b8rxh
  Jul  8 12:20:12.946: INFO: Got endpoints: latency-svc-mx6wv [750.693689ms]
  Jul  8 12:20:12.954: INFO: Created: latency-svc-d4tqm
  Jul  8 12:20:12.996: INFO: Got endpoints: latency-svc-b6w5d [748.908631ms]
  Jul  8 12:20:13.007: INFO: Created: latency-svc-g6zkq
  Jul  8 12:20:13.048: INFO: Got endpoints: latency-svc-kdgcc [749.77344ms]
  Jul  8 12:20:13.055: INFO: Created: latency-svc-d7r6h
  Jul  8 12:20:13.096: INFO: Got endpoints: latency-svc-mxjcc [748.262998ms]
  Jul  8 12:20:13.106: INFO: Created: latency-svc-dkxc8
  Jul  8 12:20:13.147: INFO: Got endpoints: latency-svc-56f62 [750.047396ms]
  Jul  8 12:20:13.156: INFO: Created: latency-svc-jshtw
  Jul  8 12:20:13.196: INFO: Got endpoints: latency-svc-c27c4 [749.526631ms]
  Jul  8 12:20:13.205: INFO: Created: latency-svc-4zfvv
  Jul  8 12:20:13.246: INFO: Got endpoints: latency-svc-gjwv7 [750.018327ms]
  Jul  8 12:20:13.256: INFO: Created: latency-svc-zm8ss
  Jul  8 12:20:13.297: INFO: Got endpoints: latency-svc-xjxbb [751.653167ms]
  Jul  8 12:20:13.307: INFO: Created: latency-svc-sfmr9
  Jul  8 12:20:13.347: INFO: Got endpoints: latency-svc-sftd7 [676.068576ms]
  Jul  8 12:20:13.355: INFO: Created: latency-svc-dtqst
  Jul  8 12:20:13.396: INFO: Got endpoints: latency-svc-4prsw [723.57047ms]
  Jul  8 12:20:13.407: INFO: Created: latency-svc-7kqn9
  Jul  8 12:20:13.446: INFO: Got endpoints: latency-svc-gp4f2 [752.055192ms]
  Jul  8 12:20:13.457: INFO: Created: latency-svc-ph676
  Jul  8 12:20:13.498: INFO: Got endpoints: latency-svc-8k6m8 [749.544825ms]
  Jul  8 12:20:13.507: INFO: Created: latency-svc-k9cqj
  Jul  8 12:20:13.547: INFO: Got endpoints: latency-svc-gfv6j [745.926668ms]
  Jul  8 12:20:13.556: INFO: Created: latency-svc-nrk6s
  Jul  8 12:20:13.597: INFO: Got endpoints: latency-svc-4ghfr [750.186969ms]
  Jul  8 12:20:13.606: INFO: Created: latency-svc-h688q
  Jul  8 12:20:13.646: INFO: Got endpoints: latency-svc-b8rxh [749.393344ms]
  Jul  8 12:20:13.654: INFO: Created: latency-svc-jpxzq
  Jul  8 12:20:13.696: INFO: Got endpoints: latency-svc-d4tqm [750.023572ms]
  Jul  8 12:20:13.706: INFO: Created: latency-svc-9jlxw
  E0708 12:20:13.712568      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:13.747: INFO: Got endpoints: latency-svc-g6zkq [751.848219ms]
  Jul  8 12:20:13.759: INFO: Created: latency-svc-pvspn
  Jul  8 12:20:13.797: INFO: Got endpoints: latency-svc-d7r6h [749.209051ms]
  Jul  8 12:20:13.807: INFO: Created: latency-svc-qcd84
  Jul  8 12:20:13.846: INFO: Got endpoints: latency-svc-dkxc8 [750.313735ms]
  Jul  8 12:20:13.856: INFO: Created: latency-svc-hmjks
  Jul  8 12:20:13.897: INFO: Got endpoints: latency-svc-jshtw [749.908805ms]
  Jul  8 12:20:13.907: INFO: Created: latency-svc-wchhg
  Jul  8 12:20:13.947: INFO: Got endpoints: latency-svc-4zfvv [751.112682ms]
  Jul  8 12:20:13.955: INFO: Created: latency-svc-p9b7x
  Jul  8 12:20:13.999: INFO: Got endpoints: latency-svc-zm8ss [752.432704ms]
  Jul  8 12:20:14.008: INFO: Created: latency-svc-t6d6g
  Jul  8 12:20:14.047: INFO: Got endpoints: latency-svc-sfmr9 [749.805193ms]
  Jul  8 12:20:14.056: INFO: Created: latency-svc-hzzjl
  Jul  8 12:20:14.097: INFO: Got endpoints: latency-svc-dtqst [749.3473ms]
  Jul  8 12:20:14.104: INFO: Created: latency-svc-jsrkg
  Jul  8 12:20:14.146: INFO: Got endpoints: latency-svc-7kqn9 [749.412569ms]
  Jul  8 12:20:14.156: INFO: Created: latency-svc-njhv2
  Jul  8 12:20:14.197: INFO: Got endpoints: latency-svc-ph676 [750.786055ms]
  Jul  8 12:20:14.208: INFO: Created: latency-svc-xbnrc
  Jul  8 12:20:14.248: INFO: Got endpoints: latency-svc-k9cqj [749.910843ms]
  Jul  8 12:20:14.256: INFO: Created: latency-svc-l95rn
  Jul  8 12:20:14.296: INFO: Got endpoints: latency-svc-nrk6s [749.593778ms]
  Jul  8 12:20:14.308: INFO: Created: latency-svc-xb92j
  Jul  8 12:20:14.347: INFO: Got endpoints: latency-svc-h688q [750.255323ms]
  Jul  8 12:20:14.356: INFO: Created: latency-svc-rjcpk
  Jul  8 12:20:14.397: INFO: Got endpoints: latency-svc-jpxzq [750.833767ms]
  Jul  8 12:20:14.405: INFO: Created: latency-svc-m5c9c
  Jul  8 12:20:14.445: INFO: Got endpoints: latency-svc-9jlxw [748.464717ms]
  Jul  8 12:20:14.454: INFO: Created: latency-svc-rmc7b
  Jul  8 12:20:14.497: INFO: Got endpoints: latency-svc-pvspn [749.058154ms]
  Jul  8 12:20:14.506: INFO: Created: latency-svc-7t8qw
  Jul  8 12:20:14.547: INFO: Got endpoints: latency-svc-qcd84 [749.861514ms]
  Jul  8 12:20:14.555: INFO: Created: latency-svc-4p7mh
  Jul  8 12:20:14.596: INFO: Got endpoints: latency-svc-hmjks [749.322716ms]
  Jul  8 12:20:14.606: INFO: Created: latency-svc-h8bwj
  Jul  8 12:20:14.646: INFO: Got endpoints: latency-svc-wchhg [749.292225ms]
  Jul  8 12:20:14.656: INFO: Created: latency-svc-d4wdq
  Jul  8 12:20:14.696: INFO: Got endpoints: latency-svc-p9b7x [748.907957ms]
  Jul  8 12:20:14.704: INFO: Created: latency-svc-stjbp
  E0708 12:20:14.712871      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:14.747: INFO: Got endpoints: latency-svc-t6d6g [747.887139ms]
  Jul  8 12:20:14.765: INFO: Created: latency-svc-jksgv
  Jul  8 12:20:14.799: INFO: Got endpoints: latency-svc-hzzjl [752.173605ms]
  Jul  8 12:20:14.811: INFO: Created: latency-svc-zvrdg
  Jul  8 12:20:14.846: INFO: Got endpoints: latency-svc-jsrkg [749.834344ms]
  Jul  8 12:20:14.859: INFO: Created: latency-svc-dspz9
  Jul  8 12:20:14.896: INFO: Got endpoints: latency-svc-njhv2 [749.907479ms]
  Jul  8 12:20:14.906: INFO: Created: latency-svc-4npfc
  Jul  8 12:20:14.946: INFO: Got endpoints: latency-svc-xbnrc [748.604511ms]
  Jul  8 12:20:14.955: INFO: Created: latency-svc-fvf5p
  Jul  8 12:20:14.997: INFO: Got endpoints: latency-svc-l95rn [749.095034ms]
  Jul  8 12:20:15.008: INFO: Created: latency-svc-jvth4
  Jul  8 12:20:15.046: INFO: Got endpoints: latency-svc-xb92j [749.29903ms]
  Jul  8 12:20:15.054: INFO: Created: latency-svc-lxbhs
  Jul  8 12:20:15.097: INFO: Got endpoints: latency-svc-rjcpk [749.480419ms]
  Jul  8 12:20:15.106: INFO: Created: latency-svc-fr8vr
  Jul  8 12:20:15.146: INFO: Got endpoints: latency-svc-m5c9c [749.647613ms]
  Jul  8 12:20:15.157: INFO: Created: latency-svc-xqcbj
  Jul  8 12:20:15.198: INFO: Got endpoints: latency-svc-rmc7b [753.084511ms]
  Jul  8 12:20:15.208: INFO: Created: latency-svc-s7q8h
  Jul  8 12:20:15.247: INFO: Got endpoints: latency-svc-7t8qw [750.071153ms]
  Jul  8 12:20:15.257: INFO: Created: latency-svc-fpcph
  Jul  8 12:20:15.297: INFO: Got endpoints: latency-svc-4p7mh [749.639782ms]
  Jul  8 12:20:15.312: INFO: Created: latency-svc-k8zgn
  Jul  8 12:20:15.345: INFO: Got endpoints: latency-svc-h8bwj [749.057665ms]
  Jul  8 12:20:15.354: INFO: Created: latency-svc-9ln4p
  Jul  8 12:20:15.397: INFO: Got endpoints: latency-svc-d4wdq [751.191921ms]
  Jul  8 12:20:15.407: INFO: Created: latency-svc-qj26g
  Jul  8 12:20:15.446: INFO: Got endpoints: latency-svc-stjbp [750.103203ms]
  Jul  8 12:20:15.455: INFO: Created: latency-svc-zd8zb
  Jul  8 12:20:15.497: INFO: Got endpoints: latency-svc-jksgv [749.728094ms]
  Jul  8 12:20:15.507: INFO: Created: latency-svc-kdfjt
  Jul  8 12:20:15.547: INFO: Got endpoints: latency-svc-zvrdg [747.790348ms]
  Jul  8 12:20:15.555: INFO: Created: latency-svc-nr29l
  Jul  8 12:20:15.596: INFO: Got endpoints: latency-svc-dspz9 [749.610911ms]
  Jul  8 12:20:15.648: INFO: Got endpoints: latency-svc-4npfc [752.103546ms]
  Jul  8 12:20:15.697: INFO: Got endpoints: latency-svc-fvf5p [750.824815ms]
  E0708 12:20:15.713310      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:15.746: INFO: Got endpoints: latency-svc-jvth4 [749.047766ms]
  Jul  8 12:20:15.795: INFO: Got endpoints: latency-svc-lxbhs [749.588375ms]
  Jul  8 12:20:15.848: INFO: Got endpoints: latency-svc-fr8vr [751.418162ms]
  Jul  8 12:20:15.900: INFO: Got endpoints: latency-svc-xqcbj [754.106923ms]
  Jul  8 12:20:15.946: INFO: Got endpoints: latency-svc-s7q8h [747.88935ms]
  Jul  8 12:20:15.997: INFO: Got endpoints: latency-svc-fpcph [750.034334ms]
  Jul  8 12:20:16.047: INFO: Got endpoints: latency-svc-k8zgn [750.645993ms]
  Jul  8 12:20:16.095: INFO: Got endpoints: latency-svc-9ln4p [750.402215ms]
  Jul  8 12:20:16.147: INFO: Got endpoints: latency-svc-qj26g [749.559693ms]
  Jul  8 12:20:16.196: INFO: Got endpoints: latency-svc-zd8zb [749.930588ms]
  Jul  8 12:20:16.246: INFO: Got endpoints: latency-svc-kdfjt [749.12101ms]
  Jul  8 12:20:16.297: INFO: Got endpoints: latency-svc-nr29l [749.866147ms]
  Jul  8 12:20:16.297: INFO: Latencies: [21.14548ms 33.906673ms 36.835761ms 36.965881ms 44.541922ms 48.601752ms 61.311663ms 61.817306ms 69.661916ms 77.485332ms 78.513117ms 84.691271ms 89.734127ms 91.396094ms 93.410963ms 110.087957ms 123.740131ms 124.584408ms 142.472866ms 144.039501ms 148.121026ms 148.444384ms 152.883719ms 156.020329ms 159.228951ms 162.70472ms 166.700747ms 177.145989ms 179.01248ms 187.592981ms 196.288509ms 198.961821ms 201.236876ms 212.596936ms 241.771594ms 245.207444ms 256.838024ms 259.628812ms 270.512027ms 283.444408ms 315.090406ms 332.293486ms 376.254409ms 426.579375ms 477.033026ms 518.801694ms 561.518939ms 600.427821ms 644.52242ms 676.068576ms 682.39166ms 722.984214ms 723.57047ms 738.369281ms 742.562647ms 745.437885ms 745.926668ms 747.24515ms 747.790348ms 747.887139ms 747.88935ms 747.921451ms 747.988305ms 748.063887ms 748.209412ms 748.260576ms 748.262998ms 748.33538ms 748.371039ms 748.464717ms 748.547245ms 748.604511ms 748.752167ms 748.907957ms 748.908631ms 748.917963ms 748.925696ms 748.99024ms 749.020702ms 749.047766ms 749.057665ms 749.058154ms 749.068143ms 749.088496ms 749.095034ms 749.117815ms 749.12101ms 749.15746ms 749.185506ms 749.205903ms 749.209051ms 749.220048ms 749.252226ms 749.28335ms 749.292225ms 749.29903ms 749.322716ms 749.3473ms 749.363926ms 749.393344ms 749.412569ms 749.420822ms 749.480419ms 749.489718ms 749.508058ms 749.526631ms 749.544825ms 749.559693ms 749.565927ms 749.571886ms 749.588375ms 749.591863ms 749.593778ms 749.610911ms 749.612184ms 749.639782ms 749.647613ms 749.681993ms 749.723214ms 749.72488ms 749.728094ms 749.77344ms 749.796739ms 749.805193ms 749.824887ms 749.834344ms 749.861514ms 749.866147ms 749.907479ms 749.908805ms 749.910159ms 749.910843ms 749.922959ms 749.930588ms 749.935573ms 750.018327ms 750.023572ms 750.034334ms 750.047396ms 750.071153ms 750.103203ms 750.124797ms 750.15947ms 750.166392ms 750.186969ms 750.196846ms 750.248335ms 750.255323ms 750.279376ms 750.281715ms 750.313735ms 750.388429ms 750.402215ms 750.410872ms 750.493722ms 750.521662ms 750.55932ms 750.565756ms 750.578365ms 750.614432ms 750.645205ms 750.645993ms 750.658092ms 750.659758ms 750.693689ms 750.736848ms 750.786055ms 750.824815ms 750.833767ms 750.874923ms 750.881195ms 750.917863ms 750.933382ms 750.955881ms 751.10488ms 751.112682ms 751.128774ms 751.138908ms 751.144065ms 751.191921ms 751.305497ms 751.323676ms 751.418162ms 751.449335ms 751.547566ms 751.62295ms 751.653167ms 751.848219ms 752.038106ms 752.055192ms 752.103546ms 752.173605ms 752.432704ms 753.084511ms 754.106923ms 754.224866ms 755.179307ms 763.282781ms 777.411369ms 823.972178ms]
  Jul  8 12:20:16.297: INFO: 50 %ile: 749.412569ms
  Jul  8 12:20:16.297: INFO: 90 %ile: 751.305497ms
  Jul  8 12:20:16.297: INFO: 99 %ile: 777.411369ms
  Jul  8 12:20:16.297: INFO: Total sample count: 200
  Jul  8 12:20:16.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-5876" for this suite. @ 07/08/23 12:20:16.302
• [9.764 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 07/08/23 12:20:16.31
  Jul  8 12:20:16.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename daemonsets @ 07/08/23 12:20:16.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:20:16.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:20:16.325
  STEP: Creating simple DaemonSet "daemon-set" @ 07/08/23 12:20:16.346
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/08/23 12:20:16.352
  Jul  8 12:20:16.357: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:20:16.357: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:20:16.361: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 12:20:16.361: INFO: Node ip-172-31-12-67 is running 0 daemon pod, expected 1
  E0708 12:20:16.714308      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:17.365: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:20:17.365: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:20:17.369: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  8 12:20:17.369: INFO: Node ip-172-31-12-67 is running 0 daemon pod, expected 1
  E0708 12:20:17.715039      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:18.364: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:20:18.364: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:20:18.368: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  8 12:20:18.368: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 07/08/23 12:20:18.37
  Jul  8 12:20:18.374: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 07/08/23 12:20:18.374
  Jul  8 12:20:18.382: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 07/08/23 12:20:18.382
  Jul  8 12:20:18.383: INFO: Observed &DaemonSet event: ADDED
  Jul  8 12:20:18.383: INFO: Observed &DaemonSet event: MODIFIED
  Jul  8 12:20:18.383: INFO: Observed &DaemonSet event: MODIFIED
  Jul  8 12:20:18.383: INFO: Observed &DaemonSet event: MODIFIED
  Jul  8 12:20:18.384: INFO: Observed &DaemonSet event: MODIFIED
  Jul  8 12:20:18.384: INFO: Found daemon set daemon-set in namespace daemonsets-5074 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul  8 12:20:18.384: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 07/08/23 12:20:18.384
  STEP: watching for the daemon set status to be patched @ 07/08/23 12:20:18.39
  Jul  8 12:20:18.391: INFO: Observed &DaemonSet event: ADDED
  Jul  8 12:20:18.391: INFO: Observed &DaemonSet event: MODIFIED
  Jul  8 12:20:18.391: INFO: Observed &DaemonSet event: MODIFIED
  Jul  8 12:20:18.392: INFO: Observed &DaemonSet event: MODIFIED
  Jul  8 12:20:18.392: INFO: Observed &DaemonSet event: MODIFIED
  Jul  8 12:20:18.392: INFO: Observed daemon set daemon-set in namespace daemonsets-5074 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul  8 12:20:18.392: INFO: Observed &DaemonSet event: MODIFIED
  Jul  8 12:20:18.392: INFO: Found daemon set daemon-set in namespace daemonsets-5074 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Jul  8 12:20:18.392: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 07/08/23 12:20:18.395
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5074, will wait for the garbage collector to delete the pods @ 07/08/23 12:20:18.395
  Jul  8 12:20:18.454: INFO: Deleting DaemonSet.extensions daemon-set took: 5.007728ms
  Jul  8 12:20:18.554: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.413545ms
  E0708 12:20:18.715213      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:19.715759      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:19.958: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 12:20:19.958: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul  8 12:20:19.961: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11404"},"items":null}

  Jul  8 12:20:19.963: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11404"},"items":null}

  Jul  8 12:20:19.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5074" for this suite. @ 07/08/23 12:20:19.976
• [3.671 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 07/08/23 12:20:19.982
  Jul  8 12:20:19.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename prestop @ 07/08/23 12:20:19.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:20:19.995
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:20:19.997
  STEP: Creating server pod server in namespace prestop-9115 @ 07/08/23 12:20:19.999
  STEP: Waiting for pods to come up. @ 07/08/23 12:20:20.007
  E0708 12:20:20.716613      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:21.716968      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-9115 @ 07/08/23 12:20:22.017
  E0708 12:20:22.717809      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:23.717964      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 07/08/23 12:20:24.032
  E0708 12:20:24.718964      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:25.719038      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:26.719142      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:27.719316      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:28.719405      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:29.043: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Jul  8 12:20:29.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 07/08/23 12:20:29.046
  STEP: Destroying namespace "prestop-9115" for this suite. @ 07/08/23 12:20:29.057
• [9.081 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 07/08/23 12:20:29.064
  Jul  8 12:20:29.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 12:20:29.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:20:29.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:20:29.084
  STEP: Setting up server cert @ 07/08/23 12:20:29.102
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 12:20:29.571
  STEP: Deploying the webhook pod @ 07/08/23 12:20:29.578
  STEP: Wait for the deployment to be ready @ 07/08/23 12:20:29.588
  Jul  8 12:20:29.595: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0708 12:20:29.720418      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:30.720505      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 12:20:31.605
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 12:20:31.615
  E0708 12:20:31.720566      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:32.615: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  E0708 12:20:32.720882      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:33.615: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  E0708 12:20:33.721497      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:34.615: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 07/08/23 12:20:34.618
  STEP: create a namespace for the webhook @ 07/08/23 12:20:34.632
  STEP: create a configmap should be unconditionally rejected by the webhook @ 07/08/23 12:20:34.653
  Jul  8 12:20:34.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0708 12:20:34.721565      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-7638" for this suite. @ 07/08/23 12:20:34.746
  STEP: Destroying namespace "webhook-markers-5847" for this suite. @ 07/08/23 12:20:34.753
  STEP: Destroying namespace "fail-closed-namespace-3471" for this suite. @ 07/08/23 12:20:34.758
• [5.699 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 07/08/23 12:20:34.763
  Jul  8 12:20:34.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename containers @ 07/08/23 12:20:34.764
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:20:34.777
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:20:34.78
  STEP: Creating a pod to test override arguments @ 07/08/23 12:20:34.782
  E0708 12:20:35.721642      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:36.721856      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:37.721966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:38.722035      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:20:38.803
  Jul  8 12:20:38.805: INFO: Trying to get logs from node ip-172-31-93-234 pod client-containers-354d79be-03a8-4d3c-aa1b-462b3a84d5a9 container agnhost-container: <nil>
  STEP: delete the pod @ 07/08/23 12:20:38.816
  Jul  8 12:20:38.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2049" for this suite. @ 07/08/23 12:20:38.833
• [4.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 07/08/23 12:20:38.839
  Jul  8 12:20:38.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename security-context-test @ 07/08/23 12:20:38.84
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:20:38.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:20:38.855
  E0708 12:20:39.722145      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:40.722250      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:41.722307      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:42.722562      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:42.883: INFO: Got logs for pod "busybox-privileged-false-e530cdde-bb14-4f75-a9cd-77f95e1210cb": "ip: RTNETLINK answers: Operation not permitted\n"
  Jul  8 12:20:42.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7662" for this suite. @ 07/08/23 12:20:42.889
• [4.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 07/08/23 12:20:42.897
  Jul  8 12:20:42.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename var-expansion @ 07/08/23 12:20:42.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:20:42.914
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:20:42.916
  STEP: Creating a pod to test env composition @ 07/08/23 12:20:42.918
  E0708 12:20:43.722633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:44.723539      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:45.724107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:46.725098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:20:46.942
  Jul  8 12:20:46.945: INFO: Trying to get logs from node ip-172-31-93-234 pod var-expansion-eaaad075-f348-4da8-8874-108f082b36e8 container dapi-container: <nil>
  STEP: delete the pod @ 07/08/23 12:20:46.951
  Jul  8 12:20:46.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5559" for this suite. @ 07/08/23 12:20:46.967
• [4.078 seconds]
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 07/08/23 12:20:46.975
  Jul  8 12:20:46.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename sched-pred @ 07/08/23 12:20:46.976
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:20:46.989
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:20:46.994
  Jul  8 12:20:46.996: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul  8 12:20:47.002: INFO: Waiting for terminating namespaces to be deleted...
  Jul  8 12:20:47.005: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-12-67 before test
  Jul  8 12:20:47.011: INFO: default-http-backend-kubernetes-worker-65fc475d49-8npt2 from ingress-nginx-kubernetes-worker started at 2023-07-08 11:50:26 +0000 UTC (1 container statuses recorded)
  Jul  8 12:20:47.011: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul  8 12:20:47.011: INFO: nginx-ingress-controller-kubernetes-worker-lsvk7 from ingress-nginx-kubernetes-worker started at 2023-07-08 11:50:26 +0000 UTC (1 container statuses recorded)
  Jul  8 12:20:47.011: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  8 12:20:47.011: INFO: calico-kube-controllers-bb564cc5-vpxwc from kube-system started at 2023-07-08 11:50:30 +0000 UTC (1 container statuses recorded)
  Jul  8 12:20:47.011: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul  8 12:20:47.011: INFO: coredns-5c7f76ccb8-mdjks from kube-system started at 2023-07-08 11:50:22 +0000 UTC (1 container statuses recorded)
  Jul  8 12:20:47.011: INFO: 	Container coredns ready: true, restart count 0
  Jul  8 12:20:47.011: INFO: kube-state-metrics-5b95b4459c-nw4hk from kube-system started at 2023-07-08 11:50:22 +0000 UTC (1 container statuses recorded)
  Jul  8 12:20:47.011: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul  8 12:20:47.011: INFO: metrics-server-v0.5.2-6cf8c8b69c-2d7pj from kube-system started at 2023-07-08 11:50:22 +0000 UTC (2 container statuses recorded)
  Jul  8 12:20:47.011: INFO: 	Container metrics-server ready: true, restart count 0
  Jul  8 12:20:47.012: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul  8 12:20:47.012: INFO: dashboard-metrics-scraper-6b8586b5c9-tjl98 from kubernetes-dashboard started at 2023-07-08 11:50:22 +0000 UTC (1 container statuses recorded)
  Jul  8 12:20:47.012: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul  8 12:20:47.012: INFO: kubernetes-dashboard-6869f4cd5f-x4wbw from kubernetes-dashboard started at 2023-07-08 11:50:22 +0000 UTC (1 container statuses recorded)
  Jul  8 12:20:47.012: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul  8 12:20:47.012: INFO: sonobuoy-systemd-logs-daemon-set-9e0399ccdc194cf6-4ll49 from sonobuoy started at 2023-07-08 12:01:20 +0000 UTC (2 container statuses recorded)
  Jul  8 12:20:47.012: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  8 12:20:47.012: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul  8 12:20:47.012: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-29-188 before test
  Jul  8 12:20:47.016: INFO: nginx-ingress-controller-kubernetes-worker-wmjdn from ingress-nginx-kubernetes-worker started at 2023-07-08 11:52:20 +0000 UTC (1 container statuses recorded)
  Jul  8 12:20:47.016: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  8 12:20:47.016: INFO: sonobuoy-e2e-job-885126f13f224642 from sonobuoy started at 2023-07-08 12:01:20 +0000 UTC (2 container statuses recorded)
  Jul  8 12:20:47.016: INFO: 	Container e2e ready: true, restart count 0
  Jul  8 12:20:47.016: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  8 12:20:47.016: INFO: sonobuoy-systemd-logs-daemon-set-9e0399ccdc194cf6-plcwc from sonobuoy started at 2023-07-08 12:01:20 +0000 UTC (2 container statuses recorded)
  Jul  8 12:20:47.016: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  8 12:20:47.016: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul  8 12:20:47.016: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-93-234 before test
  Jul  8 12:20:47.019: INFO: nginx-ingress-controller-kubernetes-worker-jdksk from ingress-nginx-kubernetes-worker started at 2023-07-08 11:56:26 +0000 UTC (1 container statuses recorded)
  Jul  8 12:20:47.020: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  8 12:20:47.020: INFO: tester from prestop-9115 started at 2023-07-08 12:20:22 +0000 UTC (1 container statuses recorded)
  Jul  8 12:20:47.020: INFO: 	Container tester ready: true, restart count 0
  Jul  8 12:20:47.020: INFO: busybox-privileged-false-e530cdde-bb14-4f75-a9cd-77f95e1210cb from security-context-test-7662 started at 2023-07-08 12:20:38 +0000 UTC (1 container statuses recorded)
  Jul  8 12:20:47.020: INFO: 	Container busybox-privileged-false-e530cdde-bb14-4f75-a9cd-77f95e1210cb ready: false, restart count 0
  Jul  8 12:20:47.020: INFO: sonobuoy from sonobuoy started at 2023-07-08 12:01:18 +0000 UTC (1 container statuses recorded)
  Jul  8 12:20:47.020: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul  8 12:20:47.020: INFO: sonobuoy-systemd-logs-daemon-set-9e0399ccdc194cf6-nrrhp from sonobuoy started at 2023-07-08 12:01:20 +0000 UTC (2 container statuses recorded)
  Jul  8 12:20:47.020: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  8 12:20:47.020: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 07/08/23 12:20:47.02
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.176fe3cb2b8b853b], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] @ 07/08/23 12:20:47.041
  E0708 12:20:47.725436      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:20:48.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-4688" for this suite. @ 07/08/23 12:20:48.045
• [1.075 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 07/08/23 12:20:48.051
  Jul  8 12:20:48.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename csiinlinevolumes @ 07/08/23 12:20:48.052
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:20:48.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:20:48.069
  STEP: creating @ 07/08/23 12:20:48.071
  STEP: getting @ 07/08/23 12:20:48.088
  STEP: listing @ 07/08/23 12:20:48.093
  STEP: deleting @ 07/08/23 12:20:48.096
  Jul  8 12:20:48.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-8910" for this suite. @ 07/08/23 12:20:48.12
• [0.079 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 07/08/23 12:20:48.13
  Jul  8 12:20:48.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename endpointslice @ 07/08/23 12:20:48.131
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:20:48.142
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:20:48.145
  E0708 12:20:48.726339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:49.726395      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:50.726470      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:51.727244      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:52.728259      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 07/08/23 12:20:53.21
  E0708 12:20:53.729191      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:54.729367      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:55.729442      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:56.729725      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:57.729914      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 07/08/23 12:20:58.216
  E0708 12:20:58.730005      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:20:59.730100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:00.730903      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:01.731287      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:02.731392      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 07/08/23 12:21:03.223
  E0708 12:21:03.732167      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:04.733103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:05.733289      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:06.734239      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:07.734409      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 07/08/23 12:21:08.231
  Jul  8 12:21:08.252: INFO: EndpointSlice for Service endpointslice-7908/example-named-port not found
  E0708 12:21:08.734908      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:09.734976      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:10.735805      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:11.736037      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:12.737092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:13.737651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:14.737761      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:15.738729      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:16.739673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:17.739766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:21:18.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7908" for this suite. @ 07/08/23 12:21:18.263
• [30.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 07/08/23 12:21:18.27
  Jul  8 12:21:18.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:21:18.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:21:18.285
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:21:18.287
  STEP: Creating configMap with name projected-configmap-test-volume-cad318c3-d59a-4021-bcb4-9cc7a16fc968 @ 07/08/23 12:21:18.289
  STEP: Creating a pod to test consume configMaps @ 07/08/23 12:21:18.292
  E0708 12:21:18.740042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:19.740117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:20.740304      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:21.741108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:21:22.314
  Jul  8 12:21:22.317: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-projected-configmaps-166260dd-5a81-4a99-b798-c2c431e236e7 container agnhost-container: <nil>
  STEP: delete the pod @ 07/08/23 12:21:22.323
  Jul  8 12:21:22.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4180" for this suite. @ 07/08/23 12:21:22.341
• [4.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 07/08/23 12:21:22.35
  Jul  8 12:21:22.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename var-expansion @ 07/08/23 12:21:22.35
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:21:22.364
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:21:22.368
  STEP: Creating a pod to test substitution in container's command @ 07/08/23 12:21:22.37
  E0708 12:21:22.741804      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:23.741890      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:24.742058      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:25.742243      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:21:26.389
  Jul  8 12:21:26.392: INFO: Trying to get logs from node ip-172-31-93-234 pod var-expansion-1b398758-5ceb-4332-a29c-0dffe633f965 container dapi-container: <nil>
  STEP: delete the pod @ 07/08/23 12:21:26.398
  Jul  8 12:21:26.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4676" for this suite. @ 07/08/23 12:21:26.415
• [4.071 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 07/08/23 12:21:26.422
  Jul  8 12:21:26.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename secrets @ 07/08/23 12:21:26.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:21:26.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:21:26.439
  STEP: Creating secret with name secret-test-dba9dd8d-d6fd-4892-ae8f-5afd299d9f5a @ 07/08/23 12:21:26.441
  STEP: Creating a pod to test consume secrets @ 07/08/23 12:21:26.444
  E0708 12:21:26.742933      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:27.743024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:28.743106      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:29.743203      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:21:30.466
  Jul  8 12:21:30.469: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-secrets-5ad90def-1ac5-4664-a67b-00b57275ebce container secret-env-test: <nil>
  STEP: delete the pod @ 07/08/23 12:21:30.474
  Jul  8 12:21:30.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7769" for this suite. @ 07/08/23 12:21:30.49
• [4.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 07/08/23 12:21:30.497
  Jul  8 12:21:30.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-runtime @ 07/08/23 12:21:30.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:21:30.512
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:21:30.514
  STEP: create the container @ 07/08/23 12:21:30.515
  W0708 12:21:30.523044      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/08/23 12:21:30.523
  E0708 12:21:30.743710      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:31.743814      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:32.744078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/08/23 12:21:33.628
  STEP: the container should be terminated @ 07/08/23 12:21:33.633
  STEP: the termination message should be set @ 07/08/23 12:21:33.633
  Jul  8 12:21:33.633: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 07/08/23 12:21:33.633
  Jul  8 12:21:33.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8104" for this suite. @ 07/08/23 12:21:33.652
• [3.163 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 07/08/23 12:21:33.66
  Jul  8 12:21:33.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename statefulset @ 07/08/23 12:21:33.661
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:21:33.674
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:21:33.678
  STEP: Creating service test in namespace statefulset-6890 @ 07/08/23 12:21:33.682
  STEP: Creating a new StatefulSet @ 07/08/23 12:21:33.688
  Jul  8 12:21:33.711: INFO: Found 0 stateful pods, waiting for 3
  E0708 12:21:33.744211      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:34.745114      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:35.745303      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:36.745633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:37.745937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:38.746025      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:39.746159      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:40.746321      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:41.746620      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:42.746775      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:21:43.714: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul  8 12:21:43.714: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul  8 12:21:43.714: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Jul  8 12:21:43.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-6890 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0708 12:21:43.746854      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:21:43.845: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  8 12:21:43.845: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  8 12:21:43.845: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0708 12:21:44.746916      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:45.746988      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:46.747249      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:47.747339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:48.747514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:49.747820      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:50.748416      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:51.749340      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:52.749431      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:53.749542      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 07/08/23 12:21:53.858
  Jul  8 12:21:53.876: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 07/08/23 12:21:53.876
  E0708 12:21:54.749643      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:55.749727      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:56.749843      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:57.750856      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:58.750948      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:21:59.751123      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:00.751213      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:01.751481      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:02.751629      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:03.751715      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 07/08/23 12:22:03.889
  Jul  8 12:22:03.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-6890 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  8 12:22:03.999: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul  8 12:22:03.999: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  8 12:22:03.999: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0708 12:22:04.752046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:05.752187      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:06.753107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:07.753293      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:08.753345      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:09.753510      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:10.753688      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:11.754068      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:12.754942      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:13.755000      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:14.755837      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:15.755946      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:16.756022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:17.757083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:18.757240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:19.757420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:20.757504      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:21.757599      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:22.757794      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:23.758623      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 07/08/23 12:22:24.019
  Jul  8 12:22:24.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-6890 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  8 12:22:24.113: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  8 12:22:24.113: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  8 12:22:24.113: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0708 12:22:24.758727      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:25.758827      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:26.758942      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:27.759024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:28.759590      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:29.759727      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:30.759790      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:31.760049      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:32.761087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:33.761890      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:22:34.145: INFO: Updating stateful set ss2
  E0708 12:22:34.762115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:35.762254      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:36.763083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:37.763251      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:38.763403      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:39.763490      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:40.763652      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:41.764023      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:42.764114      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:43.764208      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 07/08/23 12:22:44.16
  Jul  8 12:22:44.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-6890 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  8 12:22:44.263: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul  8 12:22:44.263: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  8 12:22:44.263: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0708 12:22:44.764309      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:45.765106      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:46.765324      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:47.765421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:48.765610      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:49.765699      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:50.765739      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:51.766066      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:52.767072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:53.767280      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:22:54.283: INFO: Deleting all statefulset in ns statefulset-6890
  Jul  8 12:22:54.286: INFO: Scaling statefulset ss2 to 0
  E0708 12:22:54.767444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:55.767536      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:56.768540      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:57.768619      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:58.769107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:22:59.769186      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:00.769932      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:01.770014      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:02.770270      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:03.770429      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:23:04.302: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  8 12:23:04.305: INFO: Deleting statefulset ss2
  Jul  8 12:23:04.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6890" for this suite. @ 07/08/23 12:23:04.32
• [90.668 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 07/08/23 12:23:04.328
  Jul  8 12:23:04.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename configmap @ 07/08/23 12:23:04.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:23:04.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:23:04.348
  Jul  8 12:23:04.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8679" for this suite. @ 07/08/23 12:23:04.388
• [0.064 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 07/08/23 12:23:04.393
  Jul  8 12:23:04.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir @ 07/08/23 12:23:04.393
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:23:04.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:23:04.409
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 07/08/23 12:23:04.411
  E0708 12:23:04.770919      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:05.771708      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:06.772653      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:07.772743      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:23:08.434
  Jul  8 12:23:08.437: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-087dfde2-d0ec-483c-8c6e-3c20d6d857fb container test-container: <nil>
  STEP: delete the pod @ 07/08/23 12:23:08.45
  Jul  8 12:23:08.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3863" for this suite. @ 07/08/23 12:23:08.467
• [4.080 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 07/08/23 12:23:08.473
  Jul  8 12:23:08.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:23:08.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:23:08.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:23:08.489
  STEP: Creating projection with secret that has name projected-secret-test-1a8a8247-4c32-4753-a510-8483230cd5ec @ 07/08/23 12:23:08.491
  STEP: Creating a pod to test consume secrets @ 07/08/23 12:23:08.495
  E0708 12:23:08.773587      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:09.773672      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:10.774202      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:11.774554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:23:12.513
  Jul  8 12:23:12.516: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-projected-secrets-f4e8e132-1af1-43be-a5e1-34e1f551a341 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/08/23 12:23:12.528
  Jul  8 12:23:12.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1003" for this suite. @ 07/08/23 12:23:12.547
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 07/08/23 12:23:12.553
  Jul  8 12:23:12.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename field-validation @ 07/08/23 12:23:12.554
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:23:12.566
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:23:12.568
  Jul  8 12:23:12.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 12:23:12.774539      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:13.774647      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:14.774738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0708 12:23:15.118893      20 warnings.go:70] unknown field "alpha"
  W0708 12:23:15.118910      20 warnings.go:70] unknown field "beta"
  W0708 12:23:15.118914      20 warnings.go:70] unknown field "delta"
  W0708 12:23:15.118917      20 warnings.go:70] unknown field "epsilon"
  W0708 12:23:15.118921      20 warnings.go:70] unknown field "gamma"
  Jul  8 12:23:15.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8934" for this suite. @ 07/08/23 12:23:15.152
• [2.604 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 07/08/23 12:23:15.159
  Jul  8 12:23:15.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename subpath @ 07/08/23 12:23:15.159
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:23:15.175
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:23:15.177
  STEP: Setting up data @ 07/08/23 12:23:15.179
  STEP: Creating pod pod-subpath-test-downwardapi-d6sf @ 07/08/23 12:23:15.189
  STEP: Creating a pod to test atomic-volume-subpath @ 07/08/23 12:23:15.189
  E0708 12:23:15.774804      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:16.775814      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:17.776042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:18.777104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:19.778020      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:20.778964      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:21.779886      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:22.780030      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:23.780111      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:24.781096      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:25.781801      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:26.782013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:27.782105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:28.782300      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:29.782989      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:30.783166      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:31.783341      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:32.783406      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:33.784111      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:34.785130      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:35.785205      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:36.785488      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:37.785578      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:38.785735      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:23:39.25
  Jul  8 12:23:39.253: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-subpath-test-downwardapi-d6sf container test-container-subpath-downwardapi-d6sf: <nil>
  STEP: delete the pod @ 07/08/23 12:23:39.26
  STEP: Deleting pod pod-subpath-test-downwardapi-d6sf @ 07/08/23 12:23:39.273
  Jul  8 12:23:39.273: INFO: Deleting pod "pod-subpath-test-downwardapi-d6sf" in namespace "subpath-7745"
  Jul  8 12:23:39.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7745" for this suite. @ 07/08/23 12:23:39.279
• [24.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 07/08/23 12:23:39.285
  Jul  8 12:23:39.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 07/08/23 12:23:39.286
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:23:39.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:23:39.307
  STEP: creating a target pod @ 07/08/23 12:23:39.309
  E0708 12:23:39.785915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:40.786134      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 07/08/23 12:23:41.324
  E0708 12:23:41.786741      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:42.786859      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 07/08/23 12:23:43.34
  Jul  8 12:23:43.340: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6748 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 12:23:43.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 12:23:43.340: INFO: ExecWithOptions: Clientset creation
  Jul  8 12:23:43.340: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-6748/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Jul  8 12:23:43.400: INFO: Exec stderr: ""
  Jul  8 12:23:43.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-6748" for this suite. @ 07/08/23 12:23:43.409
• [4.134 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 07/08/23 12:23:43.419
  Jul  8 12:23:43.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir @ 07/08/23 12:23:43.419
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:23:43.432
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:23:43.436
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 07/08/23 12:23:43.438
  E0708 12:23:43.786942      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:44.787100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:45.787953      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:46.788039      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:23:47.458
  Jul  8 12:23:47.461: INFO: Trying to get logs from node ip-172-31-29-188 pod pod-3f04c100-4229-4915-8e95-c6b9d61e86d6 container test-container: <nil>
  STEP: delete the pod @ 07/08/23 12:23:47.474
  Jul  8 12:23:47.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4254" for this suite. @ 07/08/23 12:23:47.491
• [4.079 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 07/08/23 12:23:47.498
  Jul  8 12:23:47.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename downward-api @ 07/08/23 12:23:47.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:23:47.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:23:47.516
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 12:23:47.518
  E0708 12:23:47.788701      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:48.788825      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:23:49.536
  Jul  8 12:23:49.539: INFO: Trying to get logs from node ip-172-31-29-188 pod downwardapi-volume-e52c7aab-0443-43e2-8e98-a547eddfc81f container client-container: <nil>
  STEP: delete the pod @ 07/08/23 12:23:49.545
  Jul  8 12:23:49.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3086" for this suite. @ 07/08/23 12:23:49.56
• [2.068 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 07/08/23 12:23:49.566
  Jul  8 12:23:49.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:23:49.567
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:23:49.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:23:49.584
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 12:23:49.586
  E0708 12:23:49.788887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:50.788984      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:51.789362      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:52.789456      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:23:53.608
  Jul  8 12:23:53.611: INFO: Trying to get logs from node ip-172-31-29-188 pod downwardapi-volume-eb6dd5f6-9679-4be5-ab02-3f958ab188c6 container client-container: <nil>
  STEP: delete the pod @ 07/08/23 12:23:53.617
  Jul  8 12:23:53.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1522" for this suite. @ 07/08/23 12:23:53.636
• [4.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 07/08/23 12:23:53.642
  Jul  8 12:23:53.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 12:23:53.642
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:23:53.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:23:53.661
  STEP: Setting up server cert @ 07/08/23 12:23:53.678
  E0708 12:23:53.789853      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 12:23:53.934
  STEP: Deploying the webhook pod @ 07/08/23 12:23:53.942
  STEP: Wait for the deployment to be ready @ 07/08/23 12:23:54.035
  Jul  8 12:23:54.042: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0708 12:23:54.789980      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:55.790072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 12:23:56.053
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 12:23:56.061
  E0708 12:23:56.790116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:23:57.061: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 07/08/23 12:23:57.064
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 07/08/23 12:23:57.079
  STEP: Creating a configMap that should not be mutated @ 07/08/23 12:23:57.086
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 07/08/23 12:23:57.096
  STEP: Creating a configMap that should be mutated @ 07/08/23 12:23:57.102
  Jul  8 12:23:57.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5483" for this suite. @ 07/08/23 12:23:57.161
  STEP: Destroying namespace "webhook-markers-3003" for this suite. @ 07/08/23 12:23:57.167
• [3.531 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 07/08/23 12:23:57.175
  Jul  8 12:23:57.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename init-container @ 07/08/23 12:23:57.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:23:57.194
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:23:57.199
  STEP: creating the pod @ 07/08/23 12:23:57.202
  Jul  8 12:23:57.202: INFO: PodSpec: initContainers in spec.initContainers
  E0708 12:23:57.791207      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:58.791986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:23:59.792042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:00.793100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:01.793498      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:02.793654      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:03.793818      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:04.793962      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:05.794045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:06.794307      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:07.794404      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:08.795206      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:09.796131      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:10.796218      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:11.796326      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:12.797103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:13.797461      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:14.797552      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:15.797657      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:16.798476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:17.798574      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:18.799295      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:19.799448      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:20.799612      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:21.800450      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:22.800628      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:23.801556      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:24.801729      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:25.802703      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:26.802889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:27.802992      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:28.803318      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:29.803471      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:30.804425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:31.804732      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:32.804836      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:33.804913      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:34.805163      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:35.805267      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:36.805821      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:24:37.341: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-f261c2fb-46e2-48cb-a91f-5d90793f1fcb", GenerateName:"", Namespace:"init-container-1962", SelfLink:"", UID:"3aea4cee-3f43-4f34-920b-94282fd7e3b6", ResourceVersion:"14274", Generation:0, CreationTimestamp:time.Date(2023, time.July, 8, 12, 23, 57, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"202299276"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 8, 12, 23, 57, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000412120), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 8, 12, 24, 37, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000412240), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-qmddj", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004774020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qmddj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qmddj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qmddj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005308368), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-29-188", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00015c000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0053083f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005308410)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005308418), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00530841c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000db8190), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 8, 12, 23, 57, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 8, 12, 23, 57, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 8, 12, 23, 57, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 8, 12, 23, 57, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.29.188", PodIP:"192.168.164.123", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.164.123"}}, StartTime:time.Date(2023, time.July, 8, 12, 23, 57, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00015c0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00015c150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://52517a0218e0b94072e7361674177122e5ff4bc4bf48ac0d37f9c4bbbba24a26", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0047740a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004774080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc005308494), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Jul  8 12:24:37.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1962" for this suite. @ 07/08/23 12:24:37.345
• [40.177 seconds]
------------------------------
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 07/08/23 12:24:37.352
  Jul  8 12:24:37.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename sysctl @ 07/08/23 12:24:37.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:24:37.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:24:37.37
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 07/08/23 12:24:37.372
  STEP: Watching for error events or started pod @ 07/08/23 12:24:37.38
  E0708 12:24:37.806576      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:38.806677      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 07/08/23 12:24:39.383
  E0708 12:24:39.806752      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:40.806881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 07/08/23 12:24:41.393
  STEP: Getting logs from the pod @ 07/08/23 12:24:41.393
  STEP: Checking that the sysctl is actually updated @ 07/08/23 12:24:41.4
  Jul  8 12:24:41.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-8529" for this suite. @ 07/08/23 12:24:41.402
• [4.055 seconds]
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 07/08/23 12:24:41.408
  Jul  8 12:24:41.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename var-expansion @ 07/08/23 12:24:41.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:24:41.421
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:24:41.428
  E0708 12:24:41.807561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:42.807660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:24:43.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  8 12:24:43.447: INFO: Deleting pod "var-expansion-ce10ec84-a752-4c26-a435-55a5d2599d16" in namespace "var-expansion-3094"
  Jul  8 12:24:43.455: INFO: Wait up to 5m0s for pod "var-expansion-ce10ec84-a752-4c26-a435-55a5d2599d16" to be fully deleted
  E0708 12:24:43.808647      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:44.809488      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-3094" for this suite. @ 07/08/23 12:24:45.46
• [4.058 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 07/08/23 12:24:45.466
  Jul  8 12:24:45.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 12:24:45.467
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:24:45.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:24:45.482
  STEP: Setting up server cert @ 07/08/23 12:24:45.502
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 12:24:45.742
  STEP: Deploying the webhook pod @ 07/08/23 12:24:45.749
  STEP: Wait for the deployment to be ready @ 07/08/23 12:24:45.76
  Jul  8 12:24:45.772: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0708 12:24:45.810204      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:46.810885      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 12:24:47.78
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 12:24:47.789
  E0708 12:24:47.811829      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:24:48.789: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 07/08/23 12:24:48.792
  STEP: create a configmap that should be updated by the webhook @ 07/08/23 12:24:48.804
  E0708 12:24:48.812510      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:24:48.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5706" for this suite. @ 07/08/23 12:24:48.868
  STEP: Destroying namespace "webhook-markers-7042" for this suite. @ 07/08/23 12:24:48.875
• [3.415 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 07/08/23 12:24:48.882
  Jul  8 12:24:48.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl @ 07/08/23 12:24:48.882
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:24:48.895
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:24:48.897
  STEP: creating all guestbook components @ 07/08/23 12:24:48.9
  Jul  8 12:24:48.900: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Jul  8 12:24:48.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-2216 create -f -'
  Jul  8 12:24:49.299: INFO: stderr: ""
  Jul  8 12:24:49.299: INFO: stdout: "service/agnhost-replica created\n"
  Jul  8 12:24:49.299: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Jul  8 12:24:49.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-2216 create -f -'
  E0708 12:24:49.813101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:24:49.899: INFO: stderr: ""
  Jul  8 12:24:49.899: INFO: stdout: "service/agnhost-primary created\n"
  Jul  8 12:24:49.899: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Jul  8 12:24:49.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-2216 create -f -'
  Jul  8 12:24:50.179: INFO: stderr: ""
  Jul  8 12:24:50.179: INFO: stdout: "service/frontend created\n"
  Jul  8 12:24:50.179: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Jul  8 12:24:50.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-2216 create -f -'
  Jul  8 12:24:50.433: INFO: stderr: ""
  Jul  8 12:24:50.433: INFO: stdout: "deployment.apps/frontend created\n"
  Jul  8 12:24:50.433: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jul  8 12:24:50.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-2216 create -f -'
  Jul  8 12:24:50.688: INFO: stderr: ""
  Jul  8 12:24:50.688: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Jul  8 12:24:50.688: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jul  8 12:24:50.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-2216 create -f -'
  E0708 12:24:50.813769      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:24:50.951: INFO: stderr: ""
  Jul  8 12:24:50.951: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 07/08/23 12:24:50.951
  Jul  8 12:24:50.951: INFO: Waiting for all frontend pods to be Running.
  E0708 12:24:51.813865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:52.813924      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:53.814028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:54.814091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:55.814241      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:24:56.002: INFO: Waiting for frontend to serve content.
  Jul  8 12:24:56.013: INFO: Trying to add a new entry to the guestbook.
  Jul  8 12:24:56.022: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 07/08/23 12:24:56.03
  Jul  8 12:24:56.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-2216 delete --grace-period=0 --force -f -'
  Jul  8 12:24:56.092: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  8 12:24:56.092: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 07/08/23 12:24:56.093
  Jul  8 12:24:56.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-2216 delete --grace-period=0 --force -f -'
  Jul  8 12:24:56.159: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  8 12:24:56.159: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 07/08/23 12:24:56.159
  Jul  8 12:24:56.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-2216 delete --grace-period=0 --force -f -'
  Jul  8 12:24:56.215: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  8 12:24:56.215: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 07/08/23 12:24:56.215
  Jul  8 12:24:56.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-2216 delete --grace-period=0 --force -f -'
  Jul  8 12:24:56.262: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  8 12:24:56.262: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 07/08/23 12:24:56.262
  Jul  8 12:24:56.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-2216 delete --grace-period=0 --force -f -'
  Jul  8 12:24:56.329: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  8 12:24:56.329: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 07/08/23 12:24:56.329
  Jul  8 12:24:56.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-2216 delete --grace-period=0 --force -f -'
  Jul  8 12:24:56.380: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  8 12:24:56.380: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Jul  8 12:24:56.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2216" for this suite. @ 07/08/23 12:24:56.383
• [7.508 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 07/08/23 12:24:56.39
  Jul  8 12:24:56.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename configmap @ 07/08/23 12:24:56.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:24:56.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:24:56.41
  STEP: Creating configMap with name configmap-test-volume-map-0315d3f8-b58a-46df-b4b7-0b5287bd16a7 @ 07/08/23 12:24:56.413
  STEP: Creating a pod to test consume configMaps @ 07/08/23 12:24:56.416
  E0708 12:24:56.814368      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:57.814496      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:58.814790      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:24:59.815111      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:25:00.436
  Jul  8 12:25:00.440: INFO: Trying to get logs from node ip-172-31-29-188 pod pod-configmaps-35a83af7-1aa8-4771-8075-18d6f1623a71 container agnhost-container: <nil>
  STEP: delete the pod @ 07/08/23 12:25:00.446
  Jul  8 12:25:00.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1116" for this suite. @ 07/08/23 12:25:00.461
• [4.075 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 07/08/23 12:25:00.465
  Jul  8 12:25:00.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename sched-pred @ 07/08/23 12:25:00.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:25:00.478
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:25:00.48
  Jul  8 12:25:00.483: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul  8 12:25:00.490: INFO: Waiting for terminating namespaces to be deleted...
  Jul  8 12:25:00.492: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-12-67 before test
  Jul  8 12:25:00.498: INFO: default-http-backend-kubernetes-worker-65fc475d49-8npt2 from ingress-nginx-kubernetes-worker started at 2023-07-08 11:50:26 +0000 UTC (1 container statuses recorded)
  Jul  8 12:25:00.498: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul  8 12:25:00.498: INFO: nginx-ingress-controller-kubernetes-worker-lsvk7 from ingress-nginx-kubernetes-worker started at 2023-07-08 11:50:26 +0000 UTC (1 container statuses recorded)
  Jul  8 12:25:00.498: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  8 12:25:00.498: INFO: calico-kube-controllers-bb564cc5-vpxwc from kube-system started at 2023-07-08 11:50:30 +0000 UTC (1 container statuses recorded)
  Jul  8 12:25:00.498: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul  8 12:25:00.498: INFO: coredns-5c7f76ccb8-mdjks from kube-system started at 2023-07-08 11:50:22 +0000 UTC (1 container statuses recorded)
  Jul  8 12:25:00.498: INFO: 	Container coredns ready: true, restart count 0
  Jul  8 12:25:00.498: INFO: kube-state-metrics-5b95b4459c-nw4hk from kube-system started at 2023-07-08 11:50:22 +0000 UTC (1 container statuses recorded)
  Jul  8 12:25:00.498: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul  8 12:25:00.498: INFO: metrics-server-v0.5.2-6cf8c8b69c-2d7pj from kube-system started at 2023-07-08 11:50:22 +0000 UTC (2 container statuses recorded)
  Jul  8 12:25:00.498: INFO: 	Container metrics-server ready: true, restart count 0
  Jul  8 12:25:00.498: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul  8 12:25:00.498: INFO: dashboard-metrics-scraper-6b8586b5c9-tjl98 from kubernetes-dashboard started at 2023-07-08 11:50:22 +0000 UTC (1 container statuses recorded)
  Jul  8 12:25:00.498: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul  8 12:25:00.498: INFO: kubernetes-dashboard-6869f4cd5f-x4wbw from kubernetes-dashboard started at 2023-07-08 11:50:22 +0000 UTC (1 container statuses recorded)
  Jul  8 12:25:00.498: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul  8 12:25:00.498: INFO: sonobuoy-systemd-logs-daemon-set-9e0399ccdc194cf6-4ll49 from sonobuoy started at 2023-07-08 12:01:20 +0000 UTC (2 container statuses recorded)
  Jul  8 12:25:00.498: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  8 12:25:00.498: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul  8 12:25:00.498: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-29-188 before test
  Jul  8 12:25:00.503: INFO: nginx-ingress-controller-kubernetes-worker-wmjdn from ingress-nginx-kubernetes-worker started at 2023-07-08 11:52:20 +0000 UTC (1 container statuses recorded)
  Jul  8 12:25:00.503: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  8 12:25:00.503: INFO: sonobuoy-e2e-job-885126f13f224642 from sonobuoy started at 2023-07-08 12:01:20 +0000 UTC (2 container statuses recorded)
  Jul  8 12:25:00.503: INFO: 	Container e2e ready: true, restart count 0
  Jul  8 12:25:00.503: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  8 12:25:00.503: INFO: sonobuoy-systemd-logs-daemon-set-9e0399ccdc194cf6-plcwc from sonobuoy started at 2023-07-08 12:01:20 +0000 UTC (2 container statuses recorded)
  Jul  8 12:25:00.503: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  8 12:25:00.503: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul  8 12:25:00.503: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-93-234 before test
  Jul  8 12:25:00.506: INFO: nginx-ingress-controller-kubernetes-worker-jdksk from ingress-nginx-kubernetes-worker started at 2023-07-08 11:56:26 +0000 UTC (1 container statuses recorded)
  Jul  8 12:25:00.506: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  8 12:25:00.506: INFO: sonobuoy from sonobuoy started at 2023-07-08 12:01:18 +0000 UTC (1 container statuses recorded)
  Jul  8 12:25:00.506: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul  8 12:25:00.506: INFO: sonobuoy-systemd-logs-daemon-set-9e0399ccdc194cf6-nrrhp from sonobuoy started at 2023-07-08 12:01:20 +0000 UTC (2 container statuses recorded)
  Jul  8 12:25:00.506: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  8 12:25:00.506: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ip-172-31-12-67 @ 07/08/23 12:25:00.518
  STEP: verifying the node has the label node ip-172-31-29-188 @ 07/08/23 12:25:00.53
  STEP: verifying the node has the label node ip-172-31-93-234 @ 07/08/23 12:25:00.544
  Jul  8 12:25:00.553: INFO: Pod default-http-backend-kubernetes-worker-65fc475d49-8npt2 requesting resource cpu=10m on Node ip-172-31-12-67
  Jul  8 12:25:00.553: INFO: Pod nginx-ingress-controller-kubernetes-worker-jdksk requesting resource cpu=0m on Node ip-172-31-93-234
  Jul  8 12:25:00.553: INFO: Pod nginx-ingress-controller-kubernetes-worker-lsvk7 requesting resource cpu=0m on Node ip-172-31-12-67
  Jul  8 12:25:00.553: INFO: Pod nginx-ingress-controller-kubernetes-worker-wmjdn requesting resource cpu=0m on Node ip-172-31-29-188
  Jul  8 12:25:00.553: INFO: Pod calico-kube-controllers-bb564cc5-vpxwc requesting resource cpu=0m on Node ip-172-31-12-67
  Jul  8 12:25:00.553: INFO: Pod coredns-5c7f76ccb8-mdjks requesting resource cpu=100m on Node ip-172-31-12-67
  Jul  8 12:25:00.553: INFO: Pod kube-state-metrics-5b95b4459c-nw4hk requesting resource cpu=0m on Node ip-172-31-12-67
  Jul  8 12:25:00.553: INFO: Pod metrics-server-v0.5.2-6cf8c8b69c-2d7pj requesting resource cpu=5m on Node ip-172-31-12-67
  Jul  8 12:25:00.553: INFO: Pod dashboard-metrics-scraper-6b8586b5c9-tjl98 requesting resource cpu=0m on Node ip-172-31-12-67
  Jul  8 12:25:00.553: INFO: Pod kubernetes-dashboard-6869f4cd5f-x4wbw requesting resource cpu=0m on Node ip-172-31-12-67
  Jul  8 12:25:00.554: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-93-234
  Jul  8 12:25:00.554: INFO: Pod sonobuoy-e2e-job-885126f13f224642 requesting resource cpu=0m on Node ip-172-31-29-188
  Jul  8 12:25:00.554: INFO: Pod sonobuoy-systemd-logs-daemon-set-9e0399ccdc194cf6-4ll49 requesting resource cpu=0m on Node ip-172-31-12-67
  Jul  8 12:25:00.554: INFO: Pod sonobuoy-systemd-logs-daemon-set-9e0399ccdc194cf6-nrrhp requesting resource cpu=0m on Node ip-172-31-93-234
  Jul  8 12:25:00.554: INFO: Pod sonobuoy-systemd-logs-daemon-set-9e0399ccdc194cf6-plcwc requesting resource cpu=0m on Node ip-172-31-29-188
  STEP: Starting Pods to consume most of the cluster CPU. @ 07/08/23 12:25:00.554
  Jul  8 12:25:00.554: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-29-188
  Jul  8 12:25:00.560: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-93-234
  Jul  8 12:25:00.567: INFO: Creating a pod which consumes cpu=1319m on Node ip-172-31-12-67
  E0708 12:25:00.816028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:01.816133      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 07/08/23 12:25:02.591
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-3df27e0d-ca62-4420-a71a-db8d6f10989b.176fe4063338b843], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9156/filler-pod-3df27e0d-ca62-4420-a71a-db8d6f10989b to ip-172-31-29-188] @ 07/08/23 12:25:02.594
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-3df27e0d-ca62-4420-a71a-db8d6f10989b.176fe40653cdf407], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/08/23 12:25:02.594
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-3df27e0d-ca62-4420-a71a-db8d6f10989b.176fe40654c30fca], Reason = [Created], Message = [Created container filler-pod-3df27e0d-ca62-4420-a71a-db8d6f10989b] @ 07/08/23 12:25:02.594
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-3df27e0d-ca62-4420-a71a-db8d6f10989b.176fe40657e0316a], Reason = [Started], Message = [Started container filler-pod-3df27e0d-ca62-4420-a71a-db8d6f10989b] @ 07/08/23 12:25:02.594
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-9656e805-d3d8-49f6-8bd6-bfaeb038b602.176fe40634007d12], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9156/filler-pod-9656e805-d3d8-49f6-8bd6-bfaeb038b602 to ip-172-31-12-67] @ 07/08/23 12:25:02.594
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-9656e805-d3d8-49f6-8bd6-bfaeb038b602.176fe4065682782c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/08/23 12:25:02.594
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-9656e805-d3d8-49f6-8bd6-bfaeb038b602.176fe40657c77a1b], Reason = [Created], Message = [Created container filler-pod-9656e805-d3d8-49f6-8bd6-bfaeb038b602] @ 07/08/23 12:25:02.594
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-9656e805-d3d8-49f6-8bd6-bfaeb038b602.176fe4065ad69a85], Reason = [Started], Message = [Started container filler-pod-9656e805-d3d8-49f6-8bd6-bfaeb038b602] @ 07/08/23 12:25:02.594
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-997abc27-dbcc-4f2d-bbd9-ec270da8709c.176fe40633859229], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9156/filler-pod-997abc27-dbcc-4f2d-bbd9-ec270da8709c to ip-172-31-93-234] @ 07/08/23 12:25:02.594
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-997abc27-dbcc-4f2d-bbd9-ec270da8709c.176fe40654b7f38e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/08/23 12:25:02.594
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-997abc27-dbcc-4f2d-bbd9-ec270da8709c.176fe40655bf59f3], Reason = [Created], Message = [Created container filler-pod-997abc27-dbcc-4f2d-bbd9-ec270da8709c] @ 07/08/23 12:25:02.594
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-997abc27-dbcc-4f2d-bbd9-ec270da8709c.176fe406591a246f], Reason = [Started], Message = [Started container filler-pod-997abc27-dbcc-4f2d-bbd9-ec270da8709c] @ 07/08/23 12:25:02.594
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.176fe406ac58ec72], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/5 nodes are available: 2 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] @ 07/08/23 12:25:02.605
  E0708 12:25:02.816215      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node ip-172-31-12-67 @ 07/08/23 12:25:03.604
  STEP: verifying the node doesn't have the label node @ 07/08/23 12:25:03.614
  STEP: removing the label node off the node ip-172-31-29-188 @ 07/08/23 12:25:03.617
  STEP: verifying the node doesn't have the label node @ 07/08/23 12:25:03.628
  STEP: removing the label node off the node ip-172-31-93-234 @ 07/08/23 12:25:03.632
  STEP: verifying the node doesn't have the label node @ 07/08/23 12:25:03.642
  Jul  8 12:25:03.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9156" for this suite. @ 07/08/23 12:25:03.649
• [3.190 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 07/08/23 12:25:03.657
  Jul  8 12:25:03.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename podtemplate @ 07/08/23 12:25:03.658
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:25:03.67
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:25:03.672
  STEP: Create a pod template @ 07/08/23 12:25:03.674
  STEP: Replace a pod template @ 07/08/23 12:25:03.681
  Jul  8 12:25:03.687: INFO: Found updated podtemplate annotation: "true"

  Jul  8 12:25:03.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-2028" for this suite. @ 07/08/23 12:25:03.689
• [0.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 07/08/23 12:25:03.697
  Jul  8 12:25:03.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename runtimeclass @ 07/08/23 12:25:03.698
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:25:03.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:25:03.71
  Jul  8 12:25:03.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3297" for this suite. @ 07/08/23 12:25:03.722
• [0.032 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 07/08/23 12:25:03.73
  Jul  8 12:25:03.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename deployment @ 07/08/23 12:25:03.731
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:25:03.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:25:03.746
  STEP: creating a Deployment @ 07/08/23 12:25:03.754
  STEP: waiting for Deployment to be created @ 07/08/23 12:25:03.76
  STEP: waiting for all Replicas to be Ready @ 07/08/23 12:25:03.761
  Jul  8 12:25:03.762: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul  8 12:25:03.762: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul  8 12:25:03.767: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul  8 12:25:03.767: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul  8 12:25:03.781: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul  8 12:25:03.781: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul  8 12:25:03.804: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul  8 12:25:03.804: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0708 12:25:03.816551      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:04.816845      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:25:04.875: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jul  8 12:25:04.875: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jul  8 12:25:05.495: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 07/08/23 12:25:05.495
  W0708 12:25:05.505939      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul  8 12:25:05.507: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 07/08/23 12:25:05.507
  Jul  8 12:25:05.509: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 0
  Jul  8 12:25:05.509: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 0
  Jul  8 12:25:05.509: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 0
  Jul  8 12:25:05.509: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 0
  Jul  8 12:25:05.509: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 0
  Jul  8 12:25:05.509: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 0
  Jul  8 12:25:05.509: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 0
  Jul  8 12:25:05.509: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 0
  Jul  8 12:25:05.509: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1
  Jul  8 12:25:05.509: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1
  Jul  8 12:25:05.509: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2
  Jul  8 12:25:05.509: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2
  Jul  8 12:25:05.509: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2
  Jul  8 12:25:05.509: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2
  Jul  8 12:25:05.516: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2
  Jul  8 12:25:05.516: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2
  Jul  8 12:25:05.532: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2
  Jul  8 12:25:05.532: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2
  Jul  8 12:25:05.549: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2
  Jul  8 12:25:05.549: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2
  Jul  8 12:25:05.556: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1
  Jul  8 12:25:05.556: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1
  E0708 12:25:05.816928      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:06.817025      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:25:06.885: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2
  Jul  8 12:25:06.885: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2
  Jul  8 12:25:06.913: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1
  STEP: listing Deployments @ 07/08/23 12:25:06.913
  Jul  8 12:25:06.915: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 07/08/23 12:25:06.915
  Jul  8 12:25:06.924: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 07/08/23 12:25:06.924
  Jul  8 12:25:06.938: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul  8 12:25:06.940: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul  8 12:25:06.961: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul  8 12:25:06.986: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul  8 12:25:06.992: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0708 12:25:07.817651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:25:07.895: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul  8 12:25:07.911: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul  8 12:25:07.918: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul  8 12:25:07.929: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0708 12:25:08.818322      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:25:09.482: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 07/08/23 12:25:09.499
  STEP: fetching the DeploymentStatus @ 07/08/23 12:25:09.505
  Jul  8 12:25:09.510: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1
  Jul  8 12:25:09.510: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1
  Jul  8 12:25:09.510: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1
  Jul  8 12:25:09.510: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1
  Jul  8 12:25:09.511: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 1
  Jul  8 12:25:09.511: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2
  Jul  8 12:25:09.511: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2
  Jul  8 12:25:09.511: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2
  Jul  8 12:25:09.511: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 2
  Jul  8 12:25:09.511: INFO: observed Deployment test-deployment in namespace deployment-1178 with ReadyReplicas 3
  STEP: deleting the Deployment @ 07/08/23 12:25:09.511
  Jul  8 12:25:09.521: INFO: observed event type MODIFIED
  Jul  8 12:25:09.521: INFO: observed event type MODIFIED
  Jul  8 12:25:09.522: INFO: observed event type MODIFIED
  Jul  8 12:25:09.522: INFO: observed event type MODIFIED
  Jul  8 12:25:09.522: INFO: observed event type MODIFIED
  Jul  8 12:25:09.522: INFO: observed event type MODIFIED
  Jul  8 12:25:09.522: INFO: observed event type MODIFIED
  Jul  8 12:25:09.522: INFO: observed event type MODIFIED
  Jul  8 12:25:09.522: INFO: observed event type MODIFIED
  Jul  8 12:25:09.522: INFO: observed event type MODIFIED
  Jul  8 12:25:09.522: INFO: observed event type MODIFIED
  Jul  8 12:25:09.526: INFO: Log out all the ReplicaSets if there is no deployment created
  Jul  8 12:25:09.529: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-1178  74175d25-81c6-4eb9-913c-42a33ae6b582 14934 3 2023-07-08 12:25:03 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment d3904039-3c88-41de-b162-977f09e515e7 0xc00541bd67 0xc00541bd68}] [] [{kube-controller-manager Update apps/v1 2023-07-08 12:25:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3904039-3c88-41de-b162-977f09e515e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 12:25:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00541bdf0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jul  8 12:25:09.533: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-1178  df01da69-94a2-4f24-8981-f4036a778157 15086 4 2023-07-08 12:25:05 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment d3904039-3c88-41de-b162-977f09e515e7 0xc00541be57 0xc00541be58}] [] [{kube-controller-manager Update apps/v1 2023-07-08 12:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3904039-3c88-41de-b162-977f09e515e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 12:25:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00541bee0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jul  8 12:25:09.536: INFO: pod: "test-deployment-5b5dcbcd95-mcp7r":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-mcp7r test-deployment-5b5dcbcd95- deployment-1178  efda9558-fdaa-4079-a5a8-eef1684f2d3a 15079 0 2023-07-08 12:25:05 +0000 UTC 2023-07-08 12:25:10 +0000 UTC 0xc005395230 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 df01da69-94a2-4f24-8981-f4036a778157 0xc005395267 0xc005395268}] [] [{kube-controller-manager Update v1 2023-07-08 12:25:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df01da69-94a2-4f24-8981-f4036a778157\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 12:25:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f6fc4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f6fc4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-93-234,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 12:25:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 12:25:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 12:25:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 12:25:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.93.234,PodIP:192.168.7.245,StartTime:2023-07-08 12:25:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-08 12:25:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://86f601f8ca06bdebcad289d577bd269dfb76ec041e7d3e3f1d346534413957e4,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.245,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul  8 12:25:09.537: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-1178  40dc83d3-3289-425f-9a86-3d18728efcac 15076 2 2023-07-08 12:25:06 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment d3904039-3c88-41de-b162-977f09e515e7 0xc00541bf47 0xc00541bf48}] [] [{kube-controller-manager Update apps/v1 2023-07-08 12:25:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3904039-3c88-41de-b162-977f09e515e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 12:25:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00541bfd0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Jul  8 12:25:09.540: INFO: pod: "test-deployment-6fc78d85c6-dps42":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-dps42 test-deployment-6fc78d85c6- deployment-1178  1b38913e-8184-4b7d-ac33-36993f1c3fc1 15075 0 2023-07-08 12:25:07 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 40dc83d3-3289-425f-9a86-3d18728efcac 0xc0052604e7 0xc0052604e8}] [] [{kube-controller-manager Update v1 2023-07-08 12:25:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40dc83d3-3289-425f-9a86-3d18728efcac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 12:25:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.164.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8mzmq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8mzmq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-29-188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 12:25:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 12:25:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 12:25:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 12:25:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.29.188,PodIP:192.168.164.68,StartTime:2023-07-08 12:25:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-08 12:25:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7ba33217729ed26fc20fe27eafedb2d2f55e99032c130d11e58ef391cf7f56a5,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.164.68,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul  8 12:25:09.540: INFO: pod: "test-deployment-6fc78d85c6-gzxd8":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-gzxd8 test-deployment-6fc78d85c6- deployment-1178  766b87a0-c506-48b5-96a8-e9c9a613d939 14982 0 2023-07-08 12:25:06 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 40dc83d3-3289-425f-9a86-3d18728efcac 0xc0052606d7 0xc0052606d8}] [] [{kube-controller-manager Update v1 2023-07-08 12:25:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40dc83d3-3289-425f-9a86-3d18728efcac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 12:25:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5bsfm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5bsfm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-93-234,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 12:25:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 12:25:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 12:25:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 12:25:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.93.234,PodIP:192.168.7.204,StartTime:2023-07-08 12:25:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-08 12:25:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ab1ddf3523fe65a83347c9e8a0d6660214d81159d1694ddd1c431b8831d6c2af,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.204,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul  8 12:25:09.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1178" for this suite. @ 07/08/23 12:25:09.545
• [5.824 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 07/08/23 12:25:09.555
  Jul  8 12:25:09.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:25:09.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:25:09.569
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:25:09.571
  STEP: Creating configMap with name projected-configmap-test-volume-map-c5760630-89b1-458d-8ea9-bf1c0dc99d47 @ 07/08/23 12:25:09.573
  STEP: Creating a pod to test consume configMaps @ 07/08/23 12:25:09.578
  E0708 12:25:09.819329      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:10.819368      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:11.820049      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:12.821095      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:25:13.597
  Jul  8 12:25:13.600: INFO: Trying to get logs from node ip-172-31-29-188 pod pod-projected-configmaps-5e9b8448-bf62-4355-b6bc-c3ccb93f1988 container agnhost-container: <nil>
  STEP: delete the pod @ 07/08/23 12:25:13.607
  Jul  8 12:25:13.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5984" for this suite. @ 07/08/23 12:25:13.625
• [4.075 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 07/08/23 12:25:13.63
  Jul  8 12:25:13.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir @ 07/08/23 12:25:13.631
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:25:13.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:25:13.647
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 07/08/23 12:25:13.649
  E0708 12:25:13.821196      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:14.821388      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:15.822396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:16.823280      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:25:17.67
  Jul  8 12:25:17.673: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-25417ce5-c163-4204-b221-b35aa003c50a container test-container: <nil>
  STEP: delete the pod @ 07/08/23 12:25:17.68
  Jul  8 12:25:17.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-131" for this suite. @ 07/08/23 12:25:17.698
• [4.073 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 07/08/23 12:25:17.703
  Jul  8 12:25:17.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/08/23 12:25:17.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:25:17.718
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:25:17.722
  STEP: fetching the /apis discovery document @ 07/08/23 12:25:17.724
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 07/08/23 12:25:17.724
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 07/08/23 12:25:17.724
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 07/08/23 12:25:17.725
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 07/08/23 12:25:17.726
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 07/08/23 12:25:17.726
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 07/08/23 12:25:17.726
  Jul  8 12:25:17.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1921" for this suite. @ 07/08/23 12:25:17.73
• [0.032 seconds]
------------------------------
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 07/08/23 12:25:17.735
  Jul  8 12:25:17.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename taint-single-pod @ 07/08/23 12:25:17.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:25:17.752
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:25:17.754
  Jul  8 12:25:17.757: INFO: Waiting up to 1m0s for all nodes to be ready
  E0708 12:25:17.824204      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:18.824319      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:19.825130      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:20.825321      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:21.826240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:22.827082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:23.828000      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:24.828056      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:25.828282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:26.829188      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:27.829560      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:28.830594      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:29.830641      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:30.830902      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:31.831399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:32.831534      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:33.831716      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:34.831966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:35.832837      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:36.833911      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:37.834492      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:38.834660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:39.835381      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:40.835516      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:41.836220      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:42.836365      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:43.836446      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:44.836523      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:45.836813      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:46.836995      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:47.837835      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:48.837947      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:49.838958      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:50.839103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:51.839655      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:52.839799      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:53.840150      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:54.841086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:55.841151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:56.841252      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:57.842259      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:58.842399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:25:59.842964      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:00.843078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:01.843563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:02.843658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:03.843883      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:04.844020      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:05.845057      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:06.845166      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:07.845647      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:08.845897      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:09.846210      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:10.846321      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:11.847199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:12.847805      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:13.848556      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:14.848674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:15.848698      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:16.849763      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:26:17.767: INFO: Waiting for terminating namespaces to be deleted...
  Jul  8 12:26:17.769: INFO: Starting informer...
  STEP: Starting pod... @ 07/08/23 12:26:17.769
  E0708 12:26:17.850595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:26:17.985: INFO: Pod is running on ip-172-31-93-234. Tainting Node
  STEP: Trying to apply a taint on the Node @ 07/08/23 12:26:17.985
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/08/23 12:26:17.997
  STEP: Waiting short time to make sure Pod is queued for deletion @ 07/08/23 12:26:18.005
  Jul  8 12:26:18.005: INFO: Pod wasn't evicted. Proceeding
  Jul  8 12:26:18.005: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/08/23 12:26:18.021
  STEP: Waiting some time to make sure that toleration time passed. @ 07/08/23 12:26:18.029
  E0708 12:26:18.850952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:19.851029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:20.851218      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:21.851337      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:22.852207      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:23.852292      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:24.853099      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:25.853570      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:26.853888      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:27.853953      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:28.854022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:29.855064      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:30.855774      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:31.856024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:32.857100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:33.857256      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:34.858213      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:35.858365      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:36.858518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:37.858673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:38.858833      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:39.859063      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:40.859211      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:41.859626      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:42.859867      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:43.860023      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:44.861089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:45.861172      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:46.861419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:47.861669      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:48.861846      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:49.862278      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:50.862358      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:51.862598      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:52.862751      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:53.862935      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:54.863105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:55.863529      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:56.863751      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:57.864514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:58.864589      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:26:59.865470      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:00.865556      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:01.865927      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:02.866079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:03.866576      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:04.866723      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:05.866887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:06.867086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:07.867238      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:08.867344      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:09.867525      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:10.867565      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:11.868533      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:12.868628      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:13.869097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:14.869253      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:15.869341      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:16.869423      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:17.870196      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:18.870312      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:19.870392      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:20.870562      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:21.870964      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:22.871017      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:23.871188      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:24.871360      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:25.871458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:26.871528      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:27.872360      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:28.873086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:29.873229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:30.873390      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:31.873690      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:32.874755      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:27:33.030: INFO: Pod wasn't evicted. Test successful
  Jul  8 12:27:33.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-4046" for this suite. @ 07/08/23 12:27:33.034
• [135.305 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 07/08/23 12:27:33.041
  Jul  8 12:27:33.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl @ 07/08/23 12:27:33.041
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:27:33.057
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:27:33.059
  STEP: creating the pod @ 07/08/23 12:27:33.062
  Jul  8 12:27:33.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7410 create -f -'
  Jul  8 12:27:33.253: INFO: stderr: ""
  Jul  8 12:27:33.253: INFO: stdout: "pod/pause created\n"
  E0708 12:27:33.875608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:34.876194      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 07/08/23 12:27:35.262
  Jul  8 12:27:35.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7410 label pods pause testing-label=testing-label-value'
  Jul  8 12:27:35.319: INFO: stderr: ""
  Jul  8 12:27:35.319: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 07/08/23 12:27:35.319
  Jul  8 12:27:35.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7410 get pod pause -L testing-label'
  Jul  8 12:27:35.368: INFO: stderr: ""
  Jul  8 12:27:35.368: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 07/08/23 12:27:35.368
  Jul  8 12:27:35.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7410 label pods pause testing-label-'
  Jul  8 12:27:35.419: INFO: stderr: ""
  Jul  8 12:27:35.419: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 07/08/23 12:27:35.419
  Jul  8 12:27:35.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7410 get pod pause -L testing-label'
  Jul  8 12:27:35.464: INFO: stderr: ""
  Jul  8 12:27:35.464: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 07/08/23 12:27:35.464
  Jul  8 12:27:35.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7410 delete --grace-period=0 --force -f -'
  Jul  8 12:27:35.518: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  8 12:27:35.518: INFO: stdout: "pod \"pause\" force deleted\n"
  Jul  8 12:27:35.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7410 get rc,svc -l name=pause --no-headers'
  Jul  8 12:27:35.566: INFO: stderr: "No resources found in kubectl-7410 namespace.\n"
  Jul  8 12:27:35.566: INFO: stdout: ""
  Jul  8 12:27:35.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7410 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul  8 12:27:35.611: INFO: stderr: ""
  Jul  8 12:27:35.611: INFO: stdout: ""
  Jul  8 12:27:35.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7410" for this suite. @ 07/08/23 12:27:35.615
• [2.579 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 07/08/23 12:27:35.62
  Jul  8 12:27:35.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/08/23 12:27:35.621
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:27:35.634
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:27:35.637
  STEP: create the container to handle the HTTPGet hook request. @ 07/08/23 12:27:35.642
  E0708 12:27:35.876988      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:36.877232      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 07/08/23 12:27:37.662
  E0708 12:27:37.878208      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:38.878327      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 07/08/23 12:27:39.68
  E0708 12:27:39.879290      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:40.879408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 07/08/23 12:27:41.693
  Jul  8 12:27:41.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8618" for this suite. @ 07/08/23 12:27:41.714
• [6.100 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 07/08/23 12:27:41.72
  Jul  8 12:27:41.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl @ 07/08/23 12:27:41.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:27:41.734
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:27:41.737
  STEP: create deployment with httpd image @ 07/08/23 12:27:41.739
  Jul  8 12:27:41.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-883 create -f -'
  E0708 12:27:41.880054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:27:41.904: INFO: stderr: ""
  Jul  8 12:27:41.904: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 07/08/23 12:27:41.904
  Jul  8 12:27:41.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-883 diff -f -'
  Jul  8 12:27:42.059: INFO: rc: 1
  Jul  8 12:27:42.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-883 delete -f -'
  Jul  8 12:27:42.109: INFO: stderr: ""
  Jul  8 12:27:42.109: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Jul  8 12:27:42.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-883" for this suite. @ 07/08/23 12:27:42.113
• [0.399 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 07/08/23 12:27:42.12
  Jul  8 12:27:42.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename replicaset @ 07/08/23 12:27:42.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:27:42.136
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:27:42.138
  Jul  8 12:27:42.140: INFO: Creating ReplicaSet my-hostname-basic-54d8718e-ae11-496d-b8fd-212177c91d87
  Jul  8 12:27:42.151: INFO: Pod name my-hostname-basic-54d8718e-ae11-496d-b8fd-212177c91d87: Found 0 pods out of 1
  E0708 12:27:42.880978      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:43.881066      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:44.881259      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:45.881606      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:46.881876      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:27:47.156: INFO: Pod name my-hostname-basic-54d8718e-ae11-496d-b8fd-212177c91d87: Found 1 pods out of 1
  Jul  8 12:27:47.156: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-54d8718e-ae11-496d-b8fd-212177c91d87" is running
  Jul  8 12:27:47.159: INFO: Pod "my-hostname-basic-54d8718e-ae11-496d-b8fd-212177c91d87-jchkv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-08 12:27:42 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-08 12:27:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-08 12:27:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-08 12:27:42 +0000 UTC Reason: Message:}])
  Jul  8 12:27:47.159: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 07/08/23 12:27:47.159
  Jul  8 12:27:47.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-945" for this suite. @ 07/08/23 12:27:47.171
• [5.057 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 07/08/23 12:27:47.177
  Jul  8 12:27:47.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename pods @ 07/08/23 12:27:47.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:27:47.192
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:27:47.194
  Jul  8 12:27:47.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: creating the pod @ 07/08/23 12:27:47.197
  STEP: submitting the pod to kubernetes @ 07/08/23 12:27:47.197
  E0708 12:27:47.882495      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:48.882593      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:27:49.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6862" for this suite. @ 07/08/23 12:27:49.239
• [2.069 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 07/08/23 12:27:49.246
  Jul  8 12:27:49.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename replication-controller @ 07/08/23 12:27:49.247
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:27:49.261
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:27:49.263
  STEP: Given a ReplicationController is created @ 07/08/23 12:27:49.264
  STEP: When the matched label of one of its pods change @ 07/08/23 12:27:49.27
  Jul  8 12:27:49.275: INFO: Pod name pod-release: Found 0 pods out of 1
  E0708 12:27:49.882677      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:50.882766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:51.882854      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:52.883059      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:53.883141      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:27:54.279: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 07/08/23 12:27:54.288
  E0708 12:27:54.884076      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:27:55.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8588" for this suite. @ 07/08/23 12:27:55.302
• [6.063 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 07/08/23 12:27:55.31
  Jul  8 12:27:55.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename crd-watch @ 07/08/23 12:27:55.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:27:55.326
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:27:55.328
  Jul  8 12:27:55.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 12:27:55.884258      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:56.885089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 07/08/23 12:27:57.866
  Jul  8 12:27:57.870: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-08T12:27:57Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-08T12:27:57Z]] name:name1 resourceVersion:15897 uid:03ddc058-147d-4019-b307-1ff260eb0d50] num:map[num1:9223372036854775807 num2:1000000]]}
  E0708 12:27:57.885972      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:58.886116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:27:59.886359      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:00.886545      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:01.886877      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:02.887163      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:03.887247      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:04.887418      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:05.888167      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:06.888256      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 07/08/23 12:28:07.871
  Jul  8 12:28:07.878: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-08T12:28:07Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-08T12:28:07Z]] name:name2 resourceVersion:15944 uid:afe26af4-9f65-4886-9338-758df70c5709] num:map[num1:9223372036854775807 num2:1000000]]}
  E0708 12:28:07.888473      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:08.889145      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:09.889240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:10.889624      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:11.889678      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:12.889777      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:13.890752      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:14.890998      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:15.891170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:16.891423      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 07/08/23 12:28:17.879
  Jul  8 12:28:17.884: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-08T12:27:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-08T12:28:17Z]] name:name1 resourceVersion:15964 uid:03ddc058-147d-4019-b307-1ff260eb0d50] num:map[num1:9223372036854775807 num2:1000000]]}
  E0708 12:28:17.892106      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:18.892222      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:19.893089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:20.893235      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:21.893643      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:22.893711      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:23.893796      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:24.893949      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:25.894963      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:26.895222      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 07/08/23 12:28:27.885
  Jul  8 12:28:27.890: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-08T12:28:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-08T12:28:27Z]] name:name2 resourceVersion:15992 uid:afe26af4-9f65-4886-9338-758df70c5709] num:map[num1:9223372036854775807 num2:1000000]]}
  E0708 12:28:27.895845      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:28.895973      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:29.896031      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:30.897089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:31.897337      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:32.897425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:33.897880      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:34.897974      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:35.898062      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:36.898113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 07/08/23 12:28:37.891
  E0708 12:28:37.898371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:28:37.899: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-08T12:27:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-08T12:28:17Z]] name:name1 resourceVersion:16015 uid:03ddc058-147d-4019-b307-1ff260eb0d50] num:map[num1:9223372036854775807 num2:1000000]]}
  E0708 12:28:38.898479      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:39.898630      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:40.899229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:41.899554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:42.899726      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:43.900521      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:44.901397      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:45.901484      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:46.902527      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 07/08/23 12:28:47.9
  E0708 12:28:47.903418      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:28:47.907: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-08T12:28:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-08T12:28:27Z]] name:name2 resourceVersion:16035 uid:afe26af4-9f65-4886-9338-758df70c5709] num:map[num1:9223372036854775807 num2:1000000]]}
  E0708 12:28:48.904026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:49.905091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:50.905981      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:51.906354      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:52.906528      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:53.906688      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:54.907008      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:55.907091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:56.907880      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:57.908036      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:28:58.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-3074" for this suite. @ 07/08/23 12:28:58.424
• [63.121 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 07/08/23 12:28:58.431
  Jul  8 12:28:58.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/08/23 12:28:58.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:28:58.446
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:28:58.448
  Jul  8 12:28:58.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 12:28:58.908383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:28:59.908581      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:00.909093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:29:01.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5793" for this suite. @ 07/08/23 12:29:01.793
• [3.367 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 07/08/23 12:29:01.799
  Jul  8 12:29:01.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename job @ 07/08/23 12:29:01.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:29:01.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:29:01.814
  STEP: Creating a suspended job @ 07/08/23 12:29:01.82
  STEP: Patching the Job @ 07/08/23 12:29:01.824
  STEP: Watching for Job to be patched @ 07/08/23 12:29:01.837
  Jul  8 12:29:01.838: INFO: Event ADDED observed for Job e2e-bcn98 in namespace job-5983 with labels: map[e2e-job-label:e2e-bcn98] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jul  8 12:29:01.839: INFO: Event MODIFIED observed for Job e2e-bcn98 in namespace job-5983 with labels: map[e2e-job-label:e2e-bcn98] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jul  8 12:29:01.839: INFO: Event MODIFIED found for Job e2e-bcn98 in namespace job-5983 with labels: map[e2e-bcn98:patched e2e-job-label:e2e-bcn98] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 07/08/23 12:29:01.839
  STEP: Watching for Job to be updated @ 07/08/23 12:29:01.847
  Jul  8 12:29:01.848: INFO: Event MODIFIED found for Job e2e-bcn98 in namespace job-5983 with labels: map[e2e-bcn98:patched e2e-job-label:e2e-bcn98] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul  8 12:29:01.848: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 07/08/23 12:29:01.848
  Jul  8 12:29:01.850: INFO: Job: e2e-bcn98 as labels: map[e2e-bcn98:patched e2e-job-label:e2e-bcn98]
  STEP: Waiting for job to complete @ 07/08/23 12:29:01.85
  E0708 12:29:01.909936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:02.910080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:03.911051      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:04.911369      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:05.911604      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:06.911694      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:07.912203      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:08.912326      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 07/08/23 12:29:09.854
  STEP: Watching for Job to be deleted @ 07/08/23 12:29:09.863
  Jul  8 12:29:09.865: INFO: Event MODIFIED observed for Job e2e-bcn98 in namespace job-5983 with labels: map[e2e-bcn98:patched e2e-job-label:e2e-bcn98] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul  8 12:29:09.865: INFO: Event MODIFIED observed for Job e2e-bcn98 in namespace job-5983 with labels: map[e2e-bcn98:patched e2e-job-label:e2e-bcn98] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul  8 12:29:09.865: INFO: Event MODIFIED observed for Job e2e-bcn98 in namespace job-5983 with labels: map[e2e-bcn98:patched e2e-job-label:e2e-bcn98] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul  8 12:29:09.865: INFO: Event MODIFIED observed for Job e2e-bcn98 in namespace job-5983 with labels: map[e2e-bcn98:patched e2e-job-label:e2e-bcn98] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul  8 12:29:09.865: INFO: Event MODIFIED observed for Job e2e-bcn98 in namespace job-5983 with labels: map[e2e-bcn98:patched e2e-job-label:e2e-bcn98] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul  8 12:29:09.865: INFO: Event MODIFIED observed for Job e2e-bcn98 in namespace job-5983 with labels: map[e2e-bcn98:patched e2e-job-label:e2e-bcn98] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul  8 12:29:09.865: INFO: Event MODIFIED observed for Job e2e-bcn98 in namespace job-5983 with labels: map[e2e-bcn98:patched e2e-job-label:e2e-bcn98] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul  8 12:29:09.865: INFO: Event DELETED found for Job e2e-bcn98 in namespace job-5983 with labels: map[e2e-bcn98:patched e2e-job-label:e2e-bcn98] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 07/08/23 12:29:09.865
  Jul  8 12:29:09.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5983" for this suite. @ 07/08/23 12:29:09.871
• [8.084 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 07/08/23 12:29:09.883
  Jul  8 12:29:09.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename gc @ 07/08/23 12:29:09.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:29:09.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:29:09.905
  STEP: create the deployment @ 07/08/23 12:29:09.907
  W0708 12:29:09.912239      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 07/08/23 12:29:09.912
  E0708 12:29:09.912383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the deployment @ 07/08/23 12:29:10.421
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 07/08/23 12:29:10.428
  E0708 12:29:10.912735      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/08/23 12:29:10.982
  W0708 12:29:10.986434      20 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul  8 12:29:10.986: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul  8 12:29:10.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5468" for this suite. @ 07/08/23 12:29:10.99
• [1.113 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 07/08/23 12:29:10.996
  Jul  8 12:29:10.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename dns @ 07/08/23 12:29:10.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:29:11.013
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:29:11.015
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9883.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9883.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 07/08/23 12:29:11.017
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9883.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9883.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 07/08/23 12:29:11.017
  STEP: creating a pod to probe /etc/hosts @ 07/08/23 12:29:11.017
  STEP: submitting the pod to kubernetes @ 07/08/23 12:29:11.017
  E0708 12:29:11.913011      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:12.913094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:13.913193      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:14.913452      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:15.913462      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:16.913696      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:17.913788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:18.913962      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/08/23 12:29:19.046
  STEP: looking for the results for each expected name from probers @ 07/08/23 12:29:19.049
  Jul  8 12:29:19.061: INFO: DNS probes using dns-9883/dns-test-81708e0a-dc47-4fcb-991e-f7cbf5bf45b1 succeeded

  Jul  8 12:29:19.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/08/23 12:29:19.063
  STEP: Destroying namespace "dns-9883" for this suite. @ 07/08/23 12:29:19.074
• [8.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 07/08/23 12:29:19.08
  Jul  8 12:29:19.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename svcaccounts @ 07/08/23 12:29:19.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:29:19.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:29:19.097
  E0708 12:29:19.914064      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:20.914163      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 07/08/23 12:29:21.117
  Jul  8 12:29:21.117: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6622 pod-service-account-91f94e96-0422-4b26-ac19-7d32cda27dd2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 07/08/23 12:29:21.241
  Jul  8 12:29:21.241: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6622 pod-service-account-91f94e96-0422-4b26-ac19-7d32cda27dd2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 07/08/23 12:29:21.35
  Jul  8 12:29:21.350: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6622 pod-service-account-91f94e96-0422-4b26-ac19-7d32cda27dd2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Jul  8 12:29:21.448: INFO: Got root ca configmap in namespace "svcaccounts-6622"
  Jul  8 12:29:21.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6622" for this suite. @ 07/08/23 12:29:21.452
• [2.377 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 07/08/23 12:29:21.458
  Jul  8 12:29:21.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename endpointslice @ 07/08/23 12:29:21.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:29:21.472
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:29:21.474
  STEP: getting /apis @ 07/08/23 12:29:21.476
  STEP: getting /apis/discovery.k8s.io @ 07/08/23 12:29:21.479
  STEP: getting /apis/discovery.k8s.iov1 @ 07/08/23 12:29:21.48
  STEP: creating @ 07/08/23 12:29:21.48
  STEP: getting @ 07/08/23 12:29:21.493
  STEP: listing @ 07/08/23 12:29:21.495
  STEP: watching @ 07/08/23 12:29:21.497
  Jul  8 12:29:21.497: INFO: starting watch
  STEP: cluster-wide listing @ 07/08/23 12:29:21.498
  STEP: cluster-wide watching @ 07/08/23 12:29:21.502
  Jul  8 12:29:21.502: INFO: starting watch
  STEP: patching @ 07/08/23 12:29:21.503
  STEP: updating @ 07/08/23 12:29:21.507
  Jul  8 12:29:21.514: INFO: waiting for watch events with expected annotations
  Jul  8 12:29:21.514: INFO: saw patched and updated annotations
  STEP: deleting @ 07/08/23 12:29:21.514
  STEP: deleting a collection @ 07/08/23 12:29:21.525
  Jul  8 12:29:21.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-2527" for this suite. @ 07/08/23 12:29:21.542
• [0.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 07/08/23 12:29:21.548
  Jul  8 12:29:21.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename svcaccounts @ 07/08/23 12:29:21.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:29:21.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:29:21.561
  Jul  8 12:29:21.579: INFO: created pod pod-service-account-defaultsa
  Jul  8 12:29:21.579: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Jul  8 12:29:21.587: INFO: created pod pod-service-account-mountsa
  Jul  8 12:29:21.587: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Jul  8 12:29:21.595: INFO: created pod pod-service-account-nomountsa
  Jul  8 12:29:21.595: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Jul  8 12:29:21.601: INFO: created pod pod-service-account-defaultsa-mountspec
  Jul  8 12:29:21.601: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Jul  8 12:29:21.610: INFO: created pod pod-service-account-mountsa-mountspec
  Jul  8 12:29:21.610: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Jul  8 12:29:21.616: INFO: created pod pod-service-account-nomountsa-mountspec
  Jul  8 12:29:21.616: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Jul  8 12:29:21.622: INFO: created pod pod-service-account-defaultsa-nomountspec
  Jul  8 12:29:21.622: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Jul  8 12:29:21.636: INFO: created pod pod-service-account-mountsa-nomountspec
  Jul  8 12:29:21.636: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Jul  8 12:29:21.643: INFO: created pod pod-service-account-nomountsa-nomountspec
  Jul  8 12:29:21.643: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Jul  8 12:29:21.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9808" for this suite. @ 07/08/23 12:29:21.648
• [0.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 07/08/23 12:29:21.658
  Jul  8 12:29:21.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename security-context-test @ 07/08/23 12:29:21.658
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:29:21.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:29:21.675
  E0708 12:29:21.915187      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:22.915415      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:23.916124      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:24.917091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:29:25.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7719" for this suite. @ 07/08/23 12:29:25.7
• [4.049 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 07/08/23 12:29:25.707
  Jul  8 12:29:25.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename security-context @ 07/08/23 12:29:25.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:29:25.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:29:25.726
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 07/08/23 12:29:25.728
  E0708 12:29:25.917722      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:26.917830      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:27.918601      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:28.918691      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:29:29.747
  Jul  8 12:29:29.751: INFO: Trying to get logs from node ip-172-31-12-67 pod security-context-521ceae4-efd8-4d19-a884-f42f8d196aa7 container test-container: <nil>
  STEP: delete the pod @ 07/08/23 12:29:29.765
  Jul  8 12:29:29.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-2382" for this suite. @ 07/08/23 12:29:29.779
• [4.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 07/08/23 12:29:29.795
  Jul  8 12:29:29.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename containers @ 07/08/23 12:29:29.796
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:29:29.81
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:29:29.812
  STEP: Creating a pod to test override all @ 07/08/23 12:29:29.814
  E0708 12:29:29.918678      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:30.918800      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:31.919137      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:32.919252      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:29:33.832
  Jul  8 12:29:33.835: INFO: Trying to get logs from node ip-172-31-29-188 pod client-containers-92aee053-ec48-4d7d-bc98-fa0169e87961 container agnhost-container: <nil>
  STEP: delete the pod @ 07/08/23 12:29:33.85
  Jul  8 12:29:33.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2045" for this suite. @ 07/08/23 12:29:33.866
• [4.079 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 07/08/23 12:29:33.874
  Jul  8 12:29:33.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename services @ 07/08/23 12:29:33.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:29:33.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:29:33.89
  STEP: creating service multi-endpoint-test in namespace services-219 @ 07/08/23 12:29:33.892
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-219 to expose endpoints map[] @ 07/08/23 12:29:33.902
  Jul  8 12:29:33.913: INFO: successfully validated that service multi-endpoint-test in namespace services-219 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-219 @ 07/08/23 12:29:33.913
  E0708 12:29:33.920043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:34.921129      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:35.921313      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-219 to expose endpoints map[pod1:[100]] @ 07/08/23 12:29:35.931
  Jul  8 12:29:35.941: INFO: successfully validated that service multi-endpoint-test in namespace services-219 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-219 @ 07/08/23 12:29:35.941
  E0708 12:29:36.921919      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:37.922023      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-219 to expose endpoints map[pod1:[100] pod2:[101]] @ 07/08/23 12:29:37.956
  Jul  8 12:29:37.966: INFO: successfully validated that service multi-endpoint-test in namespace services-219 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 07/08/23 12:29:37.966
  Jul  8 12:29:37.966: INFO: Creating new exec pod
  E0708 12:29:38.922117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:39.922356      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:40.922447      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:29:40.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-219 exec execpod2lhjb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Jul  8 12:29:41.155: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Jul  8 12:29:41.155: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 12:29:41.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-219 exec execpod2lhjb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.248 80'
  Jul  8 12:29:41.275: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.248 80\nConnection to 10.152.183.248 80 port [tcp/http] succeeded!\n"
  Jul  8 12:29:41.275: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 12:29:41.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-219 exec execpod2lhjb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Jul  8 12:29:41.413: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Jul  8 12:29:41.413: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 12:29:41.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-219 exec execpod2lhjb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.248 81'
  Jul  8 12:29:41.532: INFO: stderr: "+ nc -v -t -w 2 10.152.183.248 81\n+ echo hostName\nConnection to 10.152.183.248 81 port [tcp/*] succeeded!\n"
  Jul  8 12:29:41.532: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-219 @ 07/08/23 12:29:41.532
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-219 to expose endpoints map[pod2:[101]] @ 07/08/23 12:29:41.546
  Jul  8 12:29:41.562: INFO: successfully validated that service multi-endpoint-test in namespace services-219 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-219 @ 07/08/23 12:29:41.562
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-219 to expose endpoints map[] @ 07/08/23 12:29:41.575
  Jul  8 12:29:41.583: INFO: successfully validated that service multi-endpoint-test in namespace services-219 exposes endpoints map[]
  Jul  8 12:29:41.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-219" for this suite. @ 07/08/23 12:29:41.601
• [7.733 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 07/08/23 12:29:41.613
  Jul  8 12:29:41.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir @ 07/08/23 12:29:41.614
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:29:41.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:29:41.629
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 07/08/23 12:29:41.631
  E0708 12:29:41.922531      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:42.922698      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:43.924015      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:44.922890      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:29:45.651
  Jul  8 12:29:45.653: INFO: Trying to get logs from node ip-172-31-29-188 pod pod-77218619-dfc1-41fa-9bea-f34c182a580a container test-container: <nil>
  STEP: delete the pod @ 07/08/23 12:29:45.661
  Jul  8 12:29:45.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3540" for this suite. @ 07/08/23 12:29:45.677
• [4.071 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 07/08/23 12:29:45.684
  Jul  8 12:29:45.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 12:29:45.685
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:29:45.697
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:29:45.7
  STEP: Setting up server cert @ 07/08/23 12:29:45.723
  E0708 12:29:45.923745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 12:29:46.201
  STEP: Deploying the webhook pod @ 07/08/23 12:29:46.208
  STEP: Wait for the deployment to be ready @ 07/08/23 12:29:46.218
  Jul  8 12:29:46.226: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0708 12:29:46.923875      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:47.924054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 12:29:48.234
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 12:29:48.249
  E0708 12:29:48.925043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:29:49.249: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 07/08/23 12:29:49.252
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/08/23 12:29:49.264
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 07/08/23 12:29:49.269
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/08/23 12:29:49.277
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 07/08/23 12:29:49.286
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/08/23 12:29:49.293
  Jul  8 12:29:49.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8005" for this suite. @ 07/08/23 12:29:49.338
  STEP: Destroying namespace "webhook-markers-4947" for this suite. @ 07/08/23 12:29:49.348
• [3.670 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 07/08/23 12:29:49.355
  Jul  8 12:29:49.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename security-context-test @ 07/08/23 12:29:49.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:29:49.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:29:49.372
  E0708 12:29:49.925968      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:50.926003      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:51.926943      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:52.927032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:29:53.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-3451" for this suite. @ 07/08/23 12:29:53.402
• [4.053 seconds]
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 07/08/23 12:29:53.408
  Jul  8 12:29:53.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubelet-test @ 07/08/23 12:29:53.409
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:29:53.426
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:29:53.429
  Jul  8 12:29:53.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-96" for this suite. @ 07/08/23 12:29:53.466
• [0.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 07/08/23 12:29:53.474
  Jul  8 12:29:53.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:29:53.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:29:53.49
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:29:53.492
  STEP: Creating the pod @ 07/08/23 12:29:53.494
  E0708 12:29:53.928125      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:54.929125      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:55.929287      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:29:56.034: INFO: Successfully updated pod "annotationupdatef5edbb09-fe5a-43f4-bfd0-8e68db855644"
  E0708 12:29:56.929306      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:57.929380      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:29:58.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6401" for this suite. @ 07/08/23 12:29:58.052
• [4.584 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 07/08/23 12:29:58.059
  Jul  8 12:29:58.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename resourcequota @ 07/08/23 12:29:58.06
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:29:58.076
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:29:58.078
  STEP: Creating resourceQuota "e2e-rq-status-llwbd" @ 07/08/23 12:29:58.086
  Jul  8 12:29:58.093: INFO: Resource quota "e2e-rq-status-llwbd" reports spec: hard cpu limit of 500m
  Jul  8 12:29:58.093: INFO: Resource quota "e2e-rq-status-llwbd" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-llwbd" /status @ 07/08/23 12:29:58.093
  STEP: Confirm /status for "e2e-rq-status-llwbd" resourceQuota via watch @ 07/08/23 12:29:58.1
  Jul  8 12:29:58.101: INFO: observed resourceQuota "e2e-rq-status-llwbd" in namespace "resourcequota-7944" with hard status: v1.ResourceList(nil)
  Jul  8 12:29:58.101: INFO: Found resourceQuota "e2e-rq-status-llwbd" in namespace "resourcequota-7944" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jul  8 12:29:58.101: INFO: ResourceQuota "e2e-rq-status-llwbd" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 07/08/23 12:29:58.104
  Jul  8 12:29:58.108: INFO: Resource quota "e2e-rq-status-llwbd" reports spec: hard cpu limit of 1
  Jul  8 12:29:58.108: INFO: Resource quota "e2e-rq-status-llwbd" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-llwbd" /status @ 07/08/23 12:29:58.108
  STEP: Confirm /status for "e2e-rq-status-llwbd" resourceQuota via watch @ 07/08/23 12:29:58.113
  Jul  8 12:29:58.114: INFO: observed resourceQuota "e2e-rq-status-llwbd" in namespace "resourcequota-7944" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jul  8 12:29:58.115: INFO: Found resourceQuota "e2e-rq-status-llwbd" in namespace "resourcequota-7944" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jul  8 12:29:58.115: INFO: ResourceQuota "e2e-rq-status-llwbd" /status was patched
  STEP: Get "e2e-rq-status-llwbd" /status @ 07/08/23 12:29:58.115
  Jul  8 12:29:58.117: INFO: Resourcequota "e2e-rq-status-llwbd" reports status: hard cpu of 1
  Jul  8 12:29:58.117: INFO: Resourcequota "e2e-rq-status-llwbd" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-llwbd" /status before checking Spec is unchanged @ 07/08/23 12:29:58.121
  Jul  8 12:29:58.130: INFO: Resourcequota "e2e-rq-status-llwbd" reports status: hard cpu of 2
  Jul  8 12:29:58.130: INFO: Resourcequota "e2e-rq-status-llwbd" reports status: hard memory of 2Gi
  Jul  8 12:29:58.131: INFO: observed resourceQuota "e2e-rq-status-llwbd" in namespace "resourcequota-7944" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jul  8 12:29:58.131: INFO: Found resourceQuota "e2e-rq-status-llwbd" in namespace "resourcequota-7944" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0708 12:29:58.930462      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:29:59.930560      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:00.931243      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:01.931680      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:02.932100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:03.932188      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:04.933093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:05.933261      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:06.933536      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:07.933702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:08.934374      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:09.934547      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:10.934727      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:11.935024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:12.935114      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:13.935885      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:14.936027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:15.937089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:16.937995      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:17.938318      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:18.939280      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:19.939374      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:20.939530      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:21.939904      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:22.940017      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:23.940915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:24.941001      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:25.941090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:26.941346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:27.941520      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:28.941618      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:29.941700      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:30.942516      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:31.942607      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:32.942669      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:33.943223      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:34.943311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:35.943756      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:36.943788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:37.943958      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:38.944037      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:39.945026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:40.945111      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:41.945496      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:42.945660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:43.945725      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:44.945874      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:45.945931      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:46.946017      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:47.946106      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:48.946347      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:49.947226      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:50.947324      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:51.948212      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:52.949087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:53.949749      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:54.949982      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:55.950080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:56.950140      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:57.950334      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:58.951137      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:30:59.951283      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:00.951721      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:01.952027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:02.952122      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:03.952204      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:04.953088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:05.953263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:06.953541      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:07.953701      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:08.953788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:09.953957      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:10.954139      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:11.954225      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:12.954393      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:13.954917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:14.955005      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:15.955073      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:16.955320      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:17.955493      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:18.956542      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:19.957364      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:20.957523      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:21.957913      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:22.958011      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:23.958680      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:24.958749      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:25.959704      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:26.959792      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:27.960652      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:28.960927      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:29.961086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:30.961259      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:31.961650      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:32.961823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:33.961911      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:34.962059      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:35.962823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:36.962980      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:37.963181      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:38.963583      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:39.963736      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:40.964552      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:41.964638      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:42.965085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:43.965330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:44.965368      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:45.965879      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:46.965974      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:47.966145      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:48.967080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:49.967167      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:50.967308      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:51.967456      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:52.967550      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:53.968026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:54.969086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:55.969264      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:56.969612      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:57.969769      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:58.970388      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:31:59.970474      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:00.970745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:01.971100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:02.971272      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:03.971350      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:04.971484      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:05.971650      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:06.971843      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:07.972013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:08.972757      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:09.972808      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:10.973090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:11.973170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:12.973334      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:13.973882      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:14.974352      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:15.974435      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:16.974686      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:17.974873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:18.974924      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:19.975093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:20.975227      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:21.975589      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:22.975726      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:23.976026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:24.977090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:25.977903      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:26.978009      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:27.978051      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:28.978120      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:29.978742      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:30.978891      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:31.978978      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:32.979128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:33.979229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:34.979948      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:35.980029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:36.981087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:37.981777      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:38.982117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:39.982262      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:40.982416      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:41.982526      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:42.982897      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:43.982980      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:44.983130      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:45.983222      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:46.984107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:47.984199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:48.984276      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:49.985085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:50.985242      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:51.985884      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:52.985969      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:53.987003      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:54.987089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:55.987256      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:56.987893      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:57.988030      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:58.989092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:32:59.989180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:00.989336      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:01.989991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:02.990078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:03.990110      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:04.990630      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:05.990717      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:06.990957      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:07.991051      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:33:08.138: INFO: ResourceQuota "e2e-rq-status-llwbd" Spec was unchanged and /status reset
  Jul  8 12:33:08.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7944" for this suite. @ 07/08/23 12:33:08.142
• [190.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 07/08/23 12:33:08.148
  Jul  8 12:33:08.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename init-container @ 07/08/23 12:33:08.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:33:08.164
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:33:08.167
  STEP: creating the pod @ 07/08/23 12:33:08.169
  Jul  8 12:33:08.169: INFO: PodSpec: initContainers in spec.initContainers
  E0708 12:33:08.991163      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:09.991950      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:10.992019      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:11.993090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:33:12.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6865" for this suite. @ 07/08/23 12:33:12.93
• [4.788 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 07/08/23 12:33:12.937
  Jul  8 12:33:12.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename conformance-tests @ 07/08/23 12:33:12.937
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:33:12.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:33:12.953
  STEP: Getting node addresses @ 07/08/23 12:33:12.956
  Jul  8 12:33:12.956: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Jul  8 12:33:12.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-7588" for this suite. @ 07/08/23 12:33:12.963
• [0.033 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 07/08/23 12:33:12.97
  Jul  8 12:33:12.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename job @ 07/08/23 12:33:12.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:33:12.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:33:12.985
  STEP: Creating a job @ 07/08/23 12:33:12.987
  E0708 12:33:12.993281      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensure pods equal to parallelism count is attached to the job @ 07/08/23 12:33:12.993
  E0708 12:33:13.993897      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:14.993993      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 07/08/23 12:33:14.997
  STEP: updating /status @ 07/08/23 12:33:15.003
  STEP: get /status @ 07/08/23 12:33:15.033
  Jul  8 12:33:15.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1430" for this suite. @ 07/08/23 12:33:15.039
• [2.075 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 07/08/23 12:33:15.045
  Jul  8 12:33:15.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 12:33:15.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:33:15.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:33:15.062
  STEP: Setting up server cert @ 07/08/23 12:33:15.082
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 12:33:15.374
  STEP: Deploying the webhook pod @ 07/08/23 12:33:15.381
  STEP: Wait for the deployment to be ready @ 07/08/23 12:33:15.39
  Jul  8 12:33:15.396: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0708 12:33:15.994227      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:16.994464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 12:33:17.406
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 12:33:17.415
  E0708 12:33:17.995365      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:33:18.415: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul  8 12:33:18.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 07/08/23 12:33:18.929
  STEP: Creating a custom resource that should be denied by the webhook @ 07/08/23 12:33:18.943
  E0708 12:33:18.995357      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:19.995484      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 07/08/23 12:33:20.963
  STEP: Updating the custom resource with disallowed data should be denied @ 07/08/23 12:33:20.968
  STEP: Deleting the custom resource should be denied @ 07/08/23 12:33:20.976
  STEP: Remove the offending key and value from the custom resource data @ 07/08/23 12:33:20.981
  STEP: Deleting the updated custom resource should be successful @ 07/08/23 12:33:20.989
  E0708 12:33:20.995881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:33:20.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4862" for this suite. @ 07/08/23 12:33:21.56
  STEP: Destroying namespace "webhook-markers-1202" for this suite. @ 07/08/23 12:33:21.565
• [6.528 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 07/08/23 12:33:21.574
  Jul  8 12:33:21.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename pods @ 07/08/23 12:33:21.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:33:21.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:33:21.592
  STEP: creating pod @ 07/08/23 12:33:21.594
  E0708 12:33:21.996952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:22.997136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:33:23.614: INFO: Pod pod-hostip-504b6155-151e-4954-8b2c-19933cb36903 has hostIP: 172.31.93.234
  Jul  8 12:33:23.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1113" for this suite. @ 07/08/23 12:33:23.617
• [2.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 07/08/23 12:33:23.623
  Jul  8 12:33:23.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename secrets @ 07/08/23 12:33:23.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:33:23.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:33:23.64
  Jul  8 12:33:23.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2937" for this suite. @ 07/08/23 12:33:23.679
• [0.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 07/08/23 12:33:23.687
  Jul  8 12:33:23.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename subpath @ 07/08/23 12:33:23.688
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:33:23.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:33:23.705
  STEP: Setting up data @ 07/08/23 12:33:23.707
  STEP: Creating pod pod-subpath-test-configmap-ql66 @ 07/08/23 12:33:23.716
  STEP: Creating a pod to test atomic-volume-subpath @ 07/08/23 12:33:23.716
  E0708 12:33:23.997560      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:24.997672      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:25.998093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:26.998648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:27.998716      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:28.998812      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:29.999477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:30.999636      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:32.000188      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:33.001116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:34.001464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:35.002504      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:36.003109      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:37.003337      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:38.003682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:39.003838      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:40.004547      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:41.005116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:42.005174      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:43.005259      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:44.005603      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:45.005503      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:46.006469      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:47.006717      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:33:47.777
  Jul  8 12:33:47.781: INFO: Trying to get logs from node ip-172-31-29-188 pod pod-subpath-test-configmap-ql66 container test-container-subpath-configmap-ql66: <nil>
  STEP: delete the pod @ 07/08/23 12:33:47.796
  STEP: Deleting pod pod-subpath-test-configmap-ql66 @ 07/08/23 12:33:47.811
  Jul  8 12:33:47.811: INFO: Deleting pod "pod-subpath-test-configmap-ql66" in namespace "subpath-1576"
  Jul  8 12:33:47.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1576" for this suite. @ 07/08/23 12:33:47.818
• [24.138 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 07/08/23 12:33:47.825
  Jul  8 12:33:47.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-probe @ 07/08/23 12:33:47.825
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:33:47.842
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:33:47.845
  STEP: Creating pod liveness-dc2d6431-eeea-4464-8253-c293786b8839 in namespace container-probe-8260 @ 07/08/23 12:33:47.847
  E0708 12:33:48.007676      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:49.007823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:33:49.863: INFO: Started pod liveness-dc2d6431-eeea-4464-8253-c293786b8839 in namespace container-probe-8260
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/08/23 12:33:49.863
  Jul  8 12:33:49.867: INFO: Initial restart count of pod liveness-dc2d6431-eeea-4464-8253-c293786b8839 is 0
  E0708 12:33:50.008225      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:51.008352      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:52.009035      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:53.009537      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:54.010157      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:55.010328      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:56.011144      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:57.011408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:58.011760      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:33:59.011901      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:00.012559      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:01.013104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:02.014145      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:03.015082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:04.015116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:05.015215      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:06.015499      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:07.015795      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:08.016074      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:09.016179      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:34:09.910: INFO: Restart count of pod container-probe-8260/liveness-dc2d6431-eeea-4464-8253-c293786b8839 is now 1 (20.043526911s elapsed)
  Jul  8 12:34:09.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/08/23 12:34:09.914
  STEP: Destroying namespace "container-probe-8260" for this suite. @ 07/08/23 12:34:09.929
• [22.112 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 07/08/23 12:34:09.937
  Jul  8 12:34:09.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl @ 07/08/23 12:34:09.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:34:09.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:34:09.953
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/08/23 12:34:09.956
  Jul  8 12:34:09.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6290 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jul  8 12:34:10.012: INFO: stderr: ""
  Jul  8 12:34:10.012: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 07/08/23 12:34:10.012
  E0708 12:34:10.017029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:11.017862      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:12.017962      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:13.018031      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:14.019093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:15.019195      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 07/08/23 12:34:15.064
  Jul  8 12:34:15.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6290 get pod e2e-test-httpd-pod -o json'
  Jul  8 12:34:15.109: INFO: stderr: ""
  Jul  8 12:34:15.109: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-07-08T12:34:10Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6290\",\n        \"resourceVersion\": \"17917\",\n        \"uid\": \"2e5ab7d3-2c1d-4ae5-a889-1dd2de4df358\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-2d57r\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-93-234\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-2d57r\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-08T12:34:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-08T12:34:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-08T12:34:10Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-08T12:34:10Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://c4e10bb24b49035ac2b1213a57f652a669ad702794dc704ede13594fe3c69a3e\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-07-08T12:34:10Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.93.234\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.7.239\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.7.239\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-07-08T12:34:10Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 07/08/23 12:34:15.109
  Jul  8 12:34:15.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6290 replace -f -'
  Jul  8 12:34:15.389: INFO: stderr: ""
  Jul  8 12:34:15.389: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 07/08/23 12:34:15.389
  Jul  8 12:34:15.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6290 delete pods e2e-test-httpd-pod'
  E0708 12:34:16.019790      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:34:16.831: INFO: stderr: ""
  Jul  8 12:34:16.831: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul  8 12:34:16.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6290" for this suite. @ 07/08/23 12:34:16.834
• [6.904 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 07/08/23 12:34:16.842
  Jul  8 12:34:16.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename dns @ 07/08/23 12:34:16.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:34:16.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:34:16.861
  STEP: Creating a test externalName service @ 07/08/23 12:34:16.862
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6023.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6023.svc.cluster.local; sleep 1; done
   @ 07/08/23 12:34:16.866
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6023.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6023.svc.cluster.local; sleep 1; done
   @ 07/08/23 12:34:16.866
  STEP: creating a pod to probe DNS @ 07/08/23 12:34:16.866
  STEP: submitting the pod to kubernetes @ 07/08/23 12:34:16.866
  E0708 12:34:17.020811      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:18.020935      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/08/23 12:34:18.884
  STEP: looking for the results for each expected name from probers @ 07/08/23 12:34:18.888
  Jul  8 12:34:18.894: INFO: DNS probes using dns-test-c4314b4a-63db-4e60-b232-77aed65a5f80 succeeded

  STEP: changing the externalName to bar.example.com @ 07/08/23 12:34:18.894
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6023.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6023.svc.cluster.local; sleep 1; done
   @ 07/08/23 12:34:18.902
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6023.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6023.svc.cluster.local; sleep 1; done
   @ 07/08/23 12:34:18.902
  STEP: creating a second pod to probe DNS @ 07/08/23 12:34:18.902
  STEP: submitting the pod to kubernetes @ 07/08/23 12:34:18.902
  E0708 12:34:19.021894      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:20.022087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/08/23 12:34:20.919
  STEP: looking for the results for each expected name from probers @ 07/08/23 12:34:20.922
  Jul  8 12:34:20.925: INFO: File wheezy_udp@dns-test-service-3.dns-6023.svc.cluster.local from pod  dns-6023/dns-test-ca8ed262-a64f-44ca-97a1-56601ad5320b contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jul  8 12:34:20.929: INFO: File jessie_udp@dns-test-service-3.dns-6023.svc.cluster.local from pod  dns-6023/dns-test-ca8ed262-a64f-44ca-97a1-56601ad5320b contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jul  8 12:34:20.929: INFO: Lookups using dns-6023/dns-test-ca8ed262-a64f-44ca-97a1-56601ad5320b failed for: [wheezy_udp@dns-test-service-3.dns-6023.svc.cluster.local jessie_udp@dns-test-service-3.dns-6023.svc.cluster.local]

  E0708 12:34:21.022965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:22.023508      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:23.023981      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:24.024036      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:25.024104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:34:25.937: INFO: DNS probes using dns-test-ca8ed262-a64f-44ca-97a1-56601ad5320b succeeded

  STEP: changing the service to type=ClusterIP @ 07/08/23 12:34:25.937
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6023.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6023.svc.cluster.local; sleep 1; done
   @ 07/08/23 12:34:25.95
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6023.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6023.svc.cluster.local; sleep 1; done
   @ 07/08/23 12:34:25.95
  STEP: creating a third pod to probe DNS @ 07/08/23 12:34:25.95
  STEP: submitting the pod to kubernetes @ 07/08/23 12:34:25.954
  E0708 12:34:26.024839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:27.025045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/08/23 12:34:27.971
  STEP: looking for the results for each expected name from probers @ 07/08/23 12:34:27.974
  Jul  8 12:34:27.981: INFO: DNS probes using dns-test-34896a37-5a89-493d-bb11-cd949401dd1b succeeded

  Jul  8 12:34:27.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/08/23 12:34:27.984
  STEP: deleting the pod @ 07/08/23 12:34:27.998
  STEP: deleting the pod @ 07/08/23 12:34:28.01
  STEP: deleting the test externalName service @ 07/08/23 12:34:28.023
  E0708 12:34:28.025847      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "dns-6023" for this suite. @ 07/08/23 12:34:28.041
• [11.211 seconds]
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 07/08/23 12:34:28.053
  Jul  8 12:34:28.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename events @ 07/08/23 12:34:28.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:34:28.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:34:28.074
  STEP: Create set of events @ 07/08/23 12:34:28.077
  Jul  8 12:34:28.084: INFO: created test-event-1
  Jul  8 12:34:28.089: INFO: created test-event-2
  Jul  8 12:34:28.094: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 07/08/23 12:34:28.094
  STEP: delete collection of events @ 07/08/23 12:34:28.101
  Jul  8 12:34:28.101: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 07/08/23 12:34:28.137
  Jul  8 12:34:28.137: INFO: requesting list of events to confirm quantity
  Jul  8 12:34:28.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-2174" for this suite. @ 07/08/23 12:34:28.144
• [0.096 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 07/08/23 12:34:28.149
  Jul  8 12:34:28.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/08/23 12:34:28.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:34:28.163
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:34:28.166
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 07/08/23 12:34:28.168
  Jul  8 12:34:28.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 12:34:29.027675      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:34:29.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 12:34:30.027929      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:31.028395      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:32.028760      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:33.028772      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:34.029772      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:34:34.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6153" for this suite. @ 07/08/23 12:34:34.628
• [6.485 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 07/08/23 12:34:34.635
  Jul  8 12:34:34.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir @ 07/08/23 12:34:34.635
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:34:34.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:34:34.651
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 07/08/23 12:34:34.653
  E0708 12:34:35.030636      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:36.031387      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:37.031461      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:38.032357      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:34:38.673
  Jul  8 12:34:38.675: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-a9b6fdbc-2a32-4db7-b089-2be44055a78d container test-container: <nil>
  STEP: delete the pod @ 07/08/23 12:34:38.694
  Jul  8 12:34:38.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9262" for this suite. @ 07/08/23 12:34:38.708
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 07/08/23 12:34:38.715
  Jul  8 12:34:38.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-probe @ 07/08/23 12:34:38.715
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:34:38.729
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:34:38.732
  STEP: Creating pod test-grpc-658636d7-e8c9-4c6e-aa5d-9bf5c76a2f61 in namespace container-probe-5514 @ 07/08/23 12:34:38.734
  E0708 12:34:39.032755      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:40.032887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:34:40.751: INFO: Started pod test-grpc-658636d7-e8c9-4c6e-aa5d-9bf5c76a2f61 in namespace container-probe-5514
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/08/23 12:34:40.751
  Jul  8 12:34:40.754: INFO: Initial restart count of pod test-grpc-658636d7-e8c9-4c6e-aa5d-9bf5c76a2f61 is 0
  E0708 12:34:41.033915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:42.034438      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:43.035235      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:44.035292      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:45.035384      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:46.035555      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:47.036605      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:48.037110      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:49.038034      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:50.038881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:51.039187      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:52.039558      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:53.040175      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:54.041105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:55.041818      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:56.041897      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:57.041997      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:58.042173      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:34:59.042928      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:00.043105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:01.043563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:02.043735      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:03.044195      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:04.044306      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:05.044933      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:06.045189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:07.045267      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:08.045446      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:09.046442      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:10.046603      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:11.047646      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:12.047937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:13.048967      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:14.049148      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:15.050155      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:16.050334      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:17.051267      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:18.051420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:19.051979      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:20.052050      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:21.053043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:22.053421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:23.053973      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:24.054138      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:25.054239      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:26.054412      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:27.055305      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:28.055407      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:29.055479      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:30.055656      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:31.055745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:32.056050      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:33.056506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:34.057098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:35.058152      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:36.059100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:37.059712      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:38.060731      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:39.061545      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:40.061733      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:41.062484      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:42.062837      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:43.063769      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:44.063947      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:45.064830      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:46.065094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:47.065386      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:48.065574      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:49.066301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:50.066421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:51.066471      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:52.066592      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:53.067267      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:54.067379      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:55.067826      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:56.068065      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:57.069013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:58.069099      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:35:59.070006      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:00.070205      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:01.070254      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:02.070683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:03.070736      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:04.070845      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:05.071453      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:06.071548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:07.072082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:08.073114      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:09.073167      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:10.073337      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:11.073394      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:12.073742      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:13.074579      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:14.074737      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:15.075587      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:16.075765      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:17.076705      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:18.077100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:19.077614      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:20.077790      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:21.078617      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:22.078951      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:23.079315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:24.079490      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:25.079739      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:26.079813      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:27.080157      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:28.081088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:29.081528      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:30.081638      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:31.082612      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:32.082959      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:33.083686      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:34.083863      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:35.084922      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:36.085224      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:37.085746      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:38.085882      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:39.086026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:40.086135      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:41.086562      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:42.086677      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:43.087595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:44.087786      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:45.088570      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:46.088684      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:47.089138      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:48.089291      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:49.090004      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:50.090141      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:51.090914      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:52.091243      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:53.091674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:54.091782      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:55.092101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:56.092212      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:57.092576      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:58.093571      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:36:59.093932      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:00.094042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:01.094755      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:02.095048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:03.095401      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:04.095510      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:05.096316      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:06.096414      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:07.096700      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:08.096797      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:09.097829      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:10.097940      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:11.098447      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:12.098572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:13.099263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:14.099415      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:15.099558      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:16.099721      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:17.100301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:18.101086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:19.101284      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:20.101762      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:21.101781      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:22.102878      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:23.103466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:24.104536      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:25.104965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:26.105126      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:27.105847      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:28.105963      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:29.106005      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:30.106887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:31.107557      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:32.107700      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:33.108484      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:34.108606      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:35.109571      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:36.109708      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:37.109943      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:38.110062      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:39.110346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:40.111263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:41.111423      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:42.111472      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:43.111634      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:44.111728      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:45.111980      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:46.112018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:47.112044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:48.112136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:49.112224      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:50.112527      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:51.112603      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:52.112707      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:53.113108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:54.113191      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:55.113369      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:56.113457      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:57.113525      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:58.113646      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:37:59.113891      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:00.114626      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:01.114721      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:02.115649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:03.115725      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:04.116043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:05.117110      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:06.117246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:07.117416      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:08.118506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:09.119056      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:10.119140      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:11.120093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:12.121101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:13.121288      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:14.122242      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:15.122337      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:16.122431      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:17.122671      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:18.123253      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:19.123344      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:20.124040      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:21.125085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:22.125811      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:23.125904      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:24.126912      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:25.127083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:26.127727      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:27.128695      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:28.128798      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:29.129160      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:30.129242      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:31.129339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:32.129432      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:33.129506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:34.129597      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:35.129820      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:36.129893      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:37.130075      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:38.131072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:39.131565      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:40.132620      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:41.133083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:38:41.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/08/23 12:38:41.233
  STEP: Destroying namespace "container-probe-5514" for this suite. @ 07/08/23 12:38:41.244
• [242.538 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 07/08/23 12:38:41.253
  Jul  8 12:38:41.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename csiinlinevolumes @ 07/08/23 12:38:41.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:38:41.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:38:41.278
  STEP: creating @ 07/08/23 12:38:41.28
  STEP: getting @ 07/08/23 12:38:41.298
  STEP: listing in namespace @ 07/08/23 12:38:41.301
  STEP: patching @ 07/08/23 12:38:41.304
  STEP: deleting @ 07/08/23 12:38:41.316
  Jul  8 12:38:41.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-9167" for this suite. @ 07/08/23 12:38:41.328
• [0.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 07/08/23 12:38:41.335
  Jul  8 12:38:41.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename field-validation @ 07/08/23 12:38:41.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:38:41.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:38:41.353
  STEP: apply creating a deployment @ 07/08/23 12:38:41.356
  Jul  8 12:38:41.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9793" for this suite. @ 07/08/23 12:38:41.372
• [0.043 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 07/08/23 12:38:41.378
  Jul  8 12:38:41.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir @ 07/08/23 12:38:41.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:38:41.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:38:41.393
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 07/08/23 12:38:41.395
  E0708 12:38:42.133371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:43.133459      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:44.133550      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:45.133718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:38:45.414
  Jul  8 12:38:45.416: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-a9b30d1e-53b1-4a51-a2dc-644028f9a23a container test-container: <nil>
  STEP: delete the pod @ 07/08/23 12:38:45.43
  Jul  8 12:38:45.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1435" for this suite. @ 07/08/23 12:38:45.446
• [4.073 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 07/08/23 12:38:45.451
  Jul  8 12:38:45.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubelet-test @ 07/08/23 12:38:45.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:38:45.464
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:38:45.469
  STEP: Waiting for pod completion @ 07/08/23 12:38:45.487
  E0708 12:38:46.133815      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:47.134199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:48.135161      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:49.135193      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:38:49.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2366" for this suite. @ 07/08/23 12:38:49.522
• [4.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 07/08/23 12:38:49.53
  Jul  8 12:38:49.530: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:38:49.531
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:38:49.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:38:49.548
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 12:38:49.551
  E0708 12:38:50.135307      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:51.135832      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:52.135906      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:53.136044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:38:53.572
  Jul  8 12:38:53.578: INFO: Trying to get logs from node ip-172-31-93-234 pod downwardapi-volume-afc0a444-04dd-458e-9185-bf503bada7c7 container client-container: <nil>
  STEP: delete the pod @ 07/08/23 12:38:53.585
  Jul  8 12:38:53.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2410" for this suite. @ 07/08/23 12:38:53.602
• [4.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 07/08/23 12:38:53.609
  Jul  8 12:38:53.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename resourcequota @ 07/08/23 12:38:53.609
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:38:53.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:38:53.624
  STEP: Creating a ResourceQuota with best effort scope @ 07/08/23 12:38:53.626
  STEP: Ensuring ResourceQuota status is calculated @ 07/08/23 12:38:53.631
  E0708 12:38:54.136138      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:55.136225      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 07/08/23 12:38:55.635
  STEP: Ensuring ResourceQuota status is calculated @ 07/08/23 12:38:55.639
  E0708 12:38:56.136562      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:57.136664      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 07/08/23 12:38:57.643
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 07/08/23 12:38:57.658
  E0708 12:38:58.137144      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:38:59.137279      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 07/08/23 12:38:59.661
  E0708 12:39:00.138205      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:01.138346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 07/08/23 12:39:01.666
  STEP: Ensuring resource quota status released the pod usage @ 07/08/23 12:39:01.679
  E0708 12:39:02.139105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:03.139604      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 07/08/23 12:39:03.683
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 07/08/23 12:39:03.695
  E0708 12:39:04.140180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:05.140288      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 07/08/23 12:39:05.699
  E0708 12:39:06.140829      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:07.140908      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 07/08/23 12:39:07.701
  STEP: Ensuring resource quota status released the pod usage @ 07/08/23 12:39:07.716
  E0708 12:39:08.141627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:09.141723      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:39:09.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6430" for this suite. @ 07/08/23 12:39:09.724
• [16.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 07/08/23 12:39:09.732
  Jul  8 12:39:09.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename runtimeclass @ 07/08/23 12:39:09.732
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:39:09.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:39:09.75
  Jul  8 12:39:09.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2370" for this suite. @ 07/08/23 12:39:09.777
• [0.052 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 07/08/23 12:39:09.784
  Jul  8 12:39:09.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename statefulset @ 07/08/23 12:39:09.785
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:39:09.796
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:39:09.798
  STEP: Creating service test in namespace statefulset-110 @ 07/08/23 12:39:09.804
  STEP: Creating a new StatefulSet @ 07/08/23 12:39:09.807
  Jul  8 12:39:09.818: INFO: Found 0 stateful pods, waiting for 3
  E0708 12:39:10.142645      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:11.142802      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:12.143217      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:13.143319      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:14.143499      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:15.143576      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:16.143730      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:17.144224      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:18.145084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:19.145183      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:39:19.822: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul  8 12:39:19.822: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul  8 12:39:19.822: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 07/08/23 12:39:19.831
  Jul  8 12:39:19.851: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 07/08/23 12:39:19.851
  E0708 12:39:20.145228      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:21.145375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:22.145748      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:23.145836      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:24.146615      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:25.146778      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:26.146900      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:27.146965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:28.147137      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:29.147229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 07/08/23 12:39:29.865
  STEP: Performing a canary update @ 07/08/23 12:39:29.865
  Jul  8 12:39:29.885: INFO: Updating stateful set ss2
  Jul  8 12:39:29.893: INFO: Waiting for Pod statefulset-110/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0708 12:39:30.147530      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:31.147900      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:32.148047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:33.148161      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:34.148238      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:35.149089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:36.149185      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:37.149466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:38.150030      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:39.150115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 07/08/23 12:39:39.901
  Jul  8 12:39:39.943: INFO: Found 2 stateful pods, waiting for 3
  E0708 12:39:40.150596      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:41.150762      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:42.151788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:43.152083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:44.152195      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:45.152320      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:46.152330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:47.153131      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:48.153194      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:49.153455      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:39:49.947: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul  8 12:39:49.947: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul  8 12:39:49.947: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 07/08/23 12:39:49.952
  Jul  8 12:39:49.971: INFO: Updating stateful set ss2
  Jul  8 12:39:49.977: INFO: Waiting for Pod statefulset-110/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0708 12:39:50.154453      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:51.154756      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:52.154838      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:53.155005      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:54.155087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:55.155251      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:56.155349      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:57.155607      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:58.155845      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:39:59.155964      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:40:00.002: INFO: Updating stateful set ss2
  Jul  8 12:40:00.008: INFO: Waiting for StatefulSet statefulset-110/ss2 to complete update
  Jul  8 12:40:00.008: INFO: Waiting for Pod statefulset-110/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0708 12:40:00.156433      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:01.157098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:02.157190      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:03.157282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:04.157387      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:05.157545      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:06.158521      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:07.158612      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:08.159548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:09.159634      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:40:10.015: INFO: Deleting all statefulset in ns statefulset-110
  Jul  8 12:40:10.018: INFO: Scaling statefulset ss2 to 0
  E0708 12:40:10.160608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:11.160728      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:12.161134      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:13.161328      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:14.161422      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:15.161506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:16.161664      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:17.161756      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:18.161913      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:19.162060      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:40:20.032: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  8 12:40:20.036: INFO: Deleting statefulset ss2
  Jul  8 12:40:20.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-110" for this suite. @ 07/08/23 12:40:20.058
• [70.279 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 07/08/23 12:40:20.064
  Jul  8 12:40:20.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 12:40:20.065
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:40:20.079
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:40:20.081
  STEP: Setting up server cert @ 07/08/23 12:40:20.102
  E0708 12:40:20.162109      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 12:40:20.288
  STEP: Deploying the webhook pod @ 07/08/23 12:40:20.295
  STEP: Wait for the deployment to be ready @ 07/08/23 12:40:20.305
  Jul  8 12:40:20.311: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0708 12:40:21.162241      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:22.162693      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 12:40:22.321
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 12:40:22.334
  E0708 12:40:23.162780      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:40:23.335: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 07/08/23 12:40:23.337
  STEP: Creating a custom resource definition that should be denied by the webhook @ 07/08/23 12:40:23.349
  Jul  8 12:40:23.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 12:40:23.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4019" for this suite. @ 07/08/23 12:40:23.403
  STEP: Destroying namespace "webhook-markers-2782" for this suite. @ 07/08/23 12:40:23.408
• [3.350 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 07/08/23 12:40:23.416
  Jul  8 12:40:23.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl @ 07/08/23 12:40:23.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:40:23.429
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:40:23.431
  STEP: Starting the proxy @ 07/08/23 12:40:23.433
  Jul  8 12:40:23.433: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-1092 proxy --unix-socket=/tmp/kubectl-proxy-unix3917793070/test'
  STEP: retrieving proxy /api/ output @ 07/08/23 12:40:23.468
  Jul  8 12:40:23.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1092" for this suite. @ 07/08/23 12:40:23.473
• [0.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 07/08/23 12:40:23.48
  Jul  8 12:40:23.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename pods @ 07/08/23 12:40:23.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:40:23.493
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:40:23.495
  STEP: creating the pod @ 07/08/23 12:40:23.497
  STEP: submitting the pod to kubernetes @ 07/08/23 12:40:23.497
  E0708 12:40:24.163098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:25.163883      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 07/08/23 12:40:25.519
  STEP: updating the pod @ 07/08/23 12:40:25.523
  Jul  8 12:40:26.033: INFO: Successfully updated pod "pod-update-f9c6ad11-7b62-4e09-b079-a02842016be3"
  STEP: verifying the updated pod is in kubernetes @ 07/08/23 12:40:26.037
  Jul  8 12:40:26.040: INFO: Pod update OK
  Jul  8 12:40:26.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-313" for this suite. @ 07/08/23 12:40:26.042
• [2.567 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 07/08/23 12:40:26.047
  Jul  8 12:40:26.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename limitrange @ 07/08/23 12:40:26.048
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:40:26.06
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:40:26.062
  STEP: Creating LimitRange "e2e-limitrange-gmlt8" in namespace "limitrange-9173" @ 07/08/23 12:40:26.064
  STEP: Creating another limitRange in another namespace @ 07/08/23 12:40:26.069
  Jul  8 12:40:26.084: INFO: Namespace "e2e-limitrange-gmlt8-8174" created
  Jul  8 12:40:26.084: INFO: Creating LimitRange "e2e-limitrange-gmlt8" in namespace "e2e-limitrange-gmlt8-8174"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-gmlt8" @ 07/08/23 12:40:26.088
  Jul  8 12:40:26.092: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-gmlt8" in "limitrange-9173" namespace @ 07/08/23 12:40:26.092
  Jul  8 12:40:26.096: INFO: LimitRange "e2e-limitrange-gmlt8" has been patched
  STEP: Delete LimitRange "e2e-limitrange-gmlt8" by Collection with labelSelector: "e2e-limitrange-gmlt8=patched" @ 07/08/23 12:40:26.096
  STEP: Confirm that the limitRange "e2e-limitrange-gmlt8" has been deleted @ 07/08/23 12:40:26.103
  Jul  8 12:40:26.103: INFO: Requesting list of LimitRange to confirm quantity
  Jul  8 12:40:26.105: INFO: Found 0 LimitRange with label "e2e-limitrange-gmlt8=patched"
  Jul  8 12:40:26.105: INFO: LimitRange "e2e-limitrange-gmlt8" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-gmlt8" @ 07/08/23 12:40:26.105
  Jul  8 12:40:26.109: INFO: Found 1 limitRange
  Jul  8 12:40:26.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-9173" for this suite. @ 07/08/23 12:40:26.113
  STEP: Destroying namespace "e2e-limitrange-gmlt8-8174" for this suite. @ 07/08/23 12:40:26.119
• [0.076 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 07/08/23 12:40:26.124
  Jul  8 12:40:26.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:40:26.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:40:26.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:40:26.138
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-58fbe891-d71f-49a5-ac15-182b9d6c6fbf @ 07/08/23 12:40:26.144
  STEP: Creating the pod @ 07/08/23 12:40:26.147
  E0708 12:40:26.163853      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:27.164048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:28.164678      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-58fbe891-d71f-49a5-ac15-182b9d6c6fbf @ 07/08/23 12:40:28.179
  STEP: waiting to observe update in volume @ 07/08/23 12:40:28.183
  E0708 12:40:29.165702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:30.165912      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:31.166104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:32.166413      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:33.166515      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:34.166691      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:35.166960      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:36.166869      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:37.166922      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:38.167127      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:39.167476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:40.167563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:41.167810      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:42.168074      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:43.168433      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:44.169088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:45.169189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:46.169713      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:47.170541      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:48.170699      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:49.170797      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:50.170913      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:51.171866      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:52.172020      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:53.172110      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:54.173087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:55.173173      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:56.173379      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:57.173640      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:58.173755      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:40:59.174128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:00.174497      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:01.174867      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:02.175963      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:03.176038      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:04.177088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:05.178070      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:06.178365      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:07.178459      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:08.179134      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:09.179229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:10.179692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:11.180137      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:12.181087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:13.181167      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:14.181251      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:15.181331      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:16.181633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:17.181725      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:18.181908      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:19.181977      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:20.182141      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:21.182171      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:22.183078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:23.183678      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:24.183845      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:25.183897      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:26.184149      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:27.184240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:28.185085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:29.185166      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:30.185348      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:31.185780      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:32.185937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:33.186901      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:34.186986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:35.187067      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:36.187666      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:37.188235      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:38.188327      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:39.188410      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:40.189086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:41.189218      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:42.189367      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:43.189632      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:44.190568      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:45.191575      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:46.192079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:47.192937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:48.193046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:41:48.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9248" for this suite. @ 07/08/23 12:41:48.491
• [82.374 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 07/08/23 12:41:48.499
  Jul  8 12:41:48.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:41:48.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:41:48.517
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:41:48.519
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 12:41:48.521
  E0708 12:41:49.193146      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:50.193361      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:51.194280      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:52.194371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:41:52.544
  Jul  8 12:41:52.547: INFO: Trying to get logs from node ip-172-31-93-234 pod downwardapi-volume-73d1e51d-1d27-461b-9f73-6ac9d1945d94 container client-container: <nil>
  STEP: delete the pod @ 07/08/23 12:41:52.562
  Jul  8 12:41:52.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4537" for this suite. @ 07/08/23 12:41:52.581
• [4.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 07/08/23 12:41:52.586
  Jul  8 12:41:52.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename dns @ 07/08/23 12:41:52.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:41:52.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:41:52.602
  STEP: Creating a test headless service @ 07/08/23 12:41:52.604
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6255.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6255.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6255.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6255.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6255.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6255.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6255.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6255.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6255.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6255.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6255.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6255.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 135.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.135_udp@PTR;check="$$(dig +tcp +noall +answer +search 135.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.135_tcp@PTR;sleep 1; done
   @ 07/08/23 12:41:52.625
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6255.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6255.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6255.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6255.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6255.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6255.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6255.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6255.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6255.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6255.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6255.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6255.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 135.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.135_udp@PTR;check="$$(dig +tcp +noall +answer +search 135.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.135_tcp@PTR;sleep 1; done
   @ 07/08/23 12:41:52.625
  STEP: creating a pod to probe DNS @ 07/08/23 12:41:52.625
  STEP: submitting the pod to kubernetes @ 07/08/23 12:41:52.625
  E0708 12:41:53.195055      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:54.195265      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/08/23 12:41:54.642
  STEP: looking for the results for each expected name from probers @ 07/08/23 12:41:54.645
  Jul  8 12:41:54.649: INFO: Unable to read wheezy_udp@dns-test-service.dns-6255.svc.cluster.local from pod dns-6255/dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec: the server could not find the requested resource (get pods dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec)
  Jul  8 12:41:54.652: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6255.svc.cluster.local from pod dns-6255/dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec: the server could not find the requested resource (get pods dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec)
  Jul  8 12:41:54.654: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6255.svc.cluster.local from pod dns-6255/dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec: the server could not find the requested resource (get pods dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec)
  Jul  8 12:41:54.657: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6255.svc.cluster.local from pod dns-6255/dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec: the server could not find the requested resource (get pods dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec)
  Jul  8 12:41:54.671: INFO: Unable to read jessie_udp@dns-test-service.dns-6255.svc.cluster.local from pod dns-6255/dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec: the server could not find the requested resource (get pods dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec)
  Jul  8 12:41:54.674: INFO: Unable to read jessie_tcp@dns-test-service.dns-6255.svc.cluster.local from pod dns-6255/dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec: the server could not find the requested resource (get pods dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec)
  Jul  8 12:41:54.677: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6255.svc.cluster.local from pod dns-6255/dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec: the server could not find the requested resource (get pods dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec)
  Jul  8 12:41:54.679: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6255.svc.cluster.local from pod dns-6255/dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec: the server could not find the requested resource (get pods dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec)
  Jul  8 12:41:54.691: INFO: Lookups using dns-6255/dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec failed for: [wheezy_udp@dns-test-service.dns-6255.svc.cluster.local wheezy_tcp@dns-test-service.dns-6255.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6255.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6255.svc.cluster.local jessie_udp@dns-test-service.dns-6255.svc.cluster.local jessie_tcp@dns-test-service.dns-6255.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6255.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6255.svc.cluster.local]

  E0708 12:41:55.195704      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:56.196287      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:57.197107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:58.197248      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:41:59.197411      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:41:59.739: INFO: DNS probes using dns-6255/dns-test-79b13091-ea64-45f1-bfe0-5306b43122ec succeeded

  Jul  8 12:41:59.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/08/23 12:41:59.743
  STEP: deleting the test service @ 07/08/23 12:41:59.754
  STEP: deleting the test headless service @ 07/08/23 12:41:59.779
  STEP: Destroying namespace "dns-6255" for this suite. @ 07/08/23 12:41:59.795
• [7.215 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 07/08/23 12:41:59.802
  Jul  8 12:41:59.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:41:59.803
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:41:59.822
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:41:59.825
  STEP: Creating configMap with name projected-configmap-test-volume-map-cdcd8258-1782-4245-9fb2-80b73a6cd399 @ 07/08/23 12:41:59.83
  STEP: Creating a pod to test consume configMaps @ 07/08/23 12:41:59.835
  E0708 12:42:00.197533      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:01.198029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:02.198080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:03.198170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:42:03.857
  Jul  8 12:42:03.859: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-projected-configmaps-d202eed4-8de0-4406-ae31-e591183ba992 container agnhost-container: <nil>
  STEP: delete the pod @ 07/08/23 12:42:03.866
  Jul  8 12:42:03.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3391" for this suite. @ 07/08/23 12:42:03.884
• [4.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 07/08/23 12:42:03.889
  Jul  8 12:42:03.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename namespaces @ 07/08/23 12:42:03.89
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:42:03.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:42:03.906
  STEP: Creating namespace "e2e-ns-7pj64" @ 07/08/23 12:42:03.909
  Jul  8 12:42:03.922: INFO: Namespace "e2e-ns-7pj64-5755" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-7pj64-5755" @ 07/08/23 12:42:03.922
  Jul  8 12:42:03.931: INFO: Namespace "e2e-ns-7pj64-5755" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-7pj64-5755" @ 07/08/23 12:42:03.931
  Jul  8 12:42:03.938: INFO: Namespace "e2e-ns-7pj64-5755" has []v1.FinalizerName{"kubernetes"}
  Jul  8 12:42:03.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5305" for this suite. @ 07/08/23 12:42:03.941
  STEP: Destroying namespace "e2e-ns-7pj64-5755" for this suite. @ 07/08/23 12:42:03.948
• [0.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 07/08/23 12:42:03.955
  Jul  8 12:42:03.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir @ 07/08/23 12:42:03.955
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:42:03.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:42:03.969
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 07/08/23 12:42:03.972
  E0708 12:42:04.199045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:05.199139      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:06.199877      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:07.200044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:42:07.993
  Jul  8 12:42:07.997: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-1a3f0d35-2acf-42d8-baa7-31ba9c6d68b1 container test-container: <nil>
  STEP: delete the pod @ 07/08/23 12:42:08.002
  Jul  8 12:42:08.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7119" for this suite. @ 07/08/23 12:42:08.019
• [4.070 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 07/08/23 12:42:08.024
  Jul  8 12:42:08.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/08/23 12:42:08.025
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:42:08.038
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:42:08.041
  Jul  8 12:42:08.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 12:42:08.200661      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:09.201546      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/08/23 12:42:09.412
  Jul  8 12:42:09.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-5313 --namespace=crd-publish-openapi-5313 create -f -'
  Jul  8 12:42:10.056: INFO: stderr: ""
  Jul  8 12:42:10.056: INFO: stdout: "e2e-test-crd-publish-openapi-9339-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jul  8 12:42:10.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-5313 --namespace=crd-publish-openapi-5313 delete e2e-test-crd-publish-openapi-9339-crds test-cr'
  Jul  8 12:42:10.112: INFO: stderr: ""
  Jul  8 12:42:10.112: INFO: stdout: "e2e-test-crd-publish-openapi-9339-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Jul  8 12:42:10.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-5313 --namespace=crd-publish-openapi-5313 apply -f -'
  E0708 12:42:10.202315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:42:10.356: INFO: stderr: ""
  Jul  8 12:42:10.356: INFO: stdout: "e2e-test-crd-publish-openapi-9339-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jul  8 12:42:10.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-5313 --namespace=crd-publish-openapi-5313 delete e2e-test-crd-publish-openapi-9339-crds test-cr'
  Jul  8 12:42:10.414: INFO: stderr: ""
  Jul  8 12:42:10.414: INFO: stdout: "e2e-test-crd-publish-openapi-9339-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 07/08/23 12:42:10.414
  Jul  8 12:42:10.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-5313 explain e2e-test-crd-publish-openapi-9339-crds'
  Jul  8 12:42:10.816: INFO: stderr: ""
  Jul  8 12:42:10.817: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-9339-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0708 12:42:11.202430      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:42:12.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5313" for this suite. @ 07/08/23 12:42:12.086
• [4.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 07/08/23 12:42:12.093
  Jul  8 12:42:12.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:42:12.094
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:42:12.109
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:42:12.111
  STEP: Creating configMap with name configmap-projected-all-test-volume-75a44140-529d-4bcf-ba76-8dc3217e56bf @ 07/08/23 12:42:12.112
  STEP: Creating secret with name secret-projected-all-test-volume-c838fbd6-fffe-4a6d-aaa2-660f3d2d5b0b @ 07/08/23 12:42:12.117
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 07/08/23 12:42:12.12
  E0708 12:42:12.202867      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:13.203117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:14.204079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:15.205113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:42:16.144
  Jul  8 12:42:16.147: INFO: Trying to get logs from node ip-172-31-93-234 pod projected-volume-f2a7aa1c-1a9f-4404-8f56-e26a90246b82 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 07/08/23 12:42:16.152
  Jul  8 12:42:16.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7812" for this suite. @ 07/08/23 12:42:16.171
• [4.084 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 07/08/23 12:42:16.178
  Jul  8 12:42:16.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename svcaccounts @ 07/08/23 12:42:16.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:42:16.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:42:16.198
  STEP: Creating ServiceAccount "e2e-sa-fgr8h"  @ 07/08/23 12:42:16.201
  E0708 12:42:16.205744      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:42:16.208: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-fgr8h"  @ 07/08/23 12:42:16.208
  Jul  8 12:42:16.215: INFO: AutomountServiceAccountToken: true
  Jul  8 12:42:16.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4384" for this suite. @ 07/08/23 12:42:16.221
• [0.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 07/08/23 12:42:16.228
  Jul  8 12:42:16.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename namespaces @ 07/08/23 12:42:16.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:42:16.246
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:42:16.25
  STEP: Updating Namespace "namespaces-4711" @ 07/08/23 12:42:16.252
  Jul  8 12:42:16.260: INFO: Namespace "namespaces-4711" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"65e215ab-230f-404c-8004-75735dff06e4", "kubernetes.io/metadata.name":"namespaces-4711", "namespaces-4711":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Jul  8 12:42:16.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4711" for this suite. @ 07/08/23 12:42:16.263
• [0.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 07/08/23 12:42:16.272
  Jul  8 12:42:16.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename disruption @ 07/08/23 12:42:16.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:42:16.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:42:16.29
  STEP: Waiting for the pdb to be processed @ 07/08/23 12:42:16.299
  E0708 12:42:17.206301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:18.206397      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 07/08/23 12:42:18.331
  Jul  8 12:42:18.338: INFO: running pods: 0 < 3
  E0708 12:42:19.207288      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:20.207379      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:42:20.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4852" for this suite. @ 07/08/23 12:42:20.348
• [4.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 07/08/23 12:42:20.355
  Jul  8 12:42:20.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename events @ 07/08/23 12:42:20.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:42:20.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:42:20.374
  STEP: creating a test event @ 07/08/23 12:42:20.377
  STEP: listing all events in all namespaces @ 07/08/23 12:42:20.38
  STEP: patching the test event @ 07/08/23 12:42:20.386
  STEP: fetching the test event @ 07/08/23 12:42:20.392
  STEP: updating the test event @ 07/08/23 12:42:20.395
  STEP: getting the test event @ 07/08/23 12:42:20.403
  STEP: deleting the test event @ 07/08/23 12:42:20.406
  STEP: listing all events in all namespaces @ 07/08/23 12:42:20.413
  Jul  8 12:42:20.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5653" for this suite. @ 07/08/23 12:42:20.421
• [0.071 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 07/08/23 12:42:20.426
  Jul  8 12:42:20.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename watch @ 07/08/23 12:42:20.426
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:42:20.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:42:20.442
  STEP: creating a watch on configmaps with a certain label @ 07/08/23 12:42:20.488
  STEP: creating a new configmap @ 07/08/23 12:42:20.49
  STEP: modifying the configmap once @ 07/08/23 12:42:20.496
  STEP: changing the label value of the configmap @ 07/08/23 12:42:20.502
  STEP: Expecting to observe a delete notification for the watched object @ 07/08/23 12:42:20.511
  Jul  8 12:42:20.512: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2116  c0872f69-b6f2-4dd6-a59b-25243ff3fc61 20209 0 2023-07-08 12:42:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-08 12:42:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 12:42:20.512: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2116  c0872f69-b6f2-4dd6-a59b-25243ff3fc61 20210 0 2023-07-08 12:42:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-08 12:42:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 12:42:20.512: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2116  c0872f69-b6f2-4dd6-a59b-25243ff3fc61 20211 0 2023-07-08 12:42:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-08 12:42:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 07/08/23 12:42:20.512
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 07/08/23 12:42:20.522
  E0708 12:42:21.208126      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:22.209098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:23.209260      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:24.210287      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:25.210691      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:26.210979      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:27.211719      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:28.211812      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:29.211888      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:30.212027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 07/08/23 12:42:30.522
  STEP: modifying the configmap a third time @ 07/08/23 12:42:30.53
  STEP: deleting the configmap @ 07/08/23 12:42:30.536
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 07/08/23 12:42:30.542
  Jul  8 12:42:30.542: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2116  c0872f69-b6f2-4dd6-a59b-25243ff3fc61 20311 0 2023-07-08 12:42:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-08 12:42:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 12:42:30.542: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2116  c0872f69-b6f2-4dd6-a59b-25243ff3fc61 20312 0 2023-07-08 12:42:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-08 12:42:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 12:42:30.542: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2116  c0872f69-b6f2-4dd6-a59b-25243ff3fc61 20313 0 2023-07-08 12:42:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-08 12:42:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 12:42:30.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2116" for this suite. @ 07/08/23 12:42:30.545
• [10.125 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 07/08/23 12:42:30.552
  Jul  8 12:42:30.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 12:42:30.553
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:42:30.565
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:42:30.567
  STEP: Setting up server cert @ 07/08/23 12:42:30.585
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 12:42:30.737
  STEP: Deploying the webhook pod @ 07/08/23 12:42:30.743
  STEP: Wait for the deployment to be ready @ 07/08/23 12:42:30.76
  Jul  8 12:42:30.766: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0708 12:42:31.212430      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:32.212581      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 12:42:32.777
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 12:42:32.791
  E0708 12:42:33.213091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:42:33.791: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 07/08/23 12:42:33.796
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 07/08/23 12:42:33.811
  STEP: Creating a dummy validating-webhook-configuration object @ 07/08/23 12:42:33.821
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 07/08/23 12:42:33.828
  STEP: Creating a dummy mutating-webhook-configuration object @ 07/08/23 12:42:33.833
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 07/08/23 12:42:33.84
  Jul  8 12:42:33.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5272" for this suite. @ 07/08/23 12:42:33.9
  STEP: Destroying namespace "webhook-markers-5853" for this suite. @ 07/08/23 12:42:33.91
• [3.363 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 07/08/23 12:42:33.916
  Jul  8 12:42:33.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-runtime @ 07/08/23 12:42:33.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:42:33.929
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:42:33.932
  STEP: create the container @ 07/08/23 12:42:33.934
  W0708 12:42:33.944735      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/08/23 12:42:33.944
  E0708 12:42:34.213519      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:35.213930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:36.214518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/08/23 12:42:36.958
  STEP: the container should be terminated @ 07/08/23 12:42:36.96
  STEP: the termination message should be set @ 07/08/23 12:42:36.96
  Jul  8 12:42:36.960: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 07/08/23 12:42:36.96
  Jul  8 12:42:36.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8314" for this suite. @ 07/08/23 12:42:36.977
• [3.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 07/08/23 12:42:36.985
  Jul  8 12:42:36.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename job @ 07/08/23 12:42:36.986
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:42:36.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:42:37.006
  STEP: Creating a job @ 07/08/23 12:42:37.009
  STEP: Ensuring active pods == parallelism @ 07/08/23 12:42:37.016
  E0708 12:42:37.215542      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:38.215710      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 07/08/23 12:42:39.021
  STEP: deleting Job.batch foo in namespace job-3440, will wait for the garbage collector to delete the pods @ 07/08/23 12:42:39.021
  Jul  8 12:42:39.081: INFO: Deleting Job.batch foo took: 5.440934ms
  Jul  8 12:42:39.181: INFO: Terminating Job.batch foo pods took: 100.874697ms
  E0708 12:42:39.216116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:40.217164      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:41.217518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:42.217545      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:43.218534      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:44.219371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:45.220317      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:46.220565      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:47.221596      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:48.222585      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:49.223578      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:50.224527      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:51.224597      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:52.225576      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:53.226551      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:54.227526      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:55.228417      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:56.229403      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:57.230362      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:58.231180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:42:59.232118      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:00.233145      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:01.233359      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:02.234370      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:03.235402      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:04.235481      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:05.236424      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:06.237405      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:07.238210      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:08.239110      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:09.239525      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:10.240397      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:11.241141      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 07/08/23 12:43:11.782
  Jul  8 12:43:11.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3440" for this suite. @ 07/08/23 12:43:11.789
• [34.810 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 07/08/23 12:43:11.796
  Jul  8 12:43:11.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 12:43:11.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:43:11.811
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:43:11.814
  STEP: Setting up server cert @ 07/08/23 12:43:11.835
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 12:43:12.184
  STEP: Deploying the webhook pod @ 07/08/23 12:43:12.192
  STEP: Wait for the deployment to be ready @ 07/08/23 12:43:12.203
  Jul  8 12:43:12.209: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0708 12:43:12.241494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:13.241628      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 12:43:14.219
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 12:43:14.226
  E0708 12:43:14.241979      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:43:15.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 07/08/23 12:43:15.229
  STEP: create a pod that should be updated by the webhook @ 07/08/23 12:43:15.241
  E0708 12:43:15.242016      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:43:15.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4961" for this suite. @ 07/08/23 12:43:15.306
  STEP: Destroying namespace "webhook-markers-7816" for this suite. @ 07/08/23 12:43:15.313
• [3.522 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 07/08/23 12:43:15.318
  Jul  8 12:43:15.319: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename resourcequota @ 07/08/23 12:43:15.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:43:15.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:43:15.334
  STEP: Counting existing ResourceQuota @ 07/08/23 12:43:15.336
  E0708 12:43:16.242243      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:17.242352      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:18.242779      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:19.243162      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:20.243268      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/08/23 12:43:20.338
  STEP: Ensuring resource quota status is calculated @ 07/08/23 12:43:20.343
  E0708 12:43:21.244025      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:22.245088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 07/08/23 12:43:22.347
  STEP: Ensuring ResourceQuota status captures the pod usage @ 07/08/23 12:43:22.36
  E0708 12:43:23.245173      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:24.245330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 07/08/23 12:43:24.366
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 07/08/23 12:43:24.367
  STEP: Ensuring a pod cannot update its resource requirements @ 07/08/23 12:43:24.369
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 07/08/23 12:43:24.373
  E0708 12:43:25.245421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:26.246359      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 07/08/23 12:43:26.378
  STEP: Ensuring resource quota status released the pod usage @ 07/08/23 12:43:26.389
  E0708 12:43:27.247218      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:28.247395      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:43:28.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3388" for this suite. @ 07/08/23 12:43:28.396
• [13.083 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 07/08/23 12:43:28.402
  Jul  8 12:43:28.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename cronjob @ 07/08/23 12:43:28.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:43:28.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:43:28.418
  STEP: Creating a ForbidConcurrent cronjob @ 07/08/23 12:43:28.42
  STEP: Ensuring a job is scheduled @ 07/08/23 12:43:28.426
  E0708 12:43:29.248363      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:30.248436      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:31.249277      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:32.249449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:33.250057      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:34.250159      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:35.250248      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:36.250457      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:37.250565      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:38.250747      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:39.251670      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:40.251845      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:41.252036      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:42.253103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:43.253194      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:44.253374      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:45.253461      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:46.253727      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:47.253835      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:48.254005      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:49.254088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:50.254449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:51.255158      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:52.255312      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:53.255399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:54.255578      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:55.255656      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:56.255944      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:57.256364      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:58.256459      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:43:59.257056      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:00.257236      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 07/08/23 12:44:00.428
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 07/08/23 12:44:00.432
  STEP: Ensuring no more jobs are scheduled @ 07/08/23 12:44:00.434
  E0708 12:44:01.258062      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:02.258160      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:03.258237      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:04.258328      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:05.258492      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:06.258797      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:07.258897      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:08.259579      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:09.260500      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:10.261482      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:11.262098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:12.262328      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:13.262396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:14.262505      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:15.262583      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:16.262848      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:17.263839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:18.264000      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:19.264083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:20.265085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:21.265304      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:22.265385      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:23.265477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:24.265638      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:25.265724      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:26.265811      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:27.266818      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:28.267599      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:29.268020      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:30.268082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:31.269001      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:32.269171      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:33.269404      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:34.269990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:35.270559      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:36.270626      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:37.271133      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:38.271202      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:39.272021      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:40.273103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:41.273298      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:42.273440      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:43.273534      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:44.273700      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:45.274389      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:46.274409      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:47.274516      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:48.274674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:49.275212      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:50.275372      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:51.276293      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:52.276384      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:53.276470      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:54.277086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:55.278065      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:56.278343      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:57.279409      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:58.280042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:44:59.281088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:00.281507      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:01.282070      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:02.282166      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:03.283126      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:04.283185      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:05.283419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:06.284329      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:07.285094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:08.285523      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:09.285543      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:10.285713      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:11.286314      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:12.286465      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:13.286551      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:14.286714      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:15.287209      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:16.287252      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:17.288331      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:18.288408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:19.288494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:20.289411      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:21.290042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:22.290139      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:23.291198      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:24.291291      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:25.291367      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:26.291791      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:27.292027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:28.292116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:29.293104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:30.293272      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:31.294323      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:32.294467      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:33.294962      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:34.295055      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:35.295801      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:36.296073      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:37.296289      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:38.296382      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:39.297081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:40.297857      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:41.298311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:42.298397      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:43.299080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:44.299168      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:45.299452      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:46.300238      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:47.301082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:48.301178      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:49.301262      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:50.301420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:51.302323      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:52.303189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:53.304021      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:54.304103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:55.305079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:56.305334      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:57.305471      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:58.305632      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:45:59.306616      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:00.307062      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:01.307357      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:02.307495      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:03.308028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:04.309093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:05.309137      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:06.309374      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:07.310380      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:08.310473      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:09.311025      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:10.311182      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:11.312092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:12.312309      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:13.313108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:14.313180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:15.313817      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:16.314851      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:17.315431      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:18.315778      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:19.316026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:20.317099      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:21.317977      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:22.318061      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:23.318972      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:24.319060      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:25.319453      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:26.320391      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:27.320981      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:28.321134      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:29.321226      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:30.321394      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:31.322318      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:32.323353      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:33.324007      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:34.324099      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:35.325089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:36.325376      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:37.325470      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:38.325564      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:39.325652      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:40.326158      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:41.327105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:42.328182      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:43.328273      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:44.329096      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:45.329400      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:46.329647      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:47.329735      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:48.329895      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:49.329991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:50.330078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:51.331117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:52.331272      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:53.332092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:54.333085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:55.333197      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:56.333340      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:57.333425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:58.333515      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:46:59.333607      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:00.333770      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:01.334144      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:02.334230      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:03.334741      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:04.334843      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:05.335065      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:06.335409      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:07.335493      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:08.335588      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:09.336025      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:10.336110      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:11.337074      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:12.337290      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:13.338272      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:14.338301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:15.338820      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:16.339086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:17.340119      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:18.341093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:19.341183      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:20.341339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:21.342256      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:22.342445      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:23.343346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:24.343483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:25.343649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:26.344008      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:27.344045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:28.344175      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:29.345089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:30.345257      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:31.345323      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:32.345400      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:33.345493      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:34.345677      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:35.345775      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:36.346040      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:37.346148      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:38.346220      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:39.346311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:40.346963      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:41.347046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:42.347229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:43.347340      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:44.347442      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:45.348049      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:46.348269      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:47.348372      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:48.348477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:49.348554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:50.349205      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:51.350001      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:52.350061      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:53.350149      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:54.350336      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:55.350424      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:56.350675      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:57.350785      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:58.350967      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:47:59.351058      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:00.351245      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:01.352241      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:02.353109      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:03.353199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:04.353393      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:05.353485      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:06.353767      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:07.353875      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:08.354042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:09.354083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:10.354185      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:11.355161      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:12.355311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:13.356209      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:14.357088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:15.357907      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:16.358424      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:17.358745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:18.358917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:19.359000      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:20.359726      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:21.360373      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:22.360459      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:23.360550      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:24.361095      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:25.361624      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:26.361933      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:27.362056      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:28.362146      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:29.362242      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:30.362375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:31.362839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:32.362937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:33.363750      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:34.364604      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:35.364693      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:36.365565      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:37.365743      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:38.366596      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:39.367620      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:40.367714      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:41.368396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:42.369091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:43.369651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:44.369795      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:45.370335      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:46.370386      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:47.370595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:48.370751      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:49.371422      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:50.371496      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:51.372389      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:52.372548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:53.372633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:54.372788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:55.372886      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:56.372955      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:57.373808      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:58.373898      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:48:59.374042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:00.375040      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 07/08/23 12:49:00.44
  Jul  8 12:49:00.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1625" for this suite. @ 07/08/23 12:49:00.45
• [332.054 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 07/08/23 12:49:00.456
  Jul  8 12:49:00.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-probe @ 07/08/23 12:49:00.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:49:00.471
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:49:00.473
  E0708 12:49:01.375566      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:02.375651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:03.376043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:04.376165      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:05.377098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:06.377408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:07.377514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:08.377711      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:09.378577      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:10.378656      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:11.378984      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:12.379298      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:13.379387      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:14.379559      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:15.379646      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:16.379948      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:17.380047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:18.381109      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:19.381326      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:20.381521      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:21.381673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:22.381711      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:49:22.533: INFO: Container started at 2023-07-08 12:49:01 +0000 UTC, pod became ready at 2023-07-08 12:49:20 +0000 UTC
  Jul  8 12:49:22.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-728" for this suite. @ 07/08/23 12:49:22.537
• [22.087 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 07/08/23 12:49:22.543
  Jul  8 12:49:22.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename sched-preemption @ 07/08/23 12:49:22.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:49:22.65
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:49:22.655
  Jul  8 12:49:22.671: INFO: Waiting up to 1m0s for all nodes to be ready
  E0708 12:49:23.381779      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:24.382048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:25.383019      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:26.383188      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:27.383337      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:28.383422      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:29.384159      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:30.384257      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:31.384328      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:32.385096      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:33.386014      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:34.386086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:35.386179      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:36.386357      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:37.386420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:38.386510      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:39.386605      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:40.387226      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:41.388291      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:42.388379      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:43.389253      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:44.389324      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:45.389425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:46.389631      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:47.390596      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:48.390687      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:49.390775      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:50.390962      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:51.391439      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:52.391524      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:53.391619      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:54.392491      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:55.393108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:56.393444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:57.393545      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:58.393633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:49:59.393744      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:00.393803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:01.394188      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:02.394308      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:03.394375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:04.394551      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:05.394644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:06.394886      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:07.394988      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:08.395076      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:09.395168      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:10.395334      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:11.395884      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:12.396132      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:13.397105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:14.398019      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:15.398087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:16.398414      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:17.398522      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:18.398596      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:19.399430      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:20.399523      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:21.399882      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:22.400027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:50:22.685: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 07/08/23 12:50:22.687
  Jul  8 12:50:22.706: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jul  8 12:50:22.714: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jul  8 12:50:22.730: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jul  8 12:50:22.737: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jul  8 12:50:22.757: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jul  8 12:50:22.773: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 07/08/23 12:50:22.773
  E0708 12:50:23.400124      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:24.401090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 07/08/23 12:50:24.798
  E0708 12:50:25.401615      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:26.402671      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:27.403101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:28.403269      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:50:28.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-9722" for this suite. @ 07/08/23 12:50:28.886
• [66.347 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 07/08/23 12:50:28.894
  Jul  8 12:50:28.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename daemonsets @ 07/08/23 12:50:28.895
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:50:28.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:50:28.911
  Jul  8 12:50:28.929: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/08/23 12:50:28.933
  Jul  8 12:50:28.936: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:50:28.936: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:50:28.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 12:50:28.938: INFO: Node ip-172-31-12-67 is running 0 daemon pod, expected 1
  E0708 12:50:29.403532      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:50:29.942: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:50:29.942: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:50:29.944: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  8 12:50:29.944: INFO: Node ip-172-31-12-67 is running 0 daemon pod, expected 1
  E0708 12:50:30.404298      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:50:30.943: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:50:30.943: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:50:30.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  8 12:50:30.945: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 07/08/23 12:50:30.958
  STEP: Check that daemon pods images are updated. @ 07/08/23 12:50:30.969
  Jul  8 12:50:30.972: INFO: Wrong image for pod: daemon-set-kpjbn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  8 12:50:30.972: INFO: Wrong image for pod: daemon-set-qlndx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  8 12:50:30.972: INFO: Wrong image for pod: daemon-set-wxh54. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  8 12:50:30.979: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:50:30.979: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0708 12:50:31.404793      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:50:31.982: INFO: Wrong image for pod: daemon-set-kpjbn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  8 12:50:31.982: INFO: Wrong image for pod: daemon-set-qlndx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  8 12:50:31.984: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:50:31.984: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0708 12:50:32.405054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:50:32.984: INFO: Wrong image for pod: daemon-set-kpjbn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  8 12:50:32.984: INFO: Wrong image for pod: daemon-set-qlndx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  8 12:50:32.989: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:50:32.989: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0708 12:50:33.405433      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:50:33.983: INFO: Pod daemon-set-8hk4x is not available
  Jul  8 12:50:33.983: INFO: Wrong image for pod: daemon-set-kpjbn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  8 12:50:33.983: INFO: Wrong image for pod: daemon-set-qlndx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  8 12:50:33.987: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:50:33.987: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0708 12:50:34.406110      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:50:34.982: INFO: Wrong image for pod: daemon-set-qlndx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  8 12:50:34.986: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:50:34.986: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0708 12:50:35.407087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:50:35.983: INFO: Pod daemon-set-hgf7h is not available
  Jul  8 12:50:35.983: INFO: Wrong image for pod: daemon-set-qlndx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul  8 12:50:35.987: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:50:35.987: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0708 12:50:36.407782      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:50:36.985: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:50:36.986: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0708 12:50:37.408494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:50:37.986: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:50:37.987: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 07/08/23 12:50:37.987
  Jul  8 12:50:37.989: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:50:37.989: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:50:37.992: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  8 12:50:37.992: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/08/23 12:50:38.006
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-184, will wait for the garbage collector to delete the pods @ 07/08/23 12:50:38.006
  Jul  8 12:50:38.066: INFO: Deleting DaemonSet.extensions daemon-set took: 7.295197ms
  Jul  8 12:50:38.167: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.905294ms
  E0708 12:50:38.409477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:39.409883      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:50:39.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 12:50:39.870: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul  8 12:50:39.872: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21992"},"items":null}

  Jul  8 12:50:39.876: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21992"},"items":null}

  Jul  8 12:50:39.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-184" for this suite. @ 07/08/23 12:50:39.891
• [11.003 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 07/08/23 12:50:39.897
  Jul  8 12:50:39.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename ingress @ 07/08/23 12:50:39.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:50:39.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:50:39.915
  STEP: getting /apis @ 07/08/23 12:50:39.917
  STEP: getting /apis/networking.k8s.io @ 07/08/23 12:50:39.92
  STEP: getting /apis/networking.k8s.iov1 @ 07/08/23 12:50:39.921
  STEP: creating @ 07/08/23 12:50:39.922
  STEP: getting @ 07/08/23 12:50:39.938
  STEP: listing @ 07/08/23 12:50:39.94
  STEP: watching @ 07/08/23 12:50:39.946
  Jul  8 12:50:39.946: INFO: starting watch
  STEP: cluster-wide listing @ 07/08/23 12:50:39.947
  STEP: cluster-wide watching @ 07/08/23 12:50:39.95
  Jul  8 12:50:39.950: INFO: starting watch
  STEP: patching @ 07/08/23 12:50:39.951
  STEP: updating @ 07/08/23 12:50:39.955
  Jul  8 12:50:39.964: INFO: waiting for watch events with expected annotations
  Jul  8 12:50:39.965: INFO: saw patched and updated annotations
  STEP: patching /status @ 07/08/23 12:50:39.965
  STEP: updating /status @ 07/08/23 12:50:39.972
  STEP: get /status @ 07/08/23 12:50:39.983
  STEP: deleting @ 07/08/23 12:50:39.987
  STEP: deleting a collection @ 07/08/23 12:50:40.005
  Jul  8 12:50:40.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-3013" for this suite. @ 07/08/23 12:50:40.031
• [0.141 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 07/08/23 12:50:40.041
  Jul  8 12:50:40.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename services @ 07/08/23 12:50:40.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:50:40.057
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:50:40.061
  STEP: creating a collection of services @ 07/08/23 12:50:40.064
  Jul  8 12:50:40.064: INFO: Creating e2e-svc-a-rjkdq
  Jul  8 12:50:40.073: INFO: Creating e2e-svc-b-5dzxh
  Jul  8 12:50:40.083: INFO: Creating e2e-svc-c-v475n
  STEP: deleting service collection @ 07/08/23 12:50:40.096
  Jul  8 12:50:40.124: INFO: Collection of services has been deleted
  Jul  8 12:50:40.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3179" for this suite. @ 07/08/23 12:50:40.127
• [0.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 07/08/23 12:50:40.133
  Jul  8 12:50:40.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 12:50:40.134
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:50:40.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:50:40.156
  STEP: Setting up server cert @ 07/08/23 12:50:40.18
  E0708 12:50:40.410118      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 12:50:40.523
  STEP: Deploying the webhook pod @ 07/08/23 12:50:40.527
  STEP: Wait for the deployment to be ready @ 07/08/23 12:50:40.539
  Jul  8 12:50:40.548: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0708 12:50:41.410136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:42.410236      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 12:50:42.561
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 12:50:42.576
  E0708 12:50:43.410336      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:50:43.577: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul  8 12:50:43.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-936-crds.webhook.example.com via the AdmissionRegistration API @ 07/08/23 12:50:44.088
  STEP: Creating a custom resource that should be mutated by the webhook @ 07/08/23 12:50:44.101
  E0708 12:50:44.411309      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:45.411569      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:50:46.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0708 12:50:46.411723      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-1589" for this suite. @ 07/08/23 12:50:46.688
  STEP: Destroying namespace "webhook-markers-6128" for this suite. @ 07/08/23 12:50:46.695
• [6.568 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 07/08/23 12:50:46.701
  Jul  8 12:50:46.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/08/23 12:50:46.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:50:46.717
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:50:46.72
  STEP: set up a multi version CRD @ 07/08/23 12:50:46.722
  Jul  8 12:50:46.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 12:50:47.412467      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:48.412808      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:49.413418      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 07/08/23 12:50:49.954
  STEP: check the new version name is served @ 07/08/23 12:50:49.97
  E0708 12:50:50.413458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 07/08/23 12:50:51.217
  E0708 12:50:51.414429      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 07/08/23 12:50:51.874
  E0708 12:50:52.416880      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:53.417732      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:54.417768      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:50:54.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5782" for this suite. @ 07/08/23 12:50:54.442
• [7.748 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 07/08/23 12:50:54.45
  Jul  8 12:50:54.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename pods @ 07/08/23 12:50:54.451
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:50:54.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:50:54.471
  Jul  8 12:50:54.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: creating the pod @ 07/08/23 12:50:54.474
  STEP: submitting the pod to kubernetes @ 07/08/23 12:50:54.474
  E0708 12:50:55.417885      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:56.418401      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:50:56.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-315" for this suite. @ 07/08/23 12:50:56.566
• [2.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 07/08/23 12:50:56.575
  Jul  8 12:50:56.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename downward-api @ 07/08/23 12:50:56.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:50:56.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:50:56.594
  STEP: Creating a pod to test downward api env vars @ 07/08/23 12:50:56.597
  E0708 12:50:57.418506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:58.418590      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:50:59.418986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:00.419154      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:51:00.621
  Jul  8 12:51:00.625: INFO: Trying to get logs from node ip-172-31-29-188 pod downward-api-45396011-cd96-4908-bbd4-ba9e941239ef container dapi-container: <nil>
  STEP: delete the pod @ 07/08/23 12:51:00.643
  Jul  8 12:51:00.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4546" for this suite. @ 07/08/23 12:51:00.665
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 07/08/23 12:51:00.673
  Jul  8 12:51:00.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename security-context @ 07/08/23 12:51:00.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:51:00.687
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:51:00.69
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 07/08/23 12:51:00.694
  E0708 12:51:01.419782      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:02.419883      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:51:02.711
  Jul  8 12:51:02.715: INFO: Trying to get logs from node ip-172-31-93-234 pod security-context-5a1b2c27-06d6-4cf4-872e-b354631ad91e container test-container: <nil>
  STEP: delete the pod @ 07/08/23 12:51:02.729
  Jul  8 12:51:02.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-5057" for this suite. @ 07/08/23 12:51:02.747
• [2.081 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 07/08/23 12:51:02.754
  Jul  8 12:51:02.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 12:51:02.754
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:51:02.769
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:51:02.773
  STEP: Setting up server cert @ 07/08/23 12:51:02.799
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 12:51:03.18
  STEP: Deploying the webhook pod @ 07/08/23 12:51:03.19
  STEP: Wait for the deployment to be ready @ 07/08/23 12:51:03.202
  Jul  8 12:51:03.210: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0708 12:51:03.420796      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:04.420903      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 12:51:05.222
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 12:51:05.232
  E0708 12:51:05.421048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:51:06.234: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 07/08/23 12:51:06.237
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 07/08/23 12:51:06.239
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 07/08/23 12:51:06.239
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 07/08/23 12:51:06.239
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 07/08/23 12:51:06.241
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 07/08/23 12:51:06.241
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 07/08/23 12:51:06.242
  Jul  8 12:51:06.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4777" for this suite. @ 07/08/23 12:51:06.283
  STEP: Destroying namespace "webhook-markers-2387" for this suite. @ 07/08/23 12:51:06.292
• [3.544 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 07/08/23 12:51:06.299
  Jul  8 12:51:06.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename downward-api @ 07/08/23 12:51:06.299
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:51:06.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:51:06.317
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 12:51:06.32
  E0708 12:51:06.421961      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:07.422069      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:08.422582      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:09.422773      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:51:10.349
  Jul  8 12:51:10.353: INFO: Trying to get logs from node ip-172-31-29-188 pod downwardapi-volume-e46bab2c-bfb7-438f-b7fd-e0b10a4188db container client-container: <nil>
  STEP: delete the pod @ 07/08/23 12:51:10.36
  Jul  8 12:51:10.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9091" for this suite. @ 07/08/23 12:51:10.382
• [4.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 07/08/23 12:51:10.39
  Jul  8 12:51:10.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:51:10.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:51:10.409
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:51:10.412
  STEP: Creating secret with name projected-secret-test-c99c8e8c-a12c-439a-bb9b-ef0988e9f857 @ 07/08/23 12:51:10.416
  STEP: Creating a pod to test consume secrets @ 07/08/23 12:51:10.42
  E0708 12:51:10.423064      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:11.423448      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:12.423527      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:13.423628      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:14.423718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:51:14.444
  Jul  8 12:51:14.448: INFO: Trying to get logs from node ip-172-31-29-188 pod pod-projected-secrets-b45769de-42ed-4bea-9e91-239b6d39cebf container secret-volume-test: <nil>
  STEP: delete the pod @ 07/08/23 12:51:14.456
  Jul  8 12:51:14.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3913" for this suite. @ 07/08/23 12:51:14.478
• [4.096 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 07/08/23 12:51:14.486
  Jul  8 12:51:14.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename crd-webhook @ 07/08/23 12:51:14.487
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:51:14.502
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:51:14.505
  STEP: Setting up server cert @ 07/08/23 12:51:14.507
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 07/08/23 12:51:14.693
  STEP: Deploying the custom resource conversion webhook pod @ 07/08/23 12:51:14.699
  STEP: Wait for the deployment to be ready @ 07/08/23 12:51:14.711
  Jul  8 12:51:14.722: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0708 12:51:15.424503      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:16.425342      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 12:51:16.735
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 12:51:16.747
  E0708 12:51:17.426371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:51:17.747: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jul  8 12:51:17.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 12:51:18.426428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:19.427366      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 07/08/23 12:51:20.323
  STEP: Create a v2 custom resource @ 07/08/23 12:51:20.341
  STEP: List CRs in v1 @ 07/08/23 12:51:20.379
  STEP: List CRs in v2 @ 07/08/23 12:51:20.385
  Jul  8 12:51:20.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0708 12:51:20.428223      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-3692" for this suite. @ 07/08/23 12:51:20.953
• [6.479 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 07/08/23 12:51:20.966
  Jul  8 12:51:20.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename pods @ 07/08/23 12:51:20.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:51:20.982
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:51:20.986
  STEP: Create a pod @ 07/08/23 12:51:20.99
  E0708 12:51:21.428897      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:22.429748      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 07/08/23 12:51:23.006
  Jul  8 12:51:23.015: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Jul  8 12:51:23.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3295" for this suite. @ 07/08/23 12:51:23.019
• [2.060 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 07/08/23 12:51:23.026
  Jul  8 12:51:23.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename job @ 07/08/23 12:51:23.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:51:23.042
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:51:23.045
  STEP: Creating a job @ 07/08/23 12:51:23.048
  STEP: Ensuring job reaches completions @ 07/08/23 12:51:23.055
  E0708 12:51:23.430865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:24.430929      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:25.431008      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:26.432041      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:27.432511      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:28.433297      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:29.434279      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:30.434910      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:31.435372      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:32.435537      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:51:33.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7397" for this suite. @ 07/08/23 12:51:33.064
• [10.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 07/08/23 12:51:33.072
  Jul  8 12:51:33.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl-logs @ 07/08/23 12:51:33.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:51:33.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:51:33.092
  STEP: creating an pod @ 07/08/23 12:51:33.097
  Jul  8 12:51:33.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-logs-2828 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Jul  8 12:51:33.152: INFO: stderr: ""
  Jul  8 12:51:33.152: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 07/08/23 12:51:33.152
  Jul  8 12:51:33.152: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0708 12:51:33.435821      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:34.436046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:51:35.161: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 07/08/23 12:51:35.161
  Jul  8 12:51:35.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-logs-2828 logs logs-generator logs-generator'
  Jul  8 12:51:35.220: INFO: stderr: ""
  Jul  8 12:51:35.220: INFO: stdout: "I0708 12:51:33.787427       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/fqb 383\nI0708 12:51:33.987800       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/mv6 275\nI0708 12:51:34.188129       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/mhgb 593\nI0708 12:51:34.388419       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/zpnb 410\nI0708 12:51:34.587518       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/lqjf 467\nI0708 12:51:34.787809       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/qlgd 431\nI0708 12:51:34.988054       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/84t 522\nI0708 12:51:35.188343       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/vbr 494\n"
  STEP: limiting log lines @ 07/08/23 12:51:35.22
  Jul  8 12:51:35.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-logs-2828 logs logs-generator logs-generator --tail=1'
  Jul  8 12:51:35.277: INFO: stderr: ""
  Jul  8 12:51:35.277: INFO: stdout: "I0708 12:51:35.188343       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/vbr 494\n"
  Jul  8 12:51:35.278: INFO: got output "I0708 12:51:35.188343       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/vbr 494\n"
  STEP: limiting log bytes @ 07/08/23 12:51:35.278
  Jul  8 12:51:35.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-logs-2828 logs logs-generator logs-generator --limit-bytes=1'
  Jul  8 12:51:35.331: INFO: stderr: ""
  Jul  8 12:51:35.331: INFO: stdout: "I"
  Jul  8 12:51:35.331: INFO: got output "I"
  STEP: exposing timestamps @ 07/08/23 12:51:35.331
  Jul  8 12:51:35.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-logs-2828 logs logs-generator logs-generator --tail=1 --timestamps'
  Jul  8 12:51:35.387: INFO: stderr: ""
  Jul  8 12:51:35.387: INFO: stdout: "2023-07-08T12:51:35.188417890Z I0708 12:51:35.188343       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/vbr 494\n"
  Jul  8 12:51:35.387: INFO: got output "2023-07-08T12:51:35.188417890Z I0708 12:51:35.188343       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/vbr 494\n"
  STEP: restricting to a time range @ 07/08/23 12:51:35.387
  E0708 12:51:35.436273      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:36.436453      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:37.436547      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:51:37.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-logs-2828 logs logs-generator logs-generator --since=1s'
  Jul  8 12:51:37.941: INFO: stderr: ""
  Jul  8 12:51:37.941: INFO: stdout: "I0708 12:51:36.987743       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/7n82 376\nI0708 12:51:37.188032       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/l4q 552\nI0708 12:51:37.388273       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/v4k 384\nI0708 12:51:37.587512       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/vft 415\nI0708 12:51:37.787714       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/f7dh 352\n"
  Jul  8 12:51:37.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-logs-2828 logs logs-generator logs-generator --since=24h'
  Jul  8 12:51:37.992: INFO: stderr: ""
  Jul  8 12:51:37.992: INFO: stdout: "I0708 12:51:33.787427       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/fqb 383\nI0708 12:51:33.987800       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/mv6 275\nI0708 12:51:34.188129       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/mhgb 593\nI0708 12:51:34.388419       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/zpnb 410\nI0708 12:51:34.587518       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/lqjf 467\nI0708 12:51:34.787809       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/qlgd 431\nI0708 12:51:34.988054       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/84t 522\nI0708 12:51:35.188343       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/vbr 494\nI0708 12:51:35.387516       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/8d44 247\nI0708 12:51:35.587810       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/scq 498\nI0708 12:51:35.788103       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/x24b 348\nI0708 12:51:35.988399       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/ngp 442\nI0708 12:51:36.187627       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/kfbw 562\nI0708 12:51:36.387916       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/g7v 244\nI0708 12:51:36.588212       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/h8k 399\nI0708 12:51:36.787450       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/nh8 315\nI0708 12:51:36.987743       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/7n82 376\nI0708 12:51:37.188032       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/l4q 552\nI0708 12:51:37.388273       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/v4k 384\nI0708 12:51:37.587512       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/vft 415\nI0708 12:51:37.787714       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/f7dh 352\nI0708 12:51:37.988030       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/l7d8 229\n"
  Jul  8 12:51:37.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-logs-2828 delete pod logs-generator'
  E0708 12:51:38.437082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:51:38.512: INFO: stderr: ""
  Jul  8 12:51:38.512: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Jul  8 12:51:38.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-2828" for this suite. @ 07/08/23 12:51:38.518
• [5.452 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 07/08/23 12:51:38.525
  Jul  8 12:51:38.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename aggregator @ 07/08/23 12:51:38.526
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:51:38.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:51:38.544
  Jul  8 12:51:38.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Registering the sample API server. @ 07/08/23 12:51:38.548
  Jul  8 12:51:38.783: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Jul  8 12:51:38.810: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  E0708 12:51:39.437867      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:40.438660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:51:40.854: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0708 12:51:41.439454      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:42.439546      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:51:42.860: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0708 12:51:43.439819      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:44.439907      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:51:44.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0708 12:51:45.440067      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:46.440335      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:51:46.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0708 12:51:47.440451      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:48.440528      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:51:48.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0708 12:51:49.440612      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:50.440710      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:51:50.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0708 12:51:51.441106      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:52.441182      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:51:52.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0708 12:51:53.441457      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:54.441637      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:51:54.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0708 12:51:55.442411      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:56.442464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:51:56.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0708 12:51:57.442655      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:51:58.442746      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:51:58.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 12, 51, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0708 12:51:59.443039      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:00.443212      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:52:00.980: INFO: Waited 113.211333ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 07/08/23 12:52:01.016
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 07/08/23 12:52:01.02
  STEP: List APIServices @ 07/08/23 12:52:01.027
  Jul  8 12:52:01.032: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 07/08/23 12:52:01.032
  Jul  8 12:52:01.044: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 07/08/23 12:52:01.044
  Jul  8 12:52:01.054: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.July, 8, 12, 52, 0, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 07/08/23 12:52:01.054
  Jul  8 12:52:01.058: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-07-08 12:52:00 +0000 UTC Passed all checks passed}
  Jul  8 12:52:01.058: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul  8 12:52:01.058: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 07/08/23 12:52:01.058
  Jul  8 12:52:01.067: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-330076826" @ 07/08/23 12:52:01.067
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 07/08/23 12:52:01.08
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 07/08/23 12:52:01.087
  STEP: Patch APIService Status @ 07/08/23 12:52:01.09
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 07/08/23 12:52:01.098
  Jul  8 12:52:01.102: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-07-08 12:52:00 +0000 UTC Passed all checks passed}
  Jul  8 12:52:01.102: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul  8 12:52:01.102: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Jul  8 12:52:01.102: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 07/08/23 12:52:01.102
  STEP: Confirm that the generated APIService has been deleted @ 07/08/23 12:52:01.106
  Jul  8 12:52:01.106: INFO: Requesting list of APIServices to confirm quantity
  Jul  8 12:52:01.111: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Jul  8 12:52:01.111: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Jul  8 12:52:01.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-7023" for this suite. @ 07/08/23 12:52:01.22
• [22.703 seconds]
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 07/08/23 12:52:01.228
  Jul  8 12:52:01.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename daemonsets @ 07/08/23 12:52:01.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:52:01.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:52:01.251
  STEP: Creating a simple DaemonSet "daemon-set" @ 07/08/23 12:52:01.273
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/08/23 12:52:01.278
  Jul  8 12:52:01.284: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:52:01.284: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:52:01.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 12:52:01.287: INFO: Node ip-172-31-12-67 is running 0 daemon pod, expected 1
  E0708 12:52:01.444131      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:52:02.292: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:52:02.292: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:52:02.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  8 12:52:02.296: INFO: Node ip-172-31-12-67 is running 0 daemon pod, expected 1
  E0708 12:52:02.444460      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:52:03.292: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:52:03.292: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:52:03.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul  8 12:52:03.296: INFO: Node ip-172-31-93-234 is running 0 daemon pod, expected 1
  E0708 12:52:03.444762      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:52:04.292: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:52:04.292: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:52:04.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  8 12:52:04.296: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 07/08/23 12:52:04.3
  Jul  8 12:52:04.315: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:52:04.315: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:52:04.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul  8 12:52:04.321: INFO: Node ip-172-31-29-188 is running 0 daemon pod, expected 1
  E0708 12:52:04.445041      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:52:05.327: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:52:05.327: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:52:05.331: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul  8 12:52:05.331: INFO: Node ip-172-31-29-188 is running 0 daemon pod, expected 1
  E0708 12:52:05.445325      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:52:06.327: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:52:06.327: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:52:06.331: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  8 12:52:06.331: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 07/08/23 12:52:06.331
  STEP: Deleting DaemonSet "daemon-set" @ 07/08/23 12:52:06.338
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1395, will wait for the garbage collector to delete the pods @ 07/08/23 12:52:06.338
  Jul  8 12:52:06.400: INFO: Deleting DaemonSet.extensions daemon-set took: 8.574831ms
  E0708 12:52:06.446049      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:52:06.601: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.665678ms
  E0708 12:52:07.446716      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:52:08.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 12:52:08.005: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul  8 12:52:08.010: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23145"},"items":null}

  Jul  8 12:52:08.014: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23145"},"items":null}

  Jul  8 12:52:08.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1395" for this suite. @ 07/08/23 12:52:08.032
• [6.813 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 07/08/23 12:52:08.042
  Jul  8 12:52:08.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename dns @ 07/08/23 12:52:08.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:52:08.064
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:52:08.067
  STEP: Creating a test headless service @ 07/08/23 12:52:08.07
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9418.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9418.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 07/08/23 12:52:08.076
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9418.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9418.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 07/08/23 12:52:08.076
  STEP: creating a pod to probe DNS @ 07/08/23 12:52:08.076
  STEP: submitting the pod to kubernetes @ 07/08/23 12:52:08.076
  E0708 12:52:08.447398      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:09.447568      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/08/23 12:52:10.097
  STEP: looking for the results for each expected name from probers @ 07/08/23 12:52:10.1
  Jul  8 12:52:10.119: INFO: DNS probes using dns-9418/dns-test-ec38b001-8e8b-4f1a-9a72-232d96cc850f succeeded

  Jul  8 12:52:10.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/08/23 12:52:10.124
  STEP: deleting the test headless service @ 07/08/23 12:52:10.138
  STEP: Destroying namespace "dns-9418" for this suite. @ 07/08/23 12:52:10.15
• [2.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 07/08/23 12:52:10.159
  Jul  8 12:52:10.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:52:10.159
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:52:10.174
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:52:10.177
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 12:52:10.181
  E0708 12:52:10.448207      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:11.448387      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:12.449413      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:13.449515      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:52:14.203
  Jul  8 12:52:14.207: INFO: Trying to get logs from node ip-172-31-93-234 pod downwardapi-volume-d159f908-993f-4307-9cb9-4fb96e173e76 container client-container: <nil>
  STEP: delete the pod @ 07/08/23 12:52:14.215
  Jul  8 12:52:14.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6854" for this suite. @ 07/08/23 12:52:14.236
• [4.084 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 07/08/23 12:52:14.243
  Jul  8 12:52:14.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-probe @ 07/08/23 12:52:14.244
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:52:14.26
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:52:14.263
  STEP: Creating pod liveness-5405fd82-a448-43a7-959b-a084e9e306c1 in namespace container-probe-3706 @ 07/08/23 12:52:14.268
  E0708 12:52:14.450097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:15.450910      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:52:16.287: INFO: Started pod liveness-5405fd82-a448-43a7-959b-a084e9e306c1 in namespace container-probe-3706
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/08/23 12:52:16.287
  Jul  8 12:52:16.291: INFO: Initial restart count of pod liveness-5405fd82-a448-43a7-959b-a084e9e306c1 is 0
  E0708 12:52:16.451380      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:17.451533      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:18.451668      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:19.452019      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:20.452355      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:21.452424      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:22.453406      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:23.453499      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:24.453725      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:25.453834      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:26.454471      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:27.454632      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:28.455373      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:29.455464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:30.456405      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:31.457466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:32.458032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:33.458914      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:34.459653      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:35.459762      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:52:36.344: INFO: Restart count of pod container-probe-3706/liveness-5405fd82-a448-43a7-959b-a084e9e306c1 is now 1 (20.052833119s elapsed)
  E0708 12:52:36.460666      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:37.461083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:38.462063      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:39.462160      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:40.462664      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:41.463107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:42.463301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:43.463396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:44.463644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:45.463810      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:46.464669      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:47.465270      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:48.465660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:49.465767      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:50.466734      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:51.467164      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:52.468108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:53.468226      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:54.468679      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:55.469092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:52:56.392: INFO: Restart count of pod container-probe-3706/liveness-5405fd82-a448-43a7-959b-a084e9e306c1 is now 2 (40.101186316s elapsed)
  E0708 12:52:56.469964      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:57.470074      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:58.470677      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:52:59.470785      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:00.470985      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:01.471384      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:02.471416      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:03.471590      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:04.471934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:05.472030      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:06.472074      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:07.473087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:08.473648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:09.473810      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:10.474049      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:11.474343      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:12.474704      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:13.474818      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:14.475268      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:15.475425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:53:16.441: INFO: Restart count of pod container-probe-3706/liveness-5405fd82-a448-43a7-959b-a084e9e306c1 is now 3 (1m0.150008781s elapsed)
  E0708 12:53:16.475706      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:17.475855      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:18.476634      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:19.477088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:20.477917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:21.478185      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:22.478466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:23.479117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:24.480081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:25.481087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:26.481794      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:27.481945      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:28.482577      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:29.482742      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:30.483779      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:31.484213      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:32.484595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:33.484714      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:34.484917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:35.485036      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:36.485543      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:53:36.488: INFO: Restart count of pod container-probe-3706/liveness-5405fd82-a448-43a7-959b-a084e9e306c1 is now 4 (1m20.196542961s elapsed)
  E0708 12:53:37.485660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:38.485830      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:39.486587      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:40.486828      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:41.487533      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:42.487645      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:43.488048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:44.488119      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:45.488218      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:46.489234      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:47.489312      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:48.489474      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:49.489575      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:50.489673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:51.490122      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:52.490295      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:53.490963      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:54.491137      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:55.491234      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:56.491347      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:57.491419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:58.491587      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:53:59.491848      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:00.492219      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:01.493090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:02.493171      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:03.493221      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:04.493572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:05.493651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:06.493719      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:07.493809      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:08.494430      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:09.494548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:10.494848      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:11.495124      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:12.495298      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:13.496051      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:14.496946      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:15.497042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:16.497940      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:17.498041      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:18.498287      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:19.498338      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:20.499223      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:21.499750      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:22.499934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:23.500010      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:24.500032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:25.501023      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:26.501104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:27.501277      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:28.501367      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:29.501460      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:30.501819      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:31.502437      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:32.503287      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:33.503854      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:34.504022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:35.505086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:36.505359      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:37.506423      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:38.507422      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:39.507788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:40.508675      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:41.508983      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:42.509211      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:43.509301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:44.509391      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:45.510136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:46.511072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:54:46.662: INFO: Restart count of pod container-probe-3706/liveness-5405fd82-a448-43a7-959b-a084e9e306c1 is now 5 (2m30.37132535s elapsed)
  Jul  8 12:54:46.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/08/23 12:54:46.667
  STEP: Destroying namespace "container-probe-3706" for this suite. @ 07/08/23 12:54:46.688
• [152.457 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 07/08/23 12:54:46.702
  Jul  8 12:54:46.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename proxy @ 07/08/23 12:54:46.703
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:54:46.724
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:54:46.727
  Jul  8 12:54:46.731: INFO: Creating pod...
  E0708 12:54:47.511203      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:48.511275      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:54:48.748: INFO: Creating service...
  Jul  8 12:54:48.760: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3092/pods/agnhost/proxy/some/path/with/DELETE
  Jul  8 12:54:48.766: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul  8 12:54:48.766: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3092/pods/agnhost/proxy/some/path/with/GET
  Jul  8 12:54:48.771: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jul  8 12:54:48.771: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3092/pods/agnhost/proxy/some/path/with/HEAD
  Jul  8 12:54:48.775: INFO: http.Client request:HEAD | StatusCode:200
  Jul  8 12:54:48.775: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3092/pods/agnhost/proxy/some/path/with/OPTIONS
  Jul  8 12:54:48.779: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul  8 12:54:48.779: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3092/pods/agnhost/proxy/some/path/with/PATCH
  Jul  8 12:54:48.784: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul  8 12:54:48.784: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3092/pods/agnhost/proxy/some/path/with/POST
  Jul  8 12:54:48.788: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul  8 12:54:48.788: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3092/pods/agnhost/proxy/some/path/with/PUT
  Jul  8 12:54:48.792: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul  8 12:54:48.792: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3092/services/test-service/proxy/some/path/with/DELETE
  Jul  8 12:54:48.799: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul  8 12:54:48.799: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3092/services/test-service/proxy/some/path/with/GET
  Jul  8 12:54:48.805: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jul  8 12:54:48.805: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3092/services/test-service/proxy/some/path/with/HEAD
  Jul  8 12:54:48.811: INFO: http.Client request:HEAD | StatusCode:200
  Jul  8 12:54:48.811: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3092/services/test-service/proxy/some/path/with/OPTIONS
  Jul  8 12:54:48.818: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul  8 12:54:48.818: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3092/services/test-service/proxy/some/path/with/PATCH
  Jul  8 12:54:48.823: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul  8 12:54:48.823: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3092/services/test-service/proxy/some/path/with/POST
  Jul  8 12:54:48.829: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul  8 12:54:48.829: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3092/services/test-service/proxy/some/path/with/PUT
  Jul  8 12:54:48.836: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul  8 12:54:48.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-3092" for this suite. @ 07/08/23 12:54:48.84
• [2.144 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 07/08/23 12:54:48.847
  Jul  8 12:54:48.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename job @ 07/08/23 12:54:48.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:54:48.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:54:48.866
  STEP: Creating a job @ 07/08/23 12:54:48.869
  STEP: Ensuring active pods == parallelism @ 07/08/23 12:54:48.875
  E0708 12:54:49.512277      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:50.513107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 07/08/23 12:54:50.88
  Jul  8 12:54:51.397: INFO: Successfully updated pod "adopt-release-4kwpd"
  STEP: Checking that the Job readopts the Pod @ 07/08/23 12:54:51.397
  E0708 12:54:51.513100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:52.513260      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 07/08/23 12:54:53.407
  E0708 12:54:53.513311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:54:53.919: INFO: Successfully updated pod "adopt-release-4kwpd"
  STEP: Checking that the Job releases the Pod @ 07/08/23 12:54:53.919
  E0708 12:54:54.514410      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:55.514487      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:54:55.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6180" for this suite. @ 07/08/23 12:54:55.934
• [7.093 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 07/08/23 12:54:55.941
  Jul  8 12:54:55.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubelet-test @ 07/08/23 12:54:55.941
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:54:55.957
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:54:55.96
  E0708 12:54:56.514896      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:57.515037      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:54:57.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2564" for this suite. @ 07/08/23 12:54:58.002
• [2.068 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 07/08/23 12:54:58.009
  Jul  8 12:54:58.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename downward-api @ 07/08/23 12:54:58.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:54:58.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:54:58.029
  STEP: Creating the pod @ 07/08/23 12:54:58.032
  E0708 12:54:58.515291      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:54:59.515535      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:00.515628      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:55:00.581: INFO: Successfully updated pod "labelsupdate29635844-1606-4f85-827b-eb31265d2dd8"
  E0708 12:55:01.516093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:02.516179      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:55:02.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7185" for this suite. @ 07/08/23 12:55:02.604
• [4.603 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 07/08/23 12:55:02.612
  Jul  8 12:55:02.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename sched-preemption @ 07/08/23 12:55:02.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:55:02.629
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:55:02.632
  Jul  8 12:55:02.650: INFO: Waiting up to 1m0s for all nodes to be ready
  E0708 12:55:03.517105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:04.517193      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:05.517466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:06.517555      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:07.517645      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:08.518398      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:09.518506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:10.518583      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:11.519306      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:12.519462      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:13.519563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:14.519763      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:15.519856      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:16.520043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:17.520126      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:18.521102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:19.521189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:20.522138      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:21.522219      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:22.522311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:23.522421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:24.522509      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:25.523106      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:26.523381      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:27.523477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:28.523689      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:29.524036      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:30.525089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:31.525193      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:32.525236      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:33.525311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:34.525496      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:35.525582      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:36.525814      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:37.525891      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:38.526001      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:39.526079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:40.526862      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:41.527315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:42.528104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:43.528170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:44.529100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:45.529198      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:46.529282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:47.529369      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:48.529554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:49.529651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:50.529818      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:51.530319      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:52.530394      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:53.530502      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:54.531020      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:55.531111      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:56.531195      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:57.531270      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:58.531567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:55:59.532509      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:00.533089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:01.533697      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:02.533842      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:02.670: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 07/08/23 12:56:02.673
  Jul  8 12:56:02.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename sched-preemption-path @ 07/08/23 12:56:02.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:56:02.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:56:02.696
  Jul  8 12:56:02.717: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Jul  8 12:56:02.721: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Jul  8 12:56:02.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  8 12:56:02.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-9955" for this suite. @ 07/08/23 12:56:02.804
  STEP: Destroying namespace "sched-preemption-4584" for this suite. @ 07/08/23 12:56:02.811
• [60.207 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 07/08/23 12:56:02.82
  Jul  8 12:56:02.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename services @ 07/08/23 12:56:02.821
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:56:02.836
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:56:02.839
  STEP: creating service in namespace services-5878 @ 07/08/23 12:56:02.842
  STEP: creating service affinity-nodeport-transition in namespace services-5878 @ 07/08/23 12:56:02.842
  STEP: creating replication controller affinity-nodeport-transition in namespace services-5878 @ 07/08/23 12:56:02.858
  I0708 12:56:02.863212      20 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-5878, replica count: 3
  E0708 12:56:03.534625      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:04.534699      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:05.534902      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0708 12:56:05.914469      20 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  8 12:56:05.927: INFO: Creating new exec pod
  E0708 12:56:06.535126      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:07.535358      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:08.536346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:08.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-5878 exec execpod-affinity44zwh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Jul  8 12:56:09.059: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Jul  8 12:56:09.059: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 12:56:09.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-5878 exec execpod-affinity44zwh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.104 80'
  Jul  8 12:56:09.158: INFO: stderr: "+ nc -v -t -w 2 10.152.183.104 80\n+ echo hostName\nConnection to 10.152.183.104 80 port [tcp/http] succeeded!\n"
  Jul  8 12:56:09.158: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 12:56:09.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-5878 exec execpod-affinity44zwh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.12.67 31209'
  Jul  8 12:56:09.259: INFO: stderr: "+ nc -v -t -w 2 172.31.12.67 31209\n+ echo hostName\nConnection to 172.31.12.67 31209 port [tcp/*] succeeded!\n"
  Jul  8 12:56:09.259: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 12:56:09.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-5878 exec execpod-affinity44zwh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.29.188 31209'
  Jul  8 12:56:09.361: INFO: stderr: "+ nc -v -t -w 2 172.31.29.188 31209\n+ echo hostName\nConnection to 172.31.29.188 31209 port [tcp/*] succeeded!\n"
  Jul  8 12:56:09.361: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 12:56:09.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-5878 exec execpod-affinity44zwh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.12.67:31209/ ; done'
  Jul  8 12:56:09.529: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n"
  Jul  8 12:56:09.529: INFO: stdout: "\naffinity-nodeport-transition-2cxw5\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-2cxw5\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-dl5qz\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-2cxw5\naffinity-nodeport-transition-dl5qz\naffinity-nodeport-transition-dl5qz\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-dl5qz\naffinity-nodeport-transition-2cxw5\naffinity-nodeport-transition-2cxw5\naffinity-nodeport-transition-dl5qz\naffinity-nodeport-transition-2cxw5"
  Jul  8 12:56:09.529: INFO: Received response from host: affinity-nodeport-transition-2cxw5
  Jul  8 12:56:09.529: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.529: INFO: Received response from host: affinity-nodeport-transition-2cxw5
  Jul  8 12:56:09.529: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.529: INFO: Received response from host: affinity-nodeport-transition-dl5qz
  Jul  8 12:56:09.529: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.529: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.529: INFO: Received response from host: affinity-nodeport-transition-2cxw5
  Jul  8 12:56:09.529: INFO: Received response from host: affinity-nodeport-transition-dl5qz
  Jul  8 12:56:09.529: INFO: Received response from host: affinity-nodeport-transition-dl5qz
  Jul  8 12:56:09.529: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.529: INFO: Received response from host: affinity-nodeport-transition-dl5qz
  Jul  8 12:56:09.529: INFO: Received response from host: affinity-nodeport-transition-2cxw5
  Jul  8 12:56:09.529: INFO: Received response from host: affinity-nodeport-transition-2cxw5
  Jul  8 12:56:09.529: INFO: Received response from host: affinity-nodeport-transition-dl5qz
  Jul  8 12:56:09.529: INFO: Received response from host: affinity-nodeport-transition-2cxw5
  E0708 12:56:09.536532      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:09.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-5878 exec execpod-affinity44zwh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.12.67:31209/ ; done'
  Jul  8 12:56:09.701: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31209/\n"
  Jul  8 12:56:09.701: INFO: stdout: "\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-ls8jd\naffinity-nodeport-transition-ls8jd"
  Jul  8 12:56:09.701: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.701: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.701: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.701: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.701: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.701: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.701: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.701: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.701: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.701: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.701: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.701: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.701: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.701: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.701: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.701: INFO: Received response from host: affinity-nodeport-transition-ls8jd
  Jul  8 12:56:09.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  8 12:56:09.706: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5878, will wait for the garbage collector to delete the pods @ 07/08/23 12:56:09.721
  Jul  8 12:56:09.781: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.758872ms
  Jul  8 12:56:09.982: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 200.29625ms
  E0708 12:56:10.537492      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:11.537798      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-5878" for this suite. @ 07/08/23 12:56:12.305
• [9.492 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 07/08/23 12:56:12.314
  Jul  8 12:56:12.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename certificates @ 07/08/23 12:56:12.314
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:56:12.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:56:12.335
  E0708 12:56:12.538570      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 07/08/23 12:56:12.871
  STEP: getting /apis/certificates.k8s.io @ 07/08/23 12:56:12.876
  STEP: getting /apis/certificates.k8s.io/v1 @ 07/08/23 12:56:12.877
  STEP: creating @ 07/08/23 12:56:12.879
  STEP: getting @ 07/08/23 12:56:12.897
  STEP: listing @ 07/08/23 12:56:12.901
  STEP: watching @ 07/08/23 12:56:12.905
  Jul  8 12:56:12.905: INFO: starting watch
  STEP: patching @ 07/08/23 12:56:12.906
  STEP: updating @ 07/08/23 12:56:12.912
  Jul  8 12:56:12.918: INFO: waiting for watch events with expected annotations
  Jul  8 12:56:12.918: INFO: saw patched and updated annotations
  STEP: getting /approval @ 07/08/23 12:56:12.918
  STEP: patching /approval @ 07/08/23 12:56:12.922
  STEP: updating /approval @ 07/08/23 12:56:12.929
  STEP: getting /status @ 07/08/23 12:56:12.935
  STEP: patching /status @ 07/08/23 12:56:12.939
  STEP: updating /status @ 07/08/23 12:56:12.946
  STEP: deleting @ 07/08/23 12:56:12.953
  STEP: deleting a collection @ 07/08/23 12:56:12.967
  Jul  8 12:56:12.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-5188" for this suite. @ 07/08/23 12:56:12.988
• [0.682 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 07/08/23 12:56:12.996
  Jul  8 12:56:12.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename services @ 07/08/23 12:56:12.996
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:56:13.011
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:56:13.014
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-7947 @ 07/08/23 12:56:13.018
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 07/08/23 12:56:13.034
  STEP: creating service externalsvc in namespace services-7947 @ 07/08/23 12:56:13.034
  STEP: creating replication controller externalsvc in namespace services-7947 @ 07/08/23 12:56:13.052
  I0708 12:56:13.059902      20 runners.go:194] Created replication controller with name: externalsvc, namespace: services-7947, replica count: 2
  E0708 12:56:13.538994      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:14.539760      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:15.539959      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0708 12:56:16.110675      20 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 07/08/23 12:56:16.114
  Jul  8 12:56:16.132: INFO: Creating new exec pod
  E0708 12:56:16.540752      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:17.541122      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:18.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-7947 exec execpod7glkd -- /bin/sh -x -c nslookup nodeport-service.services-7947.svc.cluster.local'
  Jul  8 12:56:18.280: INFO: stderr: "+ nslookup nodeport-service.services-7947.svc.cluster.local\n"
  Jul  8 12:56:18.280: INFO: stdout: "Server:\t\t10.152.183.124\nAddress:\t10.152.183.124#53\n\nnodeport-service.services-7947.svc.cluster.local\tcanonical name = externalsvc.services-7947.svc.cluster.local.\nName:\texternalsvc.services-7947.svc.cluster.local\nAddress: 10.152.183.80\n\n"
  Jul  8 12:56:18.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-7947, will wait for the garbage collector to delete the pods @ 07/08/23 12:56:18.285
  Jul  8 12:56:18.347: INFO: Deleting ReplicationController externalsvc took: 7.55875ms
  Jul  8 12:56:18.448: INFO: Terminating ReplicationController externalsvc pods took: 101.011377ms
  E0708 12:56:18.541436      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:19.542191      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:20.370: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-7947" for this suite. @ 07/08/23 12:56:20.382
• [7.394 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 07/08/23 12:56:20.391
  Jul  8 12:56:20.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename configmap @ 07/08/23 12:56:20.392
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:56:20.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:56:20.419
  STEP: Creating configMap configmap-2907/configmap-test-710fe1b4-7a90-47a8-b7e5-1762a9fab1c9 @ 07/08/23 12:56:20.422
  STEP: Creating a pod to test consume configMaps @ 07/08/23 12:56:20.427
  E0708 12:56:20.542800      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:21.543306      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:22.544160      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:23.544252      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:56:24.452
  Jul  8 12:56:24.456: INFO: Trying to get logs from node ip-172-31-29-188 pod pod-configmaps-dc6d9a9a-0a61-4d31-be43-9a4f6a3b64ca container env-test: <nil>
  STEP: delete the pod @ 07/08/23 12:56:24.468
  Jul  8 12:56:24.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2907" for this suite. @ 07/08/23 12:56:24.491
• [4.107 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 07/08/23 12:56:24.498
  Jul  8 12:56:24.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename field-validation @ 07/08/23 12:56:24.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:56:24.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:56:24.517
  Jul  8 12:56:24.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  W0708 12:56:24.522199      20 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc001473660 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0708 12:56:24.545080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:25.545273      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:26.545532      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0708 12:56:27.114979      20 warnings.go:70] unknown field "alpha"
  W0708 12:56:27.114997      20 warnings.go:70] unknown field "beta"
  W0708 12:56:27.115001      20 warnings.go:70] unknown field "delta"
  W0708 12:56:27.115004      20 warnings.go:70] unknown field "epsilon"
  W0708 12:56:27.115007      20 warnings.go:70] unknown field "gamma"
  Jul  8 12:56:27.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8033" for this suite. @ 07/08/23 12:56:27.15
• [2.659 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 07/08/23 12:56:27.158
  Jul  8 12:56:27.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/08/23 12:56:27.158
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:56:27.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:56:27.181
  Jul  8 12:56:27.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 12:56:27.545537      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 07/08/23 12:56:28.486
  Jul  8 12:56:28.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-1619 --namespace=crd-publish-openapi-1619 create -f -'
  E0708 12:56:28.545809      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:29.078: INFO: stderr: ""
  Jul  8 12:56:29.078: INFO: stdout: "e2e-test-crd-publish-openapi-4587-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jul  8 12:56:29.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-1619 --namespace=crd-publish-openapi-1619 delete e2e-test-crd-publish-openapi-4587-crds test-foo'
  Jul  8 12:56:29.133: INFO: stderr: ""
  Jul  8 12:56:29.133: INFO: stdout: "e2e-test-crd-publish-openapi-4587-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Jul  8 12:56:29.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-1619 --namespace=crd-publish-openapi-1619 apply -f -'
  E0708 12:56:29.546427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:29.965: INFO: stderr: ""
  Jul  8 12:56:29.965: INFO: stdout: "e2e-test-crd-publish-openapi-4587-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jul  8 12:56:29.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-1619 --namespace=crd-publish-openapi-1619 delete e2e-test-crd-publish-openapi-4587-crds test-foo'
  Jul  8 12:56:30.024: INFO: stderr: ""
  Jul  8 12:56:30.024: INFO: stdout: "e2e-test-crd-publish-openapi-4587-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 07/08/23 12:56:30.024
  Jul  8 12:56:30.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-1619 --namespace=crd-publish-openapi-1619 create -f -'
  Jul  8 12:56:30.171: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 07/08/23 12:56:30.171
  Jul  8 12:56:30.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-1619 --namespace=crd-publish-openapi-1619 create -f -'
  Jul  8 12:56:30.424: INFO: rc: 1
  Jul  8 12:56:30.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-1619 --namespace=crd-publish-openapi-1619 apply -f -'
  E0708 12:56:30.547312      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:30.710: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 07/08/23 12:56:30.71
  Jul  8 12:56:30.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-1619 --namespace=crd-publish-openapi-1619 create -f -'
  Jul  8 12:56:30.982: INFO: rc: 1
  Jul  8 12:56:30.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-1619 --namespace=crd-publish-openapi-1619 apply -f -'
  Jul  8 12:56:31.220: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 07/08/23 12:56:31.22
  Jul  8 12:56:31.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-1619 explain e2e-test-crd-publish-openapi-4587-crds'
  Jul  8 12:56:31.462: INFO: stderr: ""
  Jul  8 12:56:31.462: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4587-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 07/08/23 12:56:31.462
  Jul  8 12:56:31.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-1619 explain e2e-test-crd-publish-openapi-4587-crds.metadata'
  E0708 12:56:31.547364      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:31.704: INFO: stderr: ""
  Jul  8 12:56:31.704: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4587-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Jul  8 12:56:31.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-1619 explain e2e-test-crd-publish-openapi-4587-crds.spec'
  Jul  8 12:56:31.947: INFO: stderr: ""
  Jul  8 12:56:31.947: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4587-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Jul  8 12:56:31.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-1619 explain e2e-test-crd-publish-openapi-4587-crds.spec.bars'
  Jul  8 12:56:32.162: INFO: stderr: ""
  Jul  8 12:56:32.162: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4587-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 07/08/23 12:56:32.162
  Jul  8 12:56:32.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-1619 explain e2e-test-crd-publish-openapi-4587-crds.spec.bars2'
  Jul  8 12:56:32.454: INFO: rc: 1
  E0708 12:56:32.547526      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:33.547735      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:33.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1619" for this suite. @ 07/08/23 12:56:33.757
• [6.607 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 07/08/23 12:56:33.765
  Jul  8 12:56:33.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl @ 07/08/23 12:56:33.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:56:33.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:56:33.785
  STEP: creating Agnhost RC @ 07/08/23 12:56:33.787
  Jul  8 12:56:33.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-9216 create -f -'
  Jul  8 12:56:34.307: INFO: stderr: ""
  Jul  8 12:56:34.307: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/08/23 12:56:34.307
  E0708 12:56:34.548488      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:35.310: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  8 12:56:35.310: INFO: Found 1 / 1
  Jul  8 12:56:35.310: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 07/08/23 12:56:35.31
  Jul  8 12:56:35.313: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  8 12:56:35.313: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul  8 12:56:35.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-9216 patch pod agnhost-primary-c2sz9 -p {"metadata":{"annotations":{"x":"y"}}}'
  Jul  8 12:56:35.370: INFO: stderr: ""
  Jul  8 12:56:35.370: INFO: stdout: "pod/agnhost-primary-c2sz9 patched\n"
  STEP: checking annotations @ 07/08/23 12:56:35.37
  Jul  8 12:56:35.373: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  8 12:56:35.373: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul  8 12:56:35.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9216" for this suite. @ 07/08/23 12:56:35.377
• [1.620 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 07/08/23 12:56:35.385
  Jul  8 12:56:35.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename gc @ 07/08/23 12:56:35.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:56:35.402
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:56:35.404
  Jul  8 12:56:35.445: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"3dd725c9-e7ae-4619-8a33-6b6431e728a2", Controller:(*bool)(0xc0067de0fe), BlockOwnerDeletion:(*bool)(0xc0067de0ff)}}
  Jul  8 12:56:35.453: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"27daa0c1-86e8-4a9d-99cc-5a8d718d7a12", Controller:(*bool)(0xc0067de386), BlockOwnerDeletion:(*bool)(0xc0067de387)}}
  Jul  8 12:56:35.458: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c76864d6-9f4f-4e61-8571-2805b4a79292", Controller:(*bool)(0xc001b783a6), BlockOwnerDeletion:(*bool)(0xc001b783a7)}}
  E0708 12:56:35.549076      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:36.549335      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:37.549440      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:38.550357      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:39.550517      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:40.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2103" for this suite. @ 07/08/23 12:56:40.483
• [5.108 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 07/08/23 12:56:40.494
  Jul  8 12:56:40.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename dns @ 07/08/23 12:56:40.494
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:56:40.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:56:40.516
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 07/08/23 12:56:40.519
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 07/08/23 12:56:40.519
  STEP: creating a pod to probe DNS @ 07/08/23 12:56:40.519
  STEP: submitting the pod to kubernetes @ 07/08/23 12:56:40.519
  E0708 12:56:40.550999      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:41.551056      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/08/23 12:56:42.54
  STEP: looking for the results for each expected name from probers @ 07/08/23 12:56:42.543
  E0708 12:56:42.551052      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:42.556: INFO: DNS probes using dns-1920/dns-test-c4c8a224-52e3-47d3-85ba-e4b15be80425 succeeded

  Jul  8 12:56:42.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/08/23 12:56:42.559
  STEP: Destroying namespace "dns-1920" for this suite. @ 07/08/23 12:56:42.572
• [2.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 07/08/23 12:56:42.58
  Jul  8 12:56:42.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename gc @ 07/08/23 12:56:42.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:56:42.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:56:42.596
  STEP: create the rc @ 07/08/23 12:56:42.598
  W0708 12:56:42.602221      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0708 12:56:43.551310      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:44.551405      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:45.551705      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:46.551779      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:47.551896      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 07/08/23 12:56:47.605
  STEP: wait for all pods to be garbage collected @ 07/08/23 12:56:47.612
  E0708 12:56:48.552283      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:49.552371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:50.552450      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:51.553530      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:56:52.553574      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/08/23 12:56:52.619
  W0708 12:56:52.623062      20 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul  8 12:56:52.623: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul  8 12:56:52.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5464" for this suite. @ 07/08/23 12:56:52.625
• [10.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 07/08/23 12:56:52.632
  Jul  8 12:56:52.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename daemonsets @ 07/08/23 12:56:52.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:56:52.647
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:56:52.65
  STEP: Creating simple DaemonSet "daemon-set" @ 07/08/23 12:56:52.667
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/08/23 12:56:52.672
  Jul  8 12:56:52.677: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:56:52.677: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:56:52.679: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 12:56:52.679: INFO: Node ip-172-31-12-67 is running 0 daemon pod, expected 1
  E0708 12:56:53.553689      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:53.684: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:56:53.684: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:56:53.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  8 12:56:53.688: INFO: Node ip-172-31-12-67 is running 0 daemon pod, expected 1
  E0708 12:56:54.553932      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:54.683: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:56:54.684: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:56:54.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  8 12:56:54.687: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 07/08/23 12:56:54.691
  Jul  8 12:56:54.704: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:56:54.704: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:56:54.707: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul  8 12:56:54.707: INFO: Node ip-172-31-12-67 is running 0 daemon pod, expected 1
  E0708 12:56:55.553952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:55.711: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:56:55.711: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:56:55.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul  8 12:56:55.713: INFO: Node ip-172-31-12-67 is running 0 daemon pod, expected 1
  E0708 12:56:56.554885      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:56.712: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:56:56.712: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:56:56.716: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul  8 12:56:56.716: INFO: Node ip-172-31-12-67 is running 0 daemon pod, expected 1
  E0708 12:56:57.554955      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:57.712: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:56:57.712: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 12:56:57.715: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  8 12:56:57.715: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/08/23 12:56:57.718
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3254, will wait for the garbage collector to delete the pods @ 07/08/23 12:56:57.718
  Jul  8 12:56:57.778: INFO: Deleting DaemonSet.extensions daemon-set took: 7.353304ms
  Jul  8 12:56:57.879: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.464749ms
  E0708 12:56:58.555703      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:56:59.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 12:56:59.383: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul  8 12:56:59.385: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24835"},"items":null}

  Jul  8 12:56:59.388: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24835"},"items":null}

  Jul  8 12:56:59.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3254" for this suite. @ 07/08/23 12:56:59.402
• [6.776 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 07/08/23 12:56:59.409
  Jul  8 12:56:59.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 12:56:59.409
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:56:59.421
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:56:59.423
  STEP: Creating projection with secret that has name projected-secret-test-401bd1ff-4bbf-49d6-becd-77a08cdd3ac2 @ 07/08/23 12:56:59.425
  STEP: Creating a pod to test consume secrets @ 07/08/23 12:56:59.43
  E0708 12:56:59.556536      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:00.556649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:01.557319      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:02.557442      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 12:57:03.45
  Jul  8 12:57:03.453: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-projected-secrets-8e3034d0-bae8-4641-a3a0-19a7a8da73d4 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/08/23 12:57:03.469
  Jul  8 12:57:03.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7351" for this suite. @ 07/08/23 12:57:03.486
• [4.082 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 07/08/23 12:57:03.491
  Jul  8 12:57:03.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename pods @ 07/08/23 12:57:03.492
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:57:03.506
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:57:03.508
  STEP: Create set of pods @ 07/08/23 12:57:03.51
  Jul  8 12:57:03.519: INFO: created test-pod-1
  Jul  8 12:57:03.522: INFO: created test-pod-2
  Jul  8 12:57:03.531: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 07/08/23 12:57:03.531
  E0708 12:57:03.558347      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:04.558527      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:05.558555      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 07/08/23 12:57:05.573
  Jul  8 12:57:05.577: INFO: Pod quantity 3 is different from expected quantity 0
  E0708 12:57:06.558883      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:57:06.580: INFO: Pod quantity 2 is different from expected quantity 0
  E0708 12:57:07.558992      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:57:07.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9076" for this suite. @ 07/08/23 12:57:07.586
• [4.101 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 07/08/23 12:57:07.592
  Jul  8 12:57:07.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename resourcequota @ 07/08/23 12:57:07.593
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:57:07.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:57:07.611
  E0708 12:57:08.559081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:09.559157      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:10.559263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:11.559974      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:12.560044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:13.560128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:14.560362      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:15.560442      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:16.560571      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:17.561100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:18.561178      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:19.561279      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:20.561366      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:21.561451      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:22.561536      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:23.561639      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:24.561722      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 07/08/23 12:57:24.618
  E0708 12:57:25.561796      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:26.562508      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:27.563473      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:28.563562      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:29.563650      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/08/23 12:57:29.621
  STEP: Ensuring resource quota status is calculated @ 07/08/23 12:57:29.626
  E0708 12:57:30.563735      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:31.564040      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 07/08/23 12:57:31.63
  STEP: Ensuring resource quota status captures configMap creation @ 07/08/23 12:57:31.64
  E0708 12:57:32.565090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:33.565189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 07/08/23 12:57:33.644
  STEP: Ensuring resource quota status released usage @ 07/08/23 12:57:33.65
  E0708 12:57:34.565256      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:35.565415      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:57:35.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1919" for this suite. @ 07/08/23 12:57:35.659
• [28.072 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 07/08/23 12:57:35.666
  Jul  8 12:57:35.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename endpointslice @ 07/08/23 12:57:35.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:57:35.683
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:57:35.685
  E0708 12:57:36.565579      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:37.565838      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:57:37.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1788" for this suite. @ 07/08/23 12:57:37.845
• [2.184 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 07/08/23 12:57:37.85
  Jul  8 12:57:37.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename services @ 07/08/23 12:57:37.851
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:57:37.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:57:37.873
  STEP: creating a Service @ 07/08/23 12:57:37.877
  STEP: watching for the Service to be added @ 07/08/23 12:57:37.884
  Jul  8 12:57:37.886: INFO: Found Service test-service-bc8pg in namespace services-467 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Jul  8 12:57:37.886: INFO: Service test-service-bc8pg created
  STEP: Getting /status @ 07/08/23 12:57:37.886
  Jul  8 12:57:37.890: INFO: Service test-service-bc8pg has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 07/08/23 12:57:37.89
  STEP: watching for the Service to be patched @ 07/08/23 12:57:37.896
  Jul  8 12:57:37.897: INFO: observed Service test-service-bc8pg in namespace services-467 with annotations: map[] & LoadBalancer: {[]}
  Jul  8 12:57:37.897: INFO: Found Service test-service-bc8pg in namespace services-467 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Jul  8 12:57:37.897: INFO: Service test-service-bc8pg has service status patched
  STEP: updating the ServiceStatus @ 07/08/23 12:57:37.897
  Jul  8 12:57:37.905: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 07/08/23 12:57:37.905
  Jul  8 12:57:37.906: INFO: Observed Service test-service-bc8pg in namespace services-467 with annotations: map[] & Conditions: {[]}
  Jul  8 12:57:37.906: INFO: Observed event: &Service{ObjectMeta:{test-service-bc8pg  services-467  3afcf05c-9541-4d2b-91d7-c1e7801cce5f 25100 0 2023-07-08 12:57:37 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-07-08 12:57:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-07-08 12:57:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.241,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.241],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Jul  8 12:57:37.906: INFO: Found Service test-service-bc8pg in namespace services-467 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul  8 12:57:37.906: INFO: Service test-service-bc8pg has service status updated
  STEP: patching the service @ 07/08/23 12:57:37.906
  STEP: watching for the Service to be patched @ 07/08/23 12:57:37.916
  Jul  8 12:57:37.917: INFO: observed Service test-service-bc8pg in namespace services-467 with labels: map[test-service-static:true]
  Jul  8 12:57:37.917: INFO: observed Service test-service-bc8pg in namespace services-467 with labels: map[test-service-static:true]
  Jul  8 12:57:37.917: INFO: observed Service test-service-bc8pg in namespace services-467 with labels: map[test-service-static:true]
  Jul  8 12:57:37.917: INFO: Found Service test-service-bc8pg in namespace services-467 with labels: map[test-service:patched test-service-static:true]
  Jul  8 12:57:37.917: INFO: Service test-service-bc8pg patched
  STEP: deleting the service @ 07/08/23 12:57:37.917
  STEP: watching for the Service to be deleted @ 07/08/23 12:57:37.931
  Jul  8 12:57:37.932: INFO: Observed event: ADDED
  Jul  8 12:57:37.932: INFO: Observed event: MODIFIED
  Jul  8 12:57:37.932: INFO: Observed event: MODIFIED
  Jul  8 12:57:37.932: INFO: Observed event: MODIFIED
  Jul  8 12:57:37.932: INFO: Found Service test-service-bc8pg in namespace services-467 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Jul  8 12:57:37.932: INFO: Service test-service-bc8pg deleted
  Jul  8 12:57:37.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-467" for this suite. @ 07/08/23 12:57:37.935
• [0.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 07/08/23 12:57:37.94
  Jul  8 12:57:37.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename replicaset @ 07/08/23 12:57:37.941
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:57:37.953
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:57:37.955
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 07/08/23 12:57:37.957
  E0708 12:57:38.565899      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:39.566058      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 07/08/23 12:57:39.976
  STEP: Then the orphan pod is adopted @ 07/08/23 12:57:39.982
  E0708 12:57:40.566148      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 07/08/23 12:57:40.989
  Jul  8 12:57:40.993: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 07/08/23 12:57:41.002
  E0708 12:57:41.566262      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:57:42.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-561" for this suite. @ 07/08/23 12:57:42.016
• [4.082 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 07/08/23 12:57:42.023
  Jul  8 12:57:42.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename replication-controller @ 07/08/23 12:57:42.023
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:57:42.037
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:57:42.04
  STEP: Creating replication controller my-hostname-basic-9d10f42f-fc39-4a5e-9c1b-5fd1894d4c19 @ 07/08/23 12:57:42.042
  Jul  8 12:57:42.050: INFO: Pod name my-hostname-basic-9d10f42f-fc39-4a5e-9c1b-5fd1894d4c19: Found 0 pods out of 1
  E0708 12:57:42.566813      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:43.566904      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:44.567353      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:45.567528      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:46.567695      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:57:47.055: INFO: Pod name my-hostname-basic-9d10f42f-fc39-4a5e-9c1b-5fd1894d4c19: Found 1 pods out of 1
  Jul  8 12:57:47.055: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9d10f42f-fc39-4a5e-9c1b-5fd1894d4c19" are running
  Jul  8 12:57:47.058: INFO: Pod "my-hostname-basic-9d10f42f-fc39-4a5e-9c1b-5fd1894d4c19-7l9sm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-08 12:57:42 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-08 12:57:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-08 12:57:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-08 12:57:42 +0000 UTC Reason: Message:}])
  Jul  8 12:57:47.058: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 07/08/23 12:57:47.058
  Jul  8 12:57:47.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8476" for this suite. @ 07/08/23 12:57:47.071
• [5.054 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 07/08/23 12:57:47.078
  Jul  8 12:57:47.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-probe @ 07/08/23 12:57:47.078
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 12:57:47.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 12:57:47.1
  STEP: Creating pod test-webserver-819cfe19-a79f-4272-b0c4-af23c20c09b2 in namespace container-probe-5647 @ 07/08/23 12:57:47.102
  E0708 12:57:47.568659      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:48.568727      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 12:57:49.120: INFO: Started pod test-webserver-819cfe19-a79f-4272-b0c4-af23c20c09b2 in namespace container-probe-5647
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/08/23 12:57:49.12
  Jul  8 12:57:49.123: INFO: Initial restart count of pod test-webserver-819cfe19-a79f-4272-b0c4-af23c20c09b2 is 0
  E0708 12:57:49.568812      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:50.569103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:51.569724      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:52.569870      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:53.570533      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:54.570713      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:55.571403      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:56.571491      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:57.572492      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:58.573093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:57:59.573864      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:00.574031      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:01.574130      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:02.574893      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:03.575216      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:04.575311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:05.576202      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:06.577183      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:07.577949      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:08.578018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:09.578822      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:10.578930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:11.579303      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:12.579484      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:13.580009      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:14.581091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:15.581163      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:16.582178      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:17.583117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:18.583265      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:19.583823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:20.583990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:21.584549      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:22.585093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:23.585405      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:24.585581      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:25.586424      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:26.586651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:27.587294      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:28.587371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:29.588183      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:30.589106      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:31.589722      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:32.589906      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:33.589940      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:34.590026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:35.590608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:36.591648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:37.592613      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:38.592710      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:39.593692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:40.594151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:41.594647      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:42.594745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:43.595149      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:44.595302      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:45.596160      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:46.596253      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:47.596808      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:48.596898      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:49.597815      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:50.598852      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:51.599173      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:52.599316      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:53.599533      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:54.600502      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:55.601061      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:56.601231      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:57.602214      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:58.602402      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:58:59.602477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:00.602853      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:01.603002      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:02.603126      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:03.603215      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:04.603388      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:05.603511      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:06.603708      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:07.604157      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:08.604246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:09.605012      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:10.605105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:11.605337      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:12.605659      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:13.606530      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:14.606626      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:15.607599      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:16.608514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:17.608555      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:18.609333      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:19.610024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:20.610199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:21.610913      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:22.611007      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:23.611877      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:24.612878      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:25.613042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:26.613135      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:27.614190      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:28.614286      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:29.614874      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:30.614978      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:31.615617      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:32.615709      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:33.616378      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:34.617087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:35.618049      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:36.618129      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:37.618598      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:38.618691      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:39.619420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:40.619682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:41.620098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:42.620198      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:43.621083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:44.621931      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:45.622290      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:46.622547      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:47.623485      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:48.623581      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:49.623653      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:50.624606      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:51.625587      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:52.625682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:53.625793      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:54.626452      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:55.627027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:56.627215      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:57.627290      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:58.627523      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 12:59:59.628463      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:00.628527      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:01.629086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:02.629247      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:03.629512      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:04.630441      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:05.630610      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:06.631667      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:07.632514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:08.633088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:09.633422      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:10.633535      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:11.633919      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:12.634649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:13.635400      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:14.635583      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:15.636594      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:16.636782      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:17.637395      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:18.637479      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:19.638435      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:20.638550      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:21.639538      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:22.639688      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:23.640072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:24.640164      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:25.641087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:26.641354      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:27.642084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:28.642229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:29.643248      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:30.643349      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:31.643421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:32.643700      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:33.644234      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:34.644596      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:35.644637      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:36.644830      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:37.644921      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:38.645072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:39.645846      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:40.646575      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:41.647422      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:42.647518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:43.647991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:44.648029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:45.648349      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:46.649232      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:47.650138      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:48.650242      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:49.650283      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:50.650418      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:51.650499      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:52.650701      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:53.651729      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:54.651865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:55.652616      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:56.652805      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:57.652968      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:58.653136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:00:59.653783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:00.653936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:01.654406      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:02.654503      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:03.655420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:04.655661      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:05.656581      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:06.656669      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:07.657080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:08.657176      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:09.657945      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:10.658029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:11.658836      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:12.659674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:13.660172      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:14.661086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:15.661715      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:16.662037      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:17.663031      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:18.663141      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:19.664152      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:20.664252      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:21.664550      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:22.664662      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:23.664977      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:24.665163      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:25.665639      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:26.665687      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:27.665986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:28.666091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:29.666298      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:30.666396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:31.666764      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:32.666849      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:33.667467      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:34.667627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:35.667668      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:36.667792      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:37.668310      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:38.668400      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:39.668569      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:40.669291      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:41.670248      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:42.670402      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:43.671006      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:44.671115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:45.671546      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:46.671676      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:47.672636      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:48.673079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:01:49.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/08/23 13:01:49.591
  STEP: Destroying namespace "container-probe-5647" for this suite. @ 07/08/23 13:01:49.603
• [242.534 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 07/08/23 13:01:49.613
  Jul  8 13:01:49.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename daemonsets @ 07/08/23 13:01:49.614
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:01:49.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:01:49.635
  STEP: Creating simple DaemonSet "daemon-set" @ 07/08/23 13:01:49.652
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/08/23 13:01:49.658
  Jul  8 13:01:49.665: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 13:01:49.665: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 13:01:49.673: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 13:01:49.673: INFO: Node ip-172-31-12-67 is running 0 daemon pod, expected 1
  E0708 13:01:49.673263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:50.673435      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:01:50.677: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 13:01:50.677: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 13:01:50.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  8 13:01:50.681: INFO: Node ip-172-31-12-67 is running 0 daemon pod, expected 1
  E0708 13:01:51.673511      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:01:51.676: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 13:01:51.676: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 13:01:51.679: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  8 13:01:51.679: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 07/08/23 13:01:51.681
  STEP: DeleteCollection of the DaemonSets @ 07/08/23 13:01:51.685
  STEP: Verify that ReplicaSets have been deleted @ 07/08/23 13:01:51.693
  Jul  8 13:01:51.704: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25786"},"items":null}

  Jul  8 13:01:51.713: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25787"},"items":[{"metadata":{"name":"daemon-set-kmd2r","generateName":"daemon-set-","namespace":"daemonsets-4912","uid":"6e894173-4990-4ee1-b864-7a3611baa326","resourceVersion":"25782","creationTimestamp":"2023-07-08T13:01:49Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0e12ceea-90f8-442a-acd8-a54e03a3d01a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-08T13:01:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e12ceea-90f8-442a-acd8-a54e03a3d01a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-08T13:01:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.164.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-d64r5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-d64r5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-29-188","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-29-188"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-08T13:01:49Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-08T13:01:50Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-08T13:01:50Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-08T13:01:49Z"}],"hostIP":"172.31.29.188","podIP":"192.168.164.119","podIPs":[{"ip":"192.168.164.119"}],"startTime":"2023-07-08T13:01:49Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-08T13:01:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://99299053c6c72a9326e362bd24923c3e8e4444789d780d11e59345fabeae0f42","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-slgqs","generateName":"daemon-set-","namespace":"daemonsets-4912","uid":"1bd45156-57b5-449a-9e72-efeca5419ee6","resourceVersion":"25787","creationTimestamp":"2023-07-08T13:01:49Z","deletionTimestamp":"2023-07-08T13:02:21Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0e12ceea-90f8-442a-acd8-a54e03a3d01a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-08T13:01:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e12ceea-90f8-442a-acd8-a54e03a3d01a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-08T13:01:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.59.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6zktz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6zktz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-12-67","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-12-67"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-08T13:01:49Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-08T13:01:50Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-08T13:01:50Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-08T13:01:49Z"}],"hostIP":"172.31.12.67","podIP":"192.168.59.7","podIPs":[{"ip":"192.168.59.7"}],"startTime":"2023-07-08T13:01:49Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-08T13:01:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://78e27f04b18a5e5d54fc1c49f1e0756a4ea41ce69c772ee58f9c3821710384e5","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-wktlv","generateName":"daemon-set-","namespace":"daemonsets-4912","uid":"f55617fd-227b-4f57-b6a8-8345dec01c75","resourceVersion":"25777","creationTimestamp":"2023-07-08T13:01:49Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0e12ceea-90f8-442a-acd8-a54e03a3d01a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-08T13:01:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e12ceea-90f8-442a-acd8-a54e03a3d01a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-08T13:01:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qzbvq","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qzbvq","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-93-234","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-93-234"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-08T13:01:49Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-08T13:01:50Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-08T13:01:50Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-08T13:01:49Z"}],"hostIP":"172.31.93.234","podIP":"192.168.7.197","podIPs":[{"ip":"192.168.7.197"}],"startTime":"2023-07-08T13:01:49Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-08T13:01:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://404e1f8f488ef12d4b303d39fb030a18b2dbffe78014578f95f56dd0535cd6f5","started":true}],"qosClass":"BestEffort"}}]}

  Jul  8 13:01:51.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4912" for this suite. @ 07/08/23 13:01:51.728
• [2.120 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 07/08/23 13:01:51.733
  Jul  8 13:01:51.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename tables @ 07/08/23 13:01:51.733
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:01:51.747
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:01:51.75
  Jul  8 13:01:51.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-5843" for this suite. @ 07/08/23 13:01:51.76
• [0.034 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 07/08/23 13:01:51.768
  Jul  8 13:01:51.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename field-validation @ 07/08/23 13:01:51.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:01:51.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:01:51.782
  Jul  8 13:01:51.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 13:01:52.673674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:53.673722      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:01:54.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4185" for this suite. @ 07/08/23 13:01:54.354
• [2.593 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 07/08/23 13:01:54.361
  Jul  8 13:01:54.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl @ 07/08/23 13:01:54.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:01:54.375
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:01:54.377
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/08/23 13:01:54.379
  Jul  8 13:01:54.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-2042 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jul  8 13:01:54.437: INFO: stderr: ""
  Jul  8 13:01:54.437: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 07/08/23 13:01:54.437
  Jul  8 13:01:54.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-2042 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Jul  8 13:01:54.486: INFO: stderr: ""
  Jul  8 13:01:54.486: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/08/23 13:01:54.486
  Jul  8 13:01:54.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-2042 delete pods e2e-test-httpd-pod'
  E0708 13:01:54.674410      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:55.674490      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:01:56.576: INFO: stderr: ""
  Jul  8 13:01:56.576: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul  8 13:01:56.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2042" for this suite. @ 07/08/23 13:01:56.579
• [2.224 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 07/08/23 13:01:56.587
  Jul  8 13:01:56.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename podtemplate @ 07/08/23 13:01:56.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:01:56.6
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:01:56.602
  Jul  8 13:01:56.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4286" for this suite. @ 07/08/23 13:01:56.632
• [0.050 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 07/08/23 13:01:56.638
  Jul  8 13:01:56.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename sched-pred @ 07/08/23 13:01:56.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:01:56.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:01:56.653
  Jul  8 13:01:56.655: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul  8 13:01:56.661: INFO: Waiting for terminating namespaces to be deleted...
  Jul  8 13:01:56.663: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-12-67 before test
  Jul  8 13:01:56.666: INFO: default-http-backend-kubernetes-worker-65fc475d49-8npt2 from ingress-nginx-kubernetes-worker started at 2023-07-08 11:50:26 +0000 UTC (1 container statuses recorded)
  Jul  8 13:01:56.666: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul  8 13:01:56.666: INFO: nginx-ingress-controller-kubernetes-worker-lsvk7 from ingress-nginx-kubernetes-worker started at 2023-07-08 11:50:26 +0000 UTC (1 container statuses recorded)
  Jul  8 13:01:56.666: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  8 13:01:56.666: INFO: calico-kube-controllers-bb564cc5-vpxwc from kube-system started at 2023-07-08 11:50:30 +0000 UTC (1 container statuses recorded)
  Jul  8 13:01:56.666: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul  8 13:01:56.666: INFO: coredns-5c7f76ccb8-mdjks from kube-system started at 2023-07-08 11:50:22 +0000 UTC (1 container statuses recorded)
  Jul  8 13:01:56.666: INFO: 	Container coredns ready: true, restart count 0
  Jul  8 13:01:56.666: INFO: kube-state-metrics-5b95b4459c-nw4hk from kube-system started at 2023-07-08 11:50:22 +0000 UTC (1 container statuses recorded)
  Jul  8 13:01:56.666: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul  8 13:01:56.666: INFO: metrics-server-v0.5.2-6cf8c8b69c-2d7pj from kube-system started at 2023-07-08 11:50:22 +0000 UTC (2 container statuses recorded)
  Jul  8 13:01:56.666: INFO: 	Container metrics-server ready: true, restart count 0
  Jul  8 13:01:56.666: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul  8 13:01:56.666: INFO: dashboard-metrics-scraper-6b8586b5c9-tjl98 from kubernetes-dashboard started at 2023-07-08 11:50:22 +0000 UTC (1 container statuses recorded)
  Jul  8 13:01:56.666: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul  8 13:01:56.666: INFO: kubernetes-dashboard-6869f4cd5f-x4wbw from kubernetes-dashboard started at 2023-07-08 11:50:22 +0000 UTC (1 container statuses recorded)
  Jul  8 13:01:56.666: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul  8 13:01:56.666: INFO: sonobuoy-systemd-logs-daemon-set-9e0399ccdc194cf6-4ll49 from sonobuoy started at 2023-07-08 12:01:20 +0000 UTC (2 container statuses recorded)
  Jul  8 13:01:56.666: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  8 13:01:56.666: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul  8 13:01:56.666: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-29-188 before test
  Jul  8 13:01:56.671: INFO: nginx-ingress-controller-kubernetes-worker-wmjdn from ingress-nginx-kubernetes-worker started at 2023-07-08 11:52:20 +0000 UTC (1 container statuses recorded)
  Jul  8 13:01:56.671: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  8 13:01:56.671: INFO: sonobuoy-e2e-job-885126f13f224642 from sonobuoy started at 2023-07-08 12:01:20 +0000 UTC (2 container statuses recorded)
  Jul  8 13:01:56.671: INFO: 	Container e2e ready: true, restart count 0
  Jul  8 13:01:56.671: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  8 13:01:56.671: INFO: sonobuoy-systemd-logs-daemon-set-9e0399ccdc194cf6-plcwc from sonobuoy started at 2023-07-08 12:01:20 +0000 UTC (2 container statuses recorded)
  Jul  8 13:01:56.671: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  8 13:01:56.671: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul  8 13:01:56.671: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-93-234 before test
  E0708 13:01:56.674603      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:01:56.675: INFO: nginx-ingress-controller-kubernetes-worker-wrlrn from ingress-nginx-kubernetes-worker started at 2023-07-08 12:26:30 +0000 UTC (1 container statuses recorded)
  Jul  8 13:01:56.675: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  8 13:01:56.675: INFO: sonobuoy from sonobuoy started at 2023-07-08 12:01:18 +0000 UTC (1 container statuses recorded)
  Jul  8 13:01:56.675: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul  8 13:01:56.675: INFO: sonobuoy-systemd-logs-daemon-set-9e0399ccdc194cf6-nrrhp from sonobuoy started at 2023-07-08 12:01:20 +0000 UTC (2 container statuses recorded)
  Jul  8 13:01:56.675: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  8 13:01:56.675: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/08/23 13:01:56.675
  E0708 13:01:57.674708      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:01:58.674796      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/08/23 13:01:58.692
  STEP: Trying to apply a random label on the found node. @ 07/08/23 13:01:58.705
  STEP: verifying the node has the label kubernetes.io/e2e-6d11d880-d803-4afa-812d-33f67d842ae3 95 @ 07/08/23 13:01:58.712
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 07/08/23 13:01:58.714
  E0708 13:01:59.674977      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:00.675146      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.93.234 on the node which pod4 resides and expect not scheduled @ 07/08/23 13:02:00.728
  E0708 13:02:01.675412      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:02.675498      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:03.676277      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:04.677100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:05.677186      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:06.677404      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:07.677506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:08.677692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:09.678532      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:10.679309      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:11.679376      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:12.679561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:13.680032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:14.681094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:15.681200      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:16.681464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:17.681567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:18.681737      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:19.681823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:20.682016      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:21.682233      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:22.682399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:23.682484      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:24.682567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:25.683567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:26.683689      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:27.683785      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:28.683988      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:29.684052      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:30.685107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:31.685954      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:32.686031      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:33.687050      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:34.687280      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:35.687592      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:36.687709      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:37.688032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:38.688220      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:39.688381      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:40.689085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:41.689132      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:42.689228      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:43.689968      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:44.690125      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:45.690389      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:46.691453      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:47.691789      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:48.691995      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:49.692030      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:50.693093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:51.693273      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:52.693415      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:53.693503      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:54.693634      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:55.694332      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:56.694424      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:57.694962      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:58.695118      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:02:59.695806      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:00.696514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:01.697091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:02.697179      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:03.698018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:04.698955      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:05.700001      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:06.700091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:07.701088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:08.702088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:09.702182      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:10.702345      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:11.702423      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:12.702615      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:13.703597      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:14.703758      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:15.704017      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:16.704208      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:17.705093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:18.705255      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:19.705476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:20.705642      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:21.706477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:22.706568      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:23.706657      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:24.706828      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:25.707085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:26.707878      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:27.708024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:28.709094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:29.709494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:30.710447      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:31.710530      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:32.710717      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:33.710839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:34.711756      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:35.712027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:36.713089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:37.713801      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:38.713891      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:39.714917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:40.715084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:41.715946      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:42.716027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:43.717082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:44.717913      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:45.718980      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:46.719063      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:47.719151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:48.719444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:49.720018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:50.720113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:51.720900      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:52.721569      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:53.721657      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:54.721803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:55.722283      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:56.722687      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:57.722716      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:58.723099      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:03:59.723189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:00.723351      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:01.723789      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:02.724452      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:03.725085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:04.725254      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:05.725969      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:06.726247      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:07.726341      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:08.726504      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:09.726883      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:10.726971      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:11.726992      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:12.727151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:13.727990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:14.728030      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:15.728120      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:16.728336      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:17.729092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:18.729590      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:19.729676      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:20.729828      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:21.730824      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:22.730931      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:23.731991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:24.732044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:25.733080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:26.733174      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:27.733361      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:28.733532      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:29.733881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:30.733965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:31.734045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:32.734204      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:33.734341      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:34.734559      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:35.735163      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:36.735408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:37.735739      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:38.735939      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:39.736747      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:40.737100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:41.737737      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:42.737810      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:43.738476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:44.738822      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:45.739319      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:46.740231      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:47.741081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:48.741167      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:49.741558      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:50.742454      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:51.742620      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:52.743335      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:53.744334      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:54.745084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:55.745972      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:56.746063      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:57.746179      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:58.746269      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:04:59.746818      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:00.746963      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:01.747732      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:02.747828      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:03.748399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:04.749092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:05.749951      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:06.750029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:07.751006      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:08.751165      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:09.751220      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:10.751803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:11.752505      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:12.752597      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:13.752689      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:14.752808      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:15.753085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:16.753357      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:17.753777      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:18.753944      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:19.754691      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:20.754855      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:21.755756      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:22.755839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:23.756017      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:24.756106      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:25.756196      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:26.757240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:27.757325      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:28.757487      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:29.757826      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:30.758869      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:31.759927      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:32.760112      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:33.761084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:34.761158      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:35.761219      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:36.761483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:37.761930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:38.762079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:39.763013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:40.763183      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:41.763278      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:42.763441      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:43.763848      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:44.764855      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:45.765269      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:46.765354      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:47.766419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:48.766509      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:49.766971      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:50.767066      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:51.767380      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:52.767481      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:53.768255      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:54.769097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:55.769706      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:56.769815      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:57.770074      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:58.770226      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:05:59.771270      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:00.771353      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:01.771534      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:02.771628      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:03.772555      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:04.773088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:05.773376      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:06.773649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:07.774671      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:08.774838      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:09.775280      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:10.775434      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:11.775508      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:12.775652      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:13.776376      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:14.777088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:15.777206      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:16.777294      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:17.777783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:18.777878      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:19.778543      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:20.778697      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:21.778784      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:22.778974      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:23.779619      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:24.779711      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:25.780503      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:26.780588      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:27.780663      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:28.780806      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:29.780883      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:30.781034      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:31.781817      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:32.781903      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:33.781998      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:34.782223      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:35.782668      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:36.782854      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:37.783710      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:38.783862      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:39.784335      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:40.784429      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:41.784515      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:42.784612      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:43.784753      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:44.785086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:45.785928      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:46.786125      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:47.786738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:48.786833      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:49.787418      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:50.787817      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:51.788807      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:52.789483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:53.789878      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:54.790040      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:55.790088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:56.790344      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:57.790692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:58.790927      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:06:59.791125      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-6d11d880-d803-4afa-812d-33f67d842ae3 off the node ip-172-31-93-234 @ 07/08/23 13:07:00.735
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-6d11d880-d803-4afa-812d-33f67d842ae3 @ 07/08/23 13:07:00.746
  Jul  8 13:07:00.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8739" for this suite. @ 07/08/23 13:07:00.753
• [304.122 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 07/08/23 13:07:00.76
  Jul  8 13:07:00.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 07/08/23 13:07:00.761
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:07:00.776
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:07:00.778
  STEP: Setting up the test @ 07/08/23 13:07:00.78
  STEP: Creating hostNetwork=false pod @ 07/08/23 13:07:00.78
  E0708 13:07:00.791135      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:01.791552      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:02.792382      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 07/08/23 13:07:02.794
  E0708 13:07:03.792514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:04.792564      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 07/08/23 13:07:04.813
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 07/08/23 13:07:04.813
  Jul  8 13:07:04.813: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1228 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:07:04.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:07:04.814: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:07:04.814: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1228/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul  8 13:07:04.868: INFO: Exec stderr: ""
  Jul  8 13:07:04.868: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1228 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:07:04.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:07:04.869: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:07:04.869: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1228/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul  8 13:07:04.924: INFO: Exec stderr: ""
  Jul  8 13:07:04.924: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1228 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:07:04.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:07:04.925: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:07:04.925: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1228/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul  8 13:07:04.975: INFO: Exec stderr: ""
  Jul  8 13:07:04.975: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1228 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:07:04.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:07:04.976: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:07:04.976: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1228/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul  8 13:07:05.037: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 07/08/23 13:07:05.037
  Jul  8 13:07:05.037: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1228 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:07:05.037: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:07:05.037: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:07:05.037: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1228/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jul  8 13:07:05.083: INFO: Exec stderr: ""
  Jul  8 13:07:05.083: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1228 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:07:05.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:07:05.084: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:07:05.084: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1228/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jul  8 13:07:05.114: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 07/08/23 13:07:05.114
  Jul  8 13:07:05.114: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1228 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:07:05.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:07:05.115: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:07:05.115: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1228/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul  8 13:07:05.174: INFO: Exec stderr: ""
  Jul  8 13:07:05.174: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1228 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:07:05.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:07:05.174: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:07:05.174: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1228/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul  8 13:07:05.213: INFO: Exec stderr: ""
  Jul  8 13:07:05.213: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1228 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:07:05.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:07:05.214: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:07:05.214: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1228/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul  8 13:07:05.261: INFO: Exec stderr: ""
  Jul  8 13:07:05.262: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1228 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:07:05.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:07:05.262: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:07:05.262: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1228/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul  8 13:07:05.310: INFO: Exec stderr: ""
  Jul  8 13:07:05.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-1228" for this suite. @ 07/08/23 13:07:05.314
• [4.559 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 07/08/23 13:07:05.32
  Jul  8 13:07:05.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename replicaset @ 07/08/23 13:07:05.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:07:05.334
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:07:05.336
  STEP: Create a ReplicaSet @ 07/08/23 13:07:05.338
  STEP: Verify that the required pods have come up @ 07/08/23 13:07:05.344
  Jul  8 13:07:05.347: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0708 13:07:05.793121      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:06.793413      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:07.793519      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:08.793583      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:09.793762      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:07:10.352: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 07/08/23 13:07:10.352
  Jul  8 13:07:10.355: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 07/08/23 13:07:10.355
  STEP: DeleteCollection of the ReplicaSets @ 07/08/23 13:07:10.358
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 07/08/23 13:07:10.366
  Jul  8 13:07:10.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-105" for this suite. @ 07/08/23 13:07:10.374
• [5.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 07/08/23 13:07:10.387
  Jul  8 13:07:10.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename daemonsets @ 07/08/23 13:07:10.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:07:10.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:07:10.405
  Jul  8 13:07:10.428: INFO: Create a RollingUpdate DaemonSet
  Jul  8 13:07:10.433: INFO: Check that daemon pods launch on every node of the cluster
  Jul  8 13:07:10.435: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 13:07:10.436: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 13:07:10.439: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 13:07:10.439: INFO: Node ip-172-31-12-67 is running 0 daemon pod, expected 1
  E0708 13:07:10.793903      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:07:11.443: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 13:07:11.443: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 13:07:11.447: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul  8 13:07:11.447: INFO: Node ip-172-31-93-234 is running 0 daemon pod, expected 1
  E0708 13:07:11.794766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:07:12.443: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 13:07:12.443: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 13:07:12.446: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul  8 13:07:12.446: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Jul  8 13:07:12.446: INFO: Update the DaemonSet to trigger a rollout
  Jul  8 13:07:12.454: INFO: Updating DaemonSet daemon-set
  E0708 13:07:12.795648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:07:13.468: INFO: Roll back the DaemonSet before rollout is complete
  Jul  8 13:07:13.478: INFO: Updating DaemonSet daemon-set
  Jul  8 13:07:13.478: INFO: Make sure DaemonSet rollback is complete
  Jul  8 13:07:13.480: INFO: Wrong image for pod: daemon-set-vnm46. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Jul  8 13:07:13.480: INFO: Pod daemon-set-vnm46 is not available
  Jul  8 13:07:13.484: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 13:07:13.484: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0708 13:07:13.795673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:07:14.491: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 13:07:14.491: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0708 13:07:14.796423      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:07:15.488: INFO: Pod daemon-set-g52fd is not available
  Jul  8 13:07:15.491: INFO: DaemonSet pods can't tolerate node ip-172-31-42-234 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul  8 13:07:15.491: INFO: DaemonSet pods can't tolerate node ip-172-31-91-17 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 07/08/23 13:07:15.496
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5320, will wait for the garbage collector to delete the pods @ 07/08/23 13:07:15.496
  Jul  8 13:07:15.555: INFO: Deleting DaemonSet.extensions daemon-set took: 5.241561ms
  Jul  8 13:07:15.655: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.116387ms
  E0708 13:07:15.796694      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:16.797037      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:07:17.358: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 13:07:17.358: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul  8 13:07:17.361: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26906"},"items":null}

  Jul  8 13:07:17.364: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26906"},"items":null}

  Jul  8 13:07:17.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5320" for this suite. @ 07/08/23 13:07:17.378
• [6.997 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 07/08/23 13:07:17.385
  Jul  8 13:07:17.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename cronjob @ 07/08/23 13:07:17.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:07:17.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:07:17.403
  STEP: Creating a cronjob @ 07/08/23 13:07:17.405
  STEP: Ensuring more than one job is running at a time @ 07/08/23 13:07:17.412
  E0708 13:07:17.798057      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:18.798152      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:19.798749      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:20.799157      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:21.799884      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:22.800043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:23.800993      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:24.801399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:25.801452      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:26.802128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:27.802967      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:28.803059      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:29.803093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:30.803175      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:31.803803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:32.804514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:33.805044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:34.805239      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:35.805561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:36.805757      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:37.806755      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:38.806918      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:39.806952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:40.808011      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:41.808025      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:42.809091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:43.810033      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:44.810655      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:45.810991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:46.811263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:47.811303      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:48.811451      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:49.812282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:50.812368      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:51.813292      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:52.813384      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:53.814375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:54.815229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:55.815852      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:56.816024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:57.816158      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:58.816249      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:07:59.817032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:00.817191      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:01.818021      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:02.818162      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:03.819152      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:04.819305      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:05.819346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:06.819624      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:07.820518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:08.820606      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:09.820699      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:10.820792      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:11.820808      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:12.821086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:13.821147      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:14.821309      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:15.822371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:16.822643      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:17.823567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:18.823655      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:19.824455      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:20.825090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:21.825745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:22.826153      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:23.827049      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:24.827139      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:25.827180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:26.827903      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:27.827969      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:28.828027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:29.828897      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:30.829065      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:31.829700      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:32.829783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:33.830708      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:34.830944      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:35.831740      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:36.832521      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:37.833090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:38.833781      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:39.834133      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:40.834290      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:41.834375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:42.834526      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:43.834635      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:44.834805      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:45.834893      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:46.835100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:47.836139      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:48.837086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:49.837190      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:50.837365      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:51.838340      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:52.838745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:53.839081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:54.839484      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:55.840258      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:56.841091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:57.841860      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:58.841951      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:08:59.842161      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:00.842315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 07/08/23 13:09:01.415
  STEP: Removing cronjob @ 07/08/23 13:09:01.42
  Jul  8 13:09:01.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4160" for this suite. @ 07/08/23 13:09:01.432
• [104.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 07/08/23 13:09:01.448
  Jul  8 13:09:01.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir @ 07/08/23 13:09:01.449
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:09:01.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:09:01.471
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 07/08/23 13:09:01.475
  E0708 13:09:01.842519      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:02.842608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:09:03.502
  Jul  8 13:09:03.504: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-d330e6be-968c-40f3-bc83-ed25b790ac8f container test-container: <nil>
  STEP: delete the pod @ 07/08/23 13:09:03.52
  Jul  8 13:09:03.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8240" for this suite. @ 07/08/23 13:09:03.537
• [2.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 07/08/23 13:09:03.546
  Jul  8 13:09:03.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename deployment @ 07/08/23 13:09:03.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:09:03.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:09:03.561
  Jul  8 13:09:03.572: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0708 13:09:03.843047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:04.843239      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:05.843330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:06.843631      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:07.843723      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:09:08.576: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/08/23 13:09:08.576
  Jul  8 13:09:08.576: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 07/08/23 13:09:08.584
  Jul  8 13:09:08.595: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9779  5307bace-52d4-4d6a-9b90-7fc1fa81618a 27264 1 2023-07-08 13:09:08 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-07-08 13:09:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d09f68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Jul  8 13:09:08.599: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Jul  8 13:09:08.599: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Jul  8 13:09:08.600: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9779  10a77e1e-17de-4180-93b8-20fa60a44f4f 27265 1 2023-07-08 13:09:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 5307bace-52d4-4d6a-9b90-7fc1fa81618a 0xc002064367 0xc002064368}] [] [{e2e.test Update apps/v1 2023-07-08 13:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 13:09:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-08 13:09:08 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"5307bace-52d4-4d6a-9b90-7fc1fa81618a\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002064428 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul  8 13:09:08.604: INFO: Pod "test-cleanup-controller-t9zkg" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-t9zkg test-cleanup-controller- deployment-9779  13e0b827-e6d7-4701-a8aa-8f388da1068d 27240 0 2023-07-08 13:09:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 10a77e1e-17de-4180-93b8-20fa60a44f4f 0xc002064a77 0xc002064a78}] [] [{kube-controller-manager Update v1 2023-07-08 13:09:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10a77e1e-17de-4180-93b8-20fa60a44f4f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:09:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rfzzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfzzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-93-234,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:09:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:09:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.93.234,PodIP:192.168.7.206,StartTime:2023-07-08 13:09:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-08 13:09:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://401ca5a838198e9d1f97cd524b74a27e0a944fa3669eed60613333fcc7c607a9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.206,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:09:08.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9779" for this suite. @ 07/08/23 13:09:08.613
• [5.081 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 07/08/23 13:09:08.626
  Jul  8 13:09:08.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl @ 07/08/23 13:09:08.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:09:08.787
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:09:08.789
  Jul  8 13:09:08.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-4715 create -f -'
  E0708 13:09:08.843888      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:09:09.285: INFO: stderr: ""
  Jul  8 13:09:09.285: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Jul  8 13:09:09.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-4715 create -f -'
  Jul  8 13:09:09.484: INFO: stderr: ""
  Jul  8 13:09:09.484: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/08/23 13:09:09.484
  E0708 13:09:09.843970      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:09:10.488: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  8 13:09:10.488: INFO: Found 1 / 1
  Jul  8 13:09:10.488: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jul  8 13:09:10.490: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul  8 13:09:10.490: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul  8 13:09:10.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-4715 describe pod agnhost-primary-v89tv'
  Jul  8 13:09:10.545: INFO: stderr: ""
  Jul  8 13:09:10.546: INFO: stdout: "Name:             agnhost-primary-v89tv\nNamespace:        kubectl-4715\nPriority:         0\nService Account:  default\nNode:             ip-172-31-93-234/172.31.93.234\nStart Time:       Sat, 08 Jul 2023 13:09:09 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.7.208\nIPs:\n  IP:           192.168.7.208\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://03e7e2b9b91b6165c4fc34ddfdd3685ff1b17443b3c9a20b9a70084d84e33e4b\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 08 Jul 2023 13:09:09 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fs58q (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-fs58q:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-4715/agnhost-primary-v89tv to ip-172-31-93-234\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Jul  8 13:09:10.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-4715 describe rc agnhost-primary'
  Jul  8 13:09:10.612: INFO: stderr: ""
  Jul  8 13:09:10.612: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4715\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-v89tv\n"
  Jul  8 13:09:10.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-4715 describe service agnhost-primary'
  Jul  8 13:09:10.683: INFO: stderr: ""
  Jul  8 13:09:10.683: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4715\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.43\nIPs:               10.152.183.43\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.7.208:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Jul  8 13:09:10.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-4715 describe node ip-172-31-12-67'
  Jul  8 13:09:10.756: INFO: stderr: ""
  Jul  8 13:09:10.756: INFO: stdout: "Name:               ip-172-31-12-67\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-worker\n                    juju-charm=kubernetes-worker\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-12-67\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 08 Jul 2023 11:50:21 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-12-67\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 08 Jul 2023 13:09:00 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 08 Jul 2023 13:05:36 +0000   Sat, 08 Jul 2023 11:50:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 08 Jul 2023 13:05:36 +0000   Sat, 08 Jul 2023 11:50:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 08 Jul 2023 13:05:36 +0000   Sat, 08 Jul 2023 11:50:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 08 Jul 2023 13:05:36 +0000   Sat, 08 Jul 2023 11:50:21 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.12.67\n  Hostname:    ip-172-31-12-67\nCapacity:\n  cpu:                2\n  ephemeral-storage:  16069568Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7982624Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  14809713845\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7880224Ki\n  pods:               110\nSystem Info:\n  Machine ID:                      ec2aa44804aac06b369f0a9122d3d71c\n  System UUID:                     ec2aa448-04aa-c06b-369f-0a9122d3d71c\n  Boot ID:                         32386060-f7f9-4ac6-b5c8-07f83f23e902\n  Kernel Version:                  5.19.0-1028-aws\n  OS Image:                        Ubuntu 22.04.2 LTS\n  Operating System:                linux\n  Architecture:                    amd64\n  Container Runtime Version:       containerd://1.6.8\n  Kubelet Version:                 v1.27.3\n  Kube-Proxy Version:              v1.27.3\nNon-terminated Pods:               (9 in total)\n  Namespace                        Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                        ----                                                       ------------  ----------  ---------------  -------------  ---\n  ingress-nginx-kubernetes-worker  default-http-backend-kubernetes-worker-65fc475d49-8npt2    10m (0%)      10m (0%)    20Mi (0%)        20Mi (0%)      78m\n  ingress-nginx-kubernetes-worker  nginx-ingress-controller-kubernetes-worker-lsvk7           0 (0%)        0 (0%)      0 (0%)           0 (0%)         78m\n  kube-system                      calico-kube-controllers-bb564cc5-vpxwc                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         78m\n  kube-system                      coredns-5c7f76ccb8-mdjks                                   100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     81m\n  kube-system                      kube-state-metrics-5b95b4459c-nw4hk                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         81m\n  kube-system                      metrics-server-v0.5.2-6cf8c8b69c-2d7pj                     5m (0%)       100m (5%)   50Mi (0%)        300Mi (3%)     81m\n  kubernetes-dashboard             dashboard-metrics-scraper-6b8586b5c9-tjl98                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         81m\n  kubernetes-dashboard             kubernetes-dashboard-6869f4cd5f-x4wbw                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         81m\n  sonobuoy                         sonobuoy-systemd-logs-daemon-set-9e0399ccdc194cf6-4ll49    0 (0%)        0 (0%)      0 (0%)           0 (0%)         67m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                115m (5%)   110m (5%)\n  memory             140Mi (1%)  490Mi (6%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
  Jul  8 13:09:10.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-4715 describe namespace kubectl-4715'
  Jul  8 13:09:10.815: INFO: stderr: ""
  Jul  8 13:09:10.815: INFO: stdout: "Name:         kubectl-4715\nLabels:       e2e-framework=kubectl\n              e2e-run=65e215ab-230f-404c-8004-75735dff06e4\n              kubernetes.io/metadata.name=kubectl-4715\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Jul  8 13:09:10.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4715" for this suite. @ 07/08/23 13:09:10.819
• [2.197 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 07/08/23 13:09:10.824
  Jul  8 13:09:10.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename resourcequota @ 07/08/23 13:09:10.825
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:09:10.839
  E0708 13:09:10.844248      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:09:10.845
  STEP: Creating a ResourceQuota @ 07/08/23 13:09:10.847
  STEP: Getting a ResourceQuota @ 07/08/23 13:09:10.851
  STEP: Updating a ResourceQuota @ 07/08/23 13:09:10.854
  STEP: Verifying a ResourceQuota was modified @ 07/08/23 13:09:10.859
  STEP: Deleting a ResourceQuota @ 07/08/23 13:09:10.861
  STEP: Verifying the deleted ResourceQuota @ 07/08/23 13:09:10.866
  Jul  8 13:09:10.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3996" for this suite. @ 07/08/23 13:09:10.873
• [0.053 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 07/08/23 13:09:10.878
  Jul  8 13:09:10.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename services @ 07/08/23 13:09:10.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:09:10.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:09:10.893
  Jul  8 13:09:10.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5520" for this suite. @ 07/08/23 13:09:10.9
• [0.029 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 07/08/23 13:09:10.908
  Jul  8 13:09:10.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename runtimeclass @ 07/08/23 13:09:10.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:09:10.919
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:09:10.921
  E0708 13:09:11.844448      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:12.845097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:09:12.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3898" for this suite. @ 07/08/23 13:09:12.954
• [2.052 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 07/08/23 13:09:12.96
  Jul  8 13:09:12.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename namespaces @ 07/08/23 13:09:12.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:09:12.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:09:12.977
  STEP: creating a Namespace @ 07/08/23 13:09:12.979
  STEP: patching the Namespace @ 07/08/23 13:09:12.992
  STEP: get the Namespace and ensuring it has the label @ 07/08/23 13:09:12.999
  Jul  8 13:09:13.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1517" for this suite. @ 07/08/23 13:09:13.007
  STEP: Destroying namespace "nspatchtest-b8e0c157-5d5a-498e-84f8-3455a7cab581-1277" for this suite. @ 07/08/23 13:09:13.028
• [0.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 07/08/23 13:09:13.035
  Jul  8 13:09:13.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename secrets @ 07/08/23 13:09:13.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:09:13.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:09:13.051
  STEP: Creating secret with name s-test-opt-del-61ebfc3e-9828-474c-a70a-87d6ce5f6945 @ 07/08/23 13:09:13.057
  STEP: Creating secret with name s-test-opt-upd-2278e693-5034-4b7f-9729-2595fe94c851 @ 07/08/23 13:09:13.061
  STEP: Creating the pod @ 07/08/23 13:09:13.064
  E0708 13:09:13.845195      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:14.845355      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-61ebfc3e-9828-474c-a70a-87d6ce5f6945 @ 07/08/23 13:09:15.103
  STEP: Updating secret s-test-opt-upd-2278e693-5034-4b7f-9729-2595fe94c851 @ 07/08/23 13:09:15.109
  STEP: Creating secret with name s-test-opt-create-cfaf36cd-678d-4430-8d3b-e22495bd7bb3 @ 07/08/23 13:09:15.113
  STEP: waiting to observe update in volume @ 07/08/23 13:09:15.116
  E0708 13:09:15.845441      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:16.845598      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:17.845675      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:18.845957      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:19.846700      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:20.846887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:21.847040      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:22.847142      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:23.847967      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:24.848035      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:25.849113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:26.849182      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:27.849565      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:28.849666      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:29.849741      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:30.849941      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:31.850026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:32.850255      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:33.850348      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:34.850543      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:35.850627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:36.850702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:37.850805      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:38.850906      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:39.850998      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:40.851167      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:41.851241      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:42.851338      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:43.852284      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:44.852377      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:45.852453      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:46.852747      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:47.852832      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:48.853077      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:49.853842      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:50.854084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:51.854780      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:52.855699      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:53.855712      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:54.855811      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:55.856653      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:56.856770      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:57.856855      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:58.857339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:09:59.857421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:00.857507      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:01.858383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:02.858486      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:03.858494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:04.858579      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:05.859170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:06.859410      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:07.859444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:08.859554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:09.860332      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:10.860424      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:11.860515      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:12.860586      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:13.860680      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:14.861113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:15.861200      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:16.861278      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:17.862130      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:18.862637      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:19.862990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:20.863093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:21.863318      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:22.863873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:23.864672      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:24.864770      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:25.865622      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:26.865780      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:27.866087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:28.866199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:29.867178      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:30.867268      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:31.867953      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:32.868115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:10:33.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5374" for this suite. @ 07/08/23 13:10:33.431
• [80.403 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 07/08/23 13:10:33.438
  Jul  8 13:10:33.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename downward-api @ 07/08/23 13:10:33.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:10:33.453
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:10:33.455
  STEP: Creating a pod to test downward api env vars @ 07/08/23 13:10:33.458
  E0708 13:10:33.869125      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:34.869316      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:35.870006      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:36.870023      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:10:37.478
  Jul  8 13:10:37.481: INFO: Trying to get logs from node ip-172-31-29-188 pod downward-api-2cf120da-a3ec-4870-b8b1-a83fa3484197 container dapi-container: <nil>
  STEP: delete the pod @ 07/08/23 13:10:37.497
  Jul  8 13:10:37.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2803" for this suite. @ 07/08/23 13:10:37.517
• [4.085 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 07/08/23 13:10:37.523
  Jul  8 13:10:37.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir-wrapper @ 07/08/23 13:10:37.523
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:10:37.535
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:10:37.538
  STEP: Creating 50 configmaps @ 07/08/23 13:10:37.54
  STEP: Creating RC which spawns configmap-volume pods @ 07/08/23 13:10:37.779
  E0708 13:10:37.870828      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:10:37.909: INFO: Pod name wrapped-volume-race-174fbb7c-23e7-4477-8aa4-853260b3f6f4: Found 3 pods out of 5
  E0708 13:10:38.870913      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:39.871004      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:40.871162      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:41.871525      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:42.872513      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:10:42.915: INFO: Pod name wrapped-volume-race-174fbb7c-23e7-4477-8aa4-853260b3f6f4: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/08/23 13:10:42.915
  STEP: Creating RC which spawns configmap-volume pods @ 07/08/23 13:10:42.932
  Jul  8 13:10:42.944: INFO: Pod name wrapped-volume-race-3495c778-0e18-4a86-a41e-32b81f2e412c: Found 0 pods out of 5
  E0708 13:10:43.873527      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:44.874161      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:45.874325      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:46.874587      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:47.874664      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:10:47.950: INFO: Pod name wrapped-volume-race-3495c778-0e18-4a86-a41e-32b81f2e412c: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/08/23 13:10:47.95
  STEP: Creating RC which spawns configmap-volume pods @ 07/08/23 13:10:47.994
  Jul  8 13:10:48.006: INFO: Pod name wrapped-volume-race-dce23acd-5ded-4875-a87a-a4cb8482dea1: Found 0 pods out of 5
  E0708 13:10:48.874833      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:49.874871      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:50.875062      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:51.875429      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:52.875580      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:10:53.013: INFO: Pod name wrapped-volume-race-dce23acd-5ded-4875-a87a-a4cb8482dea1: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/08/23 13:10:53.013
  Jul  8 13:10:53.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-dce23acd-5ded-4875-a87a-a4cb8482dea1 in namespace emptydir-wrapper-6947, will wait for the garbage collector to delete the pods @ 07/08/23 13:10:53.032
  Jul  8 13:10:53.093: INFO: Deleting ReplicationController wrapped-volume-race-dce23acd-5ded-4875-a87a-a4cb8482dea1 took: 6.560432ms
  Jul  8 13:10:53.193: INFO: Terminating ReplicationController wrapped-volume-race-dce23acd-5ded-4875-a87a-a4cb8482dea1 pods took: 100.079301ms
  E0708 13:10:53.876610      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:10:54.877458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-3495c778-0e18-4a86-a41e-32b81f2e412c in namespace emptydir-wrapper-6947, will wait for the garbage collector to delete the pods @ 07/08/23 13:10:54.893
  Jul  8 13:10:54.954: INFO: Deleting ReplicationController wrapped-volume-race-3495c778-0e18-4a86-a41e-32b81f2e412c took: 6.592467ms
  Jul  8 13:10:55.055: INFO: Terminating ReplicationController wrapped-volume-race-3495c778-0e18-4a86-a41e-32b81f2e412c pods took: 100.885276ms
  E0708 13:10:55.877855      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-174fbb7c-23e7-4477-8aa4-853260b3f6f4 in namespace emptydir-wrapper-6947, will wait for the garbage collector to delete the pods @ 07/08/23 13:10:56.756
  Jul  8 13:10:56.817: INFO: Deleting ReplicationController wrapped-volume-race-174fbb7c-23e7-4477-8aa4-853260b3f6f4 took: 6.368416ms
  E0708 13:10:56.878421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:10:56.918: INFO: Terminating ReplicationController wrapped-volume-race-174fbb7c-23e7-4477-8aa4-853260b3f6f4 pods took: 101.127286ms
  E0708 13:10:57.878951      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 07/08/23 13:10:58.719
  E0708 13:10:58.879832      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-wrapper-6947" for this suite. @ 07/08/23 13:10:58.994
• [21.477 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 07/08/23 13:10:59.001
  Jul  8 13:10:59.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 13:10:59.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:10:59.015
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:10:59.018
  STEP: Creating projection with secret that has name projected-secret-test-ea1038fb-7f50-4b3e-bda8-f2ea50a9b56c @ 07/08/23 13:10:59.02
  STEP: Creating a pod to test consume secrets @ 07/08/23 13:10:59.024
  E0708 13:10:59.880047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:00.881101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:01.881934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:02.882768      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:11:03.044
  Jul  8 13:11:03.048: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-projected-secrets-73170632-394b-4738-9582-29c99235a38d container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/08/23 13:11:03.054
  Jul  8 13:11:03.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7739" for this suite. @ 07/08/23 13:11:03.069
• [4.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 07/08/23 13:11:03.077
  Jul  8 13:11:03.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename hostport @ 07/08/23 13:11:03.078
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:11:03.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:11:03.098
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 07/08/23 13:11:03.104
  E0708 13:11:03.883055      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:04.883146      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.93.234 on the node which pod1 resides and expect scheduled @ 07/08/23 13:11:05.121
  E0708 13:11:05.884040      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:06.885087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:07.885210      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:08.885878      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:09.885989      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:10.886160      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:11.887042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:12.887140      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:13.887235      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:14.887330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:15.887431      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:16.887719      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.93.234 but use UDP protocol on the node which pod2 resides @ 07/08/23 13:11:17.154
  E0708 13:11:17.888620      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:18.889160      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:19.889874      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:20.889943      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 07/08/23 13:11:21.182
  Jul  8 13:11:21.182: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.93.234 http://127.0.0.1:54323/hostname] Namespace:hostport-2246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:11:21.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:11:21.182: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:11:21.182: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-2246/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.93.234+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.93.234, port: 54323 @ 07/08/23 13:11:21.246
  Jul  8 13:11:21.246: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.93.234:54323/hostname] Namespace:hostport-2246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:11:21.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:11:21.246: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:11:21.246: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-2246/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.93.234%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.93.234, port: 54323 UDP @ 07/08/23 13:11:21.301
  Jul  8 13:11:21.301: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.93.234 54323] Namespace:hostport-2246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:11:21.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:11:21.302: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:11:21.302: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-2246/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.93.234+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0708 13:11:21.890891      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:22.890953      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:23.891124      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:24.891225      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:25.891330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:11:26.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-2246" for this suite. @ 07/08/23 13:11:26.372
• [23.300 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 07/08/23 13:11:26.38
  Jul  8 13:11:26.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 13:11:26.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:11:26.394
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:11:26.397
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 13:11:26.399
  E0708 13:11:26.891728      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:27.892763      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:28.892853      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:29.893269      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:11:30.418
  Jul  8 13:11:30.421: INFO: Trying to get logs from node ip-172-31-29-188 pod downwardapi-volume-8e5de6e3-806a-40dd-948d-caeeac34583a container client-container: <nil>
  STEP: delete the pod @ 07/08/23 13:11:30.428
  Jul  8 13:11:30.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6507" for this suite. @ 07/08/23 13:11:30.447
• [4.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 07/08/23 13:11:30.452
  Jul  8 13:11:30.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename configmap @ 07/08/23 13:11:30.453
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:11:30.472
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:11:30.477
  STEP: Creating configMap with name configmap-test-volume-85ca21c5-5a39-4b3b-8fba-682f158def0b @ 07/08/23 13:11:30.481
  STEP: Creating a pod to test consume configMaps @ 07/08/23 13:11:30.486
  E0708 13:11:30.893797      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:31.894246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:32.895053      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:33.895291      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:11:34.575
  Jul  8 13:11:34.579: INFO: Trying to get logs from node ip-172-31-29-188 pod pod-configmaps-bc675b55-2433-4dfb-9171-31dd5f57275f container agnhost-container: <nil>
  STEP: delete the pod @ 07/08/23 13:11:34.586
  Jul  8 13:11:34.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1526" for this suite. @ 07/08/23 13:11:34.602
• [4.154 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 07/08/23 13:11:34.608
  Jul  8 13:11:34.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubelet-test @ 07/08/23 13:11:34.608
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:11:34.621
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:11:34.623
  E0708 13:11:34.896168      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:35.897118      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:11:36.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-420" for this suite. @ 07/08/23 13:11:36.653
• [2.052 seconds]
------------------------------
S
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 07/08/23 13:11:36.66
  Jul  8 13:11:36.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename events @ 07/08/23 13:11:36.661
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:11:36.674
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:11:36.678
  STEP: Create set of events @ 07/08/23 13:11:36.681
  STEP: get a list of Events with a label in the current namespace @ 07/08/23 13:11:36.696
  STEP: delete a list of events @ 07/08/23 13:11:36.698
  Jul  8 13:11:36.698: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 07/08/23 13:11:36.72
  Jul  8 13:11:36.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4108" for this suite. @ 07/08/23 13:11:36.727
• [0.074 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 07/08/23 13:11:36.734
  Jul  8 13:11:36.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename crd-webhook @ 07/08/23 13:11:36.734
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:11:36.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:11:36.75
  STEP: Setting up server cert @ 07/08/23 13:11:36.752
  E0708 13:11:36.897736      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 07/08/23 13:11:37.189
  STEP: Deploying the custom resource conversion webhook pod @ 07/08/23 13:11:37.197
  STEP: Wait for the deployment to be ready @ 07/08/23 13:11:37.209
  Jul  8 13:11:37.216: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0708 13:11:37.897865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:38.898056      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 13:11:39.226
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 13:11:39.236
  E0708 13:11:39.898758      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:11:40.237: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jul  8 13:11:40.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 13:11:40.899614      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:41.900040      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 07/08/23 13:11:42.797
  STEP: v2 custom resource should be converted @ 07/08/23 13:11:42.8
  Jul  8 13:11:42.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0708 13:11:42.900595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-388" for this suite. @ 07/08/23 13:11:43.361
• [6.635 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 07/08/23 13:11:43.369
  Jul  8 13:11:43.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename services @ 07/08/23 13:11:43.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:11:43.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:11:43.388
  STEP: fetching services @ 07/08/23 13:11:43.391
  Jul  8 13:11:43.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1926" for this suite. @ 07/08/23 13:11:43.397
• [0.035 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 07/08/23 13:11:43.404
  Jul  8 13:11:43.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/08/23 13:11:43.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:11:43.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:11:43.42
  Jul  8 13:11:43.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 13:11:43.901112      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/08/23 13:11:44.659
  Jul  8 13:11:44.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-9742 --namespace=crd-publish-openapi-9742 create -f -'
  E0708 13:11:44.901764      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:11:45.163: INFO: stderr: ""
  Jul  8 13:11:45.163: INFO: stdout: "e2e-test-crd-publish-openapi-8947-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jul  8 13:11:45.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-9742 --namespace=crd-publish-openapi-9742 delete e2e-test-crd-publish-openapi-8947-crds test-cr'
  Jul  8 13:11:45.235: INFO: stderr: ""
  Jul  8 13:11:45.235: INFO: stdout: "e2e-test-crd-publish-openapi-8947-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Jul  8 13:11:45.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-9742 --namespace=crd-publish-openapi-9742 apply -f -'
  Jul  8 13:11:45.381: INFO: stderr: ""
  Jul  8 13:11:45.381: INFO: stdout: "e2e-test-crd-publish-openapi-8947-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jul  8 13:11:45.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-9742 --namespace=crd-publish-openapi-9742 delete e2e-test-crd-publish-openapi-8947-crds test-cr'
  Jul  8 13:11:45.432: INFO: stderr: ""
  Jul  8 13:11:45.432: INFO: stdout: "e2e-test-crd-publish-openapi-8947-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 07/08/23 13:11:45.432
  Jul  8 13:11:45.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=crd-publish-openapi-9742 explain e2e-test-crd-publish-openapi-8947-crds'
  Jul  8 13:11:45.865: INFO: stderr: ""
  Jul  8 13:11:45.865: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-8947-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0708 13:11:45.902176      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:46.902785      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:11:47.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9742" for this suite. @ 07/08/23 13:11:47.212
• [3.815 seconds]
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 07/08/23 13:11:47.219
  Jul  8 13:11:47.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename proxy @ 07/08/23 13:11:47.219
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:11:47.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:11:47.24
  Jul  8 13:11:47.244: INFO: Creating pod...
  E0708 13:11:47.902911      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:48.903519      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:11:49.263: INFO: Creating service...
  Jul  8 13:11:49.275: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8415/pods/agnhost/proxy?method=DELETE
  Jul  8 13:11:49.282: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul  8 13:11:49.282: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8415/pods/agnhost/proxy?method=OPTIONS
  Jul  8 13:11:49.287: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul  8 13:11:49.287: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8415/pods/agnhost/proxy?method=PATCH
  Jul  8 13:11:49.292: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul  8 13:11:49.292: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8415/pods/agnhost/proxy?method=POST
  Jul  8 13:11:49.297: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul  8 13:11:49.297: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8415/pods/agnhost/proxy?method=PUT
  Jul  8 13:11:49.301: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul  8 13:11:49.301: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8415/services/e2e-proxy-test-service/proxy?method=DELETE
  Jul  8 13:11:49.308: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul  8 13:11:49.308: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8415/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Jul  8 13:11:49.314: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul  8 13:11:49.314: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8415/services/e2e-proxy-test-service/proxy?method=PATCH
  Jul  8 13:11:49.321: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul  8 13:11:49.321: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8415/services/e2e-proxy-test-service/proxy?method=POST
  Jul  8 13:11:49.327: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul  8 13:11:49.327: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8415/services/e2e-proxy-test-service/proxy?method=PUT
  Jul  8 13:11:49.333: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul  8 13:11:49.333: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8415/pods/agnhost/proxy?method=GET
  Jul  8 13:11:49.337: INFO: http.Client request:GET StatusCode:301
  Jul  8 13:11:49.337: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8415/services/e2e-proxy-test-service/proxy?method=GET
  Jul  8 13:11:49.342: INFO: http.Client request:GET StatusCode:301
  Jul  8 13:11:49.342: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8415/pods/agnhost/proxy?method=HEAD
  Jul  8 13:11:49.345: INFO: http.Client request:HEAD StatusCode:301
  Jul  8 13:11:49.345: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8415/services/e2e-proxy-test-service/proxy?method=HEAD
  Jul  8 13:11:49.351: INFO: http.Client request:HEAD StatusCode:301
  Jul  8 13:11:49.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-8415" for this suite. @ 07/08/23 13:11:49.355
• [2.144 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 07/08/23 13:11:49.363
  Jul  8 13:11:49.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 13:11:49.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:11:49.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:11:49.383
  STEP: Setting up server cert @ 07/08/23 13:11:49.406
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 13:11:49.582
  STEP: Deploying the webhook pod @ 07/08/23 13:11:49.59
  STEP: Wait for the deployment to be ready @ 07/08/23 13:11:49.602
  Jul  8 13:11:49.610: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0708 13:11:49.904034      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:50.904132      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 13:11:51.622
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 13:11:51.633
  E0708 13:11:51.904373      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:11:52.634: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 07/08/23 13:11:52.638
  STEP: create a pod that should be denied by the webhook @ 07/08/23 13:11:52.655
  STEP: create a pod that causes the webhook to hang @ 07/08/23 13:11:52.668
  E0708 13:11:52.904856      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:53.904930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:54.905146      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:55.905394      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:56.905548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:57.905610      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:58.905776      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:11:59.905882      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:00.906460      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:01.906548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 07/08/23 13:12:02.677
  STEP: create a configmap that should be admitted by the webhook @ 07/08/23 13:12:02.714
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 07/08/23 13:12:02.727
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 07/08/23 13:12:02.737
  STEP: create a namespace that bypass the webhook @ 07/08/23 13:12:02.743
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 07/08/23 13:12:02.762
  Jul  8 13:12:02.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3501" for this suite. @ 07/08/23 13:12:02.821
  STEP: Destroying namespace "webhook-markers-4510" for this suite. @ 07/08/23 13:12:02.829
  STEP: Destroying namespace "exempted-namespace-6150" for this suite. @ 07/08/23 13:12:02.835
• [13.480 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 07/08/23 13:12:02.843
  Jul  8 13:12:02.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir-wrapper @ 07/08/23 13:12:02.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:12:02.861
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:12:02.864
  E0708 13:12:02.906823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:03.907824      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:12:04.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 07/08/23 13:12:04.905
  E0708 13:12:04.908051      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configmap @ 07/08/23 13:12:04.912
  STEP: Cleaning up the pod @ 07/08/23 13:12:04.918
  STEP: Destroying namespace "emptydir-wrapper-833" for this suite. @ 07/08/23 13:12:04.933
• [2.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 07/08/23 13:12:04.943
  Jul  8 13:12:04.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename statefulset @ 07/08/23 13:12:04.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:12:04.961
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:12:04.965
  STEP: Creating service test in namespace statefulset-8307 @ 07/08/23 13:12:04.968
  STEP: Looking for a node to schedule stateful set and pod @ 07/08/23 13:12:04.974
  STEP: Creating pod with conflicting port in namespace statefulset-8307 @ 07/08/23 13:12:04.983
  STEP: Waiting until pod test-pod will start running in namespace statefulset-8307 @ 07/08/23 13:12:04.991
  E0708 13:12:05.909114      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:06.909386      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-8307 @ 07/08/23 13:12:07
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8307 @ 07/08/23 13:12:07.006
  Jul  8 13:12:07.029: INFO: Observed stateful pod in namespace: statefulset-8307, name: ss-0, uid: 791e8f44-b603-4485-8237-b366eec119ad, status phase: Pending. Waiting for statefulset controller to delete.
  Jul  8 13:12:07.044: INFO: Observed stateful pod in namespace: statefulset-8307, name: ss-0, uid: 791e8f44-b603-4485-8237-b366eec119ad, status phase: Failed. Waiting for statefulset controller to delete.
  Jul  8 13:12:07.052: INFO: Observed stateful pod in namespace: statefulset-8307, name: ss-0, uid: 791e8f44-b603-4485-8237-b366eec119ad, status phase: Failed. Waiting for statefulset controller to delete.
  Jul  8 13:12:07.054: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8307
  STEP: Removing pod with conflicting port in namespace statefulset-8307 @ 07/08/23 13:12:07.054
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8307 and will be in running state @ 07/08/23 13:12:07.071
  E0708 13:12:07.909489      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:08.909578      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:12:09.080: INFO: Deleting all statefulset in ns statefulset-8307
  Jul  8 13:12:09.084: INFO: Scaling statefulset ss to 0
  E0708 13:12:09.909662      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:10.909852      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:11.910260      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:12.910361      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:13.910497      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:14.910973      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:15.911061      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:16.911341      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:17.911508      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:18.911590      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:12:19.103: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  8 13:12:19.107: INFO: Deleting statefulset ss
  Jul  8 13:12:19.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8307" for this suite. @ 07/08/23 13:12:19.124
• [14.189 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 07/08/23 13:12:19.132
  Jul  8 13:12:19.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename pods @ 07/08/23 13:12:19.133
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:12:19.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:12:19.151
  E0708 13:12:19.912050      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:20.913096      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:21.913286      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:22.913366      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:23.913458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:24.913631      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:12:25.211
  Jul  8 13:12:25.214: INFO: Trying to get logs from node ip-172-31-93-234 pod client-envvars-e1e57c9e-fabf-475a-8776-3404a716b196 container env3cont: <nil>
  STEP: delete the pod @ 07/08/23 13:12:25.229
  Jul  8 13:12:25.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9863" for this suite. @ 07/08/23 13:12:25.249
• [6.124 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 07/08/23 13:12:25.256
  Jul  8 13:12:25.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename secrets @ 07/08/23 13:12:25.257
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:12:25.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:12:25.279
  STEP: Creating secret with name secret-test-map-63c0f406-e30e-4fb0-9529-0f81377d8823 @ 07/08/23 13:12:25.283
  STEP: Creating a pod to test consume secrets @ 07/08/23 13:12:25.288
  E0708 13:12:25.913731      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:26.913918      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:27.914027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:28.914117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:12:29.311
  Jul  8 13:12:29.315: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-secrets-af280918-334a-457e-b9f4-a1a6168e770b container secret-volume-test: <nil>
  STEP: delete the pod @ 07/08/23 13:12:29.322
  Jul  8 13:12:29.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8311" for this suite. @ 07/08/23 13:12:29.344
• [4.095 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 07/08/23 13:12:29.351
  Jul  8 13:12:29.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename emptydir @ 07/08/23 13:12:29.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:12:29.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:12:29.374
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 07/08/23 13:12:29.377
  E0708 13:12:29.914191      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:30.914343      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:31.915223      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:32.915786      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:12:33.4
  Jul  8 13:12:33.404: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-5c003758-571d-426f-91b1-d7888d3ce7a3 container test-container: <nil>
  STEP: delete the pod @ 07/08/23 13:12:33.412
  Jul  8 13:12:33.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3251" for this suite. @ 07/08/23 13:12:33.433
• [4.088 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 07/08/23 13:12:33.44
  Jul  8 13:12:33.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename gc @ 07/08/23 13:12:33.44
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:12:33.46
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:12:33.463
  STEP: create the rc @ 07/08/23 13:12:33.472
  W0708 13:12:33.478819      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0708 13:12:33.916522      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:34.916715      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:35.919219      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:36.919765      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:37.920715      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:38.920720      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 07/08/23 13:12:39.485
  STEP: wait for the rc to be deleted @ 07/08/23 13:12:39.496
  E0708 13:12:39.923738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:40.927113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:41.927158      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:42.927444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:43.927512      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 07/08/23 13:12:44.5
  E0708 13:12:44.928511      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:45.929213      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:46.929483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:47.929571      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:48.929672      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:49.930211      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:50.930304      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:51.930534      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:52.930721      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:53.930899      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:54.931538      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:55.931705      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:56.931791      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:57.932057      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:58.933092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:12:59.933254      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:00.933932      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:01.934285      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:02.934446      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:03.934537      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:04.934690      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:05.934871      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:06.935078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:07.935246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:08.935574      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:09.935677      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:10.935772      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:11.936032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:12.937093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:13.937268      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/08/23 13:13:14.509
  W0708 13:13:14.513826      20 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul  8 13:13:14.513: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul  8 13:13:14.513: INFO: Deleting pod "simpletest.rc-27bqx" in namespace "gc-7978"
  Jul  8 13:13:14.527: INFO: Deleting pod "simpletest.rc-27jcp" in namespace "gc-7978"
  Jul  8 13:13:14.538: INFO: Deleting pod "simpletest.rc-2kj57" in namespace "gc-7978"
  Jul  8 13:13:14.550: INFO: Deleting pod "simpletest.rc-2l5s6" in namespace "gc-7978"
  Jul  8 13:13:14.565: INFO: Deleting pod "simpletest.rc-2lvvz" in namespace "gc-7978"
  Jul  8 13:13:14.575: INFO: Deleting pod "simpletest.rc-2rshc" in namespace "gc-7978"
  Jul  8 13:13:14.589: INFO: Deleting pod "simpletest.rc-2xnrf" in namespace "gc-7978"
  Jul  8 13:13:14.605: INFO: Deleting pod "simpletest.rc-4bnh7" in namespace "gc-7978"
  Jul  8 13:13:14.618: INFO: Deleting pod "simpletest.rc-4r2vl" in namespace "gc-7978"
  Jul  8 13:13:14.628: INFO: Deleting pod "simpletest.rc-4tpjk" in namespace "gc-7978"
  Jul  8 13:13:14.642: INFO: Deleting pod "simpletest.rc-57vbk" in namespace "gc-7978"
  Jul  8 13:13:14.657: INFO: Deleting pod "simpletest.rc-5899l" in namespace "gc-7978"
  Jul  8 13:13:14.672: INFO: Deleting pod "simpletest.rc-5f2db" in namespace "gc-7978"
  Jul  8 13:13:14.684: INFO: Deleting pod "simpletest.rc-5fqt5" in namespace "gc-7978"
  Jul  8 13:13:14.696: INFO: Deleting pod "simpletest.rc-64mlb" in namespace "gc-7978"
  Jul  8 13:13:14.708: INFO: Deleting pod "simpletest.rc-6dtmh" in namespace "gc-7978"
  Jul  8 13:13:14.728: INFO: Deleting pod "simpletest.rc-6jg2f" in namespace "gc-7978"
  Jul  8 13:13:14.739: INFO: Deleting pod "simpletest.rc-6vn2k" in namespace "gc-7978"
  Jul  8 13:13:14.755: INFO: Deleting pod "simpletest.rc-76l2l" in namespace "gc-7978"
  Jul  8 13:13:14.768: INFO: Deleting pod "simpletest.rc-774hx" in namespace "gc-7978"
  Jul  8 13:13:14.778: INFO: Deleting pod "simpletest.rc-78h2g" in namespace "gc-7978"
  Jul  8 13:13:14.793: INFO: Deleting pod "simpletest.rc-7dckx" in namespace "gc-7978"
  Jul  8 13:13:14.808: INFO: Deleting pod "simpletest.rc-7sz4q" in namespace "gc-7978"
  Jul  8 13:13:14.824: INFO: Deleting pod "simpletest.rc-8468s" in namespace "gc-7978"
  Jul  8 13:13:14.838: INFO: Deleting pod "simpletest.rc-8gmpk" in namespace "gc-7978"
  Jul  8 13:13:14.857: INFO: Deleting pod "simpletest.rc-8kqnj" in namespace "gc-7978"
  Jul  8 13:13:14.868: INFO: Deleting pod "simpletest.rc-8m5r4" in namespace "gc-7978"
  Jul  8 13:13:14.884: INFO: Deleting pod "simpletest.rc-8pjfx" in namespace "gc-7978"
  Jul  8 13:13:14.899: INFO: Deleting pod "simpletest.rc-9ntpn" in namespace "gc-7978"
  Jul  8 13:13:14.911: INFO: Deleting pod "simpletest.rc-9tp7j" in namespace "gc-7978"
  Jul  8 13:13:14.928: INFO: Deleting pod "simpletest.rc-b9rkh" in namespace "gc-7978"
  E0708 13:13:14.938102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:13:14.943: INFO: Deleting pod "simpletest.rc-bgnh2" in namespace "gc-7978"
  Jul  8 13:13:14.966: INFO: Deleting pod "simpletest.rc-bvwcw" in namespace "gc-7978"
  Jul  8 13:13:14.983: INFO: Deleting pod "simpletest.rc-c59k8" in namespace "gc-7978"
  Jul  8 13:13:14.997: INFO: Deleting pod "simpletest.rc-cltzc" in namespace "gc-7978"
  Jul  8 13:13:15.015: INFO: Deleting pod "simpletest.rc-d7fl4" in namespace "gc-7978"
  Jul  8 13:13:15.026: INFO: Deleting pod "simpletest.rc-dtrml" in namespace "gc-7978"
  Jul  8 13:13:15.038: INFO: Deleting pod "simpletest.rc-f9jtk" in namespace "gc-7978"
  Jul  8 13:13:15.052: INFO: Deleting pod "simpletest.rc-fd2wl" in namespace "gc-7978"
  Jul  8 13:13:15.066: INFO: Deleting pod "simpletest.rc-fgj2s" in namespace "gc-7978"
  Jul  8 13:13:15.080: INFO: Deleting pod "simpletest.rc-fts2f" in namespace "gc-7978"
  Jul  8 13:13:15.090: INFO: Deleting pod "simpletest.rc-fxzwc" in namespace "gc-7978"
  Jul  8 13:13:15.102: INFO: Deleting pod "simpletest.rc-fz5n8" in namespace "gc-7978"
  Jul  8 13:13:15.116: INFO: Deleting pod "simpletest.rc-g97kv" in namespace "gc-7978"
  Jul  8 13:13:15.131: INFO: Deleting pod "simpletest.rc-g9qt2" in namespace "gc-7978"
  Jul  8 13:13:15.143: INFO: Deleting pod "simpletest.rc-gpgb8" in namespace "gc-7978"
  Jul  8 13:13:15.157: INFO: Deleting pod "simpletest.rc-gq4vr" in namespace "gc-7978"
  Jul  8 13:13:15.171: INFO: Deleting pod "simpletest.rc-gqn5z" in namespace "gc-7978"
  Jul  8 13:13:15.184: INFO: Deleting pod "simpletest.rc-gvhzh" in namespace "gc-7978"
  Jul  8 13:13:15.195: INFO: Deleting pod "simpletest.rc-hgj66" in namespace "gc-7978"
  Jul  8 13:13:15.212: INFO: Deleting pod "simpletest.rc-hlt7w" in namespace "gc-7978"
  Jul  8 13:13:15.224: INFO: Deleting pod "simpletest.rc-hspt7" in namespace "gc-7978"
  Jul  8 13:13:15.237: INFO: Deleting pod "simpletest.rc-hzvsx" in namespace "gc-7978"
  Jul  8 13:13:15.251: INFO: Deleting pod "simpletest.rc-j5hq6" in namespace "gc-7978"
  Jul  8 13:13:15.265: INFO: Deleting pod "simpletest.rc-jn9z2" in namespace "gc-7978"
  Jul  8 13:13:15.278: INFO: Deleting pod "simpletest.rc-jqrwp" in namespace "gc-7978"
  Jul  8 13:13:15.289: INFO: Deleting pod "simpletest.rc-jrtgc" in namespace "gc-7978"
  Jul  8 13:13:15.305: INFO: Deleting pod "simpletest.rc-jxn7p" in namespace "gc-7978"
  Jul  8 13:13:15.317: INFO: Deleting pod "simpletest.rc-jzsvd" in namespace "gc-7978"
  Jul  8 13:13:15.328: INFO: Deleting pod "simpletest.rc-k7tlb" in namespace "gc-7978"
  Jul  8 13:13:15.340: INFO: Deleting pod "simpletest.rc-kd6mv" in namespace "gc-7978"
  Jul  8 13:13:15.352: INFO: Deleting pod "simpletest.rc-kmqlv" in namespace "gc-7978"
  Jul  8 13:13:15.364: INFO: Deleting pod "simpletest.rc-krq92" in namespace "gc-7978"
  Jul  8 13:13:15.378: INFO: Deleting pod "simpletest.rc-kslhk" in namespace "gc-7978"
  Jul  8 13:13:15.391: INFO: Deleting pod "simpletest.rc-l2zfk" in namespace "gc-7978"
  Jul  8 13:13:15.407: INFO: Deleting pod "simpletest.rc-l7hr7" in namespace "gc-7978"
  Jul  8 13:13:15.421: INFO: Deleting pod "simpletest.rc-l9m6d" in namespace "gc-7978"
  Jul  8 13:13:15.462: INFO: Deleting pod "simpletest.rc-lbt7f" in namespace "gc-7978"
  Jul  8 13:13:15.514: INFO: Deleting pod "simpletest.rc-lm4hb" in namespace "gc-7978"
  Jul  8 13:13:15.563: INFO: Deleting pod "simpletest.rc-lsz2r" in namespace "gc-7978"
  Jul  8 13:13:15.615: INFO: Deleting pod "simpletest.rc-lzpg6" in namespace "gc-7978"
  Jul  8 13:13:15.668: INFO: Deleting pod "simpletest.rc-m4ccx" in namespace "gc-7978"
  Jul  8 13:13:15.716: INFO: Deleting pod "simpletest.rc-m78lv" in namespace "gc-7978"
  Jul  8 13:13:15.769: INFO: Deleting pod "simpletest.rc-m7v99" in namespace "gc-7978"
  Jul  8 13:13:15.816: INFO: Deleting pod "simpletest.rc-mfcwt" in namespace "gc-7978"
  Jul  8 13:13:15.873: INFO: Deleting pod "simpletest.rc-mk6tp" in namespace "gc-7978"
  Jul  8 13:13:15.915: INFO: Deleting pod "simpletest.rc-mm997" in namespace "gc-7978"
  E0708 13:13:15.938401      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:13:15.968: INFO: Deleting pod "simpletest.rc-n6w28" in namespace "gc-7978"
  Jul  8 13:13:16.019: INFO: Deleting pod "simpletest.rc-ndq4n" in namespace "gc-7978"
  Jul  8 13:13:16.071: INFO: Deleting pod "simpletest.rc-p8rf2" in namespace "gc-7978"
  Jul  8 13:13:16.120: INFO: Deleting pod "simpletest.rc-psh8k" in namespace "gc-7978"
  Jul  8 13:13:16.166: INFO: Deleting pod "simpletest.rc-pt5pj" in namespace "gc-7978"
  Jul  8 13:13:16.215: INFO: Deleting pod "simpletest.rc-q22z6" in namespace "gc-7978"
  Jul  8 13:13:16.265: INFO: Deleting pod "simpletest.rc-qm9m2" in namespace "gc-7978"
  Jul  8 13:13:16.315: INFO: Deleting pod "simpletest.rc-r4rl7" in namespace "gc-7978"
  Jul  8 13:13:16.362: INFO: Deleting pod "simpletest.rc-r86sn" in namespace "gc-7978"
  Jul  8 13:13:16.413: INFO: Deleting pod "simpletest.rc-rjrwb" in namespace "gc-7978"
  Jul  8 13:13:16.467: INFO: Deleting pod "simpletest.rc-rq5bw" in namespace "gc-7978"
  Jul  8 13:13:16.517: INFO: Deleting pod "simpletest.rc-sd7nl" in namespace "gc-7978"
  Jul  8 13:13:16.565: INFO: Deleting pod "simpletest.rc-v2kxs" in namespace "gc-7978"
  Jul  8 13:13:16.615: INFO: Deleting pod "simpletest.rc-vvxfq" in namespace "gc-7978"
  Jul  8 13:13:16.666: INFO: Deleting pod "simpletest.rc-w88bn" in namespace "gc-7978"
  Jul  8 13:13:16.716: INFO: Deleting pod "simpletest.rc-wbg6z" in namespace "gc-7978"
  Jul  8 13:13:16.766: INFO: Deleting pod "simpletest.rc-x5j7m" in namespace "gc-7978"
  Jul  8 13:13:16.816: INFO: Deleting pod "simpletest.rc-xjlgj" in namespace "gc-7978"
  E0708 13:13:16.939021      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:13:16.972: INFO: Deleting pod "simpletest.rc-xr9xl" in namespace "gc-7978"
  Jul  8 13:13:16.994: INFO: Deleting pod "simpletest.rc-xxgcd" in namespace "gc-7978"
  Jul  8 13:13:17.010: INFO: Deleting pod "simpletest.rc-zkl5k" in namespace "gc-7978"
  Jul  8 13:13:17.023: INFO: Deleting pod "simpletest.rc-zpsdk" in namespace "gc-7978"
  Jul  8 13:13:17.069: INFO: Deleting pod "simpletest.rc-zqfwd" in namespace "gc-7978"
  Jul  8 13:13:17.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7978" for this suite. @ 07/08/23 13:13:17.156
• [43.769 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 07/08/23 13:13:17.209
  Jul  8 13:13:17.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename sched-preemption @ 07/08/23 13:13:17.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:13:17.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:13:17.233
  Jul  8 13:13:17.251: INFO: Waiting up to 1m0s for all nodes to be ready
  E0708 13:13:17.939834      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:18.940113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:19.940404      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:20.940651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:21.941069      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:22.941920      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:23.942013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:24.942192      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:25.942282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:26.942341      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:27.942441      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:28.942614      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:29.943664      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:30.943750      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:31.944055      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:32.945115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:33.945200      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:34.945368      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:35.945411      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:36.945673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:37.945750      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:38.945911      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:39.945994      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:40.946152      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:41.946248      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:42.947231      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:43.947321      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:44.947376      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:45.947477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:46.947699      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:47.948049      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:48.948232      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:49.948354      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:50.948440      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:51.948532      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:52.949508      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:53.949594      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:54.949674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:55.949766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:56.950823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:57.950923      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:58.951028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:13:59.951865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:00.952049      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:01.952661      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:02.953128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:03.953207      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:04.953315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:05.953427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:06.953522      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:07.954483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:08.954563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:09.954666      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:10.954744      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:11.954849      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:12.954951      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:13.955037      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:14.955108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:15.955656      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:16.955746      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:14:17.270: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 07/08/23 13:14:17.274
  Jul  8 13:14:17.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename sched-preemption-path @ 07/08/23 13:14:17.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:14:17.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:14:17.295
  STEP: Finding an available node @ 07/08/23 13:14:17.298
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/08/23 13:14:17.298
  E0708 13:14:17.956531      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:18.956618      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/08/23 13:14:19.319
  Jul  8 13:14:19.334: INFO: found a healthy node: ip-172-31-93-234
  E0708 13:14:19.957105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:20.957806      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:21.957904      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:22.958766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:23.959228      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:24.959241      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:14:25.408: INFO: pods created so far: [1 1 1]
  Jul  8 13:14:25.408: INFO: length of pods created so far: 3
  E0708 13:14:25.960186      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:26.960974      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:14:27.420: INFO: pods created so far: [2 2 1]
  E0708 13:14:27.962009      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:28.962099      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:29.962209      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:30.963110      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:31.963414      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:32.963664      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:33.963758      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:14:34.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  8 13:14:34.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-7160" for this suite. @ 07/08/23 13:14:34.511
  STEP: Destroying namespace "sched-preemption-6190" for this suite. @ 07/08/23 13:14:34.524
• [77.322 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 07/08/23 13:14:34.531
  Jul  8 13:14:34.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubelet-test @ 07/08/23 13:14:34.532
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:14:34.548
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:14:34.552
  E0708 13:14:34.964846      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:35.964916      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:36.965667      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:37.965720      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:14:38.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-818" for this suite. @ 07/08/23 13:14:38.581
• [4.057 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 07/08/23 13:14:38.588
  Jul  8 13:14:38.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename services @ 07/08/23 13:14:38.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:14:38.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:14:38.61
  STEP: creating service in namespace services-5969 @ 07/08/23 13:14:38.613
  STEP: creating service affinity-clusterip in namespace services-5969 @ 07/08/23 13:14:38.614
  STEP: creating replication controller affinity-clusterip in namespace services-5969 @ 07/08/23 13:14:38.623
  I0708 13:14:38.631048      20 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-5969, replica count: 3
  E0708 13:14:38.966600      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:39.967262      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:40.967383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0708 13:14:41.682350      20 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  8 13:14:41.690: INFO: Creating new exec pod
  E0708 13:14:41.967426      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:42.967516      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:43.968540      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:14:44.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-5969 exec execpod-affinity968q7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Jul  8 13:14:44.805: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\n+ echo hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Jul  8 13:14:44.805: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 13:14:44.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-5969 exec execpod-affinity968q7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.126 80'
  Jul  8 13:14:44.897: INFO: stderr: "+ nc -v -t -w 2 10.152.183.126 80\nConnection to 10.152.183.126 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jul  8 13:14:44.897: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 13:14:44.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-5969 exec execpod-affinity968q7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.126:80/ ; done'
  E0708 13:14:44.969172      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:14:45.031: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.126:80/\n"
  Jul  8 13:14:45.031: INFO: stdout: "\naffinity-clusterip-zjw6j\naffinity-clusterip-zjw6j\naffinity-clusterip-zjw6j\naffinity-clusterip-zjw6j\naffinity-clusterip-zjw6j\naffinity-clusterip-zjw6j\naffinity-clusterip-zjw6j\naffinity-clusterip-zjw6j\naffinity-clusterip-zjw6j\naffinity-clusterip-zjw6j\naffinity-clusterip-zjw6j\naffinity-clusterip-zjw6j\naffinity-clusterip-zjw6j\naffinity-clusterip-zjw6j\naffinity-clusterip-zjw6j\naffinity-clusterip-zjw6j"
  Jul  8 13:14:45.031: INFO: Received response from host: affinity-clusterip-zjw6j
  Jul  8 13:14:45.031: INFO: Received response from host: affinity-clusterip-zjw6j
  Jul  8 13:14:45.031: INFO: Received response from host: affinity-clusterip-zjw6j
  Jul  8 13:14:45.031: INFO: Received response from host: affinity-clusterip-zjw6j
  Jul  8 13:14:45.031: INFO: Received response from host: affinity-clusterip-zjw6j
  Jul  8 13:14:45.031: INFO: Received response from host: affinity-clusterip-zjw6j
  Jul  8 13:14:45.031: INFO: Received response from host: affinity-clusterip-zjw6j
  Jul  8 13:14:45.031: INFO: Received response from host: affinity-clusterip-zjw6j
  Jul  8 13:14:45.031: INFO: Received response from host: affinity-clusterip-zjw6j
  Jul  8 13:14:45.031: INFO: Received response from host: affinity-clusterip-zjw6j
  Jul  8 13:14:45.032: INFO: Received response from host: affinity-clusterip-zjw6j
  Jul  8 13:14:45.032: INFO: Received response from host: affinity-clusterip-zjw6j
  Jul  8 13:14:45.032: INFO: Received response from host: affinity-clusterip-zjw6j
  Jul  8 13:14:45.032: INFO: Received response from host: affinity-clusterip-zjw6j
  Jul  8 13:14:45.032: INFO: Received response from host: affinity-clusterip-zjw6j
  Jul  8 13:14:45.032: INFO: Received response from host: affinity-clusterip-zjw6j
  Jul  8 13:14:45.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  8 13:14:45.036: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-5969, will wait for the garbage collector to delete the pods @ 07/08/23 13:14:45.05
  Jul  8 13:14:45.111: INFO: Deleting ReplicationController affinity-clusterip took: 6.809087ms
  Jul  8 13:14:45.211: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.492954ms
  E0708 13:14:45.970136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:46.971170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-5969" for this suite. @ 07/08/23 13:14:47.531
• [8.950 seconds]
------------------------------
S
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 07/08/23 13:14:47.538
  Jul  8 13:14:47.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename services @ 07/08/23 13:14:47.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:14:47.556
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:14:47.559
  STEP: creating service in namespace services-3574 @ 07/08/23 13:14:47.563
  STEP: creating service affinity-nodeport in namespace services-3574 @ 07/08/23 13:14:47.563
  STEP: creating replication controller affinity-nodeport in namespace services-3574 @ 07/08/23 13:14:47.579
  I0708 13:14:47.585688      20 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-3574, replica count: 3
  E0708 13:14:47.972190      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:48.972975      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:49.973074      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0708 13:14:50.636916      20 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  8 13:14:50.648: INFO: Creating new exec pod
  E0708 13:14:50.973162      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:51.973603      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:52.973684      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:14:53.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-3574 exec execpod-affinityx7d2z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Jul  8 13:14:53.767: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Jul  8 13:14:53.767: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 13:14:53.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-3574 exec execpod-affinityx7d2z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.251 80'
  Jul  8 13:14:53.861: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.251 80\nConnection to 10.152.183.251 80 port [tcp/http] succeeded!\n"
  Jul  8 13:14:53.861: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 13:14:53.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-3574 exec execpod-affinityx7d2z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.12.67 31377'
  Jul  8 13:14:53.962: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.12.67 31377\nConnection to 172.31.12.67 31377 port [tcp/*] succeeded!\n"
  Jul  8 13:14:53.962: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 13:14:53.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-3574 exec execpod-affinityx7d2z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.93.234 31377'
  E0708 13:14:53.973864      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:14:54.062: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.93.234 31377\nConnection to 172.31.93.234 31377 port [tcp/*] succeeded!\n"
  Jul  8 13:14:54.062: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 13:14:54.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-3574 exec execpod-affinityx7d2z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.12.67:31377/ ; done'
  Jul  8 13:14:54.245: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31377/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31377/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31377/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31377/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31377/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31377/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31377/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31377/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31377/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31377/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31377/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31377/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31377/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31377/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31377/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.12.67:31377/\n"
  Jul  8 13:14:54.245: INFO: stdout: "\naffinity-nodeport-jcvbs\naffinity-nodeport-jcvbs\naffinity-nodeport-jcvbs\naffinity-nodeport-jcvbs\naffinity-nodeport-jcvbs\naffinity-nodeport-jcvbs\naffinity-nodeport-jcvbs\naffinity-nodeport-jcvbs\naffinity-nodeport-jcvbs\naffinity-nodeport-jcvbs\naffinity-nodeport-jcvbs\naffinity-nodeport-jcvbs\naffinity-nodeport-jcvbs\naffinity-nodeport-jcvbs\naffinity-nodeport-jcvbs\naffinity-nodeport-jcvbs"
  Jul  8 13:14:54.245: INFO: Received response from host: affinity-nodeport-jcvbs
  Jul  8 13:14:54.245: INFO: Received response from host: affinity-nodeport-jcvbs
  Jul  8 13:14:54.245: INFO: Received response from host: affinity-nodeport-jcvbs
  Jul  8 13:14:54.245: INFO: Received response from host: affinity-nodeport-jcvbs
  Jul  8 13:14:54.245: INFO: Received response from host: affinity-nodeport-jcvbs
  Jul  8 13:14:54.245: INFO: Received response from host: affinity-nodeport-jcvbs
  Jul  8 13:14:54.245: INFO: Received response from host: affinity-nodeport-jcvbs
  Jul  8 13:14:54.245: INFO: Received response from host: affinity-nodeport-jcvbs
  Jul  8 13:14:54.245: INFO: Received response from host: affinity-nodeport-jcvbs
  Jul  8 13:14:54.245: INFO: Received response from host: affinity-nodeport-jcvbs
  Jul  8 13:14:54.245: INFO: Received response from host: affinity-nodeport-jcvbs
  Jul  8 13:14:54.245: INFO: Received response from host: affinity-nodeport-jcvbs
  Jul  8 13:14:54.245: INFO: Received response from host: affinity-nodeport-jcvbs
  Jul  8 13:14:54.245: INFO: Received response from host: affinity-nodeport-jcvbs
  Jul  8 13:14:54.245: INFO: Received response from host: affinity-nodeport-jcvbs
  Jul  8 13:14:54.245: INFO: Received response from host: affinity-nodeport-jcvbs
  Jul  8 13:14:54.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  8 13:14:54.250: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-3574, will wait for the garbage collector to delete the pods @ 07/08/23 13:14:54.26
  Jul  8 13:14:54.324: INFO: Deleting ReplicationController affinity-nodeport took: 7.141856ms
  Jul  8 13:14:54.424: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.717149ms
  E0708 13:14:54.974115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:55.974171      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-3574" for this suite. @ 07/08/23 13:14:56.548
• [9.017 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 07/08/23 13:14:56.556
  Jul  8 13:14:56.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename resourcequota @ 07/08/23 13:14:56.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:14:56.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:14:56.575
  STEP: Counting existing ResourceQuota @ 07/08/23 13:14:56.578
  E0708 13:14:56.974752      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:57.974862      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:58.975412      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:14:59.976408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:00.976728      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/08/23 13:15:01.583
  STEP: Ensuring resource quota status is calculated @ 07/08/23 13:15:01.588
  E0708 13:15:01.977737      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:02.977841      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 07/08/23 13:15:03.592
  STEP: Ensuring resource quota status captures replication controller creation @ 07/08/23 13:15:03.604
  E0708 13:15:03.977947      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:04.978026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 07/08/23 13:15:05.609
  STEP: Ensuring resource quota status released usage @ 07/08/23 13:15:05.617
  E0708 13:15:05.978918      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:06.979016      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:15:07.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4781" for this suite. @ 07/08/23 13:15:07.626
• [11.077 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 07/08/23 13:15:07.633
  Jul  8 13:15:07.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename watch @ 07/08/23 13:15:07.633
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:15:07.754
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:15:07.758
  STEP: creating a watch on configmaps @ 07/08/23 13:15:07.765
  STEP: creating a new configmap @ 07/08/23 13:15:07.766
  STEP: modifying the configmap once @ 07/08/23 13:15:07.771
  STEP: closing the watch once it receives two notifications @ 07/08/23 13:15:07.779
  Jul  8 13:15:07.779: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5337  89ee4cba-c7c6-4795-b516-b229ab8c3651 32499 0 2023-07-08 13:15:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-08 13:15:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 13:15:07.779: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5337  89ee4cba-c7c6-4795-b516-b229ab8c3651 32500 0 2023-07-08 13:15:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-08 13:15:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 07/08/23 13:15:07.779
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 07/08/23 13:15:07.788
  STEP: deleting the configmap @ 07/08/23 13:15:07.789
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 07/08/23 13:15:07.796
  Jul  8 13:15:07.796: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5337  89ee4cba-c7c6-4795-b516-b229ab8c3651 32501 0 2023-07-08 13:15:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-08 13:15:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 13:15:07.796: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5337  89ee4cba-c7c6-4795-b516-b229ab8c3651 32502 0 2023-07-08 13:15:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-08 13:15:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 13:15:07.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5337" for this suite. @ 07/08/23 13:15:07.8
• [0.174 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 07/08/23 13:15:07.807
  Jul  8 13:15:07.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename runtimeclass @ 07/08/23 13:15:07.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:15:07.829
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:15:07.832
  STEP: Deleting RuntimeClass runtimeclass-8150-delete-me @ 07/08/23 13:15:07.84
  STEP: Waiting for the RuntimeClass to disappear @ 07/08/23 13:15:07.846
  Jul  8 13:15:07.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8150" for this suite. @ 07/08/23 13:15:07.861
• [0.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 07/08/23 13:15:07.868
  Jul  8 13:15:07.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename svcaccounts @ 07/08/23 13:15:07.869
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:15:07.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:15:07.887
  STEP: creating a ServiceAccount @ 07/08/23 13:15:07.891
  STEP: watching for the ServiceAccount to be added @ 07/08/23 13:15:07.899
  STEP: patching the ServiceAccount @ 07/08/23 13:15:07.904
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 07/08/23 13:15:07.91
  STEP: deleting the ServiceAccount @ 07/08/23 13:15:07.914
  Jul  8 13:15:07.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8383" for this suite. @ 07/08/23 13:15:07.933
• [0.071 seconds]
------------------------------
SS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 07/08/23 13:15:07.939
  Jul  8 13:15:07.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename disruption @ 07/08/23 13:15:07.94
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:15:07.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:15:07.961
  STEP: creating the pdb @ 07/08/23 13:15:07.964
  STEP: Waiting for the pdb to be processed @ 07/08/23 13:15:07.97
  E0708 13:15:07.979644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:08.979795      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 07/08/23 13:15:09.979
  E0708 13:15:09.980397      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 07/08/23 13:15:09.989
  STEP: patching the pdb @ 07/08/23 13:15:09.993
  STEP: Waiting for the pdb to be processed @ 07/08/23 13:15:10.004
  STEP: Waiting for the pdb to be deleted @ 07/08/23 13:15:10.015
  Jul  8 13:15:10.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9165" for this suite. @ 07/08/23 13:15:10.023
• [2.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 07/08/23 13:15:10.033
  Jul  8 13:15:10.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-runtime @ 07/08/23 13:15:10.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:15:10.05
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:15:10.054
  STEP: create the container @ 07/08/23 13:15:10.057
  W0708 13:15:10.066866      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/08/23 13:15:10.067
  E0708 13:15:10.981110      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:11.981527      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:12.981637      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/08/23 13:15:13.084
  STEP: the container should be terminated @ 07/08/23 13:15:13.088
  STEP: the termination message should be set @ 07/08/23 13:15:13.088
  Jul  8 13:15:13.088: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 07/08/23 13:15:13.088
  Jul  8 13:15:13.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6563" for this suite. @ 07/08/23 13:15:13.115
• [3.088 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 07/08/23 13:15:13.121
  Jul  8 13:15:13.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename lease-test @ 07/08/23 13:15:13.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:15:13.139
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:15:13.142
  Jul  8 13:15:13.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-8456" for this suite. @ 07/08/23 13:15:13.217
• [0.102 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 07/08/23 13:15:13.224
  Jul  8 13:15:13.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename downward-api @ 07/08/23 13:15:13.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:15:13.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:15:13.243
  STEP: Creating a pod to test downward api env vars @ 07/08/23 13:15:13.246
  E0708 13:15:13.981711      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:14.981803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:15.982821      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:16.982892      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:15:17.268
  Jul  8 13:15:17.272: INFO: Trying to get logs from node ip-172-31-93-234 pod downward-api-530c67c9-2aee-4b48-88c3-c4ef61f657c2 container dapi-container: <nil>
  STEP: delete the pod @ 07/08/23 13:15:17.29
  Jul  8 13:15:17.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4239" for this suite. @ 07/08/23 13:15:17.308
• [4.092 seconds]
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 07/08/23 13:15:17.316
  Jul  8 13:15:17.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename configmap @ 07/08/23 13:15:17.316
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:15:17.33
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:15:17.334
  STEP: Creating configMap that has name configmap-test-emptyKey-a2db50f1-2384-4539-a068-d5604005dba4 @ 07/08/23 13:15:17.337
  Jul  8 13:15:17.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7074" for this suite. @ 07/08/23 13:15:17.343
• [0.034 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 07/08/23 13:15:17.35
  Jul  8 13:15:17.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename field-validation @ 07/08/23 13:15:17.351
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:15:17.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:15:17.371
  STEP: apply creating a deployment @ 07/08/23 13:15:17.374
  Jul  8 13:15:17.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3331" for this suite. @ 07/08/23 13:15:17.394
• [0.050 seconds]
------------------------------
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 07/08/23 13:15:17.4
  Jul  8 13:15:17.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename pods @ 07/08/23 13:15:17.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:15:17.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:15:17.42
  STEP: creating the pod @ 07/08/23 13:15:17.423
  STEP: submitting the pod to kubernetes @ 07/08/23 13:15:17.423
  STEP: verifying QOS class is set on the pod @ 07/08/23 13:15:17.432
  Jul  8 13:15:17.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1831" for this suite. @ 07/08/23 13:15:17.44
• [0.047 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 07/08/23 13:15:17.448
  Jul  8 13:15:17.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename disruption @ 07/08/23 13:15:17.448
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:15:17.465
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:15:17.468
  STEP: Creating a kubernetes client @ 07/08/23 13:15:17.472
  Jul  8 13:15:17.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename disruption-2 @ 07/08/23 13:15:17.472
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:15:17.49
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:15:17.493
  STEP: Waiting for the pdb to be processed @ 07/08/23 13:15:17.501
  E0708 13:15:17.983236      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:18.983451      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 07/08/23 13:15:19.515
  E0708 13:15:19.983697      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:20.983853      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 07/08/23 13:15:21.531
  E0708 13:15:21.984054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:22.984128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 07/08/23 13:15:23.541
  STEP: listing a collection of PDBs in namespace disruption-6316 @ 07/08/23 13:15:23.544
  STEP: deleting a collection of PDBs @ 07/08/23 13:15:23.548
  STEP: Waiting for the PDB collection to be deleted @ 07/08/23 13:15:23.562
  Jul  8 13:15:23.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  8 13:15:23.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-2139" for this suite. @ 07/08/23 13:15:23.573
  STEP: Destroying namespace "disruption-6316" for this suite. @ 07/08/23 13:15:23.58
• [6.139 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 07/08/23 13:15:23.587
  Jul  8 13:15:23.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename downward-api @ 07/08/23 13:15:23.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:15:23.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:15:23.609
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 13:15:23.613
  E0708 13:15:23.985115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:24.985210      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:25.985305      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:26.985595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:15:27.635
  Jul  8 13:15:27.639: INFO: Trying to get logs from node ip-172-31-29-188 pod downwardapi-volume-2169e576-10ee-41b9-99d0-5c9f554c6f03 container client-container: <nil>
  STEP: delete the pod @ 07/08/23 13:15:27.653
  Jul  8 13:15:27.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9192" for this suite. @ 07/08/23 13:15:27.675
• [4.096 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 07/08/23 13:15:27.683
  Jul  8 13:15:27.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename services @ 07/08/23 13:15:27.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:15:27.7
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:15:27.704
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-1424 @ 07/08/23 13:15:27.707
  STEP: changing the ExternalName service to type=ClusterIP @ 07/08/23 13:15:27.713
  STEP: creating replication controller externalname-service in namespace services-1424 @ 07/08/23 13:15:27.728
  I0708 13:15:27.737432      20 runners.go:194] Created replication controller with name: externalname-service, namespace: services-1424, replica count: 2
  E0708 13:15:27.986209      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:28.986524      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:29.987023      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0708 13:15:30.787887      20 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  8 13:15:30.787: INFO: Creating new exec pod
  E0708 13:15:30.988044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:31.988069      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:32.988540      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:15:33.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-1424 exec execpod8tbm8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul  8 13:15:33.907: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul  8 13:15:33.907: INFO: stdout: "externalname-service-87sf5"
  Jul  8 13:15:33.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-1424 exec execpod8tbm8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.130 80'
  E0708 13:15:33.989199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:15:34.008: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.130 80\nConnection to 10.152.183.130 80 port [tcp/http] succeeded!\n"
  Jul  8 13:15:34.008: INFO: stdout: "externalname-service-9v45w"
  Jul  8 13:15:34.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  8 13:15:34.013: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-1424" for this suite. @ 07/08/23 13:15:34.03
• [6.355 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 07/08/23 13:15:34.038
  Jul  8 13:15:34.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename taint-multiple-pods @ 07/08/23 13:15:34.039
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:15:34.055
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:15:34.058
  Jul  8 13:15:34.061: INFO: Waiting up to 1m0s for all nodes to be ready
  E0708 13:15:34.989308      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:35.990002      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:36.990378      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:37.990572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:38.990658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:39.990844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:40.991326      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:41.992309      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:42.993228      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:43.993314      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:44.993421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:45.994152      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:46.994228      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:47.995267      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:48.995360      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:49.995542      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:50.995634      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:51.995981      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:52.996058      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:53.997093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:54.997224      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:55.997269      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:56.997952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:57.998146      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:58.998315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:15:59.999306      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:00.999376      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:02.000208      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:03.000292      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:04.001120      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:05.001762      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:06.001962      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:07.002024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:08.002925      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:09.003015      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:10.003189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:11.003380      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:12.003713      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:13.003851      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:14.003995      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:15.004072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:16.005102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:17.005717      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:18.006042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:19.006136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:20.006364      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:21.006465      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:22.007017      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:23.007100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:24.008101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:25.008209      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:26.008311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:27.009104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:28.009198      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:29.009282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:30.009506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:31.009798      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:32.010224      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:33.010310      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:34.010391      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:16:34.077: INFO: Waiting for terminating namespaces to be deleted...
  Jul  8 13:16:34.082: INFO: Starting informer...
  STEP: Starting pods... @ 07/08/23 13:16:34.082
  Jul  8 13:16:34.300: INFO: Pod1 is running on ip-172-31-29-188. Tainting Node
  E0708 13:16:35.010426      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:36.010505      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:16:36.523: INFO: Pod2 is running on ip-172-31-29-188. Tainting Node
  STEP: Trying to apply a taint on the Node @ 07/08/23 13:16:36.523
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/08/23 13:16:36.533
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 07/08/23 13:16:36.538
  E0708 13:16:37.010692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:38.010761      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:39.010860      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:40.011639      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:41.011801      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:42.012812      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:16:42.203: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0708 13:16:43.012931      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:44.013016      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:45.013194      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:46.013273      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:47.014295      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:48.014428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:49.014592      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:50.015455      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:51.016386      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:52.016456      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:53.017083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:54.017413      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:55.017511      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:56.017660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:57.018554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:58.018657      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:16:59.018748      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:00.019194      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:01.019279      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:02.020008      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:17:02.230: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Jul  8 13:17:02.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/08/23 13:17:02.245
  STEP: Destroying namespace "taint-multiple-pods-7538" for this suite. @ 07/08/23 13:17:02.248
• [88.219 seconds]
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 07/08/23 13:17:02.258
  Jul  8 13:17:02.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename pod-network-test @ 07/08/23 13:17:02.258
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:17:02.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:17:02.284
  STEP: Performing setup for networking test in namespace pod-network-test-3844 @ 07/08/23 13:17:02.287
  STEP: creating a selector @ 07/08/23 13:17:02.287
  STEP: Creating the service pods in kubernetes @ 07/08/23 13:17:02.287
  Jul  8 13:17:02.287: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0708 13:17:03.020089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:04.021109      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:05.021639      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:06.022171      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:07.022609      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:08.022885      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:09.022965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:10.023051      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:11.023142      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:12.023242      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:13.024037      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:14.024129      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:15.025107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:16.025769      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:17.025878      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:18.026498      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:19.026737      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:20.026989      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:21.027087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:22.027459      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:23.027561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:24.027692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 07/08/23 13:17:24.403
  E0708 13:17:25.028574      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:26.028832      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:17:26.433: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul  8 13:17:26.433: INFO: Going to poll 192.168.59.47 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul  8 13:17:26.437: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.59.47:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3844 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:17:26.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:17:26.437: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:17:26.437: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3844/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.59.47%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul  8 13:17:26.505: INFO: Found all 1 expected endpoints: [netserver-0]
  Jul  8 13:17:26.505: INFO: Going to poll 192.168.164.116 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul  8 13:17:26.509: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.164.116:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3844 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:17:26.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:17:26.509: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:17:26.509: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3844/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.164.116%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul  8 13:17:26.571: INFO: Found all 1 expected endpoints: [netserver-1]
  Jul  8 13:17:26.571: INFO: Going to poll 192.168.7.214 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul  8 13:17:26.575: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.7.214:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3844 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:17:26.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:17:26.575: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:17:26.576: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3844/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.7.214%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul  8 13:17:26.627: INFO: Found all 1 expected endpoints: [netserver-2]
  Jul  8 13:17:26.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3844" for this suite. @ 07/08/23 13:17:26.632
• [24.381 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 07/08/23 13:17:26.64
  Jul  8 13:17:26.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename resourcequota @ 07/08/23 13:17:26.64
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:17:26.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:17:26.661
  STEP: Discovering how many secrets are in namespace by default @ 07/08/23 13:17:26.665
  E0708 13:17:27.029340      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:28.029452      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:29.030135      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:30.030203      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:31.030689      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 07/08/23 13:17:31.669
  E0708 13:17:32.031401      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:33.031844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:34.032436      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:35.032974      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:36.033047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/08/23 13:17:36.675
  STEP: Ensuring resource quota status is calculated @ 07/08/23 13:17:36.68
  E0708 13:17:37.033180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:38.033294      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 07/08/23 13:17:38.685
  STEP: Ensuring resource quota status captures secret creation @ 07/08/23 13:17:38.696
  E0708 13:17:39.034048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:40.034150      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 07/08/23 13:17:40.701
  STEP: Ensuring resource quota status released usage @ 07/08/23 13:17:40.709
  E0708 13:17:41.034775      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:42.035167      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:17:42.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9757" for this suite. @ 07/08/23 13:17:42.718
• [16.086 seconds]
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 07/08/23 13:17:42.725
  Jul  8 13:17:42.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename dns @ 07/08/23 13:17:42.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:17:42.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:17:42.747
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 07/08/23 13:17:42.75
  Jul  8 13:17:42.759: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5052  cd9aa2e7-edd8-4603-bcfa-3111a8a4b857 33426 0 2023-07-08 13:17:42 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-07-08 13:17:42 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gzrhh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gzrhh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0708 13:17:43.035374      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:44.035557      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 07/08/23 13:17:44.768
  Jul  8 13:17:44.768: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5052 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:17:44.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:17:44.769: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:17:44.769: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-5052/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 07/08/23 13:17:44.83
  Jul  8 13:17:44.830: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5052 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:17:44.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:17:44.831: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:17:44.831: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-5052/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul  8 13:17:44.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  8 13:17:44.903: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-5052" for this suite. @ 07/08/23 13:17:44.914
• [2.196 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 07/08/23 13:17:44.923
  Jul  8 13:17:44.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename secrets @ 07/08/23 13:17:44.924
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:17:44.989
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:17:44.993
  STEP: Creating secret with name secret-test-314bebda-774f-4307-a70e-c5cb5126a93f @ 07/08/23 13:17:44.996
  STEP: Creating a pod to test consume secrets @ 07/08/23 13:17:45.001
  E0708 13:17:45.035986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:46.037117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:47.038091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:48.038990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:17:49.022
  Jul  8 13:17:49.026: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-secrets-f3346145-210f-48cc-8e2b-c047674055aa container secret-volume-test: <nil>
  E0708 13:17:49.039252      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 07/08/23 13:17:49.04
  Jul  8 13:17:49.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5631" for this suite. @ 07/08/23 13:17:49.061
• [4.145 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 07/08/23 13:17:49.068
  Jul  8 13:17:49.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename statefulset @ 07/08/23 13:17:49.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:17:49.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:17:49.088
  STEP: Creating service test in namespace statefulset-4252 @ 07/08/23 13:17:49.092
  STEP: Creating statefulset ss in namespace statefulset-4252 @ 07/08/23 13:17:49.096
  Jul  8 13:17:49.107: INFO: Found 0 stateful pods, waiting for 1
  E0708 13:17:50.039379      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:51.039482      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:52.039618      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:53.039785      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:54.039889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:55.040026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:56.041112      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:57.042045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:58.042132      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:17:59.043170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:17:59.112: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 07/08/23 13:17:59.12
  STEP: updating a scale subresource @ 07/08/23 13:17:59.124
  STEP: verifying the statefulset Spec.Replicas was modified @ 07/08/23 13:17:59.131
  STEP: Patch a scale subresource @ 07/08/23 13:17:59.135
  STEP: verifying the statefulset Spec.Replicas was modified @ 07/08/23 13:17:59.145
  Jul  8 13:17:59.149: INFO: Deleting all statefulset in ns statefulset-4252
  Jul  8 13:17:59.153: INFO: Scaling statefulset ss to 0
  E0708 13:18:00.043275      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:01.043456      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:02.044068      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:03.044151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:04.044307      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:05.045088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:06.045200      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:07.045424      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:08.045514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:09.046377      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:09.175: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  8 13:18:09.179: INFO: Deleting statefulset ss
  Jul  8 13:18:09.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4252" for this suite. @ 07/08/23 13:18:09.197
• [20.136 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 07/08/23 13:18:09.205
  Jul  8 13:18:09.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename statefulset @ 07/08/23 13:18:09.206
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:18:09.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:18:09.223
  STEP: Creating service test in namespace statefulset-7419 @ 07/08/23 13:18:09.227
  STEP: Creating stateful set ss in namespace statefulset-7419 @ 07/08/23 13:18:09.232
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7419 @ 07/08/23 13:18:09.239
  Jul  8 13:18:09.242: INFO: Found 0 stateful pods, waiting for 1
  E0708 13:18:10.046476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:11.046578      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:12.046987      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:13.047085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:14.047611      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:15.047970      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:16.048040      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:17.048268      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:18.049088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:19.049180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:19.248: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 07/08/23 13:18:19.248
  Jul  8 13:18:19.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-7419 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  8 13:18:19.355: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  8 13:18:19.356: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  8 13:18:19.356: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  8 13:18:19.360: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0708 13:18:20.049775      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:21.049941      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:22.050331      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:23.050418      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:24.050836      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:25.050919      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:26.050995      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:27.051274      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:28.051348      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:29.052211      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:29.365: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul  8 13:18:29.365: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  8 13:18:29.381: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Jul  8 13:18:29.381: INFO: ss-0  ip-172-31-93-234  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:09 +0000 UTC  }]
  Jul  8 13:18:29.381: INFO: 
  Jul  8 13:18:29.381: INFO: StatefulSet ss has not reached scale 3, at 1
  E0708 13:18:30.052312      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:30.386: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995563908s
  E0708 13:18:31.052373      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:31.391: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991360344s
  E0708 13:18:32.052885      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:32.395: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98695924s
  E0708 13:18:33.053457      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:33.400: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982376372s
  E0708 13:18:34.054449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:34.405: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977385858s
  E0708 13:18:35.055030      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:35.409: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972808876s
  E0708 13:18:36.055621      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:36.414: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968179899s
  E0708 13:18:37.056520      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:37.420: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.962586191s
  E0708 13:18:38.056692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:38.425: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.26901ms
  E0708 13:18:39.057209      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7419 @ 07/08/23 13:18:39.425
  Jul  8 13:18:39.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-7419 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  8 13:18:39.533: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul  8 13:18:39.533: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  8 13:18:39.533: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul  8 13:18:39.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-7419 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  8 13:18:39.647: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jul  8 13:18:39.647: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  8 13:18:39.647: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul  8 13:18:39.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-7419 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  8 13:18:39.753: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jul  8 13:18:39.753: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  8 13:18:39.753: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul  8 13:18:39.758: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul  8 13:18:39.758: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul  8 13:18:39.758: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 07/08/23 13:18:39.758
  Jul  8 13:18:39.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-7419 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  8 13:18:39.860: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  8 13:18:39.860: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  8 13:18:39.860: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  8 13:18:39.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-7419 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  8 13:18:39.970: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  8 13:18:39.970: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  8 13:18:39.970: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  8 13:18:39.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-7419 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0708 13:18:40.057223      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:40.072: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  8 13:18:40.072: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  8 13:18:40.072: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  8 13:18:40.072: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  8 13:18:40.076: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0708 13:18:41.057329      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:42.057702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:43.057794      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:44.057966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:45.058715      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:46.059362      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:47.059449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:48.059545      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:49.060593      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:18:50.060682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:50.085: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul  8 13:18:50.085: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jul  8 13:18:50.085: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jul  8 13:18:50.100: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Jul  8 13:18:50.100: INFO: ss-0  ip-172-31-93-234  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:09 +0000 UTC  }]
  Jul  8 13:18:50.100: INFO: ss-1  ip-172-31-29-188  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:29 +0000 UTC  }]
  Jul  8 13:18:50.100: INFO: ss-2  ip-172-31-12-67   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:29 +0000 UTC  }]
  Jul  8 13:18:50.100: INFO: 
  Jul  8 13:18:50.100: INFO: StatefulSet ss has not reached scale 0, at 3
  E0708 13:18:51.060788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:51.105: INFO: POD   NODE              PHASE      GRACE  CONDITIONS
  Jul  8 13:18:51.105: INFO: ss-1  ip-172-31-29-188  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:29 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:40 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:40 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:18:29 +0000 UTC  }]
  Jul  8 13:18:51.105: INFO: 
  Jul  8 13:18:51.105: INFO: StatefulSet ss has not reached scale 0, at 1
  E0708 13:18:52.061166      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:52.109: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.991398151s
  E0708 13:18:53.061258      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:53.113: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.987316147s
  E0708 13:18:54.061515      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:54.118: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.983057703s
  E0708 13:18:55.061606      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:55.122: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.977902361s
  E0708 13:18:56.061693      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:56.127: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.973797637s
  E0708 13:18:57.061772      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:57.132: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.96876227s
  E0708 13:18:58.061872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:58.137: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.963613338s
  E0708 13:18:59.061964      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:18:59.142: INFO: Verifying statefulset ss doesn't scale past 0 for another 958.514874ms
  E0708 13:19:00.062032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7419 @ 07/08/23 13:19:00.143
  Jul  8 13:19:00.148: INFO: Scaling statefulset ss to 0
  Jul  8 13:19:00.163: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  8 13:19:00.167: INFO: Deleting all statefulset in ns statefulset-7419
  Jul  8 13:19:00.171: INFO: Scaling statefulset ss to 0
  Jul  8 13:19:00.186: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  8 13:19:00.190: INFO: Deleting statefulset ss
  Jul  8 13:19:00.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7419" for this suite. @ 07/08/23 13:19:00.212
• [51.015 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 07/08/23 13:19:00.22
  Jul  8 13:19:00.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename namespaces @ 07/08/23 13:19:00.221
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:19:00.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:19:00.245
  STEP: Creating a test namespace @ 07/08/23 13:19:00.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:19:00.27
  STEP: Creating a pod in the namespace @ 07/08/23 13:19:00.274
  STEP: Waiting for the pod to have running status @ 07/08/23 13:19:00.284
  E0708 13:19:01.062142      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:02.062533      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 07/08/23 13:19:02.294
  STEP: Waiting for the namespace to be removed. @ 07/08/23 13:19:02.304
  E0708 13:19:03.063461      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:04.063555      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:05.063621      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:06.063713      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:07.063809      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:08.064044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:09.064233      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:10.064306      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:11.064399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:12.064936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:13.065022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 07/08/23 13:19:13.309
  STEP: Verifying there are no pods in the namespace @ 07/08/23 13:19:13.326
  Jul  8 13:19:13.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7716" for this suite. @ 07/08/23 13:19:13.335
  STEP: Destroying namespace "nsdeletetest-5102" for this suite. @ 07/08/23 13:19:13.342
  Jul  8 13:19:13.346: INFO: Namespace nsdeletetest-5102 was already deleted
  STEP: Destroying namespace "nsdeletetest-9647" for this suite. @ 07/08/23 13:19:13.346
• [13.132 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 07/08/23 13:19:13.353
  Jul  8 13:19:13.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename resourcequota @ 07/08/23 13:19:13.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:19:13.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:19:13.374
  STEP: Counting existing ResourceQuota @ 07/08/23 13:19:13.379
  E0708 13:19:14.065195      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:15.065297      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:16.065400      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:17.065485      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:18.065578      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/08/23 13:19:18.384
  STEP: Ensuring resource quota status is calculated @ 07/08/23 13:19:18.389
  E0708 13:19:19.066402      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:20.066609      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 07/08/23 13:19:20.394
  STEP: Ensuring resource quota status captures replicaset creation @ 07/08/23 13:19:20.408
  E0708 13:19:21.067436      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:22.067685      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 07/08/23 13:19:22.412
  STEP: Ensuring resource quota status released usage @ 07/08/23 13:19:22.419
  E0708 13:19:23.068285      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:24.068383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:19:24.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7115" for this suite. @ 07/08/23 13:19:24.429
• [11.083 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 07/08/23 13:19:24.436
  Jul  8 13:19:24.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename services @ 07/08/23 13:19:24.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:19:24.456
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:19:24.46
  STEP: creating an Endpoint @ 07/08/23 13:19:24.467
  STEP: waiting for available Endpoint @ 07/08/23 13:19:24.472
  STEP: listing all Endpoints @ 07/08/23 13:19:24.473
  STEP: updating the Endpoint @ 07/08/23 13:19:24.477
  STEP: fetching the Endpoint @ 07/08/23 13:19:24.483
  STEP: patching the Endpoint @ 07/08/23 13:19:24.487
  STEP: fetching the Endpoint @ 07/08/23 13:19:24.495
  STEP: deleting the Endpoint by Collection @ 07/08/23 13:19:24.498
  STEP: waiting for Endpoint deletion @ 07/08/23 13:19:24.509
  STEP: fetching the Endpoint @ 07/08/23 13:19:24.51
  Jul  8 13:19:24.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8704" for this suite. @ 07/08/23 13:19:24.518
• [0.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 07/08/23 13:19:24.526
  Jul  8 13:19:24.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename dns @ 07/08/23 13:19:24.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:19:24.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:19:24.544
  STEP: Creating a test headless service @ 07/08/23 13:19:24.548
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1117 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1117;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1117 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1117;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1117.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1117.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1117.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1117.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1117.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1117.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1117.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1117.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1117.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1117.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1117.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1117.svc;check="$$(dig +notcp +noall +answer +search 229.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.229_udp@PTR;check="$$(dig +tcp +noall +answer +search 229.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.229_tcp@PTR;sleep 1; done
   @ 07/08/23 13:19:24.567
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1117 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1117;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1117 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1117;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1117.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1117.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1117.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1117.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1117.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1117.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1117.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1117.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1117.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1117.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1117.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1117.svc;check="$$(dig +notcp +noall +answer +search 229.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.229_udp@PTR;check="$$(dig +tcp +noall +answer +search 229.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.229_tcp@PTR;sleep 1; done
   @ 07/08/23 13:19:24.567
  STEP: creating a pod to probe DNS @ 07/08/23 13:19:24.567
  STEP: submitting the pod to kubernetes @ 07/08/23 13:19:24.567
  E0708 13:19:25.068458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:26.069101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/08/23 13:19:26.59
  STEP: looking for the results for each expected name from probers @ 07/08/23 13:19:26.594
  Jul  8 13:19:26.599: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a: the server could not find the requested resource (get pods dns-test-711baf88-a83b-4c46-b02c-967275f3af8a)
  Jul  8 13:19:26.604: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a: the server could not find the requested resource (get pods dns-test-711baf88-a83b-4c46-b02c-967275f3af8a)
  Jul  8 13:19:26.608: INFO: Unable to read wheezy_udp@dns-test-service.dns-1117 from pod dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a: the server could not find the requested resource (get pods dns-test-711baf88-a83b-4c46-b02c-967275f3af8a)
  Jul  8 13:19:26.612: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1117 from pod dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a: the server could not find the requested resource (get pods dns-test-711baf88-a83b-4c46-b02c-967275f3af8a)
  Jul  8 13:19:26.617: INFO: Unable to read wheezy_udp@dns-test-service.dns-1117.svc from pod dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a: the server could not find the requested resource (get pods dns-test-711baf88-a83b-4c46-b02c-967275f3af8a)
  Jul  8 13:19:26.621: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1117.svc from pod dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a: the server could not find the requested resource (get pods dns-test-711baf88-a83b-4c46-b02c-967275f3af8a)
  Jul  8 13:19:26.625: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1117.svc from pod dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a: the server could not find the requested resource (get pods dns-test-711baf88-a83b-4c46-b02c-967275f3af8a)
  Jul  8 13:19:26.629: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1117.svc from pod dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a: the server could not find the requested resource (get pods dns-test-711baf88-a83b-4c46-b02c-967275f3af8a)
  Jul  8 13:19:26.650: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a: the server could not find the requested resource (get pods dns-test-711baf88-a83b-4c46-b02c-967275f3af8a)
  Jul  8 13:19:26.655: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a: the server could not find the requested resource (get pods dns-test-711baf88-a83b-4c46-b02c-967275f3af8a)
  Jul  8 13:19:26.659: INFO: Unable to read jessie_udp@dns-test-service.dns-1117 from pod dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a: the server could not find the requested resource (get pods dns-test-711baf88-a83b-4c46-b02c-967275f3af8a)
  Jul  8 13:19:26.663: INFO: Unable to read jessie_tcp@dns-test-service.dns-1117 from pod dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a: the server could not find the requested resource (get pods dns-test-711baf88-a83b-4c46-b02c-967275f3af8a)
  Jul  8 13:19:26.667: INFO: Unable to read jessie_udp@dns-test-service.dns-1117.svc from pod dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a: the server could not find the requested resource (get pods dns-test-711baf88-a83b-4c46-b02c-967275f3af8a)
  Jul  8 13:19:26.672: INFO: Unable to read jessie_tcp@dns-test-service.dns-1117.svc from pod dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a: the server could not find the requested resource (get pods dns-test-711baf88-a83b-4c46-b02c-967275f3af8a)
  Jul  8 13:19:26.676: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1117.svc from pod dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a: the server could not find the requested resource (get pods dns-test-711baf88-a83b-4c46-b02c-967275f3af8a)
  Jul  8 13:19:26.680: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1117.svc from pod dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a: the server could not find the requested resource (get pods dns-test-711baf88-a83b-4c46-b02c-967275f3af8a)
  Jul  8 13:19:26.697: INFO: Lookups using dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1117 wheezy_tcp@dns-test-service.dns-1117 wheezy_udp@dns-test-service.dns-1117.svc wheezy_tcp@dns-test-service.dns-1117.svc wheezy_udp@_http._tcp.dns-test-service.dns-1117.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1117.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1117 jessie_tcp@dns-test-service.dns-1117 jessie_udp@dns-test-service.dns-1117.svc jessie_tcp@dns-test-service.dns-1117.svc jessie_udp@_http._tcp.dns-test-service.dns-1117.svc jessie_tcp@_http._tcp.dns-test-service.dns-1117.svc]

  E0708 13:19:27.070131      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:28.070240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:29.070320      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:30.070495      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:31.070688      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:19:31.800: INFO: DNS probes using dns-1117/dns-test-711baf88-a83b-4c46-b02c-967275f3af8a succeeded

  Jul  8 13:19:31.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/08/23 13:19:31.804
  STEP: deleting the test service @ 07/08/23 13:19:31.824
  STEP: deleting the test headless service @ 07/08/23 13:19:31.846
  STEP: Destroying namespace "dns-1117" for this suite. @ 07/08/23 13:19:31.857
• [7.339 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 07/08/23 13:19:31.866
  Jul  8 13:19:31.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/08/23 13:19:31.866
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:19:31.888
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:19:31.891
  Jul  8 13:19:31.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 13:19:32.070880      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:33.071178      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:34.072247      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:35.073277      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:36.073861      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:37.074842      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:38.075462      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:19:38.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2575" for this suite. @ 07/08/23 13:19:38.115
• [6.258 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 07/08/23 13:19:38.125
  Jul  8 13:19:38.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename sched-preemption @ 07/08/23 13:19:38.126
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:19:38.144
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:19:38.147
  Jul  8 13:19:38.163: INFO: Waiting up to 1m0s for all nodes to be ready
  E0708 13:19:39.076054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:40.077093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:41.077192      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:42.077658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:43.077904      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:44.078836      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:45.078916      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:46.079695      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:47.080039      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:48.081104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:49.081209      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:50.081293      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:51.082245      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:52.082578      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:53.082666      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:54.082848      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:55.082945      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:56.083115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:57.083548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:58.083652      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:19:59.083743      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:00.083959      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:01.084053      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:02.085106      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:03.085206      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:04.085396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:05.085491      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:06.085683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:07.085809      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:08.085859      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:09.085960      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:10.086162      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:11.086257      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:12.086659      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:13.086909      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:14.087668      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:15.088079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:16.089086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:17.089426      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:18.089563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:19.089663      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:20.089834      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:21.090217      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:22.090993      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:23.091085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:24.091313      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:25.091896      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:26.092031      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:27.092115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:28.093086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:29.093343      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:30.093428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:31.093519      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:32.093898      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:33.093983      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:34.094354      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:35.094647      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:36.094822      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:37.095491      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:38.095639      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:20:38.183: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 07/08/23 13:20:38.187
  Jul  8 13:20:38.206: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jul  8 13:20:38.214: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jul  8 13:20:38.232: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jul  8 13:20:38.240: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jul  8 13:20:38.259: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jul  8 13:20:38.265: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 07/08/23 13:20:38.265
  E0708 13:20:39.095742      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:40.096408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 07/08/23 13:20:40.303
  E0708 13:20:41.097092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:42.097657      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:43.098121      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:44.098767      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:20:44.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-2566" for this suite. @ 07/08/23 13:20:44.387
• [66.270 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 07/08/23 13:20:44.396
  Jul  8 13:20:44.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-probe @ 07/08/23 13:20:44.396
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:20:44.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:20:44.42
  STEP: Creating pod busybox-007be27c-8afd-49ba-99f5-1d1927637740 in namespace container-probe-6845 @ 07/08/23 13:20:44.423
  E0708 13:20:45.098852      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:46.099779      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:20:46.441: INFO: Started pod busybox-007be27c-8afd-49ba-99f5-1d1927637740 in namespace container-probe-6845
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/08/23 13:20:46.441
  Jul  8 13:20:46.445: INFO: Initial restart count of pod busybox-007be27c-8afd-49ba-99f5-1d1927637740 is 0
  E0708 13:20:47.100199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:48.100306      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:49.100762      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:50.101419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:51.101501      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:52.101902      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:53.101998      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:54.102103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:55.102930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:56.103540      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:57.104460      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:58.105307      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:20:59.105874      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:00.105979      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:01.106631      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:02.107003      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:03.107084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:04.107952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:05.108094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:06.109100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:07.109210      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:08.109402      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:09.109493      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:10.109677      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:11.109707      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:12.110775      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:13.110874      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:14.110960      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:15.111050      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:16.111148      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:17.112037      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:18.113116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:19.113200      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:20.113387      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:21.113471      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:22.113791      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:23.113881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:24.114590      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:25.115587      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:26.115712      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:27.116525      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:28.117092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:29.117726      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:30.117836      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:31.117990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:32.118061      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:33.118185      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:34.118243      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:35.119104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:36.119285      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:37.120108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:38.121094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:39.121163      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:40.121329      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:41.121596      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:42.121673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:43.121768      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:44.121865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:45.121948      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:46.122490      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:47.122555      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:48.122653      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:49.122738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:50.122910      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:51.123780      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:52.124033      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:53.125109      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:54.125290      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:55.125386      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:56.125550      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:57.126475      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:58.127311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:21:59.127409      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:00.127574      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:01.128309      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:02.129112      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:03.129197      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:04.129361      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:05.129461      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:06.129656      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:07.129726      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:08.129908      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:09.129990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:10.130084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:11.130172      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:12.130592      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:13.130944      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:14.130876      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:15.130813      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:16.130960      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:17.131541      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:18.131694      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:19.132245      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:20.133087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:21.133117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:22.133872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:23.134635      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:24.135415      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:25.135504      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:26.135655      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:27.136667      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:28.137089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:29.137720      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:30.138179      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:31.138921      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:32.139014      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:33.139168      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:34.139258      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:35.140012      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:36.140109      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:37.140724      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:38.140808      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:39.141323      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:40.141473      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:41.141813      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:42.141931      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:43.142182      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:44.142281      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:45.143090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:46.143188      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:47.143648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:48.143800      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:49.144024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:50.145088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:51.145594      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:52.145688      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:53.146586      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:54.146679      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:55.147217      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:56.147977      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:57.149019      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:58.149169      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:22:59.149467      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:00.149559      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:01.149692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:02.150090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:03.150470      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:04.150566      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:05.151332      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:06.151460      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:07.151947      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:08.152029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:09.152993      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:10.153223      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:11.153814      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:12.154301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:13.154458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:14.155055      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:15.156071      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:16.157084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:17.157165      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:18.158066      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:19.158349      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:20.158427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:21.158865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:22.158960      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:23.159055      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:24.159219      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:25.159504      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:26.159599      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:27.160321      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:28.161094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:29.161173      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:30.161330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:31.161991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:32.162087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:33.162811      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:34.162981      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:35.163064      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:36.163211      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:37.163458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:38.163628      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:39.163791      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:40.163967      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:41.164349      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:42.164438      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:43.165024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:44.165481      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:45.165941      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:46.166633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:47.167561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:48.168235      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:49.168966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:50.169061      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:51.169821      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:52.170120      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:53.170874      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:54.171035      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:55.171563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:56.171654      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:57.172593      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:58.173088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:23:59.173166      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:00.173877      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:01.174731      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:02.174838      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:03.175765      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:04.175844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:05.176439      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:06.177288      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:07.178196      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:08.178851      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:09.178984      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:10.179749      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:11.180226      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:12.180324      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:13.180548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:14.180587      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:15.181086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:16.182021      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:17.182045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:18.182191      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:19.182314      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:20.182461      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:21.183002      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:22.183146      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:23.183348      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:24.183507      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:25.183599      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:26.183899      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:27.184793      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:28.185090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:29.185995      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:30.186082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:31.186660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:32.186752      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:33.186831      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:34.186931      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:35.187093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:36.187289      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:37.188104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:38.188205      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:39.188332      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:40.189096      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:41.189471      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:42.189569      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:43.190112      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:44.190264      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:45.191219      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:46.192084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:24:47.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/08/23 13:24:47.034
  STEP: Destroying namespace "container-probe-6845" for this suite. @ 07/08/23 13:24:47.049
• [242.660 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 07/08/23 13:24:47.057
  Jul  8 13:24:47.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename services @ 07/08/23 13:24:47.058
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:24:47.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:24:47.078
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3747 @ 07/08/23 13:24:47.081
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 07/08/23 13:24:47.092
  STEP: creating service externalsvc in namespace services-3747 @ 07/08/23 13:24:47.092
  STEP: creating replication controller externalsvc in namespace services-3747 @ 07/08/23 13:24:47.107
  I0708 13:24:47.115945      20 runners.go:194] Created replication controller with name: externalsvc, namespace: services-3747, replica count: 2
  E0708 13:24:47.192730      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:48.193277      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:49.194010      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0708 13:24:50.166656      20 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 07/08/23 13:24:50.171
  Jul  8 13:24:50.185: INFO: Creating new exec pod
  E0708 13:24:50.194823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:51.195616      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:52.195889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:24:52.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-3747 exec execpodww8l5 -- /bin/sh -x -c nslookup clusterip-service.services-3747.svc.cluster.local'
  Jul  8 13:24:52.321: INFO: stderr: "+ nslookup clusterip-service.services-3747.svc.cluster.local\n"
  Jul  8 13:24:52.321: INFO: stdout: "Server:\t\t10.152.183.124\nAddress:\t10.152.183.124#53\n\nclusterip-service.services-3747.svc.cluster.local\tcanonical name = externalsvc.services-3747.svc.cluster.local.\nName:\texternalsvc.services-3747.svc.cluster.local\nAddress: 10.152.183.136\n\n"
  Jul  8 13:24:52.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-3747, will wait for the garbage collector to delete the pods @ 07/08/23 13:24:52.325
  Jul  8 13:24:52.387: INFO: Deleting ReplicationController externalsvc took: 7.566487ms
  Jul  8 13:24:52.488: INFO: Terminating ReplicationController externalsvc pods took: 100.938691ms
  E0708 13:24:53.196763      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:54.197075      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:24:54.306: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-3747" for this suite. @ 07/08/23 13:24:54.318
• [7.268 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 07/08/23 13:24:54.326
  Jul  8 13:24:54.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename downward-api @ 07/08/23 13:24:54.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:24:54.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:24:54.345
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 13:24:54.348
  E0708 13:24:55.197734      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:56.198302      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:57.198570      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:24:58.198641      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:24:58.371
  Jul  8 13:24:58.375: INFO: Trying to get logs from node ip-172-31-93-234 pod downwardapi-volume-df0cf16a-5e6e-4383-a5a5-5ae004476e54 container client-container: <nil>
  STEP: delete the pod @ 07/08/23 13:24:58.393
  Jul  8 13:24:58.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7875" for this suite. @ 07/08/23 13:24:58.415
• [4.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 07/08/23 13:24:58.424
  Jul  8 13:24:58.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename secrets @ 07/08/23 13:24:58.424
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:24:58.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:24:58.445
  STEP: creating a secret @ 07/08/23 13:24:58.448
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 07/08/23 13:24:58.453
  STEP: patching the secret @ 07/08/23 13:24:58.457
  STEP: deleting the secret using a LabelSelector @ 07/08/23 13:24:58.466
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 07/08/23 13:24:58.474
  Jul  8 13:24:58.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2041" for this suite. @ 07/08/23 13:24:58.481
• [0.064 seconds]
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 07/08/23 13:24:58.488
  Jul  8 13:24:58.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename configmap @ 07/08/23 13:24:58.488
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:24:58.503
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:24:58.506
  STEP: creating a ConfigMap @ 07/08/23 13:24:58.51
  STEP: fetching the ConfigMap @ 07/08/23 13:24:58.515
  STEP: patching the ConfigMap @ 07/08/23 13:24:58.518
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 07/08/23 13:24:58.524
  STEP: deleting the ConfigMap by collection with a label selector @ 07/08/23 13:24:58.528
  STEP: listing all ConfigMaps in test namespace @ 07/08/23 13:24:58.536
  Jul  8 13:24:58.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3252" for this suite. @ 07/08/23 13:24:58.544
• [0.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 07/08/23 13:24:58.554
  Jul  8 13:24:58.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-probe @ 07/08/23 13:24:58.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:24:58.573
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:24:58.576
  E0708 13:24:59.199466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:00.199572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:01.200002      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:02.200091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:03.200170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:04.200259      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:05.200542      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:06.200901      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:07.201011      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:08.201783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:09.201868      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:10.201966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:11.202236      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:12.202330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:13.202417      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:14.203113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:15.203191      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:16.203697      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:17.204093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:18.204187      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:19.204982      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:20.205067      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:21.205739      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:22.205843      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:23.205926      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:24.206190      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:25.206274      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:26.206904      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:27.207006      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:28.207094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:29.207672      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:30.208577      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:31.208961      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:32.209043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:33.209652      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:34.210644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:35.210728      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:36.211050      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:37.211153      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:38.212029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:39.212797      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:40.213084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:41.214016      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:42.214859      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:43.215561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:44.215646      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:45.216240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:46.216683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:47.216783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:48.216803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:49.216848      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:50.217870      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:51.218427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:52.219235      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:53.219275      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:54.219722      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:55.220272      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:56.220612      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:57.221061      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:25:58.221654      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:25:58.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-7270" for this suite. @ 07/08/23 13:25:58.597
• [60.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 07/08/23 13:25:58.606
  Jul  8 13:25:58.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename configmap @ 07/08/23 13:25:58.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:25:58.623
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:25:58.627
  STEP: Creating configMap with name configmap-test-volume-143f9803-c3ae-4c02-92cd-78151c5a8a23 @ 07/08/23 13:25:58.63
  STEP: Creating a pod to test consume configMaps @ 07/08/23 13:25:58.636
  E0708 13:25:59.223839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:00.224011      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:01.224291      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:02.224355      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:26:02.661
  Jul  8 13:26:02.665: INFO: Trying to get logs from node ip-172-31-29-188 pod pod-configmaps-3c68a9cc-9ca2-4a6b-afe7-e68862a14948 container agnhost-container: <nil>
  STEP: delete the pod @ 07/08/23 13:26:02.683
  Jul  8 13:26:02.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3349" for this suite. @ 07/08/23 13:26:02.706
• [4.108 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 07/08/23 13:26:02.714
  Jul  8 13:26:02.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename configmap @ 07/08/23 13:26:02.715
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:26:02.734
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:26:02.738
  STEP: Creating configMap configmap-1319/configmap-test-3b910971-0520-40e6-afed-382d907b2c7b @ 07/08/23 13:26:02.741
  STEP: Creating a pod to test consume configMaps @ 07/08/23 13:26:02.746
  E0708 13:26:03.225117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:04.225197      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:05.225291      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:06.225554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:26:06.768
  Jul  8 13:26:06.771: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-configmaps-fbb8d4be-8ecf-4336-80e7-fcca3aea859a container env-test: <nil>
  STEP: delete the pod @ 07/08/23 13:26:06.78
  Jul  8 13:26:06.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1319" for this suite. @ 07/08/23 13:26:06.801
• [4.094 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 07/08/23 13:26:06.808
  Jul  8 13:26:06.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename services @ 07/08/23 13:26:06.809
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:26:06.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:26:06.828
  STEP: creating service nodeport-test with type=NodePort in namespace services-2785 @ 07/08/23 13:26:06.832
  STEP: creating replication controller nodeport-test in namespace services-2785 @ 07/08/23 13:26:06.848
  I0708 13:26:06.857337      20 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-2785, replica count: 2
  E0708 13:26:07.225664      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:08.226398      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:09.226504      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0708 13:26:09.909037      20 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  8 13:26:09.909: INFO: Creating new exec pod
  E0708 13:26:10.227351      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:11.227538      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:12.228494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:26:12.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-2785 exec execpodnr66c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jul  8 13:26:13.031: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jul  8 13:26:13.031: INFO: stdout: "nodeport-test-8wqpg"
  Jul  8 13:26:13.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-2785 exec execpodnr66c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.78 80'
  Jul  8 13:26:13.138: INFO: stderr: "+ nc -v -t -w 2 10.152.183.78 80\nConnection to 10.152.183.78 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jul  8 13:26:13.138: INFO: stdout: ""
  E0708 13:26:13.228996      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:26:14.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-2785 exec execpodnr66c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.78 80'
  E0708 13:26:14.228969      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:26:14.261: INFO: stderr: "+ nc -v -t -w 2 10.152.183.78 80\n+ echo hostName\nConnection to 10.152.183.78 80 port [tcp/http] succeeded!\n"
  Jul  8 13:26:14.261: INFO: stdout: ""
  Jul  8 13:26:15.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-2785 exec execpodnr66c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.78 80'
  E0708 13:26:15.229813      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:26:15.247: INFO: stderr: "+ nc -v -t -w 2 10.152.183.78 80\n+ echo hostName\nConnection to 10.152.183.78 80 port [tcp/http] succeeded!\n"
  Jul  8 13:26:15.247: INFO: stdout: "nodeport-test-8wqpg"
  Jul  8 13:26:15.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-2785 exec execpodnr66c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.93.234 31020'
  Jul  8 13:26:15.349: INFO: stderr: "+ nc -v -t -w 2 172.31.93.234 31020\nConnection to 172.31.93.234 31020 port [tcp/*] succeeded!\n+ echo hostName\n"
  Jul  8 13:26:15.349: INFO: stdout: "nodeport-test-lf5hk"
  Jul  8 13:26:15.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-2785 exec execpodnr66c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.12.67 31020'
  Jul  8 13:26:15.463: INFO: stderr: "+ nc -v -t -w 2 172.31.12.67 31020\n+ echo hostName\nConnection to 172.31.12.67 31020 port [tcp/*] succeeded!\n"
  Jul  8 13:26:15.463: INFO: stdout: "nodeport-test-lf5hk"
  Jul  8 13:26:15.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2785" for this suite. @ 07/08/23 13:26:15.468
• [8.667 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 07/08/23 13:26:15.476
  Jul  8 13:26:15.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl @ 07/08/23 13:26:15.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:26:15.492
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:26:15.496
  STEP: validating cluster-info @ 07/08/23 13:26:15.499
  Jul  8 13:26:15.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-1893 cluster-info'
  Jul  8 13:26:15.545: INFO: stderr: ""
  Jul  8 13:26:15.545: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Jul  8 13:26:15.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1893" for this suite. @ 07/08/23 13:26:15.549
• [0.080 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 07/08/23 13:26:15.556
  Jul  8 13:26:15.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename init-container @ 07/08/23 13:26:15.557
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:26:15.571
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:26:15.575
  STEP: creating the pod @ 07/08/23 13:26:15.581
  Jul  8 13:26:15.581: INFO: PodSpec: initContainers in spec.initContainers
  E0708 13:26:16.230294      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:17.230730      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:18.230754      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:26:18.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-4637" for this suite. @ 07/08/23 13:26:18.353
• [2.804 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 07/08/23 13:26:18.36
  Jul  8 13:26:18.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename configmap @ 07/08/23 13:26:18.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:26:18.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:26:18.379
  STEP: Creating configMap with name configmap-test-volume-9f109f60-0af8-4a23-939b-d272733ea46a @ 07/08/23 13:26:18.382
  STEP: Creating a pod to test consume configMaps @ 07/08/23 13:26:18.387
  E0708 13:26:19.231684      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:20.232369      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:21.233076      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:22.233233      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:26:22.41
  Jul  8 13:26:22.414: INFO: Trying to get logs from node ip-172-31-29-188 pod pod-configmaps-a835c66e-e273-4c8e-b826-f02d237461ef container agnhost-container: <nil>
  STEP: delete the pod @ 07/08/23 13:26:22.422
  Jul  8 13:26:22.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6970" for this suite. @ 07/08/23 13:26:22.443
• [4.090 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 07/08/23 13:26:22.45
  Jul  8 13:26:22.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename pods @ 07/08/23 13:26:22.451
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:26:22.468
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:26:22.471
  STEP: creating the pod @ 07/08/23 13:26:22.475
  STEP: submitting the pod to kubernetes @ 07/08/23 13:26:22.475
  W0708 13:26:22.485151      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0708 13:26:23.233608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:24.233790      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 07/08/23 13:26:24.497
  STEP: updating the pod @ 07/08/23 13:26:24.5
  Jul  8 13:26:25.013: INFO: Successfully updated pod "pod-update-activedeadlineseconds-35813c2d-346f-4e59-8e2b-e45c0d2c4bdd"
  E0708 13:26:25.233866      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:26.234160      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:27.234548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:28.234804      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:26:29.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1203" for this suite. @ 07/08/23 13:26:29.03
• [6.586 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 07/08/23 13:26:29.037
  Jul  8 13:26:29.037: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl @ 07/08/23 13:26:29.037
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:26:29.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:26:29.056
  STEP: starting the proxy server @ 07/08/23 13:26:29.059
  Jul  8 13:26:29.059: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6685 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 07/08/23 13:26:29.092
  Jul  8 13:26:29.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6685" for this suite. @ 07/08/23 13:26:29.104
• [0.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 07/08/23 13:26:29.112
  Jul  8 13:26:29.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 13:26:29.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:26:29.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:26:29.132
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 13:26:29.136
  E0708 13:26:29.235595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:30.235722      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:31.236657      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:32.237056      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:26:33.16
  Jul  8 13:26:33.163: INFO: Trying to get logs from node ip-172-31-93-234 pod downwardapi-volume-cd0099e0-f25b-4e93-9ace-c3379765f4db container client-container: <nil>
  STEP: delete the pod @ 07/08/23 13:26:33.171
  Jul  8 13:26:33.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6404" for this suite. @ 07/08/23 13:26:33.192
• [4.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 07/08/23 13:26:33.2
  Jul  8 13:26:33.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename disruption @ 07/08/23 13:26:33.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:26:33.216
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:26:33.22
  STEP: Waiting for the pdb to be processed @ 07/08/23 13:26:33.229
  E0708 13:26:33.237446      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:34.237627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:35.238578      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating PodDisruptionBudget status @ 07/08/23 13:26:35.239
  STEP: Waiting for all pods to be running @ 07/08/23 13:26:35.247
  Jul  8 13:26:35.254: INFO: running pods: 0 < 1
  E0708 13:26:36.239266      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:37.239751      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 07/08/23 13:26:37.259
  STEP: Waiting for the pdb to be processed @ 07/08/23 13:26:37.273
  STEP: Patching PodDisruptionBudget status @ 07/08/23 13:26:37.283
  STEP: Waiting for the pdb to be processed @ 07/08/23 13:26:37.292
  Jul  8 13:26:37.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4338" for this suite. @ 07/08/23 13:26:37.301
• [4.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 07/08/23 13:26:37.31
  Jul  8 13:26:37.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename pod-network-test @ 07/08/23 13:26:37.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:26:37.327
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:26:37.33
  STEP: Performing setup for networking test in namespace pod-network-test-1464 @ 07/08/23 13:26:37.334
  STEP: creating a selector @ 07/08/23 13:26:37.334
  STEP: Creating the service pods in kubernetes @ 07/08/23 13:26:37.334
  Jul  8 13:26:37.334: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0708 13:26:38.240729      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:39.240887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:40.240981      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:41.241256      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:42.241529      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:43.241609      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:44.241700      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:45.241896      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:46.242263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:47.243216      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:48.243308      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:49.243383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 07/08/23 13:26:49.415
  E0708 13:26:50.243479      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:51.244392      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:26:51.433: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul  8 13:26:51.433: INFO: Breadth first check of 192.168.59.52 on host 172.31.12.67...
  Jul  8 13:26:51.437: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.7.204:9080/dial?request=hostname&protocol=http&host=192.168.59.52&port=8083&tries=1'] Namespace:pod-network-test-1464 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:26:51.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:26:51.437: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:26:51.438: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1464/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.7.204%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.59.52%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul  8 13:26:51.487: INFO: Waiting for responses: map[]
  Jul  8 13:26:51.487: INFO: reached 192.168.59.52 after 0/1 tries
  Jul  8 13:26:51.487: INFO: Breadth first check of 192.168.164.126 on host 172.31.29.188...
  Jul  8 13:26:51.491: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.7.204:9080/dial?request=hostname&protocol=http&host=192.168.164.126&port=8083&tries=1'] Namespace:pod-network-test-1464 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:26:51.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:26:51.492: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:26:51.492: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1464/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.7.204%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.164.126%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul  8 13:26:51.539: INFO: Waiting for responses: map[]
  Jul  8 13:26:51.539: INFO: reached 192.168.164.126 after 0/1 tries
  Jul  8 13:26:51.539: INFO: Breadth first check of 192.168.7.192 on host 172.31.93.234...
  Jul  8 13:26:51.543: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.7.204:9080/dial?request=hostname&protocol=http&host=192.168.7.192&port=8083&tries=1'] Namespace:pod-network-test-1464 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:26:51.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:26:51.543: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:26:51.543: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1464/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.7.204%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.7.192%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul  8 13:26:51.610: INFO: Waiting for responses: map[]
  Jul  8 13:26:51.610: INFO: reached 192.168.7.192 after 0/1 tries
  Jul  8 13:26:51.610: INFO: Going to retry 0 out of 3 pods....
  Jul  8 13:26:51.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1464" for this suite. @ 07/08/23 13:26:51.615
• [14.313 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 07/08/23 13:26:51.623
  Jul  8 13:26:51.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename deployment @ 07/08/23 13:26:51.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:26:51.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:26:51.647
  Jul  8 13:26:51.651: INFO: Creating deployment "test-recreate-deployment"
  Jul  8 13:26:51.656: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Jul  8 13:26:51.665: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  E0708 13:26:52.244927      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:53.245019      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:26:53.674: INFO: Waiting deployment "test-recreate-deployment" to complete
  Jul  8 13:26:53.677: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Jul  8 13:26:53.688: INFO: Updating deployment test-recreate-deployment
  Jul  8 13:26:53.688: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Jul  8 13:26:53.761: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-914  6fc0a452-64b6-453e-a102-dcdea2274807 36070 2 2023-07-08 13:26:51 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-08 13:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 13:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046a5a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-08 13:26:53 +0000 UTC,LastTransitionTime:2023-07-08 13:26:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-07-08 13:26:53 +0000 UTC,LastTransitionTime:2023-07-08 13:26:51 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Jul  8 13:26:53.765: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-914  b3fb8b25-3668-421b-8bed-f77f3a3c4445 36068 1 2023-07-08 13:26:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 6fc0a452-64b6-453e-a102-dcdea2274807 0xc0051bf487 0xc0051bf488}] [] [{kube-controller-manager Update apps/v1 2023-07-08 13:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fc0a452-64b6-453e-a102-dcdea2274807\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 13:26:53 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051bf528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul  8 13:26:53.765: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Jul  8 13:26:53.765: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-914  254e3160-c0d0-4322-9396-62bec29bc8d4 36058 2 2023-07-08 13:26:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 6fc0a452-64b6-453e-a102-dcdea2274807 0xc0051bf597 0xc0051bf598}] [] [{kube-controller-manager Update apps/v1 2023-07-08 13:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fc0a452-64b6-453e-a102-dcdea2274807\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 13:26:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051bf648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul  8 13:26:53.769: INFO: Pod "test-recreate-deployment-54757ffd6c-4xzp8" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-4xzp8 test-recreate-deployment-54757ffd6c- deployment-914  6153ffc3-6946-46e3-894a-151854ba6708 36069 0 2023-07-08 13:26:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c b3fb8b25-3668-421b-8bed-f77f3a3c4445 0xc0046a5dc7 0xc0046a5dc8}] [] [{kube-controller-manager Update v1 2023-07-08 13:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fb8b25-3668-421b-8bed-f77f3a3c4445\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:26:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vdsbd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vdsbd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-93-234,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:26:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:26:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:26:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:26:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.93.234,PodIP:,StartTime:2023-07-08 13:26:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:26:53.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-914" for this suite. @ 07/08/23 13:26:53.773
• [2.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 07/08/23 13:26:53.783
  Jul  8 13:26:53.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-runtime @ 07/08/23 13:26:53.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:26:53.798
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:26:53.802
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 07/08/23 13:26:53.813
  E0708 13:26:54.246078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:55.247080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:56.247853      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:57.248852      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:58.249103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:26:59.249875      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:00.249970      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:01.250607      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:02.250687      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:03.251617      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:04.252477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:05.253096      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:06.253331      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:07.254397      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:08.255023      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:09.255116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:10.255193      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:11.256119      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:12.256883      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 07/08/23 13:27:12.912
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 07/08/23 13:27:12.916
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 07/08/23 13:27:12.923
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 07/08/23 13:27:12.923
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 07/08/23 13:27:12.95
  E0708 13:27:13.257105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:14.257849      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:15.258189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 07/08/23 13:27:15.971
  E0708 13:27:16.258880      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 07/08/23 13:27:16.979
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 07/08/23 13:27:16.987
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 07/08/23 13:27:16.987
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 07/08/23 13:27:17.007
  E0708 13:27:17.259112      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 07/08/23 13:27:18.019
  E0708 13:27:18.259561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:19.259643      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 07/08/23 13:27:20.033
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 07/08/23 13:27:20.04
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 07/08/23 13:27:20.04
  Jul  8 13:27:20.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3764" for this suite. @ 07/08/23 13:27:20.073
• [26.296 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 07/08/23 13:27:20.081
  Jul  8 13:27:20.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename watch @ 07/08/23 13:27:20.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:27:20.098
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:27:20.101
  STEP: creating a new configmap @ 07/08/23 13:27:20.105
  STEP: modifying the configmap once @ 07/08/23 13:27:20.111
  STEP: modifying the configmap a second time @ 07/08/23 13:27:20.119
  STEP: deleting the configmap @ 07/08/23 13:27:20.127
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 07/08/23 13:27:20.134
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 07/08/23 13:27:20.136
  Jul  8 13:27:20.136: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9218  22fa5758-0d29-4c6f-b2bc-999008fed507 36278 0 2023-07-08 13:27:20 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-08 13:27:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 13:27:20.136: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9218  22fa5758-0d29-4c6f-b2bc-999008fed507 36279 0 2023-07-08 13:27:20 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-08 13:27:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul  8 13:27:20.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9218" for this suite. @ 07/08/23 13:27:20.14
• [0.067 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 07/08/23 13:27:20.148
  Jul  8 13:27:20.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/08/23 13:27:20.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:27:20.164
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:27:20.168
  Jul  8 13:27:20.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 13:27:20.260115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:27:20.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6148" for this suite. @ 07/08/23 13:27:20.724
• [0.588 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 07/08/23 13:27:20.736
  Jul  8 13:27:20.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/08/23 13:27:20.737
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:27:20.752
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:27:20.756
  STEP: create the container to handle the HTTPGet hook request. @ 07/08/23 13:27:20.764
  E0708 13:27:21.260901      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:22.261086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 07/08/23 13:27:22.785
  E0708 13:27:23.261815      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:24.261896      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 07/08/23 13:27:24.805
  E0708 13:27:25.262262      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:26.262281      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 07/08/23 13:27:26.821
  Jul  8 13:27:26.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9544" for this suite. @ 07/08/23 13:27:26.843
• [6.114 seconds]
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 07/08/23 13:27:26.851
  Jul  8 13:27:26.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename gc @ 07/08/23 13:27:26.852
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:27:26.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:27:26.875
  STEP: create the rc @ 07/08/23 13:27:26.883
  W0708 13:27:26.888088      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0708 13:27:27.262380      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:28.262469      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:29.263206      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:30.263439      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:31.264118      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:32.265139      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 07/08/23 13:27:32.894
  STEP: wait for the rc to be deleted @ 07/08/23 13:27:32.904
  E0708 13:27:33.265739      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:27:33.926: INFO: 80 pods remaining
  Jul  8 13:27:33.926: INFO: 80 pods has nil DeletionTimestamp
  Jul  8 13:27:33.926: INFO: 
  E0708 13:27:34.266939      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:27:34.917: INFO: 71 pods remaining
  Jul  8 13:27:34.918: INFO: 71 pods has nil DeletionTimestamp
  Jul  8 13:27:34.918: INFO: 
  E0708 13:27:35.267687      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:27:35.918: INFO: 60 pods remaining
  Jul  8 13:27:35.919: INFO: 60 pods has nil DeletionTimestamp
  Jul  8 13:27:35.919: INFO: 
  E0708 13:27:36.268247      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:27:36.916: INFO: 40 pods remaining
  Jul  8 13:27:36.916: INFO: 40 pods has nil DeletionTimestamp
  Jul  8 13:27:36.916: INFO: 
  E0708 13:27:37.268352      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:27:37.915: INFO: 31 pods remaining
  Jul  8 13:27:37.915: INFO: 31 pods has nil DeletionTimestamp
  Jul  8 13:27:37.915: INFO: 
  E0708 13:27:38.268589      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:27:38.915: INFO: 20 pods remaining
  Jul  8 13:27:38.915: INFO: 20 pods has nil DeletionTimestamp
  Jul  8 13:27:38.915: INFO: 
  E0708 13:27:39.268777      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/08/23 13:27:39.913
  W0708 13:27:39.917437      20 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul  8 13:27:39.917: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul  8 13:27:39.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2055" for this suite. @ 07/08/23 13:27:39.922
• [13.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 07/08/23 13:27:39.934
  Jul  8 13:27:39.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename secrets @ 07/08/23 13:27:39.934
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:27:39.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:27:39.958
  STEP: Creating secret with name secret-test-6e9c3d74-6b52-4f08-b9d1-eceabd5a9430 @ 07/08/23 13:27:39.986
  STEP: Creating a pod to test consume secrets @ 07/08/23 13:27:39.992
  E0708 13:27:40.269788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:41.270234      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:42.270312      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:43.271392      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:44.271479      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:45.271651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:46.272203      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:47.272296      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:27:48.022
  Jul  8 13:27:48.025: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-secrets-72fa0dd9-62f2-4fc9-9b04-97161702f112 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/08/23 13:27:48.036
  Jul  8 13:27:48.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8307" for this suite. @ 07/08/23 13:27:48.058
  STEP: Destroying namespace "secret-namespace-6492" for this suite. @ 07/08/23 13:27:48.066
• [8.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 07/08/23 13:27:48.075
  Jul  8 13:27:48.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename secrets @ 07/08/23 13:27:48.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:27:48.093
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:27:48.097
  STEP: creating secret secrets-6949/secret-test-48474580-488d-4f6e-a0ae-608c7c873314 @ 07/08/23 13:27:48.1
  STEP: Creating a pod to test consume secrets @ 07/08/23 13:27:48.105
  E0708 13:27:48.272380      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:49.273069      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:50.273355      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:51.274294      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:27:52.129
  Jul  8 13:27:52.133: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-configmaps-3c2158a2-a327-4d5d-b844-4df0fa24096d container env-test: <nil>
  STEP: delete the pod @ 07/08/23 13:27:52.141
  Jul  8 13:27:52.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6949" for this suite. @ 07/08/23 13:27:52.16
• [4.092 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 07/08/23 13:27:52.168
  Jul  8 13:27:52.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename daemonsets @ 07/08/23 13:27:52.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:27:52.185
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:27:52.188
  Jul  8 13:27:52.211: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 07/08/23 13:27:52.217
  Jul  8 13:27:52.222: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 13:27:52.222: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 07/08/23 13:27:52.222
  Jul  8 13:27:52.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 13:27:52.240: INFO: Node ip-172-31-29-188 is running 0 daemon pod, expected 1
  E0708 13:27:52.274694      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:27:53.244: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  8 13:27:53.244: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 07/08/23 13:27:53.249
  Jul  8 13:27:53.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  8 13:27:53.268: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0708 13:27:53.274743      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:27:54.275277      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:27:54.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 13:27:54.275: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 07/08/23 13:27:54.275
  Jul  8 13:27:54.286: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 13:27:54.286: INFO: Node ip-172-31-29-188 is running 0 daemon pod, expected 1
  E0708 13:27:55.276009      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:27:55.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 13:27:55.291: INFO: Node ip-172-31-29-188 is running 0 daemon pod, expected 1
  E0708 13:27:56.276755      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:27:56.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul  8 13:27:56.291: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/08/23 13:27:56.299
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5190, will wait for the garbage collector to delete the pods @ 07/08/23 13:27:56.299
  Jul  8 13:27:56.359: INFO: Deleting DaemonSet.extensions daemon-set took: 6.87556ms
  Jul  8 13:27:56.460: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.645214ms
  E0708 13:27:57.276931      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:27:58.265: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul  8 13:27:58.265: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul  8 13:27:58.268: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38923"},"items":null}

  Jul  8 13:27:58.271: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38923"},"items":null}

  E0708 13:27:58.277867      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:27:58.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5190" for this suite. @ 07/08/23 13:27:58.302
• [6.141 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 07/08/23 13:27:58.309
  Jul  8 13:27:58.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename pod-network-test @ 07/08/23 13:27:58.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:27:58.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:27:58.335
  STEP: Performing setup for networking test in namespace pod-network-test-3371 @ 07/08/23 13:27:58.339
  STEP: creating a selector @ 07/08/23 13:27:58.339
  STEP: Creating the service pods in kubernetes @ 07/08/23 13:27:58.339
  Jul  8 13:27:58.339: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0708 13:27:59.278293      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:00.278761      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:01.279287      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:02.280344      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:03.280442      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:04.280506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:05.280783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:06.281830      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:07.282803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:08.282893      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:09.282998      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:10.283946      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 07/08/23 13:28:10.421
  E0708 13:28:11.284346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:12.285094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:28:12.454: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul  8 13:28:12.454: INFO: Going to poll 192.168.59.29 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul  8 13:28:12.458: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.59.29 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3371 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:28:12.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:28:12.458: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:28:12.458: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3371/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.59.29+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0708 13:28:13.285439      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:28:13.528: INFO: Found all 1 expected endpoints: [netserver-0]
  Jul  8 13:28:13.528: INFO: Going to poll 192.168.164.81 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul  8 13:28:13.532: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.164.81 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3371 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:28:13.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:28:13.533: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:28:13.533: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3371/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.164.81+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0708 13:28:14.286198      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:28:14.587: INFO: Found all 1 expected endpoints: [netserver-1]
  Jul  8 13:28:14.587: INFO: Going to poll 192.168.7.216 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul  8 13:28:14.592: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.7.216 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3371 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:28:14.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:28:14.592: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:28:14.592: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3371/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.7.216+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0708 13:28:15.286736      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:28:15.653: INFO: Found all 1 expected endpoints: [netserver-2]
  Jul  8 13:28:15.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3371" for this suite. @ 07/08/23 13:28:15.659
• [17.356 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 07/08/23 13:28:15.666
  Jul  8 13:28:15.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename subpath @ 07/08/23 13:28:15.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:28:15.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:28:15.687
  STEP: Setting up data @ 07/08/23 13:28:15.691
  STEP: Creating pod pod-subpath-test-projected-979k @ 07/08/23 13:28:15.7
  STEP: Creating a pod to test atomic-volume-subpath @ 07/08/23 13:28:15.7
  E0708 13:28:16.287359      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:17.288073      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:18.289171      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:19.289194      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:20.289278      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:21.290359      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:22.290508      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:23.290609      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:24.290639      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:25.290731      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:26.291133      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:27.291307      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:28.291542      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:29.291718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:30.292041      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:31.292271      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:32.292358      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:33.293107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:34.293187      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:35.293225      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:36.293888      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:37.294079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:38.294334      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:39.294513      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:28:39.769
  Jul  8 13:28:39.773: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-subpath-test-projected-979k container test-container-subpath-projected-979k: <nil>
  STEP: delete the pod @ 07/08/23 13:28:39.781
  STEP: Deleting pod pod-subpath-test-projected-979k @ 07/08/23 13:28:39.798
  Jul  8 13:28:39.798: INFO: Deleting pod "pod-subpath-test-projected-979k" in namespace "subpath-6408"
  Jul  8 13:28:39.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6408" for this suite. @ 07/08/23 13:28:39.806
• [24.146 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 07/08/23 13:28:39.814
  Jul  8 13:28:39.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename resourcequota @ 07/08/23 13:28:39.814
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:28:39.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:28:39.835
  STEP: Counting existing ResourceQuota @ 07/08/23 13:28:39.839
  E0708 13:28:40.294637      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:41.295089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:42.295155      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:43.295219      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:44.295335      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/08/23 13:28:44.844
  STEP: Ensuring resource quota status is calculated @ 07/08/23 13:28:44.85
  E0708 13:28:45.296140      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:46.296405      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:28:46.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-125" for this suite. @ 07/08/23 13:28:46.86
• [7.055 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 07/08/23 13:28:46.869
  Jul  8 13:28:46.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename secrets @ 07/08/23 13:28:46.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:28:46.888
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:28:46.892
  STEP: Creating secret with name secret-test-76ad0c1e-0409-4faf-8ff9-ff8d107d45e3 @ 07/08/23 13:28:46.898
  STEP: Creating a pod to test consume secrets @ 07/08/23 13:28:46.904
  E0708 13:28:47.297098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:48.297188      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:28:48.933
  Jul  8 13:28:48.938: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-secrets-486d1815-7808-4edd-b727-452c4814cf40 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/08/23 13:28:48.948
  Jul  8 13:28:48.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9697" for this suite. @ 07/08/23 13:28:48.973
• [2.112 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 07/08/23 13:28:48.981
  Jul  8 13:28:48.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename downward-api @ 07/08/23 13:28:48.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:28:48.998
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:28:49.002
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 13:28:49.007
  E0708 13:28:49.297556      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:50.297659      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:51.298028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:52.298208      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:28:53.034
  Jul  8 13:28:53.039: INFO: Trying to get logs from node ip-172-31-93-234 pod downwardapi-volume-fa236a7a-234b-46c8-9749-1df2f4d498c8 container client-container: <nil>
  STEP: delete the pod @ 07/08/23 13:28:53.047
  Jul  8 13:28:53.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8518" for this suite. @ 07/08/23 13:28:53.073
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 07/08/23 13:28:53.082
  Jul  8 13:28:53.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename containers @ 07/08/23 13:28:53.082
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:28:53.1
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:28:53.104
  E0708 13:28:53.298740      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:54.299161      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:28:55.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2557" for this suite. @ 07/08/23 13:28:55.141
• [2.067 seconds]
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 07/08/23 13:28:55.149
  Jul  8 13:28:55.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename var-expansion @ 07/08/23 13:28:55.149
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:28:55.168
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:28:55.171
  STEP: Creating a pod to test substitution in volume subpath @ 07/08/23 13:28:55.176
  E0708 13:28:55.299159      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:56.299449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:57.300372      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:28:58.300554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:28:59.199
  Jul  8 13:28:59.202: INFO: Trying to get logs from node ip-172-31-93-234 pod var-expansion-05b62711-b7a5-4d4f-8f0a-656a2d00fbd5 container dapi-container: <nil>
  STEP: delete the pod @ 07/08/23 13:28:59.21
  Jul  8 13:28:59.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8550" for this suite. @ 07/08/23 13:28:59.23
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 07/08/23 13:28:59.238
  Jul  8 13:28:59.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename runtimeclass @ 07/08/23 13:28:59.238
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:28:59.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:28:59.259
  STEP: getting /apis @ 07/08/23 13:28:59.262
  STEP: getting /apis/node.k8s.io @ 07/08/23 13:28:59.266
  STEP: getting /apis/node.k8s.io/v1 @ 07/08/23 13:28:59.268
  STEP: creating @ 07/08/23 13:28:59.269
  STEP: watching @ 07/08/23 13:28:59.284
  Jul  8 13:28:59.284: INFO: starting watch
  STEP: getting @ 07/08/23 13:28:59.291
  STEP: listing @ 07/08/23 13:28:59.294
  STEP: patching @ 07/08/23 13:28:59.298
  E0708 13:28:59.300779      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating @ 07/08/23 13:28:59.304
  Jul  8 13:28:59.308: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 07/08/23 13:28:59.308
  STEP: deleting a collection @ 07/08/23 13:28:59.322
  Jul  8 13:28:59.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7831" for this suite. @ 07/08/23 13:28:59.342
• [0.111 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 07/08/23 13:28:59.349
  Jul  8 13:28:59.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename sched-pred @ 07/08/23 13:28:59.35
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:28:59.364
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:28:59.367
  Jul  8 13:28:59.371: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul  8 13:28:59.379: INFO: Waiting for terminating namespaces to be deleted...
  Jul  8 13:28:59.382: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-12-67 before test
  Jul  8 13:28:59.388: INFO: default-http-backend-kubernetes-worker-65fc475d49-8npt2 from ingress-nginx-kubernetes-worker started at 2023-07-08 11:50:26 +0000 UTC (1 container statuses recorded)
  Jul  8 13:28:59.388: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul  8 13:28:59.388: INFO: nginx-ingress-controller-kubernetes-worker-lsvk7 from ingress-nginx-kubernetes-worker started at 2023-07-08 11:50:26 +0000 UTC (1 container statuses recorded)
  Jul  8 13:28:59.388: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  8 13:28:59.388: INFO: calico-kube-controllers-bb564cc5-vpxwc from kube-system started at 2023-07-08 11:50:30 +0000 UTC (1 container statuses recorded)
  Jul  8 13:28:59.388: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul  8 13:28:59.388: INFO: coredns-5c7f76ccb8-mdjks from kube-system started at 2023-07-08 11:50:22 +0000 UTC (1 container statuses recorded)
  Jul  8 13:28:59.388: INFO: 	Container coredns ready: true, restart count 0
  Jul  8 13:28:59.388: INFO: kube-state-metrics-5b95b4459c-nw4hk from kube-system started at 2023-07-08 11:50:22 +0000 UTC (1 container statuses recorded)
  Jul  8 13:28:59.388: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul  8 13:28:59.388: INFO: metrics-server-v0.5.2-6cf8c8b69c-2d7pj from kube-system started at 2023-07-08 11:50:22 +0000 UTC (2 container statuses recorded)
  Jul  8 13:28:59.388: INFO: 	Container metrics-server ready: true, restart count 0
  Jul  8 13:28:59.388: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul  8 13:28:59.388: INFO: dashboard-metrics-scraper-6b8586b5c9-tjl98 from kubernetes-dashboard started at 2023-07-08 11:50:22 +0000 UTC (1 container statuses recorded)
  Jul  8 13:28:59.388: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul  8 13:28:59.388: INFO: kubernetes-dashboard-6869f4cd5f-x4wbw from kubernetes-dashboard started at 2023-07-08 11:50:22 +0000 UTC (1 container statuses recorded)
  Jul  8 13:28:59.388: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul  8 13:28:59.388: INFO: sonobuoy-systemd-logs-daemon-set-9e0399ccdc194cf6-4ll49 from sonobuoy started at 2023-07-08 12:01:20 +0000 UTC (2 container statuses recorded)
  Jul  8 13:28:59.388: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  8 13:28:59.388: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul  8 13:28:59.388: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-29-188 before test
  Jul  8 13:28:59.393: INFO: nginx-ingress-controller-kubernetes-worker-ptj9j from ingress-nginx-kubernetes-worker started at 2023-07-08 13:17:02 +0000 UTC (1 container statuses recorded)
  Jul  8 13:28:59.393: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  8 13:28:59.393: INFO: sonobuoy-e2e-job-885126f13f224642 from sonobuoy started at 2023-07-08 12:01:20 +0000 UTC (2 container statuses recorded)
  Jul  8 13:28:59.393: INFO: 	Container e2e ready: true, restart count 0
  Jul  8 13:28:59.393: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  8 13:28:59.393: INFO: sonobuoy-systemd-logs-daemon-set-9e0399ccdc194cf6-plcwc from sonobuoy started at 2023-07-08 12:01:20 +0000 UTC (2 container statuses recorded)
  Jul  8 13:28:59.394: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  8 13:28:59.394: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul  8 13:28:59.394: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-93-234 before test
  Jul  8 13:28:59.399: INFO: client-containers-06e8845b-e225-453e-ac48-d4be1c066ff0 from containers-2557 started at 2023-07-08 13:28:53 +0000 UTC (1 container statuses recorded)
  Jul  8 13:28:59.399: INFO: 	Container agnhost-container ready: true, restart count 0
  Jul  8 13:28:59.399: INFO: nginx-ingress-controller-kubernetes-worker-wrlrn from ingress-nginx-kubernetes-worker started at 2023-07-08 12:26:30 +0000 UTC (1 container statuses recorded)
  Jul  8 13:28:59.399: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul  8 13:28:59.399: INFO: sonobuoy from sonobuoy started at 2023-07-08 12:01:18 +0000 UTC (1 container statuses recorded)
  Jul  8 13:28:59.399: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul  8 13:28:59.399: INFO: sonobuoy-systemd-logs-daemon-set-9e0399ccdc194cf6-nrrhp from sonobuoy started at 2023-07-08 12:01:20 +0000 UTC (2 container statuses recorded)
  Jul  8 13:28:59.399: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul  8 13:28:59.399: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/08/23 13:28:59.399
  E0708 13:29:00.300934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:01.301303      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/08/23 13:29:01.421
  STEP: Trying to apply a random label on the found node. @ 07/08/23 13:29:01.433
  STEP: verifying the node has the label kubernetes.io/e2e-ddb2ee0d-5c4f-4423-84e2-658681f4453f 42 @ 07/08/23 13:29:01.442
  STEP: Trying to relaunch the pod, now with labels. @ 07/08/23 13:29:01.446
  E0708 13:29:02.301915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:03.302136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-ddb2ee0d-5c4f-4423-84e2-658681f4453f off the node ip-172-31-29-188 @ 07/08/23 13:29:03.464
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-ddb2ee0d-5c4f-4423-84e2-658681f4453f @ 07/08/23 13:29:03.475
  Jul  8 13:29:03.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8824" for this suite. @ 07/08/23 13:29:03.483
• [4.141 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 07/08/23 13:29:03.491
  Jul  8 13:29:03.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename gc @ 07/08/23 13:29:03.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:29:03.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:29:03.514
  STEP: create the deployment @ 07/08/23 13:29:03.518
  W0708 13:29:03.524399      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 07/08/23 13:29:03.524
  STEP: delete the deployment @ 07/08/23 13:29:04.033
  STEP: wait for all rs to be garbage collected @ 07/08/23 13:29:04.043
  STEP: expected 0 rs, got 1 rs @ 07/08/23 13:29:04.05
  STEP: expected 0 pods, got 2 pods @ 07/08/23 13:29:04.054
  E0708 13:29:04.302688      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/08/23 13:29:04.566
  W0708 13:29:04.570192      20 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul  8 13:29:04.570: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul  8 13:29:04.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6248" for this suite. @ 07/08/23 13:29:04.574
• [1.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 07/08/23 13:29:04.583
  Jul  8 13:29:04.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename disruption @ 07/08/23 13:29:04.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:29:04.609
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:29:04.612
  STEP: Creating a pdb that targets all three pods in a test replica set @ 07/08/23 13:29:04.615
  STEP: Waiting for the pdb to be processed @ 07/08/23 13:29:04.62
  E0708 13:29:05.303629      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:06.304397      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 07/08/23 13:29:06.634
  STEP: Waiting for all pods to be running @ 07/08/23 13:29:06.634
  Jul  8 13:29:06.637: INFO: pods: 0 < 3
  E0708 13:29:07.304692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:08.305095      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 07/08/23 13:29:08.642
  STEP: Updating the pdb to allow a pod to be evicted @ 07/08/23 13:29:08.653
  STEP: Waiting for the pdb to be processed @ 07/08/23 13:29:08.662
  E0708 13:29:09.305649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:10.305744      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 07/08/23 13:29:10.67
  STEP: Waiting for all pods to be running @ 07/08/23 13:29:10.67
  STEP: Waiting for the pdb to observed all healthy pods @ 07/08/23 13:29:10.674
  STEP: Patching the pdb to disallow a pod to be evicted @ 07/08/23 13:29:10.702
  STEP: Waiting for the pdb to be processed @ 07/08/23 13:29:10.714
  E0708 13:29:11.306719      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:12.307184      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 07/08/23 13:29:12.723
  STEP: locating a running pod @ 07/08/23 13:29:12.727
  STEP: Deleting the pdb to allow a pod to be evicted @ 07/08/23 13:29:12.738
  STEP: Waiting for the pdb to be deleted @ 07/08/23 13:29:12.744
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 07/08/23 13:29:12.748
  STEP: Waiting for all pods to be running @ 07/08/23 13:29:12.748
  Jul  8 13:29:12.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4807" for this suite. @ 07/08/23 13:29:12.773
• [8.199 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 07/08/23 13:29:12.783
  Jul  8 13:29:12.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 13:29:12.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:29:12.8
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:29:12.803
  STEP: Setting up server cert @ 07/08/23 13:29:12.83
  E0708 13:29:13.308029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 13:29:13.354
  STEP: Deploying the webhook pod @ 07/08/23 13:29:13.362
  STEP: Wait for the deployment to be ready @ 07/08/23 13:29:13.377
  Jul  8 13:29:13.386: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0708 13:29:14.308434      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:15.308537      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 13:29:15.398
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 13:29:15.408
  E0708 13:29:16.309299      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:29:16.409: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 07/08/23 13:29:16.475
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/08/23 13:29:16.513
  STEP: Deleting the collection of validation webhooks @ 07/08/23 13:29:16.542
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/08/23 13:29:16.596
  Jul  8 13:29:16.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5441" for this suite. @ 07/08/23 13:29:16.649
  STEP: Destroying namespace "webhook-markers-3090" for this suite. @ 07/08/23 13:29:16.658
• [3.884 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 07/08/23 13:29:16.668
  Jul  8 13:29:16.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename subjectreview @ 07/08/23 13:29:16.669
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:29:16.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:29:16.688
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-3020" @ 07/08/23 13:29:16.691
  Jul  8 13:29:16.696: INFO: saUsername: "system:serviceaccount:subjectreview-3020:e2e"
  Jul  8 13:29:16.696: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-3020"}
  Jul  8 13:29:16.696: INFO: saUID: "868101cd-a2b7-4ad6-82d2-e47fb8b0ce12"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-3020:e2e" @ 07/08/23 13:29:16.696
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-3020:e2e" @ 07/08/23 13:29:16.696
  Jul  8 13:29:16.698: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-3020:e2e" api 'list' configmaps in "subjectreview-3020" namespace @ 07/08/23 13:29:16.698
  Jul  8 13:29:16.699: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-3020:e2e" @ 07/08/23 13:29:16.699
  Jul  8 13:29:16.701: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Jul  8 13:29:16.702: INFO: LocalSubjectAccessReview has been verified
  Jul  8 13:29:16.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-3020" for this suite. @ 07/08/23 13:29:16.706
• [0.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 07/08/23 13:29:16.713
  Jul  8 13:29:16.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename pods @ 07/08/23 13:29:16.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:29:16.728
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:29:16.731
  STEP: creating a Pod with a static label @ 07/08/23 13:29:16.739
  STEP: watching for Pod to be ready @ 07/08/23 13:29:16.747
  Jul  8 13:29:16.749: INFO: observed Pod pod-test in namespace pods-2816 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Jul  8 13:29:16.752: INFO: observed Pod pod-test in namespace pods-2816 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:29:16 +0000 UTC  }]
  Jul  8 13:29:16.764: INFO: observed Pod pod-test in namespace pods-2816 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:29:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:29:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:29:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:29:16 +0000 UTC  }]
  E0708 13:29:17.310265      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:29:17.754: INFO: Found Pod pod-test in namespace pods-2816 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:29:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:29:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:29:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-08 13:29:16 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 07/08/23 13:29:17.758
  STEP: getting the Pod and ensuring that it's patched @ 07/08/23 13:29:17.768
  STEP: replacing the Pod's status Ready condition to False @ 07/08/23 13:29:17.773
  STEP: check the Pod again to ensure its Ready conditions are False @ 07/08/23 13:29:17.783
  STEP: deleting the Pod via a Collection with a LabelSelector @ 07/08/23 13:29:17.783
  STEP: watching for the Pod to be deleted @ 07/08/23 13:29:17.795
  Jul  8 13:29:17.797: INFO: observed event type MODIFIED
  E0708 13:29:18.310632      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:19.310960      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:29:19.756: INFO: observed event type MODIFIED
  Jul  8 13:29:20.002: INFO: observed event type MODIFIED
  E0708 13:29:20.310985      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:29:20.760: INFO: observed event type MODIFIED
  Jul  8 13:29:20.775: INFO: observed event type MODIFIED
  Jul  8 13:29:20.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2816" for this suite. @ 07/08/23 13:29:20.786
• [4.080 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 07/08/23 13:29:20.793
  Jul  8 13:29:20.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename downward-api @ 07/08/23 13:29:20.793
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:29:20.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:29:20.813
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 13:29:20.816
  E0708 13:29:21.311022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:22.311117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:23.311205      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:24.311391      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:29:24.839
  Jul  8 13:29:24.843: INFO: Trying to get logs from node ip-172-31-93-234 pod downwardapi-volume-ce1c1618-64bc-40c7-be8a-ce96c0a27d20 container client-container: <nil>
  STEP: delete the pod @ 07/08/23 13:29:24.85
  Jul  8 13:29:24.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2832" for this suite. @ 07/08/23 13:29:24.948
• [4.162 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 07/08/23 13:29:24.955
  Jul  8 13:29:24.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename deployment @ 07/08/23 13:29:24.956
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:29:24.973
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:29:24.977
  Jul  8 13:29:24.980: INFO: Creating simple deployment test-new-deployment
  Jul  8 13:29:24.993: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0708 13:29:25.312349      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:26.312673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 07/08/23 13:29:27.009
  STEP: updating a scale subresource @ 07/08/23 13:29:27.012
  STEP: verifying the deployment Spec.Replicas was modified @ 07/08/23 13:29:27.019
  STEP: Patch a scale subresource @ 07/08/23 13:29:27.022
  Jul  8 13:29:27.039: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-1347  fdb137a8-071a-49db-bb3e-6217e450d96d 39947 3 2023-07-08 13:29:24 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-07-08 13:29:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 13:29:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f04848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-08 13:29:25 +0000 UTC,LastTransitionTime:2023-07-08 13:29:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-07-08 13:29:25 +0000 UTC,LastTransitionTime:2023-07-08 13:29:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul  8 13:29:27.043: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-1347  ee183fce-8263-49e0-8ec8-703ebd5513c0 39952 2 2023-07-08 13:29:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment fdb137a8-071a-49db-bb3e-6217e450d96d 0xc0039636f7 0xc0039636f8}] [] [{kube-controller-manager Update apps/v1 2023-07-08 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdb137a8-071a-49db-bb3e-6217e450d96d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 13:29:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003963788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul  8 13:29:27.046: INFO: Pod "test-new-deployment-67bd4bf6dc-f6ls9" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-f6ls9 test-new-deployment-67bd4bf6dc- deployment-1347  6d95df1e-3f04-4b4a-8ab1-198fafa2352b 39950 0 2023-07-08 13:29:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc ee183fce-8263-49e0-8ec8-703ebd5513c0 0xc003963b77 0xc003963b78}] [] [{kube-controller-manager Update v1 2023-07-08 13:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee183fce-8263-49e0-8ec8-703ebd5513c0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4xfk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4xfk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-29-188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:29:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:29:27.047: INFO: Pod "test-new-deployment-67bd4bf6dc-jctxb" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-jctxb test-new-deployment-67bd4bf6dc- deployment-1347  8d39b365-d8d6-4900-bbd5-d15012b1c03a 39927 0 2023-07-08 13:29:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc ee183fce-8263-49e0-8ec8-703ebd5513c0 0xc003963ce0 0xc003963ce1}] [] [{kube-controller-manager Update v1 2023-07-08 13:29:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee183fce-8263-49e0-8ec8-703ebd5513c0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:29:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-92462,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-92462,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-93-234,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:29:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:29:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:29:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:29:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.93.234,PodIP:192.168.7.226,StartTime:2023-07-08 13:29:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-08 13:29:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a179512cb847e1c34385ceb9ff95e0cb1e136e9bbc6309d0898521ea11e5bd59,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.226,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:29:27.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1347" for this suite. @ 07/08/23 13:29:27.051
• [2.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 07/08/23 13:29:27.06
  Jul  8 13:29:27.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename pods @ 07/08/23 13:29:27.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:29:27.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:29:27.106
  STEP: creating the pod @ 07/08/23 13:29:27.109
  STEP: setting up watch @ 07/08/23 13:29:27.109
  STEP: submitting the pod to kubernetes @ 07/08/23 13:29:27.214
  STEP: verifying the pod is in kubernetes @ 07/08/23 13:29:27.224
  STEP: verifying pod creation was observed @ 07/08/23 13:29:27.228
  E0708 13:29:27.313017      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:28.313196      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 07/08/23 13:29:29.24
  STEP: verifying pod deletion was observed @ 07/08/23 13:29:29.247
  E0708 13:29:29.313772      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:30.314302      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:29:30.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4646" for this suite. @ 07/08/23 13:29:30.816
• [3.762 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 07/08/23 13:29:30.823
  Jul  8 13:29:30.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-probe @ 07/08/23 13:29:30.824
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:29:30.839
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:29:30.842
  STEP: Creating pod busybox-a76febf3-44bd-4cc0-9cc4-217933ab4b29 in namespace container-probe-7947 @ 07/08/23 13:29:30.845
  E0708 13:29:31.315109      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:32.315202      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:29:32.863: INFO: Started pod busybox-a76febf3-44bd-4cc0-9cc4-217933ab4b29 in namespace container-probe-7947
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/08/23 13:29:32.863
  Jul  8 13:29:32.866: INFO: Initial restart count of pod busybox-a76febf3-44bd-4cc0-9cc4-217933ab4b29 is 0
  E0708 13:29:33.316187      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:34.316303      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:35.316383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:36.316658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:37.317689      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:38.317707      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:39.317947      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:40.318949      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:41.319490      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:42.319578      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:43.320500      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:44.321107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:45.321175      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:46.321468      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:47.321572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:48.321770      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:49.321850      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:50.322071      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:51.322326      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:52.322442      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:53.323436      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:54.323633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:55.324229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:56.324397      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:57.324920      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:58.325088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:29:59.325140      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:00.325237      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:01.326220      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:02.326424      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:03.327238      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:04.327837      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:05.328380      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:06.328415      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:07.328841      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:08.328917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:09.329009      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:10.329510      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:11.330520      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:12.330715      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:13.331055      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:14.330902      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:15.330894      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:16.331157      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:17.332165      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:18.333091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:19.333292      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:20.333444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:21.334354      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:22.334512      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:30:22.996: INFO: Restart count of pod container-probe-7947/busybox-a76febf3-44bd-4cc0-9cc4-217933ab4b29 is now 1 (50.129059479s elapsed)
  Jul  8 13:30:22.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/08/23 13:30:23
  STEP: Destroying namespace "container-probe-7947" for this suite. @ 07/08/23 13:30:23.011
• [52.195 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 07/08/23 13:30:23.02
  Jul  8 13:30:23.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename deployment @ 07/08/23 13:30:23.021
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:30:23.038
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:30:23.041
  Jul  8 13:30:23.044: INFO: Creating deployment "webserver-deployment"
  Jul  8 13:30:23.049: INFO: Waiting for observed generation 1
  E0708 13:30:23.335456      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:24.335639      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:30:25.059: INFO: Waiting for all required pods to come up
  Jul  8 13:30:25.064: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 07/08/23 13:30:25.064
  Jul  8 13:30:25.064: INFO: Waiting for deployment "webserver-deployment" to complete
  Jul  8 13:30:25.071: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Jul  8 13:30:25.081: INFO: Updating deployment webserver-deployment
  Jul  8 13:30:25.081: INFO: Waiting for observed generation 2
  E0708 13:30:25.335693      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:26.335981      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:30:27.168: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Jul  8 13:30:27.172: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Jul  8 13:30:27.175: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jul  8 13:30:27.186: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Jul  8 13:30:27.186: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Jul  8 13:30:27.189: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jul  8 13:30:27.196: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Jul  8 13:30:27.196: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Jul  8 13:30:27.206: INFO: Updating deployment webserver-deployment
  Jul  8 13:30:27.206: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Jul  8 13:30:27.213: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Jul  8 13:30:27.218: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Jul  8 13:30:27.242: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-6168  746e784c-7aeb-4b82-bde5-37de0ef1b603 40440 3 2023-07-08 13:30:23 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002fbe0f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-07-08 13:30:25 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-08 13:30:27 +0000 UTC,LastTransitionTime:2023-07-08 13:30:27 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Jul  8 13:30:27.250: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-6168  85da5943-1ab2-4a55-8e72-e526da34249b 40433 3 2023-07-08 13:30:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 746e784c-7aeb-4b82-bde5-37de0ef1b603 0xc002fbe5f7 0xc002fbe5f8}] [] [{kube-controller-manager Update apps/v1 2023-07-08 13:30:25 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"746e784c-7aeb-4b82-bde5-37de0ef1b603\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002fbe6a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul  8 13:30:27.250: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Jul  8 13:30:27.251: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-6168  99d008fc-907e-429f-b690-b1a48296db29 40430 3 2023-07-08 13:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 746e784c-7aeb-4b82-bde5-37de0ef1b603 0xc002fbe507 0xc002fbe508}] [] [{kube-controller-manager Update apps/v1 2023-07-08 13:30:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"746e784c-7aeb-4b82-bde5-37de0ef1b603\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002fbe598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Jul  8 13:30:27.258: INFO: Pod "webserver-deployment-67bd4bf6dc-5nd4q" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5nd4q webserver-deployment-67bd4bf6dc- deployment-6168  c44ddc34-469d-4741-94b8-345dfa103cb0 40309 0 2023-07-08 13:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 99d008fc-907e-429f-b690-b1a48296db29 0xc005005a07 0xc005005a08}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d008fc-907e-429f-b690-b1a48296db29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:30:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.59.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hth8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hth8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.67,PodIP:192.168.59.14,StartTime:2023-07-08 13:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-08 13:30:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bd585632b4bc170fbd5f21875c2ffb6315386d9195bfdbe5a11e33248d2f30d2,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.59.14,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.258: INFO: Pod "webserver-deployment-67bd4bf6dc-79px7" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-79px7 webserver-deployment-67bd4bf6dc- deployment-6168  189987f8-2770-47a1-aaac-2a9a6fed8636 40315 0 2023-07-08 13:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 99d008fc-907e-429f-b690-b1a48296db29 0xc005005bf7 0xc005005bf8}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d008fc-907e-429f-b690-b1a48296db29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:30:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.59.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-px7f9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-px7f9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.67,PodIP:192.168.59.18,StartTime:2023-07-08 13:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-08 13:30:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3e23f744a4bd92e05e8c6c6c574efb85f1d75659e42a244406c4906309e4b7e7,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.59.18,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.258: INFO: Pod "webserver-deployment-67bd4bf6dc-9f4xl" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9f4xl webserver-deployment-67bd4bf6dc- deployment-6168  91c8877f-66bc-4283-ad28-9c597fb10ae1 40442 0 2023-07-08 13:30:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 99d008fc-907e-429f-b690-b1a48296db29 0xc005005df7 0xc005005df8}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d008fc-907e-429f-b690-b1a48296db29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w5m6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w5m6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-29-188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.258: INFO: Pod "webserver-deployment-67bd4bf6dc-9rgnb" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9rgnb webserver-deployment-67bd4bf6dc- deployment-6168  cee52bac-eae1-4221-b767-ee14ef0bd92b 40308 0 2023-07-08 13:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 99d008fc-907e-429f-b690-b1a48296db29 0xc005005f60 0xc005005f61}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d008fc-907e-429f-b690-b1a48296db29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:30:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9thwd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9thwd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-93-234,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.93.234,PodIP:192.168.7.201,StartTime:2023-07-08 13:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-08 13:30:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e4d501c911f42309257a214b215f875ea07bf00728de0e26539f78fbd967e5d4,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.201,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.258: INFO: Pod "webserver-deployment-67bd4bf6dc-gcnck" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gcnck webserver-deployment-67bd4bf6dc- deployment-6168  84fa7b8a-8c4f-40f2-a304-b1558af191ea 40297 0 2023-07-08 13:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 99d008fc-907e-429f-b690-b1a48296db29 0xc002a3a4c7 0xc002a3a4c8}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d008fc-907e-429f-b690-b1a48296db29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:30:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.164.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zhcpl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zhcpl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-29-188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.29.188,PodIP:192.168.164.99,StartTime:2023-07-08 13:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-08 13:30:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2aa756a8ed9a97cdabaee58b5130f79087412e05945a44d4089750cf2aa4859a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.164.99,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.258: INFO: Pod "webserver-deployment-67bd4bf6dc-h294h" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-h294h webserver-deployment-67bd4bf6dc- deployment-6168  f0953100-a886-4e9a-9e88-5496d700fda2 40447 0 2023-07-08 13:30:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 99d008fc-907e-429f-b690-b1a48296db29 0xc002a3aa27 0xc002a3aa28}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d008fc-907e-429f-b690-b1a48296db29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vw9kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vw9kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.259: INFO: Pod "webserver-deployment-67bd4bf6dc-h6v7w" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-h6v7w webserver-deployment-67bd4bf6dc- deployment-6168  1a79e0e9-5581-4da1-b13b-6ed181aa16e6 40448 0 2023-07-08 13:30:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 99d008fc-907e-429f-b690-b1a48296db29 0xc002a3ad57 0xc002a3ad58}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d008fc-907e-429f-b690-b1a48296db29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xbd7b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xbd7b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.259: INFO: Pod "webserver-deployment-67bd4bf6dc-h89hl" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-h89hl webserver-deployment-67bd4bf6dc- deployment-6168  169c1b64-e7fa-435c-aef9-ff6c454c7ff0 40294 0 2023-07-08 13:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 99d008fc-907e-429f-b690-b1a48296db29 0xc002a3b067 0xc002a3b068}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d008fc-907e-429f-b690-b1a48296db29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:30:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.164.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xrwgb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xrwgb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-29-188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.29.188,PodIP:192.168.164.87,StartTime:2023-07-08 13:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-08 13:30:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://65d6d89a7c504a337e4308abdc48e78cd82f462191289371c302400306950652,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.164.87,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.259: INFO: Pod "webserver-deployment-67bd4bf6dc-hhsmt" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hhsmt webserver-deployment-67bd4bf6dc- deployment-6168  2ea2afa8-7ebd-43e2-8306-4fc4be7644dc 40452 0 2023-07-08 13:30:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 99d008fc-907e-429f-b690-b1a48296db29 0xc002a3b367 0xc002a3b368}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d008fc-907e-429f-b690-b1a48296db29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mxs4j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mxs4j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.259: INFO: Pod "webserver-deployment-67bd4bf6dc-hr5l5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hr5l5 webserver-deployment-67bd4bf6dc- deployment-6168  bc04f194-c2d0-4be6-9711-902cca93ec33 40456 0 2023-07-08 13:30:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 99d008fc-907e-429f-b690-b1a48296db29 0xc002a3b587 0xc002a3b588}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d008fc-907e-429f-b690-b1a48296db29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4z52,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4z52,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.259: INFO: Pod "webserver-deployment-67bd4bf6dc-l9hd2" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-l9hd2 webserver-deployment-67bd4bf6dc- deployment-6168  0124b6f9-d949-4cc8-bdfd-638e86cabdad 40317 0 2023-07-08 13:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 99d008fc-907e-429f-b690-b1a48296db29 0xc002a3ba30 0xc002a3ba31}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d008fc-907e-429f-b690-b1a48296db29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:30:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.59.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vfkcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vfkcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.67,PodIP:192.168.59.44,StartTime:2023-07-08 13:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-08 13:30:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://09fbae6a7b7ae84b03aa7f63ef5cd68c66d421d42d1db1af9fec25a62221d98b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.59.44,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.259: INFO: Pod "webserver-deployment-67bd4bf6dc-m4lg7" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-m4lg7 webserver-deployment-67bd4bf6dc- deployment-6168  55f2c545-aa4a-4090-9216-599de19507df 40300 0 2023-07-08 13:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 99d008fc-907e-429f-b690-b1a48296db29 0xc002a3bc17 0xc002a3bc18}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d008fc-907e-429f-b690-b1a48296db29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:30:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.164.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8nwv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8nwv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-29-188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.29.188,PodIP:192.168.164.110,StartTime:2023-07-08 13:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-08 13:30:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://549a54845ce6daf9200655103f841fd8663c01caa884882277992eb987455e5a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.164.110,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.260: INFO: Pod "webserver-deployment-67bd4bf6dc-r74v2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-r74v2 webserver-deployment-67bd4bf6dc- deployment-6168  a9be6fd2-a67f-4dbe-8a56-88ed7ac131c4 40434 0 2023-07-08 13:30:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 99d008fc-907e-429f-b690-b1a48296db29 0xc002a3be07 0xc002a3be08}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d008fc-907e-429f-b690-b1a48296db29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6pgvq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6pgvq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-93-234,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.260: INFO: Pod "webserver-deployment-67bd4bf6dc-s2gwx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-s2gwx webserver-deployment-67bd4bf6dc- deployment-6168  65734fa9-bcf8-42c8-b8e5-37caab31d035 40441 0 2023-07-08 13:30:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 99d008fc-907e-429f-b690-b1a48296db29 0xc002a3bf70 0xc002a3bf71}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d008fc-907e-429f-b690-b1a48296db29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xznwn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xznwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-93-234,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.260: INFO: Pod "webserver-deployment-67bd4bf6dc-tp5px" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tp5px webserver-deployment-67bd4bf6dc- deployment-6168  86f9e7c9-b481-40cf-aad2-6a7b6dc7b521 40306 0 2023-07-08 13:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 99d008fc-907e-429f-b690-b1a48296db29 0xc004aa20d0 0xc004aa20d1}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d008fc-907e-429f-b690-b1a48296db29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:30:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.227\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fjk8h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fjk8h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-93-234,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.93.234,PodIP:192.168.7.227,StartTime:2023-07-08 13:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-08 13:30:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://938350ae4b84001859fb8a295ddb2cec44f30fa014f91462d672cd5e72cf944e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.227,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.260: INFO: Pod "webserver-deployment-7b75d79cf5-6sjdz" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-6sjdz webserver-deployment-7b75d79cf5- deployment-6168  921740c5-6ccc-4c01-a196-a9086443d3c0 40446 0 2023-07-08 13:30:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 85da5943-1ab2-4a55-8e72-e526da34249b 0xc004aa22b7 0xc004aa22b8}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85da5943-1ab2-4a55-8e72-e526da34249b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tfq6j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tfq6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.260: INFO: Pod "webserver-deployment-7b75d79cf5-d97nc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-d97nc webserver-deployment-7b75d79cf5- deployment-6168  d9aeefbf-93be-4fa0-8395-d0018da79509 40400 0 2023-07-08 13:30:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 85da5943-1ab2-4a55-8e72-e526da34249b 0xc004aa2407 0xc004aa2408}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85da5943-1ab2-4a55-8e72-e526da34249b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:30:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vxvg7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vxvg7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-93-234,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.93.234,PodIP:192.168.7.252,StartTime:2023-07-08 13:30:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.252,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.261: INFO: Pod "webserver-deployment-7b75d79cf5-drp2t" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-drp2t webserver-deployment-7b75d79cf5- deployment-6168  3b044952-c47d-4fa3-afe0-88d56f5d1594 40443 0 2023-07-08 13:30:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 85da5943-1ab2-4a55-8e72-e526da34249b 0xc004aa2627 0xc004aa2628}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85da5943-1ab2-4a55-8e72-e526da34249b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xzrh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xzrh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.261: INFO: Pod "webserver-deployment-7b75d79cf5-glfmp" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-glfmp webserver-deployment-7b75d79cf5- deployment-6168  236e8a7e-52f0-41db-a7e8-bc5ed7c83a22 40428 0 2023-07-08 13:30:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 85da5943-1ab2-4a55-8e72-e526da34249b 0xc004aa27a0 0xc004aa27a1}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85da5943-1ab2-4a55-8e72-e526da34249b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:30:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.238\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sx96w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sx96w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-93-234,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.93.234,PodIP:192.168.7.238,StartTime:2023-07-08 13:30:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.238,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.261: INFO: Pod "webserver-deployment-7b75d79cf5-jg8pl" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-jg8pl webserver-deployment-7b75d79cf5- deployment-6168  5577d6b1-f225-455a-b2ec-c6c0b1c3837a 40417 0 2023-07-08 13:30:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 85da5943-1ab2-4a55-8e72-e526da34249b 0xc004aa29c7 0xc004aa29c8}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85da5943-1ab2-4a55-8e72-e526da34249b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:30:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.164.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hxfvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxfvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-29-188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.29.188,PodIP:192.168.164.89,StartTime:2023-07-08 13:30:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.164.89,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.261: INFO: Pod "webserver-deployment-7b75d79cf5-kt2qq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-kt2qq webserver-deployment-7b75d79cf5- deployment-6168  330c5c5f-cde3-49e8-98d5-313aa0b63f42 40454 0 2023-07-08 13:30:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 85da5943-1ab2-4a55-8e72-e526da34249b 0xc004aa2be7 0xc004aa2be8}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85da5943-1ab2-4a55-8e72-e526da34249b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cr796,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cr796,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.262: INFO: Pod "webserver-deployment-7b75d79cf5-lpf7b" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lpf7b webserver-deployment-7b75d79cf5- deployment-6168  f9e8f798-956c-4525-895c-f6c0a2a0dd44 40421 0 2023-07-08 13:30:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 85da5943-1ab2-4a55-8e72-e526da34249b 0xc004aa2d37 0xc004aa2d38}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85da5943-1ab2-4a55-8e72-e526da34249b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:30:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.164.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gvx2f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gvx2f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-29-188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.29.188,PodIP:192.168.164.111,StartTime:2023-07-08 13:30:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.164.111,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.262: INFO: Pod "webserver-deployment-7b75d79cf5-lw4tj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lw4tj webserver-deployment-7b75d79cf5- deployment-6168  b336f90d-62e1-458e-886d-6ea55193dbe1 40405 0 2023-07-08 13:30:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 85da5943-1ab2-4a55-8e72-e526da34249b 0xc004aa2f77 0xc004aa2f78}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85da5943-1ab2-4a55-8e72-e526da34249b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:30:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.59.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4dbkm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4dbkm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.67,PodIP:192.168.59.19,StartTime:2023-07-08 13:30:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.59.19,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.262: INFO: Pod "webserver-deployment-7b75d79cf5-m6djv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-m6djv webserver-deployment-7b75d79cf5- deployment-6168  edf81ce8-abd8-4ea9-a685-75933b918357 40457 0 2023-07-08 13:30:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 85da5943-1ab2-4a55-8e72-e526da34249b 0xc004aa31a7 0xc004aa31a8}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85da5943-1ab2-4a55-8e72-e526da34249b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfs6p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfs6p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.262: INFO: Pod "webserver-deployment-7b75d79cf5-wg5qw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-wg5qw webserver-deployment-7b75d79cf5- deployment-6168  ece69c5c-20ae-466d-a11d-fcbf72bd580c 40455 0 2023-07-08 13:30:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 85da5943-1ab2-4a55-8e72-e526da34249b 0xc004aa32f7 0xc004aa32f8}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85da5943-1ab2-4a55-8e72-e526da34249b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-psr26,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-psr26,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.262: INFO: Pod "webserver-deployment-7b75d79cf5-x2w65" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-x2w65 webserver-deployment-7b75d79cf5- deployment-6168  66781278-a6f4-4c60-831a-5097a791f272 40453 0 2023-07-08 13:30:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 85da5943-1ab2-4a55-8e72-e526da34249b 0xc004aa3447 0xc004aa3448}] [] [{kube-controller-manager Update v1 2023-07-08 13:30:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85da5943-1ab2-4a55-8e72-e526da34249b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4k85q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4k85q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-93-234,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:30:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:30:27.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6168" for this suite. @ 07/08/23 13:30:27.27
• [4.260 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 07/08/23 13:30:27.281
  Jul  8 13:30:27.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename secrets @ 07/08/23 13:30:27.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:30:27.326
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:30:27.329
  STEP: Creating secret with name secret-test-map-4a123dea-8f99-44d8-b2c8-308261f01ffb @ 07/08/23 13:30:27.332
  E0708 13:30:27.336719      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a pod to test consume secrets @ 07/08/23 13:30:27.338
  E0708 13:30:28.337273      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:29.337356      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:30.337669      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:31.338704      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:30:31.369
  Jul  8 13:30:31.374: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-secrets-364f33c8-4378-4e2c-b1cf-b66fce71d27c container secret-volume-test: <nil>
  STEP: delete the pod @ 07/08/23 13:30:31.385
  Jul  8 13:30:31.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7778" for this suite. @ 07/08/23 13:30:31.409
• [4.136 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 07/08/23 13:30:31.417
  Jul  8 13:30:31.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/08/23 13:30:31.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:30:31.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:30:31.44
  Jul  8 13:30:31.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 13:30:32.339410      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:30:32.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2991" for this suite. @ 07/08/23 13:30:32.476
• [1.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 07/08/23 13:30:32.485
  Jul  8 13:30:32.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename server-version @ 07/08/23 13:30:32.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:30:32.503
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:30:32.506
  STEP: Request ServerVersion @ 07/08/23 13:30:32.509
  STEP: Confirm major version @ 07/08/23 13:30:32.511
  Jul  8 13:30:32.511: INFO: Major version: 1
  STEP: Confirm minor version @ 07/08/23 13:30:32.511
  Jul  8 13:30:32.511: INFO: cleanMinorVersion: 27
  Jul  8 13:30:32.511: INFO: Minor version: 27
  Jul  8 13:30:32.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-6339" for this suite. @ 07/08/23 13:30:32.515
• [0.038 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 07/08/23 13:30:32.523
  Jul  8 13:30:32.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename statefulset @ 07/08/23 13:30:32.523
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:30:32.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:30:32.546
  STEP: Creating service test in namespace statefulset-8645 @ 07/08/23 13:30:32.549
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 07/08/23 13:30:32.555
  STEP: Creating stateful set ss in namespace statefulset-8645 @ 07/08/23 13:30:32.562
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8645 @ 07/08/23 13:30:32.568
  Jul  8 13:30:32.575: INFO: Found 0 stateful pods, waiting for 1
  E0708 13:30:33.340284      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:34.340357      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:35.341103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:36.341430      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:37.341596      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:38.341676      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:39.341790      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:40.341887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:41.342198      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:42.342275      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:30:42.581: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 07/08/23 13:30:42.581
  Jul  8 13:30:42.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-8645 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  8 13:30:42.696: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  8 13:30:42.696: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  8 13:30:42.696: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  8 13:30:42.700: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0708 13:30:43.343045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:44.343147      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:45.343538      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:46.343604      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:47.343706      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:48.344700      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:49.345104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:50.345283      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:51.345653      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:30:52.345850      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:30:52.705: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul  8 13:30:52.705: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  8 13:30:52.724: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999882s
  E0708 13:30:53.346752      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:30:53.728: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996010916s
  E0708 13:30:54.346860      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:30:54.733: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991114556s
  E0708 13:30:55.346890      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:30:55.737: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986761659s
  E0708 13:30:56.347217      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:30:56.742: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982446077s
  E0708 13:30:57.347753      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:30:57.746: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.977878298s
  E0708 13:30:58.348569      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:30:58.752: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972180415s
  E0708 13:30:59.348658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:30:59.757: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.967111592s
  E0708 13:31:00.349218      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:31:00.762: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.962415019s
  E0708 13:31:01.349993      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:31:01.766: INFO: Verifying statefulset ss doesn't scale past 1 for another 957.647605ms
  E0708 13:31:02.350085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8645 @ 07/08/23 13:31:02.767
  Jul  8 13:31:02.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-8645 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  8 13:31:02.870: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul  8 13:31:02.870: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  8 13:31:02.870: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul  8 13:31:02.875: INFO: Found 1 stateful pods, waiting for 3
  E0708 13:31:03.350571      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:04.350666      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:05.351653      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:06.352249      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:07.353102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:08.353184      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:09.353273      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:10.353513      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:11.353601      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:12.353791      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:31:12.880: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul  8 13:31:12.880: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul  8 13:31:12.880: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 07/08/23 13:31:12.88
  STEP: Scale down will halt with unhealthy stateful pod @ 07/08/23 13:31:12.88
  Jul  8 13:31:12.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-8645 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  8 13:31:12.990: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  8 13:31:12.991: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  8 13:31:12.991: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  8 13:31:12.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-8645 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  8 13:31:13.093: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  8 13:31:13.093: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  8 13:31:13.093: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  8 13:31:13.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-8645 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul  8 13:31:13.203: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul  8 13:31:13.203: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul  8 13:31:13.203: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul  8 13:31:13.203: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  8 13:31:13.207: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0708 13:31:13.354726      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:14.354837      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:15.355022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:16.355331      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:17.355492      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:18.355683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:19.355815      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:20.356000      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:21.356031      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:22.357089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:31:23.217: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul  8 13:31:23.217: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jul  8 13:31:23.217: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jul  8 13:31:23.231: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999866s
  E0708 13:31:23.357301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:31:24.235: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996240751s
  E0708 13:31:24.358006      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:31:25.240: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991540654s
  E0708 13:31:25.358959      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:31:26.246: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985731689s
  E0708 13:31:26.359275      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:31:27.251: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980831161s
  E0708 13:31:27.359399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:31:28.255: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976254306s
  E0708 13:31:28.360099      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:31:29.260: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.971403749s
  E0708 13:31:29.360346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:31:30.265: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.966483279s
  E0708 13:31:30.360522      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:31:31.270: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.961289242s
  E0708 13:31:31.361444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:31:32.275: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.038114ms
  E0708 13:31:32.362045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8645 @ 07/08/23 13:31:33.276
  Jul  8 13:31:33.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-8645 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0708 13:31:33.362234      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:31:33.390: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul  8 13:31:33.390: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  8 13:31:33.390: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul  8 13:31:33.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-8645 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  8 13:31:33.492: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul  8 13:31:33.492: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  8 13:31:33.492: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul  8 13:31:33.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=statefulset-8645 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul  8 13:31:33.598: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul  8 13:31:33.598: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul  8 13:31:33.598: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul  8 13:31:33.598: INFO: Scaling statefulset ss to 0
  E0708 13:31:34.362300      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:35.363055      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:36.363532      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:37.364449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:38.365107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:39.365195      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:40.365396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:41.366395      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:42.366562      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:43.367181      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 07/08/23 13:31:43.617
  Jul  8 13:31:43.617: INFO: Deleting all statefulset in ns statefulset-8645
  Jul  8 13:31:43.621: INFO: Scaling statefulset ss to 0
  Jul  8 13:31:43.633: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  8 13:31:43.636: INFO: Deleting statefulset ss
  Jul  8 13:31:43.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8645" for this suite. @ 07/08/23 13:31:43.654
• [71.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 07/08/23 13:31:43.661
  Jul  8 13:31:43.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename svcaccounts @ 07/08/23 13:31:43.662
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:31:43.678
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:31:43.681
  Jul  8 13:31:43.697: INFO: created pod
  E0708 13:31:44.367354      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:45.367817      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:46.368802      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:47.369122      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:31:47.715
  E0708 13:31:48.369282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:49.370246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:50.370749      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:51.371155      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:52.371290      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:53.371470      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:54.371560      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:55.371694      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:56.371874      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:57.372023      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:58.373089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:31:59.373566      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:00.373659      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:01.374695      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:02.374787      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:03.375333      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:04.375423      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:05.375618      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:06.376459      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:07.377093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:08.377244      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:09.377403      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:10.377566      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:11.377997      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:12.378683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:13.379030      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:14.378988      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:15.379676      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:16.380556      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:17.381563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:32:17.716: INFO: polling logs
  Jul  8 13:32:17.732: INFO: Pod logs: 
  I0708 13:31:44.354612       1 log.go:198] OK: Got token
  I0708 13:31:44.354650       1 log.go:198] validating with in-cluster discovery
  I0708 13:31:44.354905       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0708 13:31:44.354931       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4918:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1688823703, NotBefore:1688823103, IssuedAt:1688823103, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4918", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"1c2a4a24-4c87-48a0-9cc4-2dce862e486e"}}}
  I0708 13:31:44.361742       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0708 13:31:44.365945       1 log.go:198] OK: Validated signature on JWT
  I0708 13:31:44.366029       1 log.go:198] OK: Got valid claims from token!
  I0708 13:31:44.366078       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4918:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1688823703, NotBefore:1688823103, IssuedAt:1688823103, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4918", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"1c2a4a24-4c87-48a0-9cc4-2dce862e486e"}}}

  Jul  8 13:32:17.732: INFO: completed pod
  Jul  8 13:32:17.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4918" for this suite. @ 07/08/23 13:32:17.743
• [34.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 07/08/23 13:32:17.751
  Jul  8 13:32:17.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/08/23 13:32:17.752
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:32:17.769
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:32:17.772
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 07/08/23 13:32:17.776
  Jul  8 13:32:17.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 13:32:18.382086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:32:19.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 13:32:19.382935      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:20.383575      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:21.383641      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:22.384086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:23.384503      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:32:24.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8403" for this suite. @ 07/08/23 13:32:24.168
• [6.424 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 07/08/23 13:32:24.175
  Jul  8 13:32:24.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl @ 07/08/23 13:32:24.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:32:24.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:32:24.195
  STEP: validating api versions @ 07/08/23 13:32:24.199
  Jul  8 13:32:24.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7568 api-versions'
  Jul  8 13:32:24.242: INFO: stderr: ""
  Jul  8 13:32:24.242: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Jul  8 13:32:24.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7568" for this suite. @ 07/08/23 13:32:24.247
• [0.078 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 07/08/23 13:32:24.254
  Jul  8 13:32:24.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename replication-controller @ 07/08/23 13:32:24.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:32:24.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:32:24.276
  Jul  8 13:32:24.283: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 07/08/23 13:32:24.293
  STEP: Checking rc "condition-test" has the desired failure condition set @ 07/08/23 13:32:24.298
  E0708 13:32:24.384706      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 07/08/23 13:32:25.306
  Jul  8 13:32:25.315: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 07/08/23 13:32:25.315
  E0708 13:32:25.385628      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:32:26.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4855" for this suite. @ 07/08/23 13:32:26.33
• [2.083 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 07/08/23 13:32:26.337
  Jul  8 13:32:26.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename deployment @ 07/08/23 13:32:26.338
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:32:26.354
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:32:26.358
  Jul  8 13:32:26.370: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0708 13:32:26.386504      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:27.386645      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:28.386832      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:29.387098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:30.387255      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:32:31.376: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/08/23 13:32:31.376
  Jul  8 13:32:31.377: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0708 13:32:31.388092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:32.388193      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:32:33.382: INFO: Creating deployment "test-rollover-deployment"
  E0708 13:32:33.388454      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:32:33.390: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0708 13:32:34.388835      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:35.388920      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:32:35.401: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Jul  8 13:32:35.408: INFO: Ensure that both replica sets have 1 created replica
  Jul  8 13:32:35.416: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Jul  8 13:32:35.425: INFO: Updating deployment test-rollover-deployment
  Jul  8 13:32:35.425: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0708 13:32:36.389346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:37.389983      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:32:37.435: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Jul  8 13:32:37.442: INFO: Make sure deployment "test-rollover-deployment" is complete
  Jul  8 13:32:37.452: INFO: all replica sets need to contain the pod-template-hash label
  Jul  8 13:32:37.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 13, 32, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 13, 32, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 13, 32, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 13, 32, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0708 13:32:38.390447      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:39.390623      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:32:39.460: INFO: all replica sets need to contain the pod-template-hash label
  Jul  8 13:32:39.460: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 13, 32, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 13, 32, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 13, 32, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 13, 32, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0708 13:32:40.390805      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:41.391144      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:32:41.461: INFO: all replica sets need to contain the pod-template-hash label
  Jul  8 13:32:41.461: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 13, 32, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 13, 32, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 13, 32, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 13, 32, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0708 13:32:42.391229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:43.391775      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:32:43.461: INFO: all replica sets need to contain the pod-template-hash label
  Jul  8 13:32:43.461: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 13, 32, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 13, 32, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 13, 32, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 13, 32, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0708 13:32:44.392092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:45.393103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:32:45.461: INFO: all replica sets need to contain the pod-template-hash label
  Jul  8 13:32:45.461: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 13, 32, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 13, 32, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 8, 13, 32, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 8, 13, 32, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0708 13:32:46.393475      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:47.393578      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:32:47.460: INFO: 
  Jul  8 13:32:47.460: INFO: Ensure that both old replica sets have no replicas
  Jul  8 13:32:47.471: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2762  a212e097-0ba2-4393-853c-49c166f14ae2 41592 2 2023-07-08 13:32:33 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-08 13:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 13:32:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004dd08a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-08 13:32:33 +0000 UTC,LastTransitionTime:2023-07-08 13:32:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-07-08 13:32:46 +0000 UTC,LastTransitionTime:2023-07-08 13:32:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul  8 13:32:47.475: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-2762  63c8dcc0-1a63-4801-b3dc-6bb267054a61 41581 2 2023-07-08 13:32:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment a212e097-0ba2-4393-853c-49c166f14ae2 0xc004d08f37 0xc004d08f38}] [] [{kube-controller-manager Update apps/v1 2023-07-08 13:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a212e097-0ba2-4393-853c-49c166f14ae2\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 13:32:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e86018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul  8 13:32:47.475: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Jul  8 13:32:47.475: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2762  0eb6b7ff-a913-4782-b644-4eff9f5b238d 41591 2 2023-07-08 13:32:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment a212e097-0ba2-4393-853c-49c166f14ae2 0xc0047fad37 0xc0047fad38}] [] [{e2e.test Update apps/v1 2023-07-08 13:32:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 13:32:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a212e097-0ba2-4393-853c-49c166f14ae2\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-08 13:32:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004d081e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul  8 13:32:47.475: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-2762  33098e60-d4b4-4cc7-badf-a8d8a7cfcd62 41540 2 2023-07-08 13:32:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment a212e097-0ba2-4393-853c-49c166f14ae2 0xc002e86097 0xc002e86098}] [] [{kube-controller-manager Update apps/v1 2023-07-08 13:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a212e097-0ba2-4393-853c-49c166f14ae2\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 13:32:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e86148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul  8 13:32:47.479: INFO: Pod "test-rollover-deployment-57777854c9-cd85j" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-cd85j test-rollover-deployment-57777854c9- deployment-2762  e60e2bdf-2cc3-4b66-b401-66ff9b52851f 41560 0 2023-07-08 13:32:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 63c8dcc0-1a63-4801-b3dc-6bb267054a61 0xc004aa32e7 0xc004aa32e8}] [] [{kube-controller-manager Update v1 2023-07-08 13:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"63c8dcc0-1a63-4801-b3dc-6bb267054a61\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:32:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.164.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c8pb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c8pb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-29-188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:32:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:32:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:32:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:32:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.29.188,PodIP:192.168.164.115,StartTime:2023-07-08 13:32:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-08 13:32:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://e0d7ec52385d14f14829ff9c3921ce854c62d4ce711b4a2a7a2ec7839c32a62d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.164.115,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:32:47.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2762" for this suite. @ 07/08/23 13:32:47.483
• [21.153 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 07/08/23 13:32:47.49
  Jul  8 13:32:47.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename services @ 07/08/23 13:32:47.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:32:47.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:32:47.511
  STEP: creating service in namespace services-5528 @ 07/08/23 13:32:47.514
  STEP: creating service affinity-clusterip-transition in namespace services-5528 @ 07/08/23 13:32:47.514
  STEP: creating replication controller affinity-clusterip-transition in namespace services-5528 @ 07/08/23 13:32:47.524
  I0708 13:32:47.531773      20 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-5528, replica count: 3
  E0708 13:32:48.393975      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:49.393984      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:50.394072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0708 13:32:50.582449      20 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  8 13:32:50.590: INFO: Creating new exec pod
  E0708 13:32:51.394105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:52.394195      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:53.394284      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:32:53.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-5528 exec execpod-affinityb47t8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Jul  8 13:32:53.715: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Jul  8 13:32:53.715: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 13:32:53.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-5528 exec execpod-affinityb47t8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.247 80'
  Jul  8 13:32:53.826: INFO: stderr: "+ nc -v -t -w 2 10.152.183.247 80\n+ echo hostName\nConnection to 10.152.183.247 80 port [tcp/http] succeeded!\n"
  Jul  8 13:32:53.826: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 13:32:53.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-5528 exec execpod-affinityb47t8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.247:80/ ; done'
  Jul  8 13:32:53.993: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n"
  Jul  8 13:32:53.993: INFO: stdout: "\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-5dljz\naffinity-clusterip-transition-2zgd7\naffinity-clusterip-transition-5dljz\naffinity-clusterip-transition-2zgd7\naffinity-clusterip-transition-2zgd7\naffinity-clusterip-transition-2zgd7\naffinity-clusterip-transition-2zgd7\naffinity-clusterip-transition-5dljz\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-2zgd7\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-5dljz\naffinity-clusterip-transition-pdvw4"
  Jul  8 13:32:53.993: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:53.993: INFO: Received response from host: affinity-clusterip-transition-5dljz
  Jul  8 13:32:53.993: INFO: Received response from host: affinity-clusterip-transition-2zgd7
  Jul  8 13:32:53.993: INFO: Received response from host: affinity-clusterip-transition-5dljz
  Jul  8 13:32:53.993: INFO: Received response from host: affinity-clusterip-transition-2zgd7
  Jul  8 13:32:53.993: INFO: Received response from host: affinity-clusterip-transition-2zgd7
  Jul  8 13:32:53.993: INFO: Received response from host: affinity-clusterip-transition-2zgd7
  Jul  8 13:32:53.993: INFO: Received response from host: affinity-clusterip-transition-2zgd7
  Jul  8 13:32:53.993: INFO: Received response from host: affinity-clusterip-transition-5dljz
  Jul  8 13:32:53.993: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:53.993: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:53.993: INFO: Received response from host: affinity-clusterip-transition-2zgd7
  Jul  8 13:32:53.993: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:53.993: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:53.993: INFO: Received response from host: affinity-clusterip-transition-5dljz
  Jul  8 13:32:53.993: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-5528 exec execpod-affinityb47t8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.247:80/ ; done'
  Jul  8 13:32:54.159: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.247:80/\n"
  Jul  8 13:32:54.159: INFO: stdout: "\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4\naffinity-clusterip-transition-pdvw4"
  Jul  8 13:32:54.159: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.159: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.159: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.159: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.160: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.160: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.160: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.160: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.160: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.160: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.160: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.160: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.160: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.160: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.160: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.160: INFO: Received response from host: affinity-clusterip-transition-pdvw4
  Jul  8 13:32:54.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  8 13:32:54.165: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5528, will wait for the garbage collector to delete the pods @ 07/08/23 13:32:54.178
  Jul  8 13:32:54.239: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.007048ms
  Jul  8 13:32:54.340: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.365312ms
  E0708 13:32:54.394365      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:55.395304      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-5528" for this suite. @ 07/08/23 13:32:56.257
• [8.774 seconds]
------------------------------
SSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 07/08/23 13:32:56.264
  Jul  8 13:32:56.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename ingressclass @ 07/08/23 13:32:56.265
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:32:56.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:32:56.285
  STEP: getting /apis @ 07/08/23 13:32:56.288
  STEP: getting /apis/networking.k8s.io @ 07/08/23 13:32:56.292
  STEP: getting /apis/networking.k8s.iov1 @ 07/08/23 13:32:56.294
  STEP: creating @ 07/08/23 13:32:56.295
  STEP: getting @ 07/08/23 13:32:56.309
  STEP: listing @ 07/08/23 13:32:56.313
  STEP: watching @ 07/08/23 13:32:56.316
  Jul  8 13:32:56.316: INFO: starting watch
  STEP: patching @ 07/08/23 13:32:56.318
  STEP: updating @ 07/08/23 13:32:56.323
  Jul  8 13:32:56.327: INFO: waiting for watch events with expected annotations
  Jul  8 13:32:56.327: INFO: saw patched and updated annotations
  STEP: deleting @ 07/08/23 13:32:56.327
  STEP: deleting a collection @ 07/08/23 13:32:56.34
  Jul  8 13:32:56.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-2177" for this suite. @ 07/08/23 13:32:56.361
• [0.104 seconds]
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 07/08/23 13:32:56.368
  Jul  8 13:32:56.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-probe @ 07/08/23 13:32:56.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:32:56.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:32:56.387
  STEP: Creating pod liveness-7c519bf6-6fdb-4cdc-9ba8-14f450f54bfa in namespace container-probe-3210 @ 07/08/23 13:32:56.392
  E0708 13:32:56.395861      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:57.396028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:32:58.396100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:32:58.409: INFO: Started pod liveness-7c519bf6-6fdb-4cdc-9ba8-14f450f54bfa in namespace container-probe-3210
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/08/23 13:32:58.409
  Jul  8 13:32:58.413: INFO: Initial restart count of pod liveness-7c519bf6-6fdb-4cdc-9ba8-14f450f54bfa is 0
  E0708 13:32:59.397078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:00.397162      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:01.397244      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:02.398285      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:03.398373      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:04.399072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:05.399749      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:06.399973      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:07.400055      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:08.401097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:09.401157      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:10.401335      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:11.402178      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:12.402776      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:13.403129      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:14.403168      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:15.403233      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:16.403555      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:17.404041      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:18.405099      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:19.405189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:20.405276      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:21.405371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:22.405529      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:23.405642      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:24.405715      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:25.405798      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:26.406093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:27.406972      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:28.407893      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:29.408278      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:30.408346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:31.409007      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:32.409183      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:33.409699      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:34.409788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:35.409881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:36.410068      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:37.410168      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:38.410351      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:39.410423      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:40.410599      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:41.411203      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:42.411568      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:43.412039      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:44.412214      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:45.412290      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:46.412363      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:47.412459      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:48.413104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:49.413849      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:50.414384      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:51.415046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:52.415137      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:53.415219      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:54.415806      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:55.416773      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:56.416968      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:57.417068      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:58.417200      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:33:59.417289      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:00.417469      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:01.417960      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:02.418201      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:03.418316      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:04.418408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:05.418487      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:06.418563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:07.418671      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:08.419675      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:09.419753      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:10.419930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:11.420054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:12.421097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:13.421187      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:14.421354      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:15.422130      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:16.422444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:17.422549      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:18.422714      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:19.422824      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:20.423437      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:21.424032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:22.425110      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:23.425194      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:24.425420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:25.425707      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:26.425912      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:27.425996      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:28.426087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:29.426173      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:30.426344      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:31.427357      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:32.427509      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:33.427588      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:34.427657      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:35.428576      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:36.428803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:37.429370      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:38.429535      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:39.429767      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:40.430644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:41.431443      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:42.431598      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:43.431685      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:44.431843      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:45.432724      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:46.432861      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:47.433526      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:48.433936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:49.434176      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:50.434337      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:51.434401      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:52.434544      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:53.434719      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:54.434890      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:55.435767      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:56.436513      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:57.436899      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:58.437507      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:34:59.437762      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:00.437899      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:01.438357      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:02.438468      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:03.439399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:04.439547      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:05.439663      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:06.440547      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:07.440631      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:08.441097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:09.441469      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:10.442448      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:11.442522      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:12.442592      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:13.442684      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:14.442842      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:15.443177      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:16.443428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:17.444248      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:18.444325      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:19.444415      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:20.445468      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:21.445988      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:22.446151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:23.446539      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:24.447160      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:25.448010      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:26.448025      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:27.448603      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:28.449083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:29.449305      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:30.449461      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:31.449908      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:32.450070      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:33.450206      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:34.450360      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:35.450440      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:36.450628      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:37.450835      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:38.450913      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:39.450953      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:40.451103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:41.451180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:42.451331      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:43.451419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:44.451579      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:45.452294      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:46.452456      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:47.452789      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:48.452874      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:49.452964      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:50.453115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:51.453978      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:52.454072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:53.454385      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:54.454477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:55.455066      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:56.455338      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:57.455400      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:58.455573      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:35:59.456018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:00.456134      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:01.457088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:02.457207      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:03.458262      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:04.459246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:05.460131      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:06.460216      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:07.461044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:08.461240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:09.461305      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:10.461464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:11.462270      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:12.462459      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:13.462653      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:14.462825      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:15.463110      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:16.463353      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:17.463583      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:18.463724      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:19.463817      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:20.463994      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:21.465029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:22.465185      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:23.465273      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:24.468139      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:25.468649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:26.469587      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:27.470222      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:28.470315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:29.470375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:30.470462      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:31.471039      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:32.471562      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:33.472392      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:34.472479      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:35.472563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:36.472764      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:37.473813      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:38.473906      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:39.473990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:40.474086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:41.474925      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:42.474975      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:43.475243      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:44.475441      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:45.476234      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:46.476438      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:47.476595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:48.477081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:49.477319      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:50.477470      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:51.478136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:52.478499      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:53.479085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:54.479239      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:55.479838      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:56.480555      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:57.480637      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:36:58.481084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:36:58.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/08/23 13:36:59.001
  STEP: Destroying namespace "container-probe-3210" for this suite. @ 07/08/23 13:36:59.017
• [242.656 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 07/08/23 13:36:59.024
  Jul  8 13:36:59.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename statefulset @ 07/08/23 13:36:59.024
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:36:59.041
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:36:59.044
  STEP: Creating service test in namespace statefulset-4905 @ 07/08/23 13:36:59.048
  STEP: Creating statefulset ss in namespace statefulset-4905 @ 07/08/23 13:36:59.058
  Jul  8 13:36:59.067: INFO: Found 0 stateful pods, waiting for 1
  E0708 13:36:59.481267      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:00.481374      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:01.481470      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:02.481656      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:03.481741      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:04.481895      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:05.482008      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:06.482281      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:07.482399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:08.482487      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:37:09.072: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 07/08/23 13:37:09.08
  STEP: Getting /status @ 07/08/23 13:37:09.087
  Jul  8 13:37:09.091: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 07/08/23 13:37:09.091
  Jul  8 13:37:09.100: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 07/08/23 13:37:09.1
  Jul  8 13:37:09.102: INFO: Observed &StatefulSet event: ADDED
  Jul  8 13:37:09.102: INFO: Found Statefulset ss in namespace statefulset-4905 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul  8 13:37:09.102: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 07/08/23 13:37:09.102
  Jul  8 13:37:09.102: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul  8 13:37:09.110: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 07/08/23 13:37:09.11
  Jul  8 13:37:09.112: INFO: Observed &StatefulSet event: ADDED
  Jul  8 13:37:09.112: INFO: Observed Statefulset ss in namespace statefulset-4905 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul  8 13:37:09.112: INFO: Observed &StatefulSet event: MODIFIED
  Jul  8 13:37:09.112: INFO: Deleting all statefulset in ns statefulset-4905
  Jul  8 13:37:09.116: INFO: Scaling statefulset ss to 0
  E0708 13:37:09.483104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:10.483199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:11.483260      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:12.483359      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:13.483541      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:14.483783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:15.483952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:16.484042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:17.485088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:18.485241      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:37:19.134: INFO: Waiting for statefulset status.replicas updated to 0
  Jul  8 13:37:19.138: INFO: Deleting statefulset ss
  Jul  8 13:37:19.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4905" for this suite. @ 07/08/23 13:37:19.156
• [20.139 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 07/08/23 13:37:19.163
  Jul  8 13:37:19.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename resourcequota @ 07/08/23 13:37:19.163
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:37:19.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:37:19.183
  STEP: Counting existing ResourceQuota @ 07/08/23 13:37:19.186
  E0708 13:37:19.486010      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:20.487025      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:21.487420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:22.487805      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:23.487896      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/08/23 13:37:24.19
  STEP: Ensuring resource quota status is calculated @ 07/08/23 13:37:24.197
  E0708 13:37:24.488711      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:25.488826      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 07/08/23 13:37:26.201
  STEP: Creating a NodePort Service @ 07/08/23 13:37:26.217
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 07/08/23 13:37:26.238
  STEP: Ensuring resource quota status captures service creation @ 07/08/23 13:37:26.262
  E0708 13:37:26.489837      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:27.489963      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 07/08/23 13:37:28.266
  STEP: Ensuring resource quota status released usage @ 07/08/23 13:37:28.299
  E0708 13:37:28.490193      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:29.490300      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:37:30.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-537" for this suite. @ 07/08/23 13:37:30.308
• [11.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 07/08/23 13:37:30.315
  Jul  8 13:37:30.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename subpath @ 07/08/23 13:37:30.316
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:37:30.335
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:37:30.338
  STEP: Setting up data @ 07/08/23 13:37:30.341
  STEP: Creating pod pod-subpath-test-secret-tx75 @ 07/08/23 13:37:30.352
  STEP: Creating a pod to test atomic-volume-subpath @ 07/08/23 13:37:30.352
  E0708 13:37:30.491325      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:31.491576      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:32.492648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:33.493104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:34.493670      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:35.493907      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:36.494386      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:37.494506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:38.495412      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:39.495604      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:40.496111      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:41.496413      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:42.497040      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:43.497131      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:44.497302      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:45.497428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:46.498015      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:47.498194      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:48.499222      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:49.499329      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:50.499665      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:51.499931      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:52.500234      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:53.501107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:37:54.426
  Jul  8 13:37:54.430: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-subpath-test-secret-tx75 container test-container-subpath-secret-tx75: <nil>
  STEP: delete the pod @ 07/08/23 13:37:54.45
  STEP: Deleting pod pod-subpath-test-secret-tx75 @ 07/08/23 13:37:54.464
  Jul  8 13:37:54.464: INFO: Deleting pod "pod-subpath-test-secret-tx75" in namespace "subpath-4699"
  Jul  8 13:37:54.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4699" for this suite. @ 07/08/23 13:37:54.472
• [24.163 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 07/08/23 13:37:54.479
  Jul  8 13:37:54.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename downward-api @ 07/08/23 13:37:54.48
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:37:54.495
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:37:54.498
  E0708 13:37:54.501650      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a pod to test downward API volume plugin @ 07/08/23 13:37:54.502
  E0708 13:37:55.501795      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:56.502111      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:57.502273      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:37:58.502484      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:37:58.525
  Jul  8 13:37:58.528: INFO: Trying to get logs from node ip-172-31-93-234 pod downwardapi-volume-d6aa3023-bb3e-4b4a-a0f6-cba96442280c container client-container: <nil>
  STEP: delete the pod @ 07/08/23 13:37:58.536
  Jul  8 13:37:58.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-665" for this suite. @ 07/08/23 13:37:58.557
• [4.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 07/08/23 13:37:58.567
  Jul  8 13:37:58.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename replication-controller @ 07/08/23 13:37:58.567
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:37:58.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:37:58.588
  STEP: Creating ReplicationController "e2e-rc-pnbn6" @ 07/08/23 13:37:58.592
  Jul  8 13:37:58.597: INFO: Get Replication Controller "e2e-rc-pnbn6" to confirm replicas
  E0708 13:37:59.503086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:37:59.601: INFO: Get Replication Controller "e2e-rc-pnbn6" to confirm replicas
  Jul  8 13:37:59.605: INFO: Found 1 replicas for "e2e-rc-pnbn6" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-pnbn6" @ 07/08/23 13:37:59.605
  STEP: Updating a scale subresource @ 07/08/23 13:37:59.609
  STEP: Verifying replicas where modified for replication controller "e2e-rc-pnbn6" @ 07/08/23 13:37:59.615
  Jul  8 13:37:59.615: INFO: Get Replication Controller "e2e-rc-pnbn6" to confirm replicas
  E0708 13:38:00.503189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:38:00.621: INFO: Get Replication Controller "e2e-rc-pnbn6" to confirm replicas
  Jul  8 13:38:00.626: INFO: Found 2 replicas for "e2e-rc-pnbn6" replication controller
  Jul  8 13:38:00.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1905" for this suite. @ 07/08/23 13:38:00.63
• [2.070 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 07/08/23 13:38:00.637
  Jul  8 13:38:00.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename deployment @ 07/08/23 13:38:00.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:38:00.66
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:38:00.663
  Jul  8 13:38:00.667: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Jul  8 13:38:00.676: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0708 13:38:01.503256      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:02.503448      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:03.503540      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:04.503647      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:05.503817      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:38:05.681: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/08/23 13:38:05.681
  Jul  8 13:38:05.681: INFO: Creating deployment "test-rolling-update-deployment"
  Jul  8 13:38:05.686: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Jul  8 13:38:05.695: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0708 13:38:06.504033      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:07.504134      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:38:07.705: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Jul  8 13:38:07.709: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Jul  8 13:38:07.720: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8090  52dc1d8d-017b-4756-828a-a5645f643e86 42742 1 2023-07-08 13:38:05 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-07-08 13:38:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 13:38:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00414ffd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-08 13:38:05 +0000 UTC,LastTransitionTime:2023-07-08 13:38:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-07-08 13:38:06 +0000 UTC,LastTransitionTime:2023-07-08 13:38:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul  8 13:38:07.723: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-8090  8180d26b-a796-4e17-a269-48ea0977e5a4 42732 1 2023-07-08 13:38:05 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 52dc1d8d-017b-4756-828a-a5645f643e86 0xc004483e17 0xc004483e18}] [] [{kube-controller-manager Update apps/v1 2023-07-08 13:38:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52dc1d8d-017b-4756-828a-a5645f643e86\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 13:38:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004483fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul  8 13:38:07.723: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Jul  8 13:38:07.724: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8090  e377875a-d7bb-4210-807a-883dc9c31793 42741 2 2023-07-08 13:38:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 52dc1d8d-017b-4756-828a-a5645f643e86 0xc004483ce7 0xc004483ce8}] [] [{e2e.test Update apps/v1 2023-07-08 13:38:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-08 13:38:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52dc1d8d-017b-4756-828a-a5645f643e86\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-08 13:38:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004483da8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul  8 13:38:07.727: INFO: Pod "test-rolling-update-deployment-656d657cd8-md7mh" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-md7mh test-rolling-update-deployment-656d657cd8- deployment-8090  e8988679-9503-4cbb-839e-ee44464797e9 42731 0 2023-07-08 13:38:05 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 8180d26b-a796-4e17-a269-48ea0977e5a4 0xc004dd0347 0xc004dd0348}] [] [{kube-controller-manager Update v1 2023-07-08 13:38:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8180d26b-a796-4e17-a269-48ea0977e5a4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-08 13:38:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lllr4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lllr4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-93-234,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:38:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:38:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:38:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-08 13:38:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.93.234,PodIP:192.168.7.251,StartTime:2023-07-08 13:38:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-08 13:38:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://d9393449da51f77c08777de4552be4d4df393612020a743bd82c8c48e754f480,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.251,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul  8 13:38:07.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8090" for this suite. @ 07/08/23 13:38:07.731
• [7.100 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 07/08/23 13:38:07.738
  Jul  8 13:38:07.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl @ 07/08/23 13:38:07.739
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:38:07.757
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:38:07.761
  STEP: creating a replication controller @ 07/08/23 13:38:07.764
  Jul  8 13:38:07.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 create -f -'
  E0708 13:38:08.505021      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:38:08.551: INFO: stderr: ""
  Jul  8 13:38:08.551: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/08/23 13:38:08.551
  Jul  8 13:38:08.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul  8 13:38:08.606: INFO: stderr: ""
  Jul  8 13:38:08.606: INFO: stdout: "update-demo-nautilus-bsrw7 update-demo-nautilus-sfksz "
  Jul  8 13:38:08.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods update-demo-nautilus-bsrw7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  8 13:38:08.654: INFO: stderr: ""
  Jul  8 13:38:08.654: INFO: stdout: ""
  Jul  8 13:38:08.654: INFO: update-demo-nautilus-bsrw7 is created but not running
  E0708 13:38:09.505075      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:10.505193      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:11.505341      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:12.505505      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:13.505698      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:38:13.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul  8 13:38:13.708: INFO: stderr: ""
  Jul  8 13:38:13.708: INFO: stdout: "update-demo-nautilus-bsrw7 update-demo-nautilus-sfksz "
  Jul  8 13:38:13.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods update-demo-nautilus-bsrw7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  8 13:38:13.756: INFO: stderr: ""
  Jul  8 13:38:13.756: INFO: stdout: "true"
  Jul  8 13:38:13.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods update-demo-nautilus-bsrw7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul  8 13:38:13.806: INFO: stderr: ""
  Jul  8 13:38:13.806: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul  8 13:38:13.806: INFO: validating pod update-demo-nautilus-bsrw7
  Jul  8 13:38:13.812: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul  8 13:38:13.812: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul  8 13:38:13.812: INFO: update-demo-nautilus-bsrw7 is verified up and running
  Jul  8 13:38:13.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods update-demo-nautilus-sfksz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  8 13:38:13.857: INFO: stderr: ""
  Jul  8 13:38:13.857: INFO: stdout: "true"
  Jul  8 13:38:13.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods update-demo-nautilus-sfksz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul  8 13:38:13.918: INFO: stderr: ""
  Jul  8 13:38:13.919: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul  8 13:38:13.919: INFO: validating pod update-demo-nautilus-sfksz
  Jul  8 13:38:13.925: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul  8 13:38:13.925: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul  8 13:38:13.925: INFO: update-demo-nautilus-sfksz is verified up and running
  STEP: scaling down the replication controller @ 07/08/23 13:38:13.925
  Jul  8 13:38:13.926: INFO: scanned /root for discovery docs: <nil>
  Jul  8 13:38:13.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0708 13:38:14.506510      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:38:14.989: INFO: stderr: ""
  Jul  8 13:38:14.989: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/08/23 13:38:14.989
  Jul  8 13:38:14.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul  8 13:38:15.040: INFO: stderr: ""
  Jul  8 13:38:15.040: INFO: stdout: "update-demo-nautilus-bsrw7 update-demo-nautilus-sfksz "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 07/08/23 13:38:15.04
  E0708 13:38:15.506583      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:16.506808      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:17.506968      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:18.507070      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:19.507257      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:38:20.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul  8 13:38:20.089: INFO: stderr: ""
  Jul  8 13:38:20.089: INFO: stdout: "update-demo-nautilus-bsrw7 "
  Jul  8 13:38:20.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods update-demo-nautilus-bsrw7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  8 13:38:20.139: INFO: stderr: ""
  Jul  8 13:38:20.139: INFO: stdout: "true"
  Jul  8 13:38:20.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods update-demo-nautilus-bsrw7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul  8 13:38:20.185: INFO: stderr: ""
  Jul  8 13:38:20.185: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul  8 13:38:20.185: INFO: validating pod update-demo-nautilus-bsrw7
  Jul  8 13:38:20.190: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul  8 13:38:20.190: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul  8 13:38:20.190: INFO: update-demo-nautilus-bsrw7 is verified up and running
  STEP: scaling up the replication controller @ 07/08/23 13:38:20.19
  Jul  8 13:38:20.191: INFO: scanned /root for discovery docs: <nil>
  Jul  8 13:38:20.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0708 13:38:20.508185      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:38:21.255: INFO: stderr: ""
  Jul  8 13:38:21.255: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/08/23 13:38:21.255
  Jul  8 13:38:21.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul  8 13:38:21.305: INFO: stderr: ""
  Jul  8 13:38:21.305: INFO: stdout: "update-demo-nautilus-bsrw7 update-demo-nautilus-gjmpc "
  Jul  8 13:38:21.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods update-demo-nautilus-bsrw7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  8 13:38:21.352: INFO: stderr: ""
  Jul  8 13:38:21.352: INFO: stdout: "true"
  Jul  8 13:38:21.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods update-demo-nautilus-bsrw7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul  8 13:38:21.399: INFO: stderr: ""
  Jul  8 13:38:21.399: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul  8 13:38:21.399: INFO: validating pod update-demo-nautilus-bsrw7
  Jul  8 13:38:21.404: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul  8 13:38:21.404: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul  8 13:38:21.405: INFO: update-demo-nautilus-bsrw7 is verified up and running
  Jul  8 13:38:21.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods update-demo-nautilus-gjmpc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul  8 13:38:21.452: INFO: stderr: ""
  Jul  8 13:38:21.452: INFO: stdout: "true"
  Jul  8 13:38:21.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods update-demo-nautilus-gjmpc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul  8 13:38:21.496: INFO: stderr: ""
  Jul  8 13:38:21.496: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul  8 13:38:21.496: INFO: validating pod update-demo-nautilus-gjmpc
  Jul  8 13:38:21.502: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul  8 13:38:21.502: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul  8 13:38:21.502: INFO: update-demo-nautilus-gjmpc is verified up and running
  STEP: using delete to clean up resources @ 07/08/23 13:38:21.503
  Jul  8 13:38:21.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 delete --grace-period=0 --force -f -'
  E0708 13:38:21.508368      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:38:21.552: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul  8 13:38:21.552: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jul  8 13:38:21.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get rc,svc -l name=update-demo --no-headers'
  Jul  8 13:38:21.601: INFO: stderr: "No resources found in kubectl-6350 namespace.\n"
  Jul  8 13:38:21.601: INFO: stdout: ""
  Jul  8 13:38:21.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-6350 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul  8 13:38:21.650: INFO: stderr: ""
  Jul  8 13:38:21.650: INFO: stdout: ""
  Jul  8 13:38:21.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6350" for this suite. @ 07/08/23 13:38:21.657
• [13.925 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 07/08/23 13:38:21.664
  Jul  8 13:38:21.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-probe @ 07/08/23 13:38:21.665
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:38:21.682
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:38:21.686
  STEP: Creating pod test-grpc-70e69774-83bd-4a76-bfe1-f88a3e66806b in namespace container-probe-3844 @ 07/08/23 13:38:21.689
  E0708 13:38:22.508422      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:23.509112      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:38:23.707: INFO: Started pod test-grpc-70e69774-83bd-4a76-bfe1-f88a3e66806b in namespace container-probe-3844
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/08/23 13:38:23.707
  Jul  8 13:38:23.711: INFO: Initial restart count of pod test-grpc-70e69774-83bd-4a76-bfe1-f88a3e66806b is 0
  E0708 13:38:24.509604      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:25.509313      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:26.509380      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:27.509536      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:28.509617      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:29.509788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:30.510070      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:31.510376      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:32.511124      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:33.511291      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:34.511368      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:35.511509      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:36.511690      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:37.511838      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:38.512021      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:39.512114      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:40.512220      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:41.512453      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:42.513092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:43.513504      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:44.513831      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:45.513985      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:46.514159      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:47.514317      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:48.515360      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:49.515443      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:50.515658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:51.516094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:52.516992      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:53.517081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:54.517969      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:55.518061      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:56.518144      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:57.518241      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:58.518634      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:38:59.518814      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:00.519474      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:01.520541      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:02.521088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:03.521270      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:04.521541      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:05.521708      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:06.522289      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:07.523102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:08.523186      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:09.523357      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:10.523445      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:11.523489      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:12.524332      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:13.525099      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:14.525208      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:15.525508      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:16.526054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:17.526206      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:18.526447      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:19.526742      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:20.526797      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:21.526890      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:22.527684      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:23.527784      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:24.528712      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:25.528807      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:26.528930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:27.529079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:39:27.868: INFO: Restart count of pod container-probe-3844/test-grpc-70e69774-83bd-4a76-bfe1-f88a3e66806b is now 1 (1m4.15631522s elapsed)
  Jul  8 13:39:27.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/08/23 13:39:27.872
  STEP: Destroying namespace "container-probe-3844" for this suite. @ 07/08/23 13:39:27.887
• [66.229 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 07/08/23 13:39:27.9
  Jul  8 13:39:27.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename container-runtime @ 07/08/23 13:39:27.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:39:27.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:39:27.921
  STEP: create the container @ 07/08/23 13:39:27.924
  W0708 13:39:27.934930      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 07/08/23 13:39:27.935
  E0708 13:39:28.529186      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:29.529752      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:30.529842      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/08/23 13:39:30.953
  STEP: the container should be terminated @ 07/08/23 13:39:30.957
  STEP: the termination message should be set @ 07/08/23 13:39:30.957
  Jul  8 13:39:30.957: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 07/08/23 13:39:30.957
  Jul  8 13:39:30.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3026" for this suite. @ 07/08/23 13:39:30.978
• [3.086 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 07/08/23 13:39:30.985
  Jul  8 13:39:30.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename var-expansion @ 07/08/23 13:39:30.986
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:39:31.003
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:39:31.006
  STEP: creating the pod @ 07/08/23 13:39:31.01
  STEP: waiting for pod running @ 07/08/23 13:39:31.017
  E0708 13:39:31.530184      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:32.530273      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 07/08/23 13:39:33.028
  Jul  8 13:39:33.032: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-937 PodName:var-expansion-a0c7bec0-19e1-4881-ae0a-6834d05796c7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:39:33.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:39:33.032: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:39:33.032: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-937/pods/var-expansion-a0c7bec0-19e1-4881-ae0a-6834d05796c7/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 07/08/23 13:39:33.093
  Jul  8 13:39:33.097: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-937 PodName:var-expansion-a0c7bec0-19e1-4881-ae0a-6834d05796c7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:39:33.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:39:33.097: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:39:33.097: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-937/pods/var-expansion-a0c7bec0-19e1-4881-ae0a-6834d05796c7/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 07/08/23 13:39:33.139
  E0708 13:39:33.531200      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:39:33.651: INFO: Successfully updated pod "var-expansion-a0c7bec0-19e1-4881-ae0a-6834d05796c7"
  STEP: waiting for annotated pod running @ 07/08/23 13:39:33.651
  STEP: deleting the pod gracefully @ 07/08/23 13:39:33.655
  Jul  8 13:39:33.655: INFO: Deleting pod "var-expansion-a0c7bec0-19e1-4881-ae0a-6834d05796c7" in namespace "var-expansion-937"
  Jul  8 13:39:33.664: INFO: Wait up to 5m0s for pod "var-expansion-a0c7bec0-19e1-4881-ae0a-6834d05796c7" to be fully deleted
  E0708 13:39:34.531319      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:35.531416      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:36.531486      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:37.531597      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:38.531677      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:39.531777      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:40.532090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:41.532181      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:42.532271      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:43.533087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:44.534030      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:45.534417      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:46.534535      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:47.534629      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:48.534717      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:49.534900      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:50.534989      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:51.535332      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:52.535438      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:53.535621      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:54.535664      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:55.535840      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:56.536026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:57.537088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:58.537202      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:39:59.537686      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:00.537782      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:01.537917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:02.538023      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:03.538251      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:04.538671      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:05.538773      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:40:05.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-937" for this suite. @ 07/08/23 13:40:05.751
• [34.773 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 07/08/23 13:40:05.761
  Jul  8 13:40:05.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename cronjob @ 07/08/23 13:40:05.762
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:40:05.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:40:05.786
  STEP: Creating a cronjob @ 07/08/23 13:40:05.789
  STEP: creating @ 07/08/23 13:40:05.789
  STEP: getting @ 07/08/23 13:40:05.797
  STEP: listing @ 07/08/23 13:40:05.801
  STEP: watching @ 07/08/23 13:40:05.804
  Jul  8 13:40:05.804: INFO: starting watch
  STEP: cluster-wide listing @ 07/08/23 13:40:05.806
  STEP: cluster-wide watching @ 07/08/23 13:40:05.809
  Jul  8 13:40:05.809: INFO: starting watch
  STEP: patching @ 07/08/23 13:40:05.811
  STEP: updating @ 07/08/23 13:40:05.818
  Jul  8 13:40:05.827: INFO: waiting for watch events with expected annotations
  Jul  8 13:40:05.827: INFO: saw patched and updated annotations
  STEP: patching /status @ 07/08/23 13:40:05.827
  STEP: updating /status @ 07/08/23 13:40:05.834
  STEP: get /status @ 07/08/23 13:40:05.842
  STEP: deleting @ 07/08/23 13:40:05.846
  STEP: deleting a collection @ 07/08/23 13:40:05.86
  Jul  8 13:40:05.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4929" for this suite. @ 07/08/23 13:40:05.876
• [0.121 seconds]
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 07/08/23 13:40:05.883
  Jul  8 13:40:05.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename var-expansion @ 07/08/23 13:40:05.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:40:05.899
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:40:05.902
  E0708 13:40:06.539646      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:07.539750      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:40:07.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  8 13:40:07.929: INFO: Deleting pod "var-expansion-31f20770-768c-4cf5-996a-5a166be2747e" in namespace "var-expansion-5655"
  Jul  8 13:40:07.936: INFO: Wait up to 5m0s for pod "var-expansion-31f20770-768c-4cf5-996a-5a166be2747e" to be fully deleted
  E0708 13:40:08.540096      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:09.540184      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-5655" for this suite. @ 07/08/23 13:40:09.945
• [4.069 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 07/08/23 13:40:09.953
  Jul  8 13:40:09.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename watch @ 07/08/23 13:40:09.953
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:40:09.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:40:09.974
  STEP: getting a starting resourceVersion @ 07/08/23 13:40:09.977
  STEP: starting a background goroutine to produce watch events @ 07/08/23 13:40:09.981
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 07/08/23 13:40:09.981
  E0708 13:40:10.541030      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:11.541580      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:12.542045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:40:12.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-6954" for this suite. @ 07/08/23 13:40:12.808
• [2.908 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 07/08/23 13:40:12.861
  Jul  8 13:40:12.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename webhook @ 07/08/23 13:40:12.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:40:12.99
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:40:12.993
  STEP: Setting up server cert @ 07/08/23 13:40:13.019
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/08/23 13:40:13.355
  STEP: Deploying the webhook pod @ 07/08/23 13:40:13.363
  STEP: Wait for the deployment to be ready @ 07/08/23 13:40:13.375
  Jul  8 13:40:13.385: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0708 13:40:13.542126      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:14.542255      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/08/23 13:40:15.397
  STEP: Verifying the service has paired with the endpoint @ 07/08/23 13:40:15.409
  E0708 13:40:15.543293      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:40:16.410: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 07/08/23 13:40:16.414
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/08/23 13:40:16.414
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 07/08/23 13:40:16.428
  E0708 13:40:16.544282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 07/08/23 13:40:17.439
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/08/23 13:40:17.439
  E0708 13:40:17.545005      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 07/08/23 13:40:18.468
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/08/23 13:40:18.468
  E0708 13:40:18.545457      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:19.545956      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:20.546874      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:21.547946      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:22.548715      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 07/08/23 13:40:23.504
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/08/23 13:40:23.504
  E0708 13:40:23.549362      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:24.549814      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:25.549577      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:26.549643      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:27.549820      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:40:28.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0708 13:40:28.550843      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-8990" for this suite. @ 07/08/23 13:40:28.598
  STEP: Destroying namespace "webhook-markers-7486" for this suite. @ 07/08/23 13:40:28.606
• [15.751 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 07/08/23 13:40:28.613
  Jul  8 13:40:28.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename services @ 07/08/23 13:40:28.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:40:28.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:40:28.633
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-3980 @ 07/08/23 13:40:28.636
  STEP: changing the ExternalName service to type=NodePort @ 07/08/23 13:40:28.642
  STEP: creating replication controller externalname-service in namespace services-3980 @ 07/08/23 13:40:28.664
  I0708 13:40:28.669866      20 runners.go:194] Created replication controller with name: externalname-service, namespace: services-3980, replica count: 2
  E0708 13:40:29.551204      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:30.551293      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:31.551394      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0708 13:40:31.720722      20 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  8 13:40:31.720: INFO: Creating new exec pod
  E0708 13:40:32.551475      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:33.551570      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:34.551659      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:40:34.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-3980 exec execpod7gzqc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul  8 13:40:34.843: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul  8 13:40:34.843: INFO: stdout: "externalname-service-2wqhg"
  Jul  8 13:40:34.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-3980 exec execpod7gzqc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.206 80'
  Jul  8 13:40:34.950: INFO: stderr: "+ nc -v -t -w 2 10.152.183.206 80\n+ echo hostName\nConnection to 10.152.183.206 80 port [tcp/http] succeeded!\n"
  Jul  8 13:40:34.950: INFO: stdout: "externalname-service-2wqhg"
  Jul  8 13:40:34.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-3980 exec execpod7gzqc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.29.188 31532'
  Jul  8 13:40:35.042: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.29.188 31532\nConnection to 172.31.29.188 31532 port [tcp/*] succeeded!\n"
  Jul  8 13:40:35.042: INFO: stdout: "externalname-service-2wqhg"
  Jul  8 13:40:35.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-3980 exec execpod7gzqc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.12.67 31532'
  Jul  8 13:40:35.134: INFO: stderr: "+ nc -v -t -w 2 172.31.12.67 31532\n+ echo hostName\nConnection to 172.31.12.67 31532 port [tcp/*] succeeded!\n"
  Jul  8 13:40:35.134: INFO: stdout: "externalname-service-2wqhg"
  Jul  8 13:40:35.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul  8 13:40:35.139: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-3980" for this suite. @ 07/08/23 13:40:35.16
• [6.555 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 07/08/23 13:40:35.168
  Jul  8 13:40:35.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename kubectl @ 07/08/23 13:40:35.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:40:35.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:40:35.188
  Jul  8 13:40:35.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=kubectl-7931 version'
  Jul  8 13:40:35.232: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Jul  8 13:40:35.232: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.3\", GitCommit:\"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:53:42Z\", GoVersion:\"go1.20.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.3\", GitCommit:\"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", GitTreeState:\"clean\", BuildDate:\"2023-06-15T02:06:40Z\", GoVersion:\"go1.20.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Jul  8 13:40:35.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7931" for this suite. @ 07/08/23 13:40:35.236
• [0.076 seconds]
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 07/08/23 13:40:35.244
  Jul  8 13:40:35.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 13:40:35.245
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:40:35.26
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:40:35.263
  STEP: Creating the pod @ 07/08/23 13:40:35.266
  E0708 13:40:35.552607      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:36.553084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:37.553150      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:40:37.818: INFO: Successfully updated pod "labelsupdate9cc81b91-eb66-49ac-a0f1-6ed10fde790e"
  E0708 13:40:38.554151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:39.554863      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:40:39.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9810" for this suite. @ 07/08/23 13:40:39.841
• [4.604 seconds]
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 07/08/23 13:40:39.848
  Jul  8 13:40:39.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename pod-network-test @ 07/08/23 13:40:39.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:40:39.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:40:39.872
  STEP: Performing setup for networking test in namespace pod-network-test-1867 @ 07/08/23 13:40:39.875
  STEP: creating a selector @ 07/08/23 13:40:39.875
  STEP: Creating the service pods in kubernetes @ 07/08/23 13:40:39.876
  Jul  8 13:40:39.876: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0708 13:40:40.555162      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:41.555508      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:42.556250      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:43.556343      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:44.556424      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:45.557107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:46.557662      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:47.557735      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:48.557898      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:49.558878      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:50.559649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:51.560026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 07/08/23 13:40:51.957
  E0708 13:40:52.560944      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:53.561156      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:40:53.975: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul  8 13:40:53.975: INFO: Breadth first check of 192.168.59.24 on host 172.31.12.67...
  Jul  8 13:40:53.979: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.7.245:9080/dial?request=hostname&protocol=udp&host=192.168.59.24&port=8081&tries=1'] Namespace:pod-network-test-1867 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:40:53.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:40:53.979: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:40:53.979: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1867/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.7.245%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.59.24%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul  8 13:40:54.039: INFO: Waiting for responses: map[]
  Jul  8 13:40:54.039: INFO: reached 192.168.59.24 after 0/1 tries
  Jul  8 13:40:54.040: INFO: Breadth first check of 192.168.164.122 on host 172.31.29.188...
  Jul  8 13:40:54.043: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.7.245:9080/dial?request=hostname&protocol=udp&host=192.168.164.122&port=8081&tries=1'] Namespace:pod-network-test-1867 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:40:54.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:40:54.044: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:40:54.044: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1867/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.7.245%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.164.122%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul  8 13:40:54.094: INFO: Waiting for responses: map[]
  Jul  8 13:40:54.094: INFO: reached 192.168.164.122 after 0/1 tries
  Jul  8 13:40:54.094: INFO: Breadth first check of 192.168.7.206 on host 172.31.93.234...
  Jul  8 13:40:54.098: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.7.245:9080/dial?request=hostname&protocol=udp&host=192.168.7.206&port=8081&tries=1'] Namespace:pod-network-test-1867 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul  8 13:40:54.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  Jul  8 13:40:54.099: INFO: ExecWithOptions: Clientset creation
  Jul  8 13:40:54.099: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1867/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.7.245%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.7.206%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul  8 13:40:54.149: INFO: Waiting for responses: map[]
  Jul  8 13:40:54.149: INFO: reached 192.168.7.206 after 0/1 tries
  Jul  8 13:40:54.149: INFO: Going to retry 0 out of 3 pods....
  Jul  8 13:40:54.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1867" for this suite. @ 07/08/23 13:40:54.154
• [14.313 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 07/08/23 13:40:54.163
  Jul  8 13:40:54.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename services @ 07/08/23 13:40:54.163
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:40:54.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:40:54.182
  STEP: creating service endpoint-test2 in namespace services-993 @ 07/08/23 13:40:54.186
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-993 to expose endpoints map[] @ 07/08/23 13:40:54.196
  Jul  8 13:40:54.200: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E0708 13:40:54.562045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:40:55.209: INFO: successfully validated that service endpoint-test2 in namespace services-993 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-993 @ 07/08/23 13:40:55.209
  E0708 13:40:55.562656      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:56.563636      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-993 to expose endpoints map[pod1:[80]] @ 07/08/23 13:40:57.229
  Jul  8 13:40:57.242: INFO: successfully validated that service endpoint-test2 in namespace services-993 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 07/08/23 13:40:57.242
  Jul  8 13:40:57.242: INFO: Creating new exec pod
  E0708 13:40:57.563757      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:58.563898      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:40:59.564053      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:41:00.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-993 exec execpodgkqgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul  8 13:41:00.379: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul  8 13:41:00.379: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 13:41:00.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-993 exec execpodgkqgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.99 80'
  Jul  8 13:41:00.489: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.99 80\nConnection to 10.152.183.99 80 port [tcp/http] succeeded!\n"
  Jul  8 13:41:00.489: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-993 @ 07/08/23 13:41:00.489
  E0708 13:41:00.564223      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:01.564311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-993 to expose endpoints map[pod1:[80] pod2:[80]] @ 07/08/23 13:41:02.509
  Jul  8 13:41:02.524: INFO: successfully validated that service endpoint-test2 in namespace services-993 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 07/08/23 13:41:02.524
  E0708 13:41:02.564444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:41:03.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-993 exec execpodgkqgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0708 13:41:03.565171      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:41:03.616: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul  8 13:41:03.616: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 13:41:03.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-993 exec execpodgkqgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.99 80'
  Jul  8 13:41:03.723: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.99 80\nConnection to 10.152.183.99 80 port [tcp/http] succeeded!\n"
  Jul  8 13:41:03.723: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-993 @ 07/08/23 13:41:03.723
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-993 to expose endpoints map[pod2:[80]] @ 07/08/23 13:41:03.739
  E0708 13:41:04.565263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:41:04.761: INFO: successfully validated that service endpoint-test2 in namespace services-993 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 07/08/23 13:41:04.761
  E0708 13:41:05.565348      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:41:05.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-993 exec execpodgkqgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul  8 13:41:05.868: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul  8 13:41:05.868: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul  8 13:41:05.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3675114709 --namespace=services-993 exec execpodgkqgg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.99 80'
  Jul  8 13:41:05.971: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.99 80\nConnection to 10.152.183.99 80 port [tcp/http] succeeded!\n"
  Jul  8 13:41:05.971: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-993 @ 07/08/23 13:41:05.971
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-993 to expose endpoints map[] @ 07/08/23 13:41:05.986
  Jul  8 13:41:05.996: INFO: successfully validated that service endpoint-test2 in namespace services-993 exposes endpoints map[]
  Jul  8 13:41:05.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-993" for this suite. @ 07/08/23 13:41:06.016
• [11.860 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 07/08/23 13:41:06.023
  Jul  8 13:41:06.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename proxy @ 07/08/23 13:41:06.024
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:41:06.041
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:41:06.045
  STEP: starting an echo server on multiple ports @ 07/08/23 13:41:06.057
  STEP: creating replication controller proxy-service-vccp6 in namespace proxy-4768 @ 07/08/23 13:41:06.057
  I0708 13:41:06.064569      20 runners.go:194] Created replication controller with name: proxy-service-vccp6, namespace: proxy-4768, replica count: 1
  E0708 13:41:06.566362      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0708 13:41:07.116091      20 runners.go:194] proxy-service-vccp6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0708 13:41:07.566640      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0708 13:41:08.116183      20 runners.go:194] proxy-service-vccp6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul  8 13:41:08.119: INFO: setup took 2.071420369s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 07/08/23 13:41:08.119
  Jul  8 13:41:08.126: INFO: (0) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 6.05354ms)
  Jul  8 13:41:08.129: INFO: (0) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 8.984187ms)
  Jul  8 13:41:08.129: INFO: (0) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 9.655486ms)
  Jul  8 13:41:08.129: INFO: (0) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 9.328887ms)
  Jul  8 13:41:08.133: INFO: (0) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 13.253692ms)
  Jul  8 13:41:08.133: INFO: (0) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 13.402477ms)
  Jul  8 13:41:08.133: INFO: (0) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 13.85131ms)
  Jul  8 13:41:08.133: INFO: (0) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 13.712401ms)
  Jul  8 13:41:08.134: INFO: (0) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 14.171112ms)
  Jul  8 13:41:08.134: INFO: (0) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 14.163877ms)
  Jul  8 13:41:08.134: INFO: (0) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 14.190385ms)
  Jul  8 13:41:08.134: INFO: (0) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 14.475401ms)
  Jul  8 13:41:08.134: INFO: (0) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 14.573408ms)
  Jul  8 13:41:08.134: INFO: (0) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 14.338426ms)
  Jul  8 13:41:08.134: INFO: (0) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 14.444228ms)
  Jul  8 13:41:08.135: INFO: (0) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 14.744861ms)
  Jul  8 13:41:08.139: INFO: (1) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 4.523029ms)
  Jul  8 13:41:08.140: INFO: (1) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 5.658925ms)
  Jul  8 13:41:08.141: INFO: (1) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 6.422011ms)
  Jul  8 13:41:08.142: INFO: (1) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 7.047887ms)
  Jul  8 13:41:08.143: INFO: (1) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 7.841826ms)
  Jul  8 13:41:08.143: INFO: (1) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 8.351257ms)
  Jul  8 13:41:08.143: INFO: (1) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 8.709904ms)
  Jul  8 13:41:08.144: INFO: (1) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 9.018119ms)
  Jul  8 13:41:08.144: INFO: (1) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 9.478126ms)
  Jul  8 13:41:08.145: INFO: (1) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 9.807387ms)
  Jul  8 13:41:08.145: INFO: (1) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 10.003256ms)
  Jul  8 13:41:08.145: INFO: (1) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 9.893189ms)
  Jul  8 13:41:08.145: INFO: (1) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 9.932914ms)
  Jul  8 13:41:08.145: INFO: (1) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 10.268179ms)
  Jul  8 13:41:08.145: INFO: (1) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 10.285774ms)
  Jul  8 13:41:08.145: INFO: (1) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 10.715205ms)
  Jul  8 13:41:08.150: INFO: (2) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 4.646556ms)
  Jul  8 13:41:08.152: INFO: (2) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 6.606001ms)
  Jul  8 13:41:08.153: INFO: (2) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 7.53381ms)
  Jul  8 13:41:08.153: INFO: (2) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 7.500537ms)
  Jul  8 13:41:08.153: INFO: (2) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 7.4434ms)
  Jul  8 13:41:08.153: INFO: (2) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 7.784063ms)
  Jul  8 13:41:08.154: INFO: (2) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 8.061573ms)
  Jul  8 13:41:08.155: INFO: (2) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 9.081172ms)
  Jul  8 13:41:08.155: INFO: (2) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 9.041386ms)
  Jul  8 13:41:08.155: INFO: (2) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 8.866954ms)
  Jul  8 13:41:08.155: INFO: (2) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 8.919819ms)
  Jul  8 13:41:08.155: INFO: (2) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 9.331847ms)
  Jul  8 13:41:08.155: INFO: (2) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 9.622683ms)
  Jul  8 13:41:08.156: INFO: (2) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 9.827116ms)
  Jul  8 13:41:08.156: INFO: (2) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 10.037093ms)
  Jul  8 13:41:08.156: INFO: (2) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 10.344219ms)
  Jul  8 13:41:08.161: INFO: (3) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 4.396789ms)
  Jul  8 13:41:08.163: INFO: (3) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 6.3163ms)
  Jul  8 13:41:08.163: INFO: (3) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 6.571576ms)
  Jul  8 13:41:08.164: INFO: (3) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 7.239971ms)
  Jul  8 13:41:08.164: INFO: (3) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 7.731194ms)
  Jul  8 13:41:08.164: INFO: (3) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 7.91073ms)
  Jul  8 13:41:08.165: INFO: (3) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 8.282562ms)
  Jul  8 13:41:08.165: INFO: (3) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 8.892189ms)
  Jul  8 13:41:08.166: INFO: (3) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 9.213704ms)
  Jul  8 13:41:08.166: INFO: (3) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 9.565063ms)
  Jul  8 13:41:08.166: INFO: (3) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 9.873251ms)
  Jul  8 13:41:08.167: INFO: (3) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 10.418115ms)
  Jul  8 13:41:08.167: INFO: (3) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 10.34083ms)
  Jul  8 13:41:08.168: INFO: (3) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 11.730763ms)
  Jul  8 13:41:08.168: INFO: (3) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 12.026278ms)
  Jul  8 13:41:08.170: INFO: (3) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 13.003289ms)
  Jul  8 13:41:08.175: INFO: (4) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 4.950086ms)
  Jul  8 13:41:08.175: INFO: (4) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 5.572143ms)
  Jul  8 13:41:08.177: INFO: (4) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 7.717786ms)
  Jul  8 13:41:08.177: INFO: (4) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 7.576612ms)
  Jul  8 13:41:08.178: INFO: (4) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 7.828126ms)
  Jul  8 13:41:08.178: INFO: (4) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 8.042755ms)
  Jul  8 13:41:08.178: INFO: (4) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 8.015206ms)
  Jul  8 13:41:08.178: INFO: (4) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 7.847492ms)
  Jul  8 13:41:08.178: INFO: (4) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 8.523289ms)
  Jul  8 13:41:08.178: INFO: (4) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 8.798701ms)
  Jul  8 13:41:08.178: INFO: (4) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 8.607899ms)
  Jul  8 13:41:08.179: INFO: (4) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 8.679998ms)
  Jul  8 13:41:08.179: INFO: (4) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 9.201438ms)
  Jul  8 13:41:08.179: INFO: (4) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 9.419981ms)
  Jul  8 13:41:08.180: INFO: (4) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 9.712411ms)
  Jul  8 13:41:08.180: INFO: (4) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 10.005206ms)
  Jul  8 13:41:08.184: INFO: (5) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 4.47974ms)
  Jul  8 13:41:08.187: INFO: (5) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 7.316306ms)
  Jul  8 13:41:08.188: INFO: (5) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 7.575917ms)
  Jul  8 13:41:08.188: INFO: (5) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 8.064867ms)
  Jul  8 13:41:08.189: INFO: (5) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 8.61907ms)
  Jul  8 13:41:08.189: INFO: (5) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 8.557336ms)
  Jul  8 13:41:08.189: INFO: (5) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 8.327683ms)
  Jul  8 13:41:08.189: INFO: (5) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 8.36254ms)
  Jul  8 13:41:08.189: INFO: (5) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 8.937558ms)
  Jul  8 13:41:08.189: INFO: (5) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 9.134568ms)
  Jul  8 13:41:08.190: INFO: (5) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 9.350952ms)
  Jul  8 13:41:08.190: INFO: (5) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 9.22945ms)
  Jul  8 13:41:08.190: INFO: (5) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 9.777956ms)
  Jul  8 13:41:08.190: INFO: (5) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 9.769532ms)
  Jul  8 13:41:08.191: INFO: (5) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 10.394863ms)
  Jul  8 13:41:08.191: INFO: (5) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 10.781983ms)
  Jul  8 13:41:08.195: INFO: (6) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 4.275377ms)
  Jul  8 13:41:08.197: INFO: (6) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 5.883844ms)
  Jul  8 13:41:08.198: INFO: (6) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 6.790174ms)
  Jul  8 13:41:08.198: INFO: (6) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 6.973227ms)
  Jul  8 13:41:08.199: INFO: (6) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 7.381289ms)
  Jul  8 13:41:08.199: INFO: (6) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 8.202666ms)
  Jul  8 13:41:08.199: INFO: (6) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 7.959263ms)
  Jul  8 13:41:08.200: INFO: (6) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 8.750372ms)
  Jul  8 13:41:08.200: INFO: (6) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 9.065743ms)
  Jul  8 13:41:08.201: INFO: (6) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 9.4739ms)
  Jul  8 13:41:08.201: INFO: (6) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 9.721081ms)
  Jul  8 13:41:08.201: INFO: (6) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 9.606839ms)
  Jul  8 13:41:08.201: INFO: (6) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 10.02607ms)
  Jul  8 13:41:08.201: INFO: (6) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 10.247276ms)
  Jul  8 13:41:08.201: INFO: (6) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 10.364306ms)
  Jul  8 13:41:08.202: INFO: (6) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 10.340135ms)
  Jul  8 13:41:08.206: INFO: (7) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 4.622135ms)
  Jul  8 13:41:08.207: INFO: (7) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 4.87534ms)
  Jul  8 13:41:08.207: INFO: (7) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 5.460016ms)
  Jul  8 13:41:08.208: INFO: (7) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 6.006325ms)
  Jul  8 13:41:08.208: INFO: (7) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 6.424096ms)
  Jul  8 13:41:08.208: INFO: (7) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 6.658069ms)
  Jul  8 13:41:08.209: INFO: (7) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 7.537476ms)
  Jul  8 13:41:08.210: INFO: (7) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 7.730217ms)
  Jul  8 13:41:08.210: INFO: (7) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 8.46396ms)
  Jul  8 13:41:08.210: INFO: (7) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 8.439475ms)
  Jul  8 13:41:08.211: INFO: (7) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 8.792226ms)
  Jul  8 13:41:08.211: INFO: (7) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 8.71736ms)
  Jul  8 13:41:08.211: INFO: (7) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 8.860549ms)
  Jul  8 13:41:08.211: INFO: (7) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 9.31658ms)
  Jul  8 13:41:08.212: INFO: (7) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 9.809275ms)
  Jul  8 13:41:08.212: INFO: (7) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 9.953487ms)
  Jul  8 13:41:08.217: INFO: (8) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 4.588355ms)
  Jul  8 13:41:08.217: INFO: (8) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 5.130531ms)
  Jul  8 13:41:08.218: INFO: (8) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 5.872532ms)
  Jul  8 13:41:08.219: INFO: (8) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 6.751559ms)
  Jul  8 13:41:08.219: INFO: (8) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 7.241906ms)
  Jul  8 13:41:08.219: INFO: (8) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 6.958687ms)
  Jul  8 13:41:08.220: INFO: (8) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 7.487141ms)
  Jul  8 13:41:08.220: INFO: (8) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 7.413785ms)
  Jul  8 13:41:08.220: INFO: (8) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 7.788347ms)
  Jul  8 13:41:08.221: INFO: (8) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 8.418897ms)
  Jul  8 13:41:08.221: INFO: (8) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 8.510531ms)
  Jul  8 13:41:08.221: INFO: (8) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 8.76279ms)
  Jul  8 13:41:08.222: INFO: (8) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 9.358311ms)
  Jul  8 13:41:08.222: INFO: (8) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 9.737137ms)
  Jul  8 13:41:08.223: INFO: (8) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 10.427997ms)
  Jul  8 13:41:08.223: INFO: (8) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 10.564054ms)
  Jul  8 13:41:08.227: INFO: (9) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 4.351922ms)
  Jul  8 13:41:08.228: INFO: (9) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 5.482819ms)
  Jul  8 13:41:08.229: INFO: (9) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 6.193253ms)
  Jul  8 13:41:08.229: INFO: (9) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 6.259176ms)
  Jul  8 13:41:08.229: INFO: (9) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 6.312302ms)
  Jul  8 13:41:08.230: INFO: (9) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 6.901552ms)
  Jul  8 13:41:08.230: INFO: (9) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 7.313388ms)
  Jul  8 13:41:08.231: INFO: (9) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 7.711557ms)
  Jul  8 13:41:08.231: INFO: (9) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 7.735718ms)
  Jul  8 13:41:08.231: INFO: (9) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 7.846522ms)
  Jul  8 13:41:08.231: INFO: (9) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 8.092876ms)
  Jul  8 13:41:08.232: INFO: (9) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 8.627984ms)
  Jul  8 13:41:08.232: INFO: (9) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 9.426713ms)
  Jul  8 13:41:08.233: INFO: (9) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 9.853377ms)
  Jul  8 13:41:08.233: INFO: (9) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 9.871321ms)
  Jul  8 13:41:08.233: INFO: (9) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 10.175311ms)
  Jul  8 13:41:08.239: INFO: (10) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 5.178796ms)
  Jul  8 13:41:08.240: INFO: (10) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 5.98206ms)
  Jul  8 13:41:08.240: INFO: (10) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 6.187922ms)
  Jul  8 13:41:08.240: INFO: (10) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 6.193235ms)
  Jul  8 13:41:08.241: INFO: (10) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 6.918039ms)
  Jul  8 13:41:08.241: INFO: (10) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 7.424087ms)
  Jul  8 13:41:08.242: INFO: (10) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 8.389262ms)
  Jul  8 13:41:08.242: INFO: (10) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 9.121687ms)
  Jul  8 13:41:08.243: INFO: (10) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 8.9275ms)
  Jul  8 13:41:08.243: INFO: (10) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 8.870026ms)
  Jul  8 13:41:08.243: INFO: (10) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 9.120786ms)
  Jul  8 13:41:08.243: INFO: (10) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 9.270072ms)
  Jul  8 13:41:08.243: INFO: (10) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 9.318743ms)
  Jul  8 13:41:08.243: INFO: (10) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 9.351277ms)
  Jul  8 13:41:08.243: INFO: (10) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 9.757007ms)
  Jul  8 13:41:08.244: INFO: (10) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 10.028686ms)
  Jul  8 13:41:08.250: INFO: (11) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 6.423343ms)
  Jul  8 13:41:08.250: INFO: (11) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 6.223885ms)
  Jul  8 13:41:08.251: INFO: (11) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 7.724732ms)
  Jul  8 13:41:08.251: INFO: (11) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 7.458859ms)
  Jul  8 13:41:08.252: INFO: (11) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 7.776789ms)
  Jul  8 13:41:08.252: INFO: (11) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 8.030203ms)
  Jul  8 13:41:08.253: INFO: (11) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 8.91179ms)
  Jul  8 13:41:08.253: INFO: (11) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 8.906937ms)
  Jul  8 13:41:08.253: INFO: (11) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 8.816624ms)
  Jul  8 13:41:08.253: INFO: (11) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 9.068123ms)
  Jul  8 13:41:08.253: INFO: (11) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 9.121879ms)
  Jul  8 13:41:08.253: INFO: (11) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 9.076786ms)
  Jul  8 13:41:08.254: INFO: (11) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 9.600476ms)
  Jul  8 13:41:08.254: INFO: (11) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 10.050142ms)
  Jul  8 13:41:08.254: INFO: (11) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 9.787201ms)
  Jul  8 13:41:08.254: INFO: (11) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 9.891516ms)
  Jul  8 13:41:08.259: INFO: (12) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 4.967365ms)
  Jul  8 13:41:08.259: INFO: (12) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 5.092764ms)
  Jul  8 13:41:08.259: INFO: (12) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 5.326246ms)
  Jul  8 13:41:08.259: INFO: (12) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 5.465711ms)
  Jul  8 13:41:08.261: INFO: (12) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 6.674186ms)
  Jul  8 13:41:08.261: INFO: (12) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 7.132018ms)
  Jul  8 13:41:08.262: INFO: (12) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 7.429376ms)
  Jul  8 13:41:08.262: INFO: (12) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 7.496258ms)
  Jul  8 13:41:08.263: INFO: (12) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 8.226373ms)
  Jul  8 13:41:08.263: INFO: (12) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 8.849675ms)
  Jul  8 13:41:08.263: INFO: (12) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 8.868609ms)
  Jul  8 13:41:08.264: INFO: (12) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 9.230066ms)
  Jul  8 13:41:08.264: INFO: (12) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 9.703122ms)
  Jul  8 13:41:08.264: INFO: (12) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 9.761678ms)
  Jul  8 13:41:08.264: INFO: (12) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 10.0943ms)
  Jul  8 13:41:08.265: INFO: (12) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 10.3909ms)
  Jul  8 13:41:08.270: INFO: (13) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 4.598753ms)
  Jul  8 13:41:08.271: INFO: (13) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 5.831108ms)
  Jul  8 13:41:08.271: INFO: (13) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 5.788052ms)
  Jul  8 13:41:08.271: INFO: (13) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 6.219925ms)
  Jul  8 13:41:08.271: INFO: (13) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 6.266044ms)
  Jul  8 13:41:08.272: INFO: (13) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 6.764436ms)
  Jul  8 13:41:08.272: INFO: (13) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 6.901626ms)
  Jul  8 13:41:08.273: INFO: (13) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 7.713527ms)
  Jul  8 13:41:08.274: INFO: (13) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 8.784159ms)
  Jul  8 13:41:08.274: INFO: (13) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 8.985971ms)
  Jul  8 13:41:08.274: INFO: (13) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 9.069086ms)
  Jul  8 13:41:08.274: INFO: (13) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 9.191382ms)
  Jul  8 13:41:08.274: INFO: (13) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 9.316914ms)
  Jul  8 13:41:08.275: INFO: (13) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 9.769588ms)
  Jul  8 13:41:08.275: INFO: (13) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 10.022505ms)
  Jul  8 13:41:08.276: INFO: (13) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 10.547021ms)
  Jul  8 13:41:08.281: INFO: (14) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 5.397887ms)
  Jul  8 13:41:08.282: INFO: (14) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 6.038982ms)
  Jul  8 13:41:08.282: INFO: (14) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 6.375383ms)
  Jul  8 13:41:08.282: INFO: (14) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 6.556355ms)
  Jul  8 13:41:08.283: INFO: (14) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 7.051462ms)
  Jul  8 13:41:08.283: INFO: (14) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 6.873623ms)
  Jul  8 13:41:08.284: INFO: (14) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 7.886806ms)
  Jul  8 13:41:08.284: INFO: (14) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 8.199605ms)
  Jul  8 13:41:08.285: INFO: (14) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 9.345089ms)
  Jul  8 13:41:08.285: INFO: (14) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 9.427696ms)
  Jul  8 13:41:08.285: INFO: (14) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 9.610927ms)
  Jul  8 13:41:08.285: INFO: (14) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 9.742653ms)
  Jul  8 13:41:08.285: INFO: (14) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 9.795574ms)
  Jul  8 13:41:08.286: INFO: (14) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 9.620215ms)
  Jul  8 13:41:08.286: INFO: (14) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 10.25681ms)
  Jul  8 13:41:08.287: INFO: (14) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 10.666434ms)
  Jul  8 13:41:08.291: INFO: (15) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 4.742975ms)
  Jul  8 13:41:08.291: INFO: (15) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 4.799209ms)
  Jul  8 13:41:08.294: INFO: (15) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 7.500816ms)
  Jul  8 13:41:08.295: INFO: (15) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 7.819033ms)
  Jul  8 13:41:08.295: INFO: (15) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 7.983306ms)
  Jul  8 13:41:08.295: INFO: (15) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 8.212415ms)
  Jul  8 13:41:08.295: INFO: (15) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 8.325462ms)
  Jul  8 13:41:08.296: INFO: (15) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 8.779918ms)
  Jul  8 13:41:08.296: INFO: (15) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 9.135027ms)
  Jul  8 13:41:08.296: INFO: (15) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 9.372842ms)
  Jul  8 13:41:08.296: INFO: (15) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 9.475814ms)
  Jul  8 13:41:08.296: INFO: (15) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 9.71373ms)
  Jul  8 13:41:08.296: INFO: (15) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 9.586743ms)
  Jul  8 13:41:08.297: INFO: (15) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 10.017266ms)
  Jul  8 13:41:08.297: INFO: (15) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 10.265899ms)
  Jul  8 13:41:08.297: INFO: (15) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 10.778453ms)
  Jul  8 13:41:08.302: INFO: (16) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 4.361166ms)
  Jul  8 13:41:08.302: INFO: (16) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 4.589569ms)
  Jul  8 13:41:08.304: INFO: (16) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 5.575379ms)
  Jul  8 13:41:08.304: INFO: (16) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 5.944833ms)
  Jul  8 13:41:08.304: INFO: (16) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 6.308515ms)
  Jul  8 13:41:08.305: INFO: (16) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 6.535778ms)
  Jul  8 13:41:08.305: INFO: (16) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 5.898293ms)
  Jul  8 13:41:08.305: INFO: (16) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 6.257869ms)
  Jul  8 13:41:08.306: INFO: (16) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 7.72496ms)
  Jul  8 13:41:08.306: INFO: (16) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 6.95684ms)
  Jul  8 13:41:08.306: INFO: (16) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 8.071948ms)
  Jul  8 13:41:08.307: INFO: (16) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 7.592658ms)
  Jul  8 13:41:08.307: INFO: (16) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 8.845344ms)
  Jul  8 13:41:08.307: INFO: (16) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 9.231901ms)
  Jul  8 13:41:08.307: INFO: (16) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 9.641633ms)
  Jul  8 13:41:08.308: INFO: (16) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 9.192343ms)
  Jul  8 13:41:08.313: INFO: (17) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 4.557993ms)
  Jul  8 13:41:08.313: INFO: (17) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 4.787647ms)
  Jul  8 13:41:08.314: INFO: (17) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 5.384334ms)
  Jul  8 13:41:08.314: INFO: (17) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 5.855159ms)
  Jul  8 13:41:08.315: INFO: (17) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 6.767841ms)
  Jul  8 13:41:08.315: INFO: (17) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 7.000574ms)
  Jul  8 13:41:08.316: INFO: (17) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 7.51255ms)
  Jul  8 13:41:08.316: INFO: (17) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 7.766283ms)
  Jul  8 13:41:08.320: INFO: (17) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 11.159195ms)
  Jul  8 13:41:08.320: INFO: (17) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 11.693832ms)
  Jul  8 13:41:08.320: INFO: (17) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 11.862673ms)
  Jul  8 13:41:08.320: INFO: (17) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 11.930466ms)
  Jul  8 13:41:08.321: INFO: (17) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 12.332998ms)
  Jul  8 13:41:08.321: INFO: (17) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 12.358715ms)
  Jul  8 13:41:08.321: INFO: (17) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 12.314639ms)
  Jul  8 13:41:08.321: INFO: (17) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 12.308369ms)
  Jul  8 13:41:08.326: INFO: (18) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 4.726425ms)
  Jul  8 13:41:08.329: INFO: (18) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 7.189448ms)
  Jul  8 13:41:08.329: INFO: (18) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 7.580211ms)
  Jul  8 13:41:08.329: INFO: (18) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 7.761635ms)
  Jul  8 13:41:08.330: INFO: (18) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 8.568319ms)
  Jul  8 13:41:08.330: INFO: (18) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 8.917321ms)
  Jul  8 13:41:08.331: INFO: (18) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 9.989718ms)
  Jul  8 13:41:08.332: INFO: (18) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 10.20009ms)
  Jul  8 13:41:08.332: INFO: (18) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 10.225768ms)
  Jul  8 13:41:08.332: INFO: (18) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 10.450489ms)
  Jul  8 13:41:08.332: INFO: (18) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 10.289071ms)
  Jul  8 13:41:08.332: INFO: (18) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 10.371014ms)
  Jul  8 13:41:08.332: INFO: (18) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 10.460161ms)
  Jul  8 13:41:08.332: INFO: (18) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 10.182586ms)
  Jul  8 13:41:08.332: INFO: (18) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 10.598131ms)
  Jul  8 13:41:08.332: INFO: (18) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 10.754423ms)
  Jul  8 13:41:08.338: INFO: (19) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:160/proxy/: foo (200; 5.241566ms)
  Jul  8 13:41:08.339: INFO: (19) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt/proxy/rewriteme">test</a> (200; 6.284298ms)
  Jul  8 13:41:08.339: INFO: (19) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:460/proxy/: tls baz (200; 6.348319ms)
  Jul  8 13:41:08.340: INFO: (19) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:443/proxy/tlsrewritem... (200; 6.976636ms)
  Jul  8 13:41:08.340: INFO: (19) /api/v1/namespaces/proxy-4768/pods/https:proxy-service-vccp6-srftt:462/proxy/: tls qux (200; 7.161916ms)
  Jul  8 13:41:08.340: INFO: (19) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:1080/proxy/rewriteme">... (200; 7.03ms)
  Jul  8 13:41:08.340: INFO: (19) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:162/proxy/: bar (200; 7.677665ms)
  Jul  8 13:41:08.341: INFO: (19) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:1080/proxy/rewriteme">test<... (200; 8.265442ms)
  Jul  8 13:41:08.341: INFO: (19) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname1/proxy/: tls baz (200; 8.533573ms)
  Jul  8 13:41:08.341: INFO: (19) /api/v1/namespaces/proxy-4768/pods/proxy-service-vccp6-srftt:160/proxy/: foo (200; 8.814727ms)
  Jul  8 13:41:08.342: INFO: (19) /api/v1/namespaces/proxy-4768/pods/http:proxy-service-vccp6-srftt:162/proxy/: bar (200; 8.809816ms)
  Jul  8 13:41:08.342: INFO: (19) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname2/proxy/: bar (200; 9.066393ms)
  Jul  8 13:41:08.342: INFO: (19) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname1/proxy/: foo (200; 9.27511ms)
  Jul  8 13:41:08.342: INFO: (19) /api/v1/namespaces/proxy-4768/services/proxy-service-vccp6:portname2/proxy/: bar (200; 9.522121ms)
  Jul  8 13:41:08.343: INFO: (19) /api/v1/namespaces/proxy-4768/services/https:proxy-service-vccp6:tlsportname2/proxy/: tls qux (200; 10.219966ms)
  Jul  8 13:41:08.343: INFO: (19) /api/v1/namespaces/proxy-4768/services/http:proxy-service-vccp6:portname1/proxy/: foo (200; 10.337179ms)
  Jul  8 13:41:08.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-vccp6 in namespace proxy-4768, will wait for the garbage collector to delete the pods @ 07/08/23 13:41:08.347
  Jul  8 13:41:08.408: INFO: Deleting ReplicationController proxy-service-vccp6 took: 7.31322ms
  Jul  8 13:41:08.509: INFO: Terminating ReplicationController proxy-service-vccp6 pods took: 101.10647ms
  E0708 13:41:08.566952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:09.567992      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:10.568413      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-4768" for this suite. @ 07/08/23 13:41:11.11
• [5.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 07/08/23 13:41:11.119
  Jul  8 13:41:11.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename init-container @ 07/08/23 13:41:11.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:41:11.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:41:11.138
  STEP: creating the pod @ 07/08/23 13:41:11.141
  Jul  8 13:41:11.141: INFO: PodSpec: initContainers in spec.initContainers
  E0708 13:41:11.568955      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:12.569803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:13.569892      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:41:14.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3763" for this suite. @ 07/08/23 13:41:14.268
• [3.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 07/08/23 13:41:14.277
  Jul  8 13:41:14.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename downward-api @ 07/08/23 13:41:14.278
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:41:14.296
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:41:14.3
  STEP: Creating a pod to test downward api env vars @ 07/08/23 13:41:14.305
  E0708 13:41:14.570682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:15.570822      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:41:16.325
  Jul  8 13:41:16.329: INFO: Trying to get logs from node ip-172-31-93-234 pod downward-api-3daad029-1d09-42a4-918c-3afe077712ad container dapi-container: <nil>
  STEP: delete the pod @ 07/08/23 13:41:16.346
  Jul  8 13:41:16.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-922" for this suite. @ 07/08/23 13:41:16.376
• [2.106 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 07/08/23 13:41:16.384
  Jul  8 13:41:16.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 13:41:16.385
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:41:16.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:41:16.406
  STEP: Creating configMap with name projected-configmap-test-volume-0aea675e-23fd-474c-b583-f8667eafb901 @ 07/08/23 13:41:16.41
  STEP: Creating a pod to test consume configMaps @ 07/08/23 13:41:16.414
  E0708 13:41:16.570837      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:17.570966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:18.571576      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:19.571678      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/08/23 13:41:20.436
  Jul  8 13:41:20.440: INFO: Trying to get logs from node ip-172-31-93-234 pod pod-projected-configmaps-d2d65545-2de9-4e1f-b1b6-e9889008f8c6 container agnhost-container: <nil>
  STEP: delete the pod @ 07/08/23 13:41:20.448
  Jul  8 13:41:20.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6601" for this suite. @ 07/08/23 13:41:20.471
• [4.093 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 07/08/23 13:41:20.478
  Jul  8 13:41:20.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/08/23 13:41:20.478
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:41:20.495
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:41:20.498
  STEP: set up a multi version CRD @ 07/08/23 13:41:20.502
  Jul  8 13:41:20.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  E0708 13:41:20.571930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:21.572164      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:22.572692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:23.573714      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 07/08/23 13:41:23.747
  STEP: check the unserved version gets removed @ 07/08/23 13:41:23.765
  E0708 13:41:24.574751      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 07/08/23 13:41:25.106
  E0708 13:41:25.575121      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:26.575873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:27.576209      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:41:27.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5215" for this suite. @ 07/08/23 13:41:27.687
• [7.216 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 07/08/23 13:41:27.694
  Jul  8 13:41:27.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3675114709
  STEP: Building a namespace api object, basename projected @ 07/08/23 13:41:27.695
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/08/23 13:41:27.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/08/23 13:41:27.716
  STEP: Creating configMap with name cm-test-opt-del-f53bdf3e-c912-48bb-b0e9-e5208e077383 @ 07/08/23 13:41:27.726
  STEP: Creating configMap with name cm-test-opt-upd-6a25b7ea-917e-42f3-bae4-b01121d1e136 @ 07/08/23 13:41:27.732
  STEP: Creating the pod @ 07/08/23 13:41:27.738
  E0708 13:41:28.577125      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:29.577192      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:30.577822      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:31.578400      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-f53bdf3e-c912-48bb-b0e9-e5208e077383 @ 07/08/23 13:41:31.789
  STEP: Updating configmap cm-test-opt-upd-6a25b7ea-917e-42f3-bae4-b01121d1e136 @ 07/08/23 13:41:31.795
  STEP: Creating configMap with name cm-test-opt-create-a2263248-f1d8-48b3-97fe-7359801da28c @ 07/08/23 13:41:31.801
  STEP: waiting to observe update in volume @ 07/08/23 13:41:31.806
  E0708 13:41:32.578474      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:33.578677      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:34.579320      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:35.579406      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:36.579641      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:37.579718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:38.580015      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:39.581087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:40.581247      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:41.581538      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:42.581682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:43.582491      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:44.582587      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:45.582762      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:46.583617      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:47.583791      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:48.584020      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:49.585086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:50.585192      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:51.585480      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:52.586455      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:53.586621      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:54.587595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:55.587854      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:56.588786      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:57.588894      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:58.588969      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:41:59.589143      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:00.589206      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:01.589523      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:02.589615      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:03.589788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:04.589876      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:05.590426      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:06.590606      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:07.590694      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:08.590789      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:09.590959      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:10.591038      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:11.591113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:12.591207      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:13.591356      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:14.591621      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:15.591709      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:16.592480      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:17.593102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:18.594107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:19.594292      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:20.594377      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:21.594458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:22.595021      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:23.595119      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:24.595617      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:25.595748      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:26.595815      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:27.595986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:28.597047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:29.597125      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:30.597533      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:31.597937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:32.598622      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:33.599196      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:34.599318      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:35.599441      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:36.599746      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:37.600342      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:38.601064      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:39.601180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:40.601889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:41.602239      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:42.603120      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:43.603301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:44.604198      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:45.604288      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:46.605098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:47.605234      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:48.605725      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0708 13:42:49.605816      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul  8 13:42:50.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2502" for this suite. @ 07/08/23 13:42:50.183
• [82.496 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Jul  8 13:42:50.191: INFO: Running AfterSuite actions on node 1
  Jul  8 13:42:50.191: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.030 seconds]
------------------------------

Ran 378 of 7207 Specs in 6078.851 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h41m19.110083552s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

